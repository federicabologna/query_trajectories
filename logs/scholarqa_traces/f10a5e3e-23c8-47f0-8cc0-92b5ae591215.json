{
    "query": "dark knowledge that gets standard formulation knowledge distillation information from a teacher network to student network",
    "user_id": "lib_user",
    "task_id": "f10a5e3e-23c8-47f0-8cc0-92b5ae591215",
    "timestamp": "2025-06-24T00:07:16.015063",
    "n_retrieval": 256,
    "n_retrieved": 273,
    "n_candidates": 49,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.436332,
    "decomposed_query": {
        "rewritten_query": "Dark knowledge in knowledge distillation for transferring information from teacher network to student network.",
        "keyword_query": "dark knowledge standard formulation knowledge distillation teacher network student network",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009663,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Knowledge Distillation Meets Self-Supervision",
            "venue": "European Conference on Computer Vision",
            "year": 2020,
            "reference_count": 54,
            "citation_count": 285,
            "influential_citation_count": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2006.07114",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.07114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46538811",
                    "name": "Guodong Xu"
                },
                {
                    "authorId": "2117940996",
                    "name": "Ziwei Liu"
                },
                {
                    "authorId": "2108536754",
                    "name": "Xiaoxiao Li"
                },
                {
                    "authorId": "1717179",
                    "name": "Chen Change Loy"
                }
            ],
            "abstract": "Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting \"richer dark knowledge\" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs.",
            "corpus_id": 219636179,
            "sentences": [
                {
                    "corpus_id": "219636179",
                    "title": "Knowledge Distillation Meets Self-Supervision",
                    "text": "Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting \"richer dark knowledge\" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs.",
                    "score": 0.4980829148799004,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation Meets Self-Supervision\n# Venue: European Conference on Computer Vision\n# Authors: Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy\n## Abstract\nKnowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting \"richer dark knowledge\" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs.\n",
            "reference_string": "[219636179 | Xu et al. | 2020 | Citations: 285]"
        },
        {
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2020,
            "reference_count": 35,
            "citation_count": 35,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6509/6365",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/AAAI.V34I05.6509?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/AAAI.V34I05.6509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47295143",
                    "name": "Zhenyu Zhang"
                },
                {
                    "authorId": "2269366",
                    "name": "Xiaobo Shu"
                },
                {
                    "authorId": "48613402",
                    "name": "Yu Bowen"
                },
                {
                    "authorId": "2079682",
                    "name": "Tingwen Liu"
                },
                {
                    "authorId": "48019474",
                    "name": "Jiapeng Zhao"
                },
                {
                    "authorId": "2108645146",
                    "name": "Quangang Li"
                },
                {
                    "authorId": "48358041",
                    "name": "Li Guo"
                }
            ],
            "abstract": "Extracting relations from plain text is an important task with wide application. Most existing methods formulate it as a supervised problem and utilize one-hot hard labels as the sole target in training, neglecting the rich semantic information among relations. In this paper, we aim to explore the supervision with soft labels in relation extraction, which makes it possible to integrate prior knowledge. Specifically, a bipartite graph is first devised to discover type constraints between entities and relations based on the entire corpus. Then, we combine such type constraints with neural networks to achieve a knowledgeable model. Furthermore, this model is regarded as teacher to generate well-informed soft labels and guide the optimization of a student network via knowledge distillation. Besides, a multi-aspect attention mechanism is introduced to help student mine latent information from text. In this way, the enhanced student inherits the dark knowledge (e.g., type constraints and relevance among relations) from teacher, and directly serves the testing scenarios without any extra constraints. We conduct extensive experiments on the TACRED and SemEval datasets, the experimental results justify the effectiveness of our approach.",
            "corpus_id": 212855595,
            "sentences": [
                {
                    "corpus_id": "212855595",
                    "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                    "text": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
                    "score": 0.5855641613997551,
                    "section_title": "Objective Functions",
                    "char_start_offset": 15526,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 276
                        },
                        {
                            "start": 279,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 838
                        },
                        {
                            "start": 841,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1151
                        },
                        {
                            "start": 1154,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1606
                        },
                        {
                            "start": 1609,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2078
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 524,
                            "end": 542,
                            "matchedPaperCorpusId": "54986302"
                        },
                        {
                            "start": 1267,
                            "end": 1285,
                            "matchedPaperCorpusId": "29308926"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.818359375
                },
                {
                    "corpus_id": "212855595",
                    "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                    "text": "knowledge distillation is an effective framework to transfer knowledge from a neural network to another, which typically consists of two branches: a teacher T , which is usually a complex model or accompanied by some extra knowledge, and a student S, which is a small network that learns from the teacher (Hinton, Vinyals, and Dean 2015). In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model: \n\nwhere L S GT is the ground-truth loss using one-hot labels, L KD is the knowledge distillation loss using teacher's soft labels and \u03bb is the coefficient to trade off such two terms. Typically, L GT is often the cross entropy loss in classification problems, and L KD is the Kullback-Leibler divergence to quantify the difference of output distribution from student to teacher: \n\nHenebry was 86 and lived in St Martinville. [0, \u2026, 0.69, \u2026, 0.17, ..., 0.14 where G is the one-hot distribution of ground-truth and G(i) is the i-th element of G. The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al. 2017).",
                    "score": 0.5444055664263173,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 7274,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 816
                        },
                        {
                            "start": 819,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1195
                        },
                        {
                            "start": 1198,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1782
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 337,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1763,
                            "end": 1780,
                            "matchedPaperCorpusId": "29308926"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79052734375
                }
            ],
            "relevance_judgement": 0.818359375,
            "relevance_judgment_input_expanded": "# Title: Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zhenyu Zhang, Xiaobo Shu, Yu Bowen, Tingwen Liu, Jiapeng Zhao, Quangang Li, Li Guo\n## Abstract\nExtracting relations from plain text is an important task with wide application. Most existing methods formulate it as a supervised problem and utilize one-hot hard labels as the sole target in training, neglecting the rich semantic information among relations. In this paper, we aim to explore the supervision with soft labels in relation extraction, which makes it possible to integrate prior knowledge. Specifically, a bipartite graph is first devised to discover type constraints between entities and relations based on the entire corpus. Then, we combine such type constraints with neural networks to achieve a knowledgeable model. Furthermore, this model is regarded as teacher to generate well-informed soft labels and guide the optimization of a student network via knowledge distillation. Besides, a multi-aspect attention mechanism is introduced to help student mine latent information from text. In this way, the enhanced student inherits the dark knowledge (e.g., type constraints and relevance among relations) from teacher, and directly serves the testing scenarios without any extra constraints. We conduct extensive experiments on the TACRED and SemEval datasets, the experimental results justify the effectiveness of our approach.\n## Knowledge Distillation\nknowledge distillation is an effective framework to transfer knowledge from a neural network to another, which typically consists of two branches: a teacher T , which is usually a complex model or accompanied by some extra knowledge, and a student S, which is a small network that learns from the teacher (Hinton, Vinyals, and Dean 2015). In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model: \n\nwhere L S GT is the ground-truth loss using one-hot labels, L KD is the knowledge distillation loss using teacher's soft labels and \u03bb is the coefficient to trade off such two terms. Typically, L GT is often the cross entropy loss in classification problems, and L KD is the Kullback-Leibler divergence to quantify the difference of output distribution from student to teacher: \n\nHenebry was 86 and lived in St Martinville. [0, \u2026, 0.69, \u2026, 0.17, ..., 0.14 where G is the one-hot distribution of ground-truth and G(i) is the i-th element of G. The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al. 2017).\n\n## Objective Functions\nThis subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
            "reference_string": "[212855595 | Zhang et al. | 2020 | Citations: 35]"
        },
        {
            "title": "Similarity Transfer for Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 56,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10047, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "2058266174",
                    "name": "Kun Gong"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "145429878",
                    "name": "Hui Yu"
                }
            ],
            "abstract": "Knowledge distillation is a popular paradigm for learning portable neural networks by transferring the knowledge from a large model into a smaller one. Most existing approaches enhance the student model by utilizing the similarity information between the categories of instance level provided by the teacher model. However, these works ignore the similarity correlation between different instances that plays an important role in confidence prediction. To tackle this issue, we propose a novel method in this paper, called similarity transfer for knowledge distillation (STKD), which aims to fully utilize the similarities between categories of multiple samples. Furthermore, we propose to better capture the similarity correlation between different instances by the mixup technique, which creates virtual samples by a weighted linear interpolation. Note that, our distillation loss can fully utilize the incorrect classes similarities by the mixed labels. The proposed approach promotes the performance of student model as the virtual sample created by multiple images produces a similar probability distribution in the teacher and student networks. Experiments and ablation studies on several public classification datasets including CIFAR-10,CIFAR-100,CINIC-10 and Tiny-ImageNet verify that this light-weight method can effectively boost the performance of the compact student model. It shows that STKD substantially has outperformed the vanilla knowledge distillation and has achieved superior accuracy over the state-of-the-art knowledge distillation methods.",
            "corpus_id": 232269823,
            "sentences": [
                {
                    "corpus_id": "232269823",
                    "title": "Similarity Transfer for Knowledge Distillation",
                    "text": "Different from above methods, knowledge distillation enrich and get the student model by extracting kinds of knowledge from the fixed teacher model. To address the challenge of deploying CNNs in resource-constrained edge devices, Bucilua et al. [38] first propose to transfer the knowledge of an ensemble of models to a small model. Then Caruana et al. [39] propose to train student model by mimicking the teacher model's logits. Later, Hinton et al. [12] popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network. Afterward, some recent works [14] [15] extend KD by distilling knowledge from intermediate feature representations instead of soft labels. For example, FitNets [14] propose to train student network by mimicking the intermediate feature maps of teacher network, which are defined as hints. Inspired by this, Zagoruyko et al. [15] propose to match the attention maps between the teacher and the student, which are defined from the original feature maps as knowledge. Wang et al. [40] propose to improve the performance of student network by matching the distributions of spatial neuron activations between the teacher and the student. Recently, Heo et al. [41] introduce the activation boundary of the hidden neuron as knowledge for distilling the compact student network. \n\nHowever, the aforementioned knowledge distillation methods only utilize the knowledge contained in the output of specific layers of the teacher network. More richer knowledge between different layers is explored and utilized for knowledge distillation. For example, Yim et al. [16] propose to use Gram matrix between different feature layers as distilled knowledge, which named flow of solution process (FSP) that reflects the relations of different features maps.",
                    "score": 0.5309611783025652,
                    "section_title": "B. Knowledge Distillation.",
                    "char_start_offset": 9178,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1763
                        },
                        {
                            "start": 1766,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2018
                        },
                        {
                            "start": 2019,
                            "end": 2230
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 245,
                            "end": 249,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 353,
                            "end": 357,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 451,
                            "end": 455,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1022,
                            "end": 1026,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1153,
                            "end": 1157,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1317,
                            "end": 1321,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1470,
                            "end": 1474,
                            "matchedPaperCorpusId": "30307744"
                        },
                        {
                            "start": 1647,
                            "end": 1651,
                            "matchedPaperCorpusId": "53213211"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81787109375
                }
            ],
            "relevance_judgement": 0.81787109375,
            "relevance_judgment_input_expanded": "# Title: Similarity Transfer for Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Haoran Zhao, Kun Gong, Xin Sun, Junyu Dong, Hui Yu\n## Abstract\nKnowledge distillation is a popular paradigm for learning portable neural networks by transferring the knowledge from a large model into a smaller one. Most existing approaches enhance the student model by utilizing the similarity information between the categories of instance level provided by the teacher model. However, these works ignore the similarity correlation between different instances that plays an important role in confidence prediction. To tackle this issue, we propose a novel method in this paper, called similarity transfer for knowledge distillation (STKD), which aims to fully utilize the similarities between categories of multiple samples. Furthermore, we propose to better capture the similarity correlation between different instances by the mixup technique, which creates virtual samples by a weighted linear interpolation. Note that, our distillation loss can fully utilize the incorrect classes similarities by the mixed labels. The proposed approach promotes the performance of student model as the virtual sample created by multiple images produces a similar probability distribution in the teacher and student networks. Experiments and ablation studies on several public classification datasets including CIFAR-10,CIFAR-100,CINIC-10 and Tiny-ImageNet verify that this light-weight method can effectively boost the performance of the compact student model. It shows that STKD substantially has outperformed the vanilla knowledge distillation and has achieved superior accuracy over the state-of-the-art knowledge distillation methods.\n## B. Knowledge Distillation.\nDifferent from above methods, knowledge distillation enrich and get the student model by extracting kinds of knowledge from the fixed teacher model. To address the challenge of deploying CNNs in resource-constrained edge devices, Bucilua et al. [38] first propose to transfer the knowledge of an ensemble of models to a small model. Then Caruana et al. [39] propose to train student model by mimicking the teacher model's logits. Later, Hinton et al. [12] popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network. Afterward, some recent works [14] [15] extend KD by distilling knowledge from intermediate feature representations instead of soft labels. For example, FitNets [14] propose to train student network by mimicking the intermediate feature maps of teacher network, which are defined as hints. Inspired by this, Zagoruyko et al. [15] propose to match the attention maps between the teacher and the student, which are defined from the original feature maps as knowledge. Wang et al. [40] propose to improve the performance of student network by matching the distributions of spatial neuron activations between the teacher and the student. Recently, Heo et al. [41] introduce the activation boundary of the hidden neuron as knowledge for distilling the compact student network. \n\nHowever, the aforementioned knowledge distillation methods only utilize the knowledge contained in the output of specific layers of the teacher network. More richer knowledge between different layers is explored and utilized for knowledge distillation. For example, Yim et al. [16] propose to use Gram matrix between different feature layers as distilled knowledge, which named flow of solution process (FSP) that reflects the relations of different features maps.",
            "reference_string": "[232269823 | Zhao et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "venue": "Knowledge-Based Systems",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2106.10885",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.10885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "2087106420",
                    "name": "Zihe Dong"
                },
                {
                    "authorId": "2108144626",
                    "name": "Qiong Li"
                }
            ],
            "abstract": null,
            "corpus_id": 235489777,
            "sentences": [
                {
                    "corpus_id": "235489777",
                    "title": "Knowledge Distillation via Instance-level Sequence Learning",
                    "text": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
                    "score": 0.5423963456851287,
                    "section_title": "B. Formulation",
                    "char_start_offset": 12183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 361
                        },
                        {
                            "start": 364,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1084
                        },
                        {
                            "start": 1087,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1399
                        },
                        {
                            "start": 1402,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1655
                        },
                        {
                            "start": 1658,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2084
                        },
                        {
                            "start": 2085,
                            "end": 2231
                        },
                        {
                            "start": 2234,
                            "end": 2294
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1101,
                            "end": 1105,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1128,
                            "end": 1132,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 1879,
                            "end": 1883,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8056640625
                }
            ],
            "relevance_judgement": 0.8056640625,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation via Instance-level Sequence Learning\n# Venue: Knowledge-Based Systems\n# Authors: Haoran Zhao, Xin Sun, Junyu Dong, Zihe Dong, Qiong Li\n## Abstract\nNone\n## B. Formulation\nThe idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
            "reference_string": "[235489777 | Zhao et al. | 2021 | Citations: 24]"
        },
        {
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.08038",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.08038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135847074",
                    "name": "Xiongtao Zhang"
                },
                {
                    "authorId": "2206403890",
                    "name": "Zezong Yin"
                },
                {
                    "authorId": "3247526",
                    "name": "Yunliang Jiang"
                },
                {
                    "authorId": "1390650781",
                    "name": "Yizhang Jiang"
                },
                {
                    "authorId": "2340356",
                    "name": "Da-Song Sun"
                },
                {
                    "authorId": "2189281",
                    "name": "Yong Liu"
                }
            ],
            "abstract": "High-order Takagi-Sugeno-Kang (TSK) fuzzy classifiers possess powerful classification performance yet have fewer fuzzy rules, but always be impaired by its exponential growth training time and poorer interpretability owing to High-order polynomial used in consequent part of fuzzy rule, while Low-order TSK fuzzy classifiers run quickly with high interpretability, however they usually require more fuzzy rules and perform relatively not very well. Address this issue, a novel TSK fuzzy classifier embeded with knowledge distillation in deep learning called HTSK-LLM-DKD is proposed in this study. HTSK-LLM-DKD achieves the following distinctive characteristics: 1) It takes High-order TSK classifier as teacher model and Low-order TSK fuzzy classifier as student model, and leverages the proposed LLM-DKD (Least Learning Machine based Decoupling Knowledge Distillation) to distill the fuzzy dark knowledge from High-order TSK fuzzy classifier to Low-order TSK fuzzy classifier, which resulting in Low-order TSK fuzzy classifier endowed with enhanced performance surpassing or at least comparable to High-order TSK classifier, as well as high interpretability; specifically 2) The Negative Euclidean distance between the output of teacher model and each class is employed to obtain the teacher logits, and then it compute teacher/student soft labels by the softmax function with distillating temperature parameter; 3) By reformulating the Kullback-Leibler divergence, it decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge, and transfers them to student model. The advantages of HTSK-LLM-DKD are verified on the benchmarking UCI datasets and a real dataset Cleveland heart disease, in terms of classification performance and model interpretability.",
            "corpus_id": 256900863,
            "sentences": [
                {
                    "corpus_id": "256900863",
                    "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                    "text": "Knowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
                    "score": 0.5965737149579841,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 8754,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 617
                        },
                        {
                            "start": 620,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1583
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 730,
                            "end": 734,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 759,
                            "end": 763,
                            "matchedPaperCorpusId": "233033712"
                        },
                        {
                            "start": 798,
                            "end": 802,
                            "matchedPaperCorpusId": "209405263"
                        },
                        {
                            "start": 959,
                            "end": 963,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1188,
                            "end": 1192,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80419921875
                }
            ],
            "relevance_judgement": 0.80419921875,
            "relevance_judgment_input_expanded": "# Title: Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK\n# Venue: arXiv.org\n# Authors: Xiongtao Zhang, Zezong Yin, Yunliang Jiang, Yizhang Jiang, Da-Song Sun, Yong Liu\n## Abstract\nHigh-order Takagi-Sugeno-Kang (TSK) fuzzy classifiers possess powerful classification performance yet have fewer fuzzy rules, but always be impaired by its exponential growth training time and poorer interpretability owing to High-order polynomial used in consequent part of fuzzy rule, while Low-order TSK fuzzy classifiers run quickly with high interpretability, however they usually require more fuzzy rules and perform relatively not very well. Address this issue, a novel TSK fuzzy classifier embeded with knowledge distillation in deep learning called HTSK-LLM-DKD is proposed in this study. HTSK-LLM-DKD achieves the following distinctive characteristics: 1) It takes High-order TSK classifier as teacher model and Low-order TSK fuzzy classifier as student model, and leverages the proposed LLM-DKD (Least Learning Machine based Decoupling Knowledge Distillation) to distill the fuzzy dark knowledge from High-order TSK fuzzy classifier to Low-order TSK fuzzy classifier, which resulting in Low-order TSK fuzzy classifier endowed with enhanced performance surpassing or at least comparable to High-order TSK classifier, as well as high interpretability; specifically 2) The Negative Euclidean distance between the output of teacher model and each class is employed to obtain the teacher logits, and then it compute teacher/student soft labels by the softmax function with distillating temperature parameter; 3) By reformulating the Kullback-Leibler divergence, it decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge, and transfers them to student model. The advantages of HTSK-LLM-DKD are verified on the benchmarking UCI datasets and a real dataset Cleveland heart disease, in terms of classification performance and model interpretability.\n## B. Knowledge Distillation\nKnowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
            "reference_string": "[256900863 | Zhang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction",
            "venue": "International Conference on Database Systems for Advanced Applications",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2000918408",
                    "name": "Kaixi Hu"
                },
                {
                    "authorId": "2155688849",
                    "name": "Lin Li"
                },
                {
                    "authorId": "2027162851",
                    "name": "Qing Xie"
                },
                {
                    "authorId": "2070898550",
                    "name": "Xiaohui Tao"
                },
                {
                    "authorId": "2256933615",
                    "name": "Guandong Xu"
                }
            ],
            "abstract": "Granularity and accuracy are two crucial factors for crime event prediction. Within fine-grained event classification, multiple criminal intents may alternately exhibit in preceding sequential events, and progress differently in next. Such intensive intent dynamics makes training models hard to capture unobserved intents, and thus leads to sub-optimal generalization performance, especially in the intertwining of numerous potential events. To capture comprehensive criminal intents, this paper proposes a fine-grained sequential crime prediction framework, CrimeAlarm, that equips with a novel mutual distillation strategy inspired by curriculum learning. During the early training phase, spot-shared criminal intents are captured through high-confidence sequence samples. In the later phase, spot-specific intents are gradually learned by increasing the contribution of low-confidence sequences. Meanwhile, the output probability distributions are reciprocally learned between prediction networks to model unobserved criminal intents. Extensive experiments show that CrimeAlarm outperforms state-of-the-art methods in terms of NDCG@5, with improvements of 4.51% for the NYC16 and 7.73% for the CHI18 in accuracy measures.",
            "corpus_id": 269033278,
            "sentences": [
                {
                    "corpus_id": "269033278",
                    "title": "CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction",
                    "text": "Why knowledge distillation works?Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance.\n\nRecent decoupled knowledge distillation (DKD) [33] reveals that the target distillation is related to training difficulty, and the non-target distillation provides knowledge among classes.For the kth prediction network, given its logits z (k) , the cross-entropy (CE) of DKD can be written as:\n\nwhere \u03b2 is a hyper-parameter of non-target distillation, and the target probability distribution of a student model is\n\nwhile the non-target probability distribution of a student is\n\ni , ..., q\n\nand \" * \" is the index of the target (ground-truth).For simplicity, we use tilde (\u223c) as the symbol of corresponding probability distribution from its teacher.Basically, each sequence sample is involved with a target crime event (observed intent) while other unobserved intents are covered in numerous non-target crime events.It is appropriate to model them separately through DKD.However, based on our follow-up analysis in Sec.3.2 and Sec.3.3 about target and non-target parts in Eq. ( 2), we find it is intractable for existing KD to handle numerous candidate crime events.This further inspire our CrimeAlarm.",
                    "score": 0.5009363760250231,
                    "section_title": "Decoupled Knowledge Distillation",
                    "char_start_offset": 4475,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 33
                        },
                        {
                            "start": 33,
                            "end": 189
                        },
                        {
                            "start": 189,
                            "end": 209
                        },
                        {
                            "start": 209,
                            "end": 376
                        },
                        {
                            "start": 378,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 671
                        },
                        {
                            "start": 673,
                            "end": 791
                        },
                        {
                            "start": 793,
                            "end": 854
                        },
                        {
                            "start": 856,
                            "end": 866
                        },
                        {
                            "start": 868,
                            "end": 920
                        },
                        {
                            "start": 920,
                            "end": 1026
                        },
                        {
                            "start": 1026,
                            "end": 1193
                        },
                        {
                            "start": 1193,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1296
                        },
                        {
                            "start": 1296,
                            "end": 1308
                        },
                        {
                            "start": 1308,
                            "end": 1443
                        },
                        {
                            "start": 1443,
                            "end": 1479
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.794921875
                }
            ],
            "relevance_judgement": 0.794921875,
            "relevance_judgment_input_expanded": "# Title: CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction\n# Venue: International Conference on Database Systems for Advanced Applications\n# Authors: Kaixi Hu, Lin Li, Qing Xie, Xiaohui Tao, Guandong Xu\n## Abstract\nGranularity and accuracy are two crucial factors for crime event prediction. Within fine-grained event classification, multiple criminal intents may alternately exhibit in preceding sequential events, and progress differently in next. Such intensive intent dynamics makes training models hard to capture unobserved intents, and thus leads to sub-optimal generalization performance, especially in the intertwining of numerous potential events. To capture comprehensive criminal intents, this paper proposes a fine-grained sequential crime prediction framework, CrimeAlarm, that equips with a novel mutual distillation strategy inspired by curriculum learning. During the early training phase, spot-shared criminal intents are captured through high-confidence sequence samples. In the later phase, spot-specific intents are gradually learned by increasing the contribution of low-confidence sequences. Meanwhile, the output probability distributions are reciprocally learned between prediction networks to model unobserved criminal intents. Extensive experiments show that CrimeAlarm outperforms state-of-the-art methods in terms of NDCG@5, with improvements of 4.51% for the NYC16 and 7.73% for the CHI18 in accuracy measures.\n## Decoupled Knowledge Distillation\nWhy knowledge distillation works?Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance.\n\nRecent decoupled knowledge distillation (DKD) [33] reveals that the target distillation is related to training difficulty, and the non-target distillation provides knowledge among classes.For the kth prediction network, given its logits z (k) , the cross-entropy (CE) of DKD can be written as:\n\nwhere \u03b2 is a hyper-parameter of non-target distillation, and the target probability distribution of a student model is\n\nwhile the non-target probability distribution of a student is\n\ni , ..., q\n\nand \" * \" is the index of the target (ground-truth).For simplicity, we use tilde (\u223c) as the symbol of corresponding probability distribution from its teacher.Basically, each sequence sample is involved with a target crime event (observed intent) while other unobserved intents are covered in numerous non-target crime events.It is appropriate to model them separately through DKD.However, based on our follow-up analysis in Sec.3.2 and Sec.3.3 about target and non-target parts in Eq. ( 2), we find it is intractable for existing KD to handle numerous candidate crime events.This further inspire our CrimeAlarm.",
            "reference_string": "[269033278 | Hu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Determining Top Fully Connected Layer\u2019s Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19",
            "venue": "Journal of digital imaging",
            "year": 2021,
            "reference_count": 65,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8480458",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8480458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "29592319",
                    "name": "Ritwick Ghosh"
                }
            ],
            "abstract": "Deep convolutional neural network (CNN)-assisted classification of images is one of the most discussed topics in recent years. Continuously innovation of neural network architectures is making it more correct and efficient every day. But training a neural network from scratch is very time-consuming and requires a lot of sophisticated computational equipment and power. So, using some pre-trained neural network as feature extractor for any image classification task or \u201ctransfer learning\u201d is a very popular approach that saves time and computational power for practical use of CNNs. In this paper, an efficient way of building full model from any pre-trained model with high accuracy and low memory is proposed using knowledge distillation. Using the distilled knowledge of the last layer of pre-trained networks passes through fully connected layers with different hidden layers, followed by Softmax layer. The accuracies of student networks are mildly lesser than the whole models, but accuracy of student models clearly indicates the accuracy of the real network. In this way, the best number of hidden layers for dense layer for that pre-trained network with best accuracy and no-overfitting can be found with less time. Here, VGG16 and VGG19 (pre-trained upon \u201cImageNet\u201d dataset) is tested upon chest X-rays (pneumonia and COVID-19). For finding the best total number of hidden layers, it saves nearly 44 min for VGG19 and 36 min and 37 s for VGG16 feature extractor.",
            "corpus_id": 238223320,
            "sentences": [
                {
                    "corpus_id": "238223320",
                    "title": "Determining Top Fully Connected Layer\u2019s Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19",
                    "text": "Neural networks use distributed representation of feature map of input to interpret and processing in hidden layers [26]. These representations are hard to understand and represent in general form. Knowledge distillation (KD) was first applied on deep neural networks by Hinton et al. [27]. KD was first proposed by Bucila et al. [28]. \n\nKnowledge distillation is the process to transfer the representations and abilities learned by a large network (teacher network) to any smaller network (student network). Main drawback of knowledge distillation is that it can only be applied for classification tasks, not for regression tasks [29]. KD uses the \"dark knowledge\" (softened logit output of the bottom output layer of teacher network) that is transferred to student network. This dark knowledge is more than interlabel correlations and one-hot encoding of labels. In case of regression, the deep network interprets the continuous values, which has a tendency of unknown error distribution; as a result, there is no dark knowledge for deep networks trained for regression. But later [30] present their knowledge distillation process for pose regression. Multiple papers try to propose a framework to train the teacher and student network in parallel. Yim et al. [31] and You et al. [32] proposed a shared layer-representation for it. Czarnecki et al. [33] narrowed down the variance between teacher and student derivatives of the loss shared with the discrepancy from teacher predictions. Tarvainen and Valpola [34] proposed an averaging procedure for model weights for the same purpose. Urban et al. [35] trained a network of convolutional neural networks and teach shallow multilayer perceptron as student networks. Sau and Balasubramanian [36] injected noise into teacher logits for making the student network more robust. Employing several teacher networks is always a way to decrease the accuracy difference between teacher and student network. Zhang et al. [37] proposed deep mutual learning so that teacher and student networks can learn side by side during training. \n\nHinton et al. [27] claim that the success of knowledge distillation is credited to the logit distribution of the incorrect outputs.",
                    "score": 0.6181973133641059,
                    "section_title": "Distilling Neural Networks",
                    "char_start_offset": 5608,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 335
                        },
                        {
                            "start": 338,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1250
                        },
                        {
                            "start": 1251,
                            "end": 1333
                        },
                        {
                            "start": 1334,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 1949
                        },
                        {
                            "start": 1950,
                            "end": 2074
                        },
                        {
                            "start": 2077,
                            "end": 2208
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 116,
                            "end": 120,
                            "matchedPaperCorpusId": "1779661"
                        },
                        {
                            "start": 285,
                            "end": 289,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 330,
                            "end": 334,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 631,
                            "end": 635,
                            "matchedPaperCorpusId": "32588614"
                        },
                        {
                            "start": 1262,
                            "end": 1266,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1282,
                            "end": 1286,
                            "matchedPaperCorpusId": "26021416"
                        },
                        {
                            "start": 1351,
                            "end": 1355,
                            "matchedPaperCorpusId": "21596346"
                        },
                        {
                            "start": 1511,
                            "end": 1515,
                            "matchedPaperCorpusId": "263861232"
                        },
                        {
                            "start": 1601,
                            "end": 1605,
                            "matchedPaperCorpusId": "16550689"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.771484375
                }
            ],
            "relevance_judgement": 0.771484375,
            "relevance_judgment_input_expanded": "# Title: Determining Top Fully Connected Layer\u2019s Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19\n# Venue: Journal of digital imaging\n# Authors: Ritwick Ghosh\n## Abstract\nDeep convolutional neural network (CNN)-assisted classification of images is one of the most discussed topics in recent years. Continuously innovation of neural network architectures is making it more correct and efficient every day. But training a neural network from scratch is very time-consuming and requires a lot of sophisticated computational equipment and power. So, using some pre-trained neural network as feature extractor for any image classification task or \u201ctransfer learning\u201d is a very popular approach that saves time and computational power for practical use of CNNs. In this paper, an efficient way of building full model from any pre-trained model with high accuracy and low memory is proposed using knowledge distillation. Using the distilled knowledge of the last layer of pre-trained networks passes through fully connected layers with different hidden layers, followed by Softmax layer. The accuracies of student networks are mildly lesser than the whole models, but accuracy of student models clearly indicates the accuracy of the real network. In this way, the best number of hidden layers for dense layer for that pre-trained network with best accuracy and no-overfitting can be found with less time. Here, VGG16 and VGG19 (pre-trained upon \u201cImageNet\u201d dataset) is tested upon chest X-rays (pneumonia and COVID-19). For finding the best total number of hidden layers, it saves nearly 44 min for VGG19 and 36 min and 37 s for VGG16 feature extractor.\n## Distilling Neural Networks\nNeural networks use distributed representation of feature map of input to interpret and processing in hidden layers [26]. These representations are hard to understand and represent in general form. Knowledge distillation (KD) was first applied on deep neural networks by Hinton et al. [27]. KD was first proposed by Bucila et al. [28]. \n\nKnowledge distillation is the process to transfer the representations and abilities learned by a large network (teacher network) to any smaller network (student network). Main drawback of knowledge distillation is that it can only be applied for classification tasks, not for regression tasks [29]. KD uses the \"dark knowledge\" (softened logit output of the bottom output layer of teacher network) that is transferred to student network. This dark knowledge is more than interlabel correlations and one-hot encoding of labels. In case of regression, the deep network interprets the continuous values, which has a tendency of unknown error distribution; as a result, there is no dark knowledge for deep networks trained for regression. But later [30] present their knowledge distillation process for pose regression. Multiple papers try to propose a framework to train the teacher and student network in parallel. Yim et al. [31] and You et al. [32] proposed a shared layer-representation for it. Czarnecki et al. [33] narrowed down the variance between teacher and student derivatives of the loss shared with the discrepancy from teacher predictions. Tarvainen and Valpola [34] proposed an averaging procedure for model weights for the same purpose. Urban et al. [35] trained a network of convolutional neural networks and teach shallow multilayer perceptron as student networks. Sau and Balasubramanian [36] injected noise into teacher logits for making the student network more robust. Employing several teacher networks is always a way to decrease the accuracy difference between teacher and student network. Zhang et al. [37] proposed deep mutual learning so that teacher and student networks can learn side by side during training. \n\nHinton et al. [27] claim that the success of knowledge distillation is credited to the logit distribution of the incorrect outputs.",
            "reference_string": "[238223320 | Ghosh | 2021 | Citations: 3]"
        },
        {
            "title": "Generalized Knowledge Distillation via Relationship Matching",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2022,
            "reference_count": 113,
            "citation_count": 21,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.01915",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.01915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151459740",
                    "name": "Han-Jia Ye"
                },
                {
                    "authorId": "2115435395",
                    "name": "Su Lu"
                },
                {
                    "authorId": "1721819",
                    "name": "De-chuan Zhan"
                }
            ],
            "abstract": "The knowledge of a well-trained deep neural network (a.k.a. the \u201cteacher\u201d) is valuable for learning similar tasks. Knowledge distillation extracts knowledge from the teacher and integrates it with the target model (a.k.a. the \u201cstudent\u201d), which expands the student\u2019s knowledge and improves its learning efficacy. Instead of enforcing the teacher to work on the same task as the student, we borrow the knowledge from a teacher trained from a general label space \u2014 in this \u201cGeneralized Knowledge Distillation (GKD),\u201d the classes of the teacher and the student may be the same, completely different, or partially overlapped. We claim that the comparison ability between instances acts as an essential factor threading knowledge across tasks, and propose the RElationship FacIlitated Local cLassifiEr Distillation (ReFilled) approach, which decouples the GKD flow of the embedding and the top-layer classifier. In particular, different from reconciling the instance-label confidence between models, ReFilled requires the teacher to reweight the hard tuples pushed forward by the student and then matches the similarity comparison levels between instances. An embedding-induced classifier based on the teacher model supervises the student\u2019s classification confidence and adaptively emphasizes the most related supervision from the teacher. ReFilled demonstrates strong discriminative ability when the classes of the teacher vary from the same to a fully non-overlapped set w.r.t. the student. It also achieves state-of-the-art performance on standard knowledge distillation, one-step incremental learning, and few-shot learning tasks.",
            "corpus_id": 247521335,
            "sentences": [
                {
                    "corpus_id": "247521335",
                    "title": "Generalized Knowledge Distillation via Relationship Matching",
                    "text": "S UPERVISED deep learning has demonstrated success in a variety of fields [1]. Given the instances and corresponding annotations from the target task, we train a deep neural network to minimize the discrepancy between the model predictions and the ground-truth labels. Knowledge distillation (KD) [2], [3], [4] facilitates the learning efficiency of a deep neural network via taking advantage of the \"dark knowledge\" from another well-trained model. In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task. KD improves the discriminative ability of the target student model [9], [10], relieves the burden of model storage [3], [5], [4], [7], [11], [12] and enables the training of a deep neural network in low-resource environments [13], [14]. Applications of KD have been witnessed in a wide range of domains such as model/dataset compression [15], [16], [17], [18], [19], [20], multi-task learning [21], [22], and incremental image classification [23], [24]. \n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress. Although prediction matching allows knowledge to be transferred across different architectures [3], [17], its dependence on Figure 1: An illustration of strengthening a student model on the target task via distilling the knowledge from a teacher model. In standard Knowledge Distillation (KD), teacher and student share the same set of classes.",
                    "score": 0.5306139987986842,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 79,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1238
                        },
                        {
                            "start": 1241,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1824
                        },
                        {
                            "start": 1825,
                            "end": 1916
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 74,
                            "end": 77,
                            "matchedPaperCorpusId": "195908774"
                        },
                        {
                            "start": 297,
                            "end": 300,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 302,
                            "end": 305,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 307,
                            "end": 310,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 564,
                            "end": 567,
                            "matchedPaperCorpusId": "102351826"
                        },
                        {
                            "start": 604,
                            "end": 607,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 609,
                            "end": 612,
                            "matchedPaperCorpusId": "54436113"
                        },
                        {
                            "start": 852,
                            "end": 855,
                            "matchedPaperCorpusId": "226841742"
                        },
                        {
                            "start": 857,
                            "end": 861,
                            "matchedPaperCorpusId": "218487294"
                        },
                        {
                            "start": 900,
                            "end": 903,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 910,
                            "end": 913,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 915,
                            "end": 918,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1010,
                            "end": 1014,
                            "matchedPaperCorpusId": "219965421"
                        },
                        {
                            "start": 1122,
                            "end": 1126,
                            "matchedPaperCorpusId": "53763883"
                        },
                        {
                            "start": 1128,
                            "end": 1132,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 1134,
                            "end": 1138,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 1140,
                            "end": 1144,
                            "matchedPaperCorpusId": "159041346"
                        },
                        {
                            "start": 1146,
                            "end": 1150,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 1178,
                            "end": 1182,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 1184,
                            "end": 1188,
                            "matchedPaperCorpusId": "199543275"
                        },
                        {
                            "start": 1227,
                            "end": 1231,
                            "matchedPaperCorpusId": "102486799"
                        },
                        {
                            "start": 1233,
                            "end": 1237,
                            "matchedPaperCorpusId": "49655438"
                        },
                        {
                            "start": 1667,
                            "end": 1670,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1672,
                            "end": 1676,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.771484375
                }
            ],
            "relevance_judgement": 0.771484375,
            "relevance_judgment_input_expanded": "# Title: Generalized Knowledge Distillation via Relationship Matching\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Han-Jia Ye, Su Lu, De-chuan Zhan\n## Abstract\nThe knowledge of a well-trained deep neural network (a.k.a. the \u201cteacher\u201d) is valuable for learning similar tasks. Knowledge distillation extracts knowledge from the teacher and integrates it with the target model (a.k.a. the \u201cstudent\u201d), which expands the student\u2019s knowledge and improves its learning efficacy. Instead of enforcing the teacher to work on the same task as the student, we borrow the knowledge from a teacher trained from a general label space \u2014 in this \u201cGeneralized Knowledge Distillation (GKD),\u201d the classes of the teacher and the student may be the same, completely different, or partially overlapped. We claim that the comparison ability between instances acts as an essential factor threading knowledge across tasks, and propose the RElationship FacIlitated Local cLassifiEr Distillation (ReFilled) approach, which decouples the GKD flow of the embedding and the top-layer classifier. In particular, different from reconciling the instance-label confidence between models, ReFilled requires the teacher to reweight the hard tuples pushed forward by the student and then matches the similarity comparison levels between instances. An embedding-induced classifier based on the teacher model supervises the student\u2019s classification confidence and adaptively emphasizes the most related supervision from the teacher. ReFilled demonstrates strong discriminative ability when the classes of the teacher vary from the same to a fully non-overlapped set w.r.t. the student. It also achieves state-of-the-art performance on standard knowledge distillation, one-step incremental learning, and few-shot learning tasks.\n## INTRODUCTION\nS UPERVISED deep learning has demonstrated success in a variety of fields [1]. Given the instances and corresponding annotations from the target task, we train a deep neural network to minimize the discrepancy between the model predictions and the ground-truth labels. Knowledge distillation (KD) [2], [3], [4] facilitates the learning efficiency of a deep neural network via taking advantage of the \"dark knowledge\" from another well-trained model. In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task. KD improves the discriminative ability of the target student model [9], [10], relieves the burden of model storage [3], [5], [4], [7], [11], [12] and enables the training of a deep neural network in low-resource environments [13], [14]. Applications of KD have been witnessed in a wide range of domains such as model/dataset compression [15], [16], [17], [18], [19], [20], multi-task learning [21], [22], and incremental image classification [23], [24]. \n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress. Although prediction matching allows knowledge to be transferred across different architectures [3], [17], its dependence on Figure 1: An illustration of strengthening a student model on the target task via distilling the knowledge from a teacher model. In standard Knowledge Distillation (KD), teacher and student share the same set of classes.",
            "reference_string": "[247521335 | Ye et al. | 2022 | Citations: 21]"
        },
        {
            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 76,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.13063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2212036699",
                    "name": "Jiao Xie"
                },
                {
                    "authorId": "3431378",
                    "name": "Shaohui Lin"
                },
                {
                    "authorId": "2121310989",
                    "name": "Yichen Zhang"
                },
                {
                    "authorId": "39378434",
                    "name": "Linkai Luo"
                }
            ],
            "abstract": "The large memory and computation consumption in convolutional neural networks (CNNs) has been one of the main barriers for deploying them on resource-limited systems. To this end, most cheap convolutions (e.g., group convolution, depth-wise convolution, and shift convolution) have recently been used for memory and computation reduction but with the specific architecture designing. Furthermore, it results in a low discriminability of the compressed networks by directly replacing the standard convolution with these cheap ones. In this paper, we propose to use knowledge distillation to improve the performance of the compact student networks with cheap convolutions. In our case, the teacher is a network with the standard convolution, while the student is a simple transformation of the teacher architecture without complicated redesigning. In particular, we propose a novel online distillation method, which online constructs the teacher network without pre-training and conducts mutual learning between the teacher and student network, to improve the performance of the student model. Extensive experiments demonstrate that the proposed approach achieves superior performance to simultaneously reduce memory and computation overhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and ImageNet ILSVRC 2012, compared to the state-of-the-art CNN compression and acceleration methods. The codes are publicly available at this https URL.",
            "corpus_id": 203593636,
            "sentences": [
                {
                    "corpus_id": "203593636",
                    "title": "Training convolutional neural networks with cheap convolutions and online distillation",
                    "text": "By replacing the standard convolution with several cheap convolutions, a variety of CNN models can be compressed and accelerated substantially. It is simple and straightforward to train the compressed models with cheap convolutions from scratch to improve the accuracy. By doing this, it however leads to a limited improvement on accuracy, which is due to a limited knowledge used only by the ground-truth labels. \n\nAlternatively, knowledge distillation [30,32] is becoming a promising solution, which aims to transfer more knowledge from a teacher network to a student network to boost the accuracy of the student network. For this paper, we first review two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: dark knowledge (DK) [30] and attention transfer (AT) [32]. In that case, we can select a CNN model with the standard convolution as a teacher, while a model with cheap convolution by keeping the teacher's architecture is regarded as a student. Then, we propose our online distillation (OD) method to replace the pretrained teacher network by constructing online from the multiple student networks with the same architecture, and train both teacher and student networks in a one-shot manner. \n\nDark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels. \n\nAttention transfer.",
                    "score": 0.49923301362947653,
                    "section_title": "Online Distillation",
                    "char_start_offset": 18581,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 413
                        },
                        {
                            "start": 416,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1506
                        },
                        {
                            "start": 1509,
                            "end": 1679
                        },
                        {
                            "start": 1682,
                            "end": 1759
                        },
                        {
                            "start": 1762,
                            "end": 1858
                        },
                        {
                            "start": 1861,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2100
                        },
                        {
                            "start": 2103,
                            "end": 2122
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 458,
                            "end": 461,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 826,
                            "end": 830,
                            "matchedPaperCorpusId": "829159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7490234375
                }
            ],
            "relevance_judgement": 0.7490234375,
            "relevance_judgment_input_expanded": "# Title: Training convolutional neural networks with cheap convolutions and online distillation\n# Venue: arXiv.org\n# Authors: Jiao Xie, Shaohui Lin, Yichen Zhang, Linkai Luo\n## Abstract\nThe large memory and computation consumption in convolutional neural networks (CNNs) has been one of the main barriers for deploying them on resource-limited systems. To this end, most cheap convolutions (e.g., group convolution, depth-wise convolution, and shift convolution) have recently been used for memory and computation reduction but with the specific architecture designing. Furthermore, it results in a low discriminability of the compressed networks by directly replacing the standard convolution with these cheap ones. In this paper, we propose to use knowledge distillation to improve the performance of the compact student networks with cheap convolutions. In our case, the teacher is a network with the standard convolution, while the student is a simple transformation of the teacher architecture without complicated redesigning. In particular, we propose a novel online distillation method, which online constructs the teacher network without pre-training and conducts mutual learning between the teacher and student network, to improve the performance of the student model. Extensive experiments demonstrate that the proposed approach achieves superior performance to simultaneously reduce memory and computation overhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and ImageNet ILSVRC 2012, compared to the state-of-the-art CNN compression and acceleration methods. The codes are publicly available at this https URL.\n## Online Distillation\nBy replacing the standard convolution with several cheap convolutions, a variety of CNN models can be compressed and accelerated substantially. It is simple and straightforward to train the compressed models with cheap convolutions from scratch to improve the accuracy. By doing this, it however leads to a limited improvement on accuracy, which is due to a limited knowledge used only by the ground-truth labels. \n\nAlternatively, knowledge distillation [30,32] is becoming a promising solution, which aims to transfer more knowledge from a teacher network to a student network to boost the accuracy of the student network. For this paper, we first review two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: dark knowledge (DK) [30] and attention transfer (AT) [32]. In that case, we can select a CNN model with the standard convolution as a teacher, while a model with cheap convolution by keeping the teacher's architecture is regarded as a student. Then, we propose our online distillation (OD) method to replace the pretrained teacher network by constructing online from the multiple student networks with the same architecture, and train both teacher and student networks in a one-shot manner. \n\nDark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels. \n\nAttention transfer.",
            "reference_string": "[203593636 | Xie et al. | 2019 | Citations: 12]"
        },
        {
            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 85,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186275686",
                    "name": "Jiatai Wang"
                },
                {
                    "authorId": "2244022079",
                    "name": "Zhiwei Xu"
                },
                {
                    "authorId": "2262805047",
                    "name": "Xin Wang"
                }
            ],
            "abstract": "Existing multi-stage clustering methods independently learn the salient features from multiple views and then perform the clustering task. Particularly, multi-view clustering (MVC) has attracted a lot of attention in multi-view or multi-modal scenarios. MVC aims at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that mis-guide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.",
            "corpus_id": 264590688,
            "sentences": [
                {
                    "corpus_id": "264590688",
                    "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
                    "text": "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT [30] uses convolutional operations to transfer dark knowledge; 2) DeiT [31] proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP [32] exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD [34] exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes [36]. With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge [36]. This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks. \n\nIn the next section, we consider this observation and leverage knowledge self-distillation in Multi-stage MVC.",
                    "score": 0.6231205753332721,
                    "section_title": "B. Contrastive Learning",
                    "char_start_offset": 10461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 56
                        },
                        {
                            "start": 57,
                            "end": 706
                        },
                        {
                            "start": 709,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1651
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 118,
                            "end": 122,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 189,
                            "end": 193,
                            "matchedPaperCorpusId": "229363322"
                        },
                        {
                            "start": 337,
                            "end": 341,
                            "matchedPaperCorpusId": "245006036"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "252918735"
                        },
                        {
                            "start": 1051,
                            "end": 1055,
                            "matchedPaperCorpusId": "219962714"
                        },
                        {
                            "start": 1418,
                            "end": 1422,
                            "matchedPaperCorpusId": "219962714"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7421875
                }
            ],
            "relevance_judgement": 0.7421875,
            "relevance_judgment_input_expanded": "# Title: Towards Generalized Multi-stage Clustering: Multi-view Self-distillation\n# Venue: arXiv.org\n# Authors: Jiatai Wang, Zhiwei Xu, Xin Wang\n## Abstract\nExisting multi-stage clustering methods independently learn the salient features from multiple views and then perform the clustering task. Particularly, multi-view clustering (MVC) has attracted a lot of attention in multi-view or multi-modal scenarios. MVC aims at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that mis-guide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.\n## B. Contrastive Learning\nThe specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT [30] uses convolutional operations to transfer dark knowledge; 2) DeiT [31] proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP [32] exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD [34] exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes [36]. With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge [36]. This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks. \n\nIn the next section, we consider this observation and leverage knowledge self-distillation in Multi-stage MVC.",
            "reference_string": "[264590688 | Wang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
            "venue": "European Conference on Computer Vision",
            "year": 2022,
            "reference_count": 36,
            "citation_count": 19,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12980",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1387852255",
                    "name": "Jiajun Liang"
                },
                {
                    "authorId": "2304362074",
                    "name": "Linze Li"
                },
                {
                    "authorId": "2056410266",
                    "name": "Z. Bing"
                },
                {
                    "authorId": "2112526021",
                    "name": "Borui Zhao"
                },
                {
                    "authorId": "2179286812",
                    "name": "Yao Tang"
                },
                {
                    "authorId": "2179192518",
                    "name": "Bo Lin"
                },
                {
                    "authorId": "1934546",
                    "name": "Haoqiang Fan"
                }
            ],
            "abstract": "Self-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models' era. This paper proposes an efficient self-distillation method named Zipf's Label Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network's final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf's Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61% accuracy gain compared to the vanilla baseline, and 0.88% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https://github.com/megvii-research/zipfls.",
            "corpus_id": 251066725,
            "sentences": [
                {
                    "corpus_id": "251066725",
                    "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
                    "text": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,5,18,34], designing student-friendly architecture [16,20], improving the distillation efficiency [7,14,27,29] and explaining the distillation's working mechanism [1,23]. \n\nIn this work, we focus on how to transfer the \"dark knowledge\" in an almost free manner. Furlanello et al. [7] proposed to improve the performance of the student network by distilling a teacher network with the same architecture. However, it is still a two-stage approach, which first trains the teacher and then distills knowledge to the student. To reduce the training time, many selfdistillation methods were proposed. They gain soft label supervision on the fly without the pretraining step.",
                    "score": 0.681922972997853,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5454,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 678
                        },
                        {
                            "start": 681,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1176
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 509,
                            "end": 511,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 511,
                            "end": 514,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 514,
                            "end": 517,
                            "matchedPaperCorpusId": "244680427"
                        },
                        {
                            "start": 563,
                            "end": 566,
                            "matchedPaperCorpusId": "231925118"
                        },
                        {
                            "start": 606,
                            "end": 609,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 609,
                            "end": 612,
                            "matchedPaperCorpusId": "233714221"
                        },
                        {
                            "start": 612,
                            "end": 615,
                            "matchedPaperCorpusId": "70335318"
                        },
                        {
                            "start": 615,
                            "end": 618,
                            "matchedPaperCorpusId": "214727822"
                        },
                        {
                            "start": 674,
                            "end": 677,
                            "matchedPaperCorpusId": "235390933"
                        },
                        {
                            "start": 788,
                            "end": 791,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7392578125
                }
            ],
            "relevance_judgement": 0.7392578125,
            "relevance_judgment_input_expanded": "# Title: Efficient One Pass Self-distillation with Zipf's Label Smoothing\n# Venue: European Conference on Computer Vision\n# Authors: Jiajun Liang, Linze Li, Z. Bing, Borui Zhao, Yao Tang, Bo Lin, Haoqiang Fan\n## Abstract\nSelf-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models' era. This paper proposes an efficient self-distillation method named Zipf's Label Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network's final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf's Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61% accuracy gain compared to the vanilla baseline, and 0.88% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https://github.com/megvii-research/zipfls.\n## Knowledge Distillation\nInstead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,5,18,34], designing student-friendly architecture [16,20], improving the distillation efficiency [7,14,27,29] and explaining the distillation's working mechanism [1,23]. \n\nIn this work, we focus on how to transfer the \"dark knowledge\" in an almost free manner. Furlanello et al. [7] proposed to improve the performance of the student network by distilling a teacher network with the same architecture. However, it is still a two-stage approach, which first trains the teacher and then distills knowledge to the student. To reduce the training time, many selfdistillation methods were proposed. They gain soft label supervision on the fly without the pretraining step.",
            "reference_string": "[251066725 | Liang et al. | 2022 | Citations: 19]"
        },
        {
            "title": "Class-aware Information for Logit-based Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 2,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.14773",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.14773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Shuoxi Zhang"
                },
                {
                    "authorId": "29901869",
                    "name": "Hanpeng Liu"
                },
                {
                    "authorId": "1706504",
                    "name": "J. Hopcroft"
                },
                {
                    "authorId": "1702188",
                    "name": "Kun He"
                }
            ],
            "abstract": "Knowledge distillation aims to transfer knowledge to the student model by utilizing the predictions/features of the teacher model, and feature-based distillation has recently shown its superiority over logit-based distillation. However, due to the cumbersome computation and storage of extra feature transformation, the training overhead of feature-based methods is much higher than that of logit-based distillation. In this work, we revisit the logit-based knowledge distillation, and observe that the existing logit-based distillation methods treat the prediction logits only in the instance level, while many other useful semantic information is overlooked. To address this issue, we propose a Class-aware Logit Knowledge Distillation (CLKD) method, that extents the logit distillation in both instance-level and class-level. CLKD enables the student model mimic higher semantic information from the teacher model, hence improving the distillation performance. We further introduce a novel loss called Class Correlation Loss to force the student learn the inherent class-level correlation of the teacher. Empirical comparisons demonstrate the superiority of the proposed method over several prevailing logit-based methods and feature-based methods, in which CLKD achieves compelling results on various visual classification tasks and outperforms the state-of-the-art baselines.",
            "corpus_id": 254044469,
            "sentences": [
                {
                    "corpus_id": "254044469",
                    "title": "Class-aware Information for Logit-based Knowledge Distillation",
                    "text": "The concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits. Therefore, the studies on KD can be divided into two categories, i.e., logit-based distillation and feature-based distillation.",
                    "score": 0.5950559247251486,
                    "section_title": "Related Work",
                    "char_start_offset": 4417,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 59
                        },
                        {
                            "start": 60,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 533
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72900390625
                }
            ],
            "relevance_judgement": 0.72900390625,
            "relevance_judgment_input_expanded": "# Title: Class-aware Information for Logit-based Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Shuoxi Zhang, Hanpeng Liu, J. Hopcroft, Kun He\n## Abstract\nKnowledge distillation aims to transfer knowledge to the student model by utilizing the predictions/features of the teacher model, and feature-based distillation has recently shown its superiority over logit-based distillation. However, due to the cumbersome computation and storage of extra feature transformation, the training overhead of feature-based methods is much higher than that of logit-based distillation. In this work, we revisit the logit-based knowledge distillation, and observe that the existing logit-based distillation methods treat the prediction logits only in the instance level, while many other useful semantic information is overlooked. To address this issue, we propose a Class-aware Logit Knowledge Distillation (CLKD) method, that extents the logit distillation in both instance-level and class-level. CLKD enables the student model mimic higher semantic information from the teacher model, hence improving the distillation performance. We further introduce a novel loss called Class Correlation Loss to force the student learn the inherent class-level correlation of the teacher. Empirical comparisons demonstrate the superiority of the proposed method over several prevailing logit-based methods and feature-based methods, in which CLKD achieves compelling results on various visual classification tasks and outperforms the state-of-the-art baselines.\n## Related Work\nThe concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits. Therefore, the studies on KD can be divided into two categories, i.e., logit-based distillation and feature-based distillation.",
            "reference_string": "[254044469 | Zhang et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262406221",
                    "name": "Yeongseo Jung"
                },
                {
                    "authorId": "2262217139",
                    "name": "Eunseo Jung"
                },
                {
                    "authorId": "2262372006",
                    "name": "Lei Chen"
                }
            ],
            "abstract": "In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity.",
            "corpus_id": 264555654,
            "sentences": [
                {
                    "corpus_id": "264555654",
                    "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
                    "text": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
                    "score": 0.7872691246349993,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6986,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 585
                        },
                        {
                            "start": 588,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 857
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.720703125
                }
            ],
            "relevance_judgement": 0.720703125,
            "relevance_judgment_input_expanded": "# Title: Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yeongseo Jung, Eunseo Jung, Lei Chen\n## Abstract\nIn Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity.\n## Knowledge Distillation\nThe core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
            "reference_string": "[264555654 | Jung et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Improving the Latent Space of Image Style Transfer",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.12135",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.12135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48442720",
                    "name": "Yun-Hao Bai"
                },
                {
                    "authorId": "47073960",
                    "name": "Cairong Wang"
                },
                {
                    "authorId": "2117729099",
                    "name": "C. Yuan"
                },
                {
                    "authorId": "2140245719",
                    "name": "Yanbo Fan"
                },
                {
                    "authorId": "2144537318",
                    "name": "Jue Wang"
                }
            ],
            "abstract": "Existing neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. For example, the style distance between images of different styles is less than that of the same style. In such an inappropriate latent space, the objective function of the existing methods will be optimized in the wrong direction, resulting in bad stylization results. In addition, the lack of content details in the features extracted by the pre-trained encoder also leads to the content leak problem. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task. The style contrastive loss pulls the stylized result closer to the same visual style image and pushes it away from the content image. The content contrastive loss enables the encoder to retain more available details. We can directly add our training scheme to some existing style transfer methods and significantly improve their results. Extensive experimental results demonstrate the effectiveness and superiority of our methods.",
            "corpus_id": 249017724,
            "sentences": [
                {
                    "corpus_id": "249017724",
                    "title": "Improving the Latent Space of Image Style Transfer",
                    "text": "Knowledge distillation (KD) [26,27,28] is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability [28,29], which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted. This dark knowledge is mainly related to labels, so they are rarely used in low-level vision tasks (e.g., neural style transfer). Wang et al. [10] developed a collaborative knowledge distillation method to learn a much smaller model from pre-trained redundant VGG-19 for ultra-resolution style transfer. In our method, the pre-trained encoder is regarded as a regularizer to guarantee that the features extracted by the new encoder are near a suitable value.",
                    "score": 0.526540929603609,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 8201,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1036
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 35,
                            "end": 38,
                            "matchedPaperCorpusId": "102351826"
                        },
                        {
                            "start": 221,
                            "end": 225,
                            "matchedPaperCorpusId": "102351826"
                        },
                        {
                            "start": 225,
                            "end": 228,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 720,
                            "end": 724,
                            "matchedPaperCorpusId": "213178672"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71826171875
                }
            ],
            "relevance_judgement": 0.71826171875,
            "relevance_judgment_input_expanded": "# Title: Improving the Latent Space of Image Style Transfer\n# Venue: arXiv.org\n# Authors: Yun-Hao Bai, Cairong Wang, C. Yuan, Yanbo Fan, Jue Wang\n## Abstract\nExisting neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. For example, the style distance between images of different styles is less than that of the same style. In such an inappropriate latent space, the objective function of the existing methods will be optimized in the wrong direction, resulting in bad stylization results. In addition, the lack of content details in the features extracted by the pre-trained encoder also leads to the content leak problem. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task. The style contrastive loss pulls the stylized result closer to the same visual style image and pushes it away from the content image. The content contrastive loss enables the encoder to retain more available details. We can directly add our training scheme to some existing style transfer methods and significantly improve their results. Extensive experimental results demonstrate the effectiveness and superiority of our methods.\n## Knowledge Distillation\nKnowledge distillation (KD) [26,27,28] is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability [28,29], which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted. This dark knowledge is mainly related to labels, so they are rarely used in low-level vision tasks (e.g., neural style transfer). Wang et al. [10] developed a collaborative knowledge distillation method to learn a much smaller model from pre-trained redundant VGG-19 for ultra-resolution style transfer. In our method, the pre-trained encoder is regarded as a regularizer to guarantee that the features extracted by the new encoder are near a suitable value.",
            "reference_string": "[249017724 | Bai et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Task Integration Distillation for Object Detectors",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152173244",
                    "name": "Hai Su"
                },
                {
                    "authorId": "2294574600",
                    "name": "Zhenwen Jian"
                },
                {
                    "authorId": "2112454661",
                    "name": "Songsen Yu"
                }
            ],
            "abstract": "Knowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.",
            "corpus_id": 268857025,
            "sentences": [
                {
                    "corpus_id": "268857025",
                    "title": "Task Integration Distillation for Object Detectors",
                    "text": "Knowledge distillation is an effective model compression technique that facilitates the transfer of knowledge from a large model to a smaller one, enabling the smaller model to achieve, or even approximate, the performance of the larger model.Initially introduced by Hinton et al. [12], knowledge distillation has been predominantly applied in the image classification domain.The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.\n\nKnowledge distillation was initially proposed in the domain of image classification and has since been widely applied across various fields.Currently, knowledge distillation can be categorized into three types based on the source of knowledge: response-based knowledge distillation [3,5,22,35,42], relation-based knowledge distillation [23,24], and feature-based knowledge distillation [10,11,13,14].Response-based knowledge distillation, the earliest proposed method, involves extracting the output of the teacher network's last layer and directly mimicking the teacher's final prediction.Relation-based knowledge distillation extracts relationships between different layers or data samples within the teacher network, transferring such relational knowledge to the student to achieve distillation.Feature-based knowledge distillation involves extracting and aligning feature maps from both the teacher and student models to the same size, then having the student model's feature maps fit those of the teacher model, facilitating the transfer of knowledge.Due to its ease of handling knowledge sources and almost uniform performance across most domains, feature-based knowledge distillation has garnered widespread attention from scholars for its strong versatility.\n\nKnowledge distillation methods are not only widely applied in the field of computer vision, such as image retrieval [34] and face recognition [17], but also in natural language processing [20,30], speech recognition [15], recommendation systems [28], information security [44], multimodal data [43], and finance and securities [8].Knowledge distillation, with its capability to transfer knowledge between different models, has garnered significant attention across multiple domains due to its versatility.",
                    "score": 0.5173764328872882,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 7226,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 243,
                            "end": 376
                        },
                        {
                            "start": 376,
                            "end": 493
                        },
                        {
                            "start": 493,
                            "end": 588
                        },
                        {
                            "start": 590,
                            "end": 730
                        },
                        {
                            "start": 730,
                            "end": 990
                        },
                        {
                            "start": 990,
                            "end": 1180
                        },
                        {
                            "start": 1180,
                            "end": 1388
                        },
                        {
                            "start": 1388,
                            "end": 1646
                        },
                        {
                            "start": 1646,
                            "end": 1856
                        },
                        {
                            "start": 1858,
                            "end": 2189
                        },
                        {
                            "start": 2189,
                            "end": 2363
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 872,
                            "end": 875,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 875,
                            "end": 877,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 877,
                            "end": 880,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 880,
                            "end": 883,
                            "matchedPaperCorpusId": "54436113"
                        },
                        {
                            "start": 883,
                            "end": 886,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 926,
                            "end": 930,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 930,
                            "end": 933,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 976,
                            "end": 980,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 980,
                            "end": 983,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 986,
                            "end": 989,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 1974,
                            "end": 1978,
                            "matchedPaperCorpusId": "250581330"
                        },
                        {
                            "start": 2000,
                            "end": 2004,
                            "matchedPaperCorpusId": "260068567"
                        },
                        {
                            "start": 2074,
                            "end": 2078,
                            "matchedPaperCorpusId": "235417286"
                        },
                        {
                            "start": 2103,
                            "end": 2107,
                            "matchedPaperCorpusId": "2552056"
                        },
                        {
                            "start": 2130,
                            "end": 2134,
                            "matchedPaperCorpusId": "245693198"
                        },
                        {
                            "start": 2152,
                            "end": 2156,
                            "matchedPaperCorpusId": "237490408"
                        },
                        {
                            "start": 2185,
                            "end": 2188,
                            "matchedPaperCorpusId": "203605465"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71826171875
                }
            ],
            "relevance_judgement": 0.71826171875,
            "relevance_judgment_input_expanded": "# Title: Task Integration Distillation for Object Detectors\n# Venue: arXiv.org\n# Authors: Hai Su, Zhenwen Jian, Songsen Yu\n## Abstract\nKnowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.\n## Knowledge distillation\nKnowledge distillation is an effective model compression technique that facilitates the transfer of knowledge from a large model to a smaller one, enabling the smaller model to achieve, or even approximate, the performance of the larger model.Initially introduced by Hinton et al. [12], knowledge distillation has been predominantly applied in the image classification domain.The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.\n\nKnowledge distillation was initially proposed in the domain of image classification and has since been widely applied across various fields.Currently, knowledge distillation can be categorized into three types based on the source of knowledge: response-based knowledge distillation [3,5,22,35,42], relation-based knowledge distillation [23,24], and feature-based knowledge distillation [10,11,13,14].Response-based knowledge distillation, the earliest proposed method, involves extracting the output of the teacher network's last layer and directly mimicking the teacher's final prediction.Relation-based knowledge distillation extracts relationships between different layers or data samples within the teacher network, transferring such relational knowledge to the student to achieve distillation.Feature-based knowledge distillation involves extracting and aligning feature maps from both the teacher and student models to the same size, then having the student model's feature maps fit those of the teacher model, facilitating the transfer of knowledge.Due to its ease of handling knowledge sources and almost uniform performance across most domains, feature-based knowledge distillation has garnered widespread attention from scholars for its strong versatility.\n\nKnowledge distillation methods are not only widely applied in the field of computer vision, such as image retrieval [34] and face recognition [17], but also in natural language processing [20,30], speech recognition [15], recommendation systems [28], information security [44], multimodal data [43], and finance and securities [8].Knowledge distillation, with its capability to transfer knowledge between different models, has garnered significant attention across multiple domains due to its versatility.",
            "reference_string": "[268857025 | Su et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16910/16717",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.08106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152751276",
                    "name": "Lingyun Feng"
                },
                {
                    "authorId": "2642333",
                    "name": "Minghui Qiu"
                },
                {
                    "authorId": "2110479359",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "16215052",
                    "name": "Haitao Zheng"
                },
                {
                    "authorId": "2115382645",
                    "name": "Ying Shen"
                }
            ],
            "abstract": "Despite pre-trained language models such as BERT have achieved appealing performance in a wide range of Natural Language Processing (NLP) tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment data for BERT Knowledge Distillation in target domains with scarce labeled data, by learning a cross-domain manipulation scheme that automatically augments the target domain with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced controller to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on different NLP tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ~13.3%) when only a few labeled examples available.",
            "corpus_id": 231648215,
            "sentences": [
                {
                    "corpus_id": "231648215",
                    "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
                    "text": "Before we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
                    "score": 0.6011706705271493,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 7626,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 479,
                            "end": 496,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 951,
                            "end": 972,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71337890625
                }
            ],
            "relevance_judgement": 0.71337890625,
            "relevance_judgment_input_expanded": "# Title: Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Lingyun Feng, Minghui Qiu, Yaliang Li, Haitao Zheng, Ying Shen\n## Abstract\nDespite pre-trained language models such as BERT have achieved appealing performance in a wide range of Natural Language Processing (NLP) tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment data for BERT Knowledge Distillation in target domains with scarce labeled data, by learning a cross-domain manipulation scheme that automatically augments the target domain with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced controller to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on different NLP tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ~13.3%) when only a few labeled examples available.\n## Knowledge Distillation\nBefore we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
            "reference_string": "[231648215 | Feng et al. | 2021 | Citations: 10]"
        },
        {
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.01547",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.01547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163284473",
                    "name": "Chengting Yu"
                },
                {
                    "authorId": "2274194299",
                    "name": "Fengzhao Zhang"
                },
                {
                    "authorId": "2255346632",
                    "name": "Ruizhe Chen"
                },
                {
                    "authorId": "2311458018",
                    "name": "Zuozhu Liu"
                },
                {
                    "authorId": "2110408145",
                    "name": "Shurun Tan"
                },
                {
                    "authorId": "2326182494",
                    "name": "Erping Li"
                },
                {
                    "authorId": "2115787618",
                    "name": "Aili Wang"
                }
            ],
            "abstract": "Knowledge distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the logit-based method, bringing the simple KD form based on logits back into the limelight. Features or logits? They partially implement the KD with entirely distinct perspectives; therefore, choosing between logits and features is not straightforward. This article provides a unified perspective of feature alignment to obtain a better comprehension of their fundamental distinction. Inheriting the design philosophy and insights of feature-based and logit-based methods, we introduce a block-wise logit distillation framework to apply implicit logit-based feature alignment by gradually replacing teacher's blocks as intermediate stepping-stone models to bridge the gap between the student and the teacher. Our method obtains comparable or superior results to state-of-the-art distillation methods. This article demonstrates the great potential of combining logit and features, and we hope it will inspire future research to revisit KD from a higher vantage point.",
            "corpus_id": 273811396,
            "sentences": [
                {
                    "corpus_id": "273811396",
                    "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
                    "text": "The concept of knowledge distillation [16] was first proposed as a learning strategy that employs a larger teacher network to steer the training process of a smaller student network for various tasks [27,28]. The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" [51], whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student. Recent studies examine how to select, express, and transfer the \"dark knowledge\" more practically and effectively, which can be categorized into two main types: logit-and feature-based distillations. \n\nLogits Distillation. In the wake of the earliest KD method based on temperature-regulated distillation [16], previous logit-based methods have concentrated mainly on effective regularization and optimization methods. DML [64] proposes a mutual learning method to train students and teachers simultaneously. TAKD [34] proposes using intermediary \"teacher assistants\" to transmit knowledge in a step-by-step manner in order to narrow the performance disparity between teachers and students. Additionally, several works [6,39] focus on interpreting the principles underlying KD. Recently, DKD [65] introduces an improved logit-based objective by decoupling the classical KD loss, which re-explores the potential of logitbased methods with comparable performance gains. \n\nFeature Distillation. To further enhance knowledge distillation, feature distillation is proposed to perform alignments on intermediate features as well as logit outputs, which can directly transfer teacher representations [5,14,15,41] or the correlation [36,37,47,48] from the teacher to the student. Feature methods are more likely to obtain high performance with extensive information from the teacher; however, tight feature alignment frequently relies on prior empirical observation and meticulous adjustment of hyperparameters.",
                    "score": 0.5311675335419309,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 5421,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 639
                        },
                        {
                            "start": 642,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1407
                        },
                        {
                            "start": 1410,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1943
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 200,
                            "end": 204,
                            "matchedPaperCorpusId": "267213"
                        },
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "236912875"
                        },
                        {
                            "start": 297,
                            "end": 301,
                            "matchedPaperCorpusId": "215745611"
                        },
                        {
                            "start": 863,
                            "end": 867,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 954,
                            "end": 958,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 1159,
                            "end": 1162,
                            "matchedPaperCorpusId": "212633769"
                        },
                        {
                            "start": 1162,
                            "end": 1165,
                            "matchedPaperCorpusId": "174800711"
                        },
                        {
                            "start": 1232,
                            "end": 1236,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1633,
                            "end": 1636,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 1636,
                            "end": 1639,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 1665,
                            "end": 1669,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1669,
                            "end": 1672,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1675,
                            "end": 1678,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6953125
                }
            ],
            "relevance_judgement": 0.6953125,
            "relevance_judgment_input_expanded": "# Title: Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: Chengting Yu, Fengzhao Zhang, Ruizhe Chen, Zuozhu Liu, Shurun Tan, Erping Li, Aili Wang\n## Abstract\nKnowledge distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the logit-based method, bringing the simple KD form based on logits back into the limelight. Features or logits? They partially implement the KD with entirely distinct perspectives; therefore, choosing between logits and features is not straightforward. This article provides a unified perspective of feature alignment to obtain a better comprehension of their fundamental distinction. Inheriting the design philosophy and insights of feature-based and logit-based methods, we introduce a block-wise logit distillation framework to apply implicit logit-based feature alignment by gradually replacing teacher's blocks as intermediate stepping-stone models to bridge the gap between the student and the teacher. Our method obtains comparable or superior results to state-of-the-art distillation methods. This article demonstrates the great potential of combining logit and features, and we hope it will inspire future research to revisit KD from a higher vantage point.\n## II. RELATED WORK\nThe concept of knowledge distillation [16] was first proposed as a learning strategy that employs a larger teacher network to steer the training process of a smaller student network for various tasks [27,28]. The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" [51], whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student. Recent studies examine how to select, express, and transfer the \"dark knowledge\" more practically and effectively, which can be categorized into two main types: logit-and feature-based distillations. \n\nLogits Distillation. In the wake of the earliest KD method based on temperature-regulated distillation [16], previous logit-based methods have concentrated mainly on effective regularization and optimization methods. DML [64] proposes a mutual learning method to train students and teachers simultaneously. TAKD [34] proposes using intermediary \"teacher assistants\" to transmit knowledge in a step-by-step manner in order to narrow the performance disparity between teachers and students. Additionally, several works [6,39] focus on interpreting the principles underlying KD. Recently, DKD [65] introduces an improved logit-based objective by decoupling the classical KD loss, which re-explores the potential of logitbased methods with comparable performance gains. \n\nFeature Distillation. To further enhance knowledge distillation, feature distillation is proposed to perform alignments on intermediate features as well as logit outputs, which can directly transfer teacher representations [5,14,15,41] or the correlation [36,37,47,48] from the teacher to the student. Feature methods are more likely to obtain high performance with extensive information from the teacher; however, tight feature alignment frequently relies on prior empirical observation and meticulous adjustment of hyperparameters.",
            "reference_string": "[273811396 | Yu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "venue": "Knowledge-Based Systems",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2311.14334",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.14334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268350036",
                    "name": "Seonghak Kim"
                },
                {
                    "authorId": "2156910329",
                    "name": "Gyeongdo Ham"
                },
                {
                    "authorId": "2268370058",
                    "name": "Suin Lee"
                },
                {
                    "authorId": "2268310103",
                    "name": "Donggon Jang"
                },
                {
                    "authorId": "2145154407",
                    "name": "Daeshik Kim"
                }
            ],
            "abstract": null,
            "corpus_id": 267657497,
            "sentences": [
                {
                    "corpus_id": "267657497",
                    "title": "Maximizing discrimination capability of knowledge distillation with energy function",
                    "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
                    "score": 0.5517006978554995,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 3764,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 473
                        },
                        {
                            "start": 476,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 2234
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 366,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 366,
                            "end": 369,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 439,
                            "end": 442,
                            "matchedPaperCorpusId": "219169868"
                        },
                        {
                            "start": 442,
                            "end": 445,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 451,
                            "end": 454,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 454,
                            "end": 457,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 457,
                            "end": 460,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 460,
                            "end": 463,
                            "matchedPaperCorpusId": "258298441"
                        },
                        {
                            "start": 463,
                            "end": 466,
                            "matchedPaperCorpusId": "258309453"
                        },
                        {
                            "start": 466,
                            "end": 469,
                            "matchedPaperCorpusId": "269167845"
                        },
                        {
                            "start": 469,
                            "end": 472,
                            "matchedPaperCorpusId": "269206209"
                        },
                        {
                            "start": 821,
                            "end": 825,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1058,
                            "end": 1062,
                            "matchedPaperCorpusId": "260933721"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68896484375
                }
            ],
            "relevance_judgement": 0.68896484375,
            "relevance_judgment_input_expanded": "# Title: Maximizing discrimination capability of knowledge distillation with energy function\n# Venue: Knowledge-Based Systems\n# Authors: Seonghak Kim, Gyeongdo Ham, Suin Lee, Donggon Jang, Daeshik Kim\n## Abstract\nNone\n## Knowledge Distillation\nKnowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "reference_string": "[267657497 | Kim et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Contrastive Representation Distillation via Multi-Scale Feature Decoupling",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.05835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344789398",
                    "name": "Cuipeng Wang"
                },
                {
                    "authorId": "2344967066",
                    "name": "Tieyuan Chen"
                },
                {
                    "authorId": "2344788684",
                    "name": "Haipeng Wang"
                }
            ],
            "abstract": "Knowledge distillation is a technique aimed at enhancing the performance of a small student network without increasing its parameter size by transferring knowledge from a large, pre-trained teacher network. In the feature space, different local regions within an individual global feature map often encode distinct yet interdependent semantic information. However, previous methods mainly focus on transferring global feature knowledge, neglecting the decoupling of interdependent local regions within an individual global feature, which often results in suboptimal performance. To address this limitation, we propose MSDCRD, a novel contrastive representation distillation approach that explicitly performs multi-scale decoupling within the feature space. MSDCRD employs a multi-scale sliding-window pooling approach within the feature space to capture representations at various granularities effectively. This, in conjunction with sample categorization, facilitates efficient multi-scale feature decoupling. When integrated with a novel and effective contrastive loss function, this forms the core of MSDCRD. Feature representations differ significantly across network architectures, and this divergence becomes more pronounced in heterogeneous models, rendering feature distillation particularly challenging. Despite this, our method not only achieves superior performance in homogeneous models but also enables efficient feature knowledge transfer across a variety of heterogeneous teacher-student pairs, highlighting its strong generalizability. Moreover, its plug-and-play and parameter-free nature enables flexible integration with different visual tasks. Extensive experiments on different visual benchmarks consistently confirm the superiority of our method in enhancing the performance of student models.",
            "corpus_id": 276249293,
            "sentences": [
                {
                    "corpus_id": "276249293",
                    "title": "Contrastive Representation Distillation via Multi-Scale Feature Decoupling",
                    "text": "Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network. \n\nDepending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits (Hinton, 2015;Luo, 2024;Zhao et al., 2022;Sun et al., 2024;Jin et al., 2023;Li et al., 2023), features (Romero et al., 2014;Tian et al., 2019;Chen et al., 2022;2021;Heo et al., 2019;Park et al., 2019;Ahn et al., 2019), and attention (Zagoruyko & Komodakis, 2016;Guo et al., 2023). \n\nMany transferring features methods followed Fit-Net (Romero et al., 2014) by utilizing single-stage features for knowledge distillation. PKT (Passalis et al., 2020) aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. SimKD (Chen et al., 2022) decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. CRD (Tian et al., 2019) combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives. \n\nIn the feature-based distillation methods, many works proposed to utilize multi-level feature distillation. OFD (Heo et al., 2019) enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric. ReviewKD (Chen et al., 2021) improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student. \n\nHowever, previous methods primarily focused on global feature information, without addressing the decoupling of rela-tionships between global and local features.",
                    "score": 0.6117875378820371,
                    "section_title": "Related Work",
                    "char_start_offset": 6016,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 169,
                            "end": 614
                        },
                        {
                            "start": 617,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1267
                        },
                        {
                            "start": 1270,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1623
                        },
                        {
                            "start": 1624,
                            "end": 1816
                        },
                        {
                            "start": 1819,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 358,
                            "end": 376,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 376,
                            "end": 393,
                            "matchedPaperCorpusId": "268247468"
                        },
                        {
                            "start": 393,
                            "end": 410,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 410,
                            "end": 426,
                            "matchedPaperCorpusId": "254069919"
                        },
                        {
                            "start": 476,
                            "end": 494,
                            "matchedPaperCorpusId": "247762862"
                        },
                        {
                            "start": 494,
                            "end": 499,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 499,
                            "end": 516,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 516,
                            "end": 534,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 534,
                            "end": 551,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 596,
                            "end": 613,
                            "matchedPaperCorpusId": "258309453"
                        },
                        {
                            "start": 758,
                            "end": 781,
                            "matchedPaperCorpusId": "219169868"
                        },
                        {
                            "start": 914,
                            "end": 933,
                            "matchedPaperCorpusId": "247762862"
                        },
                        {
                            "start": 1382,
                            "end": 1400,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 1633,
                            "end": 1652,
                            "matchedPaperCorpusId": "233296935"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6796875
                }
            ],
            "relevance_judgement": 0.6796875,
            "relevance_judgment_input_expanded": "# Title: Contrastive Representation Distillation via Multi-Scale Feature Decoupling\n# Venue: arXiv.org\n# Authors: Cuipeng Wang, Tieyuan Chen, Haipeng Wang\n## Abstract\nKnowledge distillation is a technique aimed at enhancing the performance of a small student network without increasing its parameter size by transferring knowledge from a large, pre-trained teacher network. In the feature space, different local regions within an individual global feature map often encode distinct yet interdependent semantic information. However, previous methods mainly focus on transferring global feature knowledge, neglecting the decoupling of interdependent local regions within an individual global feature, which often results in suboptimal performance. To address this limitation, we propose MSDCRD, a novel contrastive representation distillation approach that explicitly performs multi-scale decoupling within the feature space. MSDCRD employs a multi-scale sliding-window pooling approach within the feature space to capture representations at various granularities effectively. This, in conjunction with sample categorization, facilitates efficient multi-scale feature decoupling. When integrated with a novel and effective contrastive loss function, this forms the core of MSDCRD. Feature representations differ significantly across network architectures, and this divergence becomes more pronounced in heterogeneous models, rendering feature distillation particularly challenging. Despite this, our method not only achieves superior performance in homogeneous models but also enables efficient feature knowledge transfer across a variety of heterogeneous teacher-student pairs, highlighting its strong generalizability. Moreover, its plug-and-play and parameter-free nature enables flexible integration with different visual tasks. Extensive experiments on different visual benchmarks consistently confirm the superiority of our method in enhancing the performance of student models.\n## Related Work\nKnowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network. \n\nDepending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits (Hinton, 2015;Luo, 2024;Zhao et al., 2022;Sun et al., 2024;Jin et al., 2023;Li et al., 2023), features (Romero et al., 2014;Tian et al., 2019;Chen et al., 2022;2021;Heo et al., 2019;Park et al., 2019;Ahn et al., 2019), and attention (Zagoruyko & Komodakis, 2016;Guo et al., 2023). \n\nMany transferring features methods followed Fit-Net (Romero et al., 2014) by utilizing single-stage features for knowledge distillation. PKT (Passalis et al., 2020) aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. SimKD (Chen et al., 2022) decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. CRD (Tian et al., 2019) combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives. \n\nIn the feature-based distillation methods, many works proposed to utilize multi-level feature distillation. OFD (Heo et al., 2019) enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric. ReviewKD (Chen et al., 2021) improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student. \n\nHowever, previous methods primarily focused on global feature information, without addressing the decoupling of rela-tionships between global and local features.",
            "reference_string": "[276249293 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Locally Linear Region Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 63,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.04812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150478789",
                    "name": "Xiang Deng"
                },
                {
                    "authorId": "2118748124",
                    "name": "Zhongfei Zhang"
                }
            ],
            "abstract": "Knowledge distillation (KD) is an effective technique to transfer knowledge from one neural network (teacher) to another (student), thus improving the performance of the student. To make the student better mimic the behavior of the teacher, the existing work focuses on designing different criteria to align their logits or representations. Different from these efforts, we address knowledge distillation from a novel data perspective. We argue that transferring knowledge at sparse training data points cannot enable the student to well capture the local shape of the teacher function. To address this issue, we propose locally linear region knowledge distillation ($\\rm L^2$RKD) which transfers the knowledge in local, linear regions from a teacher to a student. This is achieved by enforcing the student to mimic the outputs of the teacher function in local, linear regions. To the end, the student is able to better capture the local shape of the teacher function and thus achieves a better performance. Despite its simplicity, extensive experiments demonstrate that $\\rm L^2$RKD is superior to the original KD in many aspects as it outperforms KD and the other state-of-the-art approaches by a large margin, shows robustness and superiority under few-shot settings, and is more compatible with the existing distillation approaches to further improve their performances significantly.",
            "corpus_id": 224801420,
            "sentences": [
                {
                    "corpus_id": "224801420",
                    "title": "Locally Linear Region Knowledge Distillation",
                    "text": "Logit-based approaches [6] construct the distillation objective based on output logits. Hinton et al. [6] propose KD which uses the softened logits of a teacher network as the targets to train a student network. The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance. Park et al. [11] point out that KD only considers knowledge transfer on individual samples and thus they propose to transfer mutual relations of data examples from a teacher to a student by penalizing logit-based structural differences between them. Zhao et al. [12] consider the information in training process for knowledge distillation by employing two teachers. One teacher uses its temporary output logits during the training process to supervise the student step by step, which assists the student to find the optimal path towards the final logits. The other teacher provides the information about a critical region that is more useful for the task.",
                    "score": 0.5538353148245415,
                    "section_title": "A. Logit-based Distillation Approaches",
                    "char_start_offset": 6947,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1010
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 367,
                            "end": 371,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 617,
                            "end": 621,
                            "matchedPaperCorpusId": "198179767"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67919921875
                }
            ],
            "relevance_judgement": 0.67919921875,
            "relevance_judgment_input_expanded": "# Title: Locally Linear Region Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Xiang Deng, Zhongfei Zhang\n## Abstract\nKnowledge distillation (KD) is an effective technique to transfer knowledge from one neural network (teacher) to another (student), thus improving the performance of the student. To make the student better mimic the behavior of the teacher, the existing work focuses on designing different criteria to align their logits or representations. Different from these efforts, we address knowledge distillation from a novel data perspective. We argue that transferring knowledge at sparse training data points cannot enable the student to well capture the local shape of the teacher function. To address this issue, we propose locally linear region knowledge distillation ($\\rm L^2$RKD) which transfers the knowledge in local, linear regions from a teacher to a student. This is achieved by enforcing the student to mimic the outputs of the teacher function in local, linear regions. To the end, the student is able to better capture the local shape of the teacher function and thus achieves a better performance. Despite its simplicity, extensive experiments demonstrate that $\\rm L^2$RKD is superior to the original KD in many aspects as it outperforms KD and the other state-of-the-art approaches by a large margin, shows robustness and superiority under few-shot settings, and is more compatible with the existing distillation approaches to further improve their performances significantly.\n## A. Logit-based Distillation Approaches\nLogit-based approaches [6] construct the distillation objective based on output logits. Hinton et al. [6] propose KD which uses the softened logits of a teacher network as the targets to train a student network. The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance. Park et al. [11] point out that KD only considers knowledge transfer on individual samples and thus they propose to transfer mutual relations of data examples from a teacher to a student by penalizing logit-based structural differences between them. Zhao et al. [12] consider the information in training process for knowledge distillation by employing two teachers. One teacher uses its temporary output logits during the training process to supervise the student step by step, which assists the student to find the optimal path towards the final logits. The other teacher provides the information about a critical region that is more useful for the task.",
            "reference_string": "[224801420 | Deng et al. | 2020 | Citations: 0]"
        },
        {
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "venue": "Applied intelligence (Boston)",
            "year": 2022,
            "reference_count": 47,
            "citation_count": 44,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10489-022-03486-4.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-022-03486-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-022-03486-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2817613",
                    "name": "Chuanyun Xu"
                },
                {
                    "authorId": "2164051968",
                    "name": "Wenjian Gao"
                },
                {
                    "authorId": "2164318078",
                    "name": "Tian Li"
                },
                {
                    "authorId": "2164821600",
                    "name": "Nanlan Bai"
                },
                {
                    "authorId": "2155121570",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2145954082",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "A single model usually cannot learn all the appropriate features with limited data, thus leading to poor performance when test data are used. To improve model performance, we propose a teacher-student collaborative knowledge distillation (TSKD) method based on knowledge distillation and self-distillation. The method consists of two parts: learning in the teacher network and self-teaching in the student network. Learning in the teacher network allows the student network to use knowledge from the teacher network. Self-teaching in the student network is to build a multi-exit network based on self-distillation and provide deep features as supervised information for training. In the inference stage, we use ensembles to vote on the classification results of multiple sub-models in the student network. The experimental results demonstrate the superior performance of our method compared with a traditional knowledge distillation method and a self-distillation-based multi-exit network.",
            "corpus_id": 248683566,
            "sentences": [
                {
                    "corpus_id": "248683566",
                    "title": "Teacher-student collaborative knowledge distillation for image classification",
                    "text": "Knowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
                    "score": 0.5939056397517228,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 4620,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1930
                        },
                        {
                            "start": 1931,
                            "end": 2178
                        },
                        {
                            "start": 2179,
                            "end": 2311
                        },
                        {
                            "start": 2312,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 70,
                            "end": 73,
                            "matchedPaperCorpusId": "167217261"
                        },
                        {
                            "start": 73,
                            "end": 76,
                            "matchedPaperCorpusId": "32588614"
                        },
                        {
                            "start": 76,
                            "end": 79,
                            "matchedPaperCorpusId": "222310537"
                        },
                        {
                            "start": 260,
                            "end": 263,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 263,
                            "end": 266,
                            "matchedPaperCorpusId": "8451212"
                        },
                        {
                            "start": 266,
                            "end": 269,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 306,
                            "end": 309,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 862,
                            "end": 866,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 866,
                            "end": 870,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 870,
                            "end": 874,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 874,
                            "end": 878,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 878,
                            "end": 882,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 882,
                            "end": 886,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1075,
                            "end": 1079,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1461,
                            "end": 1465,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1765,
                            "end": 1769,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 2091,
                            "end": 2095,
                            "matchedPaperCorpusId": "237734820"
                        },
                        {
                            "start": 2179,
                            "end": 2183,
                            "matchedPaperCorpusId": "225257208"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6728515625
                }
            ],
            "relevance_judgement": 0.6728515625,
            "relevance_judgment_input_expanded": "# Title: Teacher-student collaborative knowledge distillation for image classification\n# Venue: Applied intelligence (Boston)\n# Authors: Chuanyun Xu, Wenjian Gao, Tian Li, Nanlan Bai, Gang Li, Yang Zhang\n## Abstract\nA single model usually cannot learn all the appropriate features with limited data, thus leading to poor performance when test data are used. To improve model performance, we propose a teacher-student collaborative knowledge distillation (TSKD) method based on knowledge distillation and self-distillation. The method consists of two parts: learning in the teacher network and self-teaching in the student network. Learning in the teacher network allows the student network to use knowledge from the teacher network. Self-teaching in the student network is to build a multi-exit network based on self-distillation and provide deep features as supervised information for training. In the inference stage, we use ensembles to vote on the classification results of multiple sub-models in the student network. The experimental results demonstrate the superior performance of our method compared with a traditional knowledge distillation method and a self-distillation-based multi-exit network.\n## Knowledge distillation\nKnowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
            "reference_string": "[248683566 | Xu et al. | 2022 | Citations: 44]"
        },
        {
            "title": "Using Knowledge Distillation to improve interpretable models in a retail banking context",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 31,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.15496",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.15496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186740719",
                    "name": "Maxime Biehler"
                },
                {
                    "authorId": "2186740508",
                    "name": "Mohamed Guermazi"
                },
                {
                    "authorId": "2186740498",
                    "name": "C'elim Starck"
                }
            ],
            "abstract": "This article sets forth a review of knowledge distillation techniques with a focus on their applicability to retail banking contexts. Predictive machine learning algorithms used in banking environments, especially in risk and control functions, are generally subject to regulatory and technical constraints limiting their complexity. Knowledge distillation gives the opportunity to improve the performances of simple models without burdening their application, using the results of other - generally more complex and better-performing - models. Parsing recent advances in this \ufb01eld, we highlight three main approaches: Soft Targets, Sample Selection and Data Augmentation. We assess the relevance of a subset of such techniques by applying them to open source datasets, before putting them to the test on the use cases of BPCE, a major French institution in the retail banking sector. As such, we demonstrate the potential of knowledge distillation to improve the performance of these models without altering their form and simplicity.",
            "corpus_id": 252668749,
            "sentences": [
                {
                    "corpus_id": "252668749",
                    "title": "Using Knowledge Distillation to improve interpretable models in a retail banking context",
                    "text": "Dark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation. While the available body of research for those is thinner, other approaches are thus included in this paper, and will be detailed alongside the seminal technique.",
                    "score": 0.49506449754752213,
                    "section_title": "Multiple flavors of knowledge distillation",
                    "char_start_offset": 4539,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 753
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67138671875
                }
            ],
            "relevance_judgement": 0.67138671875,
            "relevance_judgment_input_expanded": "# Title: Using Knowledge Distillation to improve interpretable models in a retail banking context\n# Venue: arXiv.org\n# Authors: Maxime Biehler, Mohamed Guermazi, C'elim Starck\n## Abstract\nThis article sets forth a review of knowledge distillation techniques with a focus on their applicability to retail banking contexts. Predictive machine learning algorithms used in banking environments, especially in risk and control functions, are generally subject to regulatory and technical constraints limiting their complexity. Knowledge distillation gives the opportunity to improve the performances of simple models without burdening their application, using the results of other - generally more complex and better-performing - models. Parsing recent advances in this \ufb01eld, we highlight three main approaches: Soft Targets, Sample Selection and Data Augmentation. We assess the relevance of a subset of such techniques by applying them to open source datasets, before putting them to the test on the use cases of BPCE, a major French institution in the retail banking sector. As such, we demonstrate the potential of knowledge distillation to improve the performance of these models without altering their form and simplicity.\n## Multiple flavors of knowledge distillation\nDark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation. While the available body of research for those is thinner, other approaches are thus included in this paper, and will be detailed alongside the seminal technique.",
            "reference_string": "[252668749 | Biehler et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Relational Representation Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2196360101",
                    "name": "Nikolaos Giakoumoglou"
                },
                {
                    "authorId": "2292259667",
                    "name": "Tania Stathaki"
                }
            ],
            "abstract": "Knowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.",
            "corpus_id": 271244914,
            "sentences": [
                {
                    "corpus_id": "271244914",
                    "title": "Relational Representation Distillation",
                    "text": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
                    "score": 0.49581009610294546,
                    "section_title": "Related Work",
                    "char_start_offset": 4321,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 722
                        },
                        {
                            "start": 725,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1059
                        },
                        {
                            "start": 1062,
                            "end": 1210
                        },
                        {
                            "start": 1213,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2119
                        },
                        {
                            "start": 2122,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2271
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 686,
                            "end": 690,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 693,
                            "end": 696,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 696,
                            "end": 699,
                            "matchedPaperCorpusId": "829159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66064453125
                }
            ],
            "relevance_judgement": 0.66064453125,
            "relevance_judgment_input_expanded": "# Title: Relational Representation Distillation\n# Venue: arXiv.org\n# Authors: Nikolaos Giakoumoglou, Tania Stathaki\n## Abstract\nKnowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.\n## Related Work\nThe concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
            "reference_string": "[271244914 | Giakoumoglou et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
            "venue": "IEEE Transactions on Communications",
            "year": 2022,
            "reference_count": 63,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.17113",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.17113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2172485573",
                    "name": "Yiming Cui"
                },
                {
                    "authorId": "47093519",
                    "name": "Jiajia Guo"
                },
                {
                    "authorId": "2113999930",
                    "name": "Zheng Cao"
                },
                {
                    "authorId": "120710335",
                    "name": "Huaze Tang"
                },
                {
                    "authorId": "2257212132",
                    "name": "Chao-Kai Wen"
                },
                {
                    "authorId": "2227268421",
                    "name": "Shi Jin"
                },
                {
                    "authorId": "2288090155",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2240356164",
                    "name": "Xiaolin Hou"
                }
            ],
            "abstract": "Deep learning has shown promise in enhancing channel state information (CSI) feedback. However, many studies indicate that better feedback performance often accompanies higher computational complexity. Pursuing better performance-complexity tradeoffs is crucial to facilitate practical deployment, especially on computation-limited devices, which may have to use lightweight autoencoder with unfavorable performance. To achieve this goal, this paper introduces knowledge distillation (KD) to achieve better tradeoffs, where knowledge from a complicated teacher autoencoder is transferred to a lightweight student autoencoder for performance improvement. Specifically, two methods are proposed for implementation. Firstly, an autoencoder KD-based method is introduced by training a student autoencoder to mimic the reconstructed CSI of a pretrained teacher autoencoder. Secondly, an encoder KD-based method is proposed to reduce training overhead by performing KD only on the student encoder. Additionally, a variant of encoder KD is introduced to protect user equipment and base station vendor intellectual property. Numerical simulations demonstrate that the proposed methods can significantly improve the student autoencoder\u2019s performance, while reducing the number of floating point operations and inference time to 3.05%\u20135.28% and 13.80%\u201314.76% of the teacher network, respectively. Furthermore, the variant encoder KD method effectively enhances the student autoencoder\u2019s generalization capability across different scenarios, environments, and bandwidths.",
            "corpus_id": 260447668,
            "sentences": [
                {
                    "corpus_id": "260447668",
                    "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                    "text": "In this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
                    "score": 0.516232880397338,
                    "section_title": "B. Key Idea and Training Framework",
                    "char_start_offset": 18765,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 187
                        },
                        {
                            "start": 190,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1205
                        },
                        {
                            "start": 1208,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1543
                        },
                        {
                            "start": 1546,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1912
                        },
                        {
                            "start": 1913,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2175
                        },
                        {
                            "start": 2178,
                            "end": 2202
                        },
                        {
                            "start": 2203,
                            "end": 2281
                        },
                        {
                            "start": 2282,
                            "end": 2414
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66015625
                }
            ],
            "relevance_judgement": 0.66015625,
            "relevance_judgment_input_expanded": "# Title: Lightweight Neural Network With Knowledge Distillation for CSI Feedback\n# Venue: IEEE Transactions on Communications\n# Authors: Yiming Cui, Jiajia Guo, Zheng Cao, Huaze Tang, Chao-Kai Wen, Shi Jin, Xin Wang, Xiaolin Hou\n## Abstract\nDeep learning has shown promise in enhancing channel state information (CSI) feedback. However, many studies indicate that better feedback performance often accompanies higher computational complexity. Pursuing better performance-complexity tradeoffs is crucial to facilitate practical deployment, especially on computation-limited devices, which may have to use lightweight autoencoder with unfavorable performance. To achieve this goal, this paper introduces knowledge distillation (KD) to achieve better tradeoffs, where knowledge from a complicated teacher autoencoder is transferred to a lightweight student autoencoder for performance improvement. Specifically, two methods are proposed for implementation. Firstly, an autoencoder KD-based method is introduced by training a student autoencoder to mimic the reconstructed CSI of a pretrained teacher autoencoder. Secondly, an encoder KD-based method is proposed to reduce training overhead by performing KD only on the student encoder. Additionally, a variant of encoder KD is introduced to protect user equipment and base station vendor intellectual property. Numerical simulations demonstrate that the proposed methods can significantly improve the student autoencoder\u2019s performance, while reducing the number of floating point operations and inference time to 3.05%\u20135.28% and 13.80%\u201314.76% of the teacher network, respectively. Furthermore, the variant encoder KD method effectively enhances the student autoencoder\u2019s generalization capability across different scenarios, environments, and bandwidths.\n## B. Key Idea and Training Framework\nIn this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
            "reference_string": "[260447668 | Cui et al. | 2022 | Citations: 3]"
        },
        {
            "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i21.34457",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i21.34457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i21.34457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331698741",
                    "name": "Junjie Zhou"
                },
                {
                    "authorId": "2273931950",
                    "name": "Ke Zhu"
                },
                {
                    "authorId": "2274078411",
                    "name": "Jianxin Wu"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.",
            "corpus_id": 277754927,
            "sentences": [
                {
                    "corpus_id": "277754927",
                    "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
                    "text": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.",
                    "score": 0.4902311551688844,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66015625
                }
            ],
            "relevance_judgement": 0.66015625,
            "relevance_judgment_input_expanded": "# Title: All You Need in Knowledge Distillation Is a Tailored Coordinate System\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Junjie Zhou, Ke Zhu, Jianxin Wu\n## Abstract\nKnowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.\n",
            "reference_string": "[277754927 | Zhou et al. | 2025 | Citations: 0]"
        },
        {
            "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.09388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331698741",
                    "name": "Junjie Zhou"
                },
                {
                    "authorId": "2273931950",
                    "name": "Ke Zhu"
                },
                {
                    "authorId": "2274078411",
                    "name": "Jianxin Wu"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, and allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.",
            "corpus_id": 274655700,
            "sentences": [],
            "relevance_judgement": 0.65869140625,
            "relevance_judgment_input_expanded": "# Title: All You Need in Knowledge Distillation Is a Tailored Coordinate System\n# Venue: arXiv.org\n# Authors: Junjie Zhou, Ke Zhu, Jianxin Wu\n## Abstract\nKnowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, and allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.\n",
            "reference_string": "[274655700 | Zhou et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Two-Stage Approach for Targeted Knowledge Transfer in Self-Knowledge Distillation",
            "venue": "IEEE/CAA Journal of Automatica Sinica",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2024.124629?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2024.124629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290035516",
                    "name": "Zimo Yin"
                },
                {
                    "authorId": "2142824615",
                    "name": "Jian Pu"
                },
                {
                    "authorId": "2261082542",
                    "name": "Yijie Zhou"
                },
                {
                    "authorId": "2251995827",
                    "name": "Xiangyang Xue"
                }
            ],
            "abstract": "Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network. To optimize computational expenditure and memory utilization, self-knowledge distillation (SKD) extracts dark knowledge from the model itself rather than an external teacher network. However, previous SKD methods performed distillation indiscriminately on full datasets, overlooking the analysis of representative samples. In this work, we present a novel two-stage approach to providing targeted knowledge on specific samples, named two-stage approach self-knowledge distillation (TOAST). We first soften the hard targets using class medoids generated based on logit vectors per class. Then, we iteratively distill the under-trained data with past predictions of half the batch size. The two-stage knowledge is linearly combined, efficiently enhancing model performance. Extensive experiments conducted on five backbone architectures show our method is model-agnostic and achieves the best generalization performance. Besides, TOAST is strongly compatible with existing augmentation-based regularization methods. Our method also obtains a speedup of up to 2.95x compared with a recent state-of-the-art method.",
            "corpus_id": 273227005,
            "sentences": [],
            "relevance_judgement": 0.650390625,
            "relevance_judgment_input_expanded": "# Title: Two-Stage Approach for Targeted Knowledge Transfer in Self-Knowledge Distillation\n# Venue: IEEE/CAA Journal of Automatica Sinica\n# Authors: Zimo Yin, Jian Pu, Yijie Zhou, Xiangyang Xue\n## Abstract\nKnowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network. To optimize computational expenditure and memory utilization, self-knowledge distillation (SKD) extracts dark knowledge from the model itself rather than an external teacher network. However, previous SKD methods performed distillation indiscriminately on full datasets, overlooking the analysis of representative samples. In this work, we present a novel two-stage approach to providing targeted knowledge on specific samples, named two-stage approach self-knowledge distillation (TOAST). We first soften the hard targets using class medoids generated based on logit vectors per class. Then, we iteratively distill the under-trained data with past predictions of half the batch size. The two-stage knowledge is linearly combined, efficiently enhancing model performance. Extensive experiments conducted on five backbone architectures show our method is model-agnostic and achieves the best generalization performance. Besides, TOAST is strongly compatible with existing augmentation-based regularization methods. Our method also obtains a speedup of up to 2.95x compared with a recent state-of-the-art method.\n",
            "reference_string": "[273227005 | Yin et al. | 2024 | Citations: 0]"
        },
        {
            "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 52,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.16226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281746039",
                    "name": "Kaito Takanami"
                },
                {
                    "authorId": "2342462960",
                    "name": "Takashi Takahashi"
                },
                {
                    "authorId": "2342406436",
                    "name": "Ayaka Sakata"
                }
            ],
            "abstract": "Self-distillation (SD), a technique where a model improves itself using its own predictions, has attracted attention as a simple yet powerful approach in machine learning. Despite its widespread use, the mechanisms underlying its effectiveness remain unclear. In this study, we investigate the efficacy of hyperparameter-tuned multi-stage SD with a linear classifier for binary classification on noisy Gaussian mixture data. For the analysis, we employ the replica method from statistical physics. Our findings reveal that the primary driver of SD's performance improvement is denoising through hard pseudo-labels, with the most notable gains observed in moderately sized datasets. We also identify two practical heuristics to enhance SD: early stopping that limits the number of stages, which is broadly effective, and bias parameter fixing, which helps under label imbalance. To empirically validate our theoretical findings derived from our toy model, we conduct additional experiments on CIFAR-10 classification using pretrained ResNet backbone. These results provide both theoretical and practical insights, advancing our understanding and application of SD in noisy settings.",
            "corpus_id": 275920765,
            "sentences": [
                {
                    "corpus_id": "275920765",
                    "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
                    "text": "Knowledge distillation (KD) Hinton et al. [2015] is a technique in machine learning that transfers the learned information from a complex model (often referred to as the teacher) to a simpler model (the student). This method attracted attention for achieving model compression with minimal performance loss, and has been applied across various domains, including image classification Liu et al. [2018], Xu et al. [2020], object detection Chen et al. [2017], and natural language processing Calderon et al. [2023], Gu et al. [2024]. \n\nAmong the various forms of KD, self-distillation (SD), originally termed born again neural network Furlanello et al. [2018] is particularly intriguing. In SD, the teacher and student models share Preprint. Under review. arXiv:2501.16226v3 [stat.ML] 17 May 2025 identical architectures. This means that SD does not attempt the model compression; rather, it retrains the student model using the teacher's output. SD presents a intriguing paradox: despite training an identical model on the same dataset, the student model can outperform the teacher Furlanello et al. [2018], Hahn and Choi [2019], Clark et al. [2019]. \n\nTwo main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data. The second hypothesis attributes the improvement to a denoising effect Das and Sanghavi [2023], Das et al. [2024] where the teacher model reduces the influence of the incorrect noisy labels in the training data, enabling the student model to learn a more reliable representation of the underlying patterns Pareek et al. [2024]. \n\nAlthough these hypotheses offer plausible explanations, the optimal behavior of SD, achieved through hyperparameter optimization and repeated iterations Pareek et al. [2024], remains poorly understood.",
                    "score": 0.48773099076943,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 531
                        },
                        {
                            "start": 534,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 772
                        },
                        {
                            "start": 773,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1149
                        },
                        {
                            "start": 1152,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1354
                        },
                        {
                            "start": 1357,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1961
                        },
                        {
                            "start": 1964,
                            "end": 2165
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 384,
                            "end": 401,
                            "matchedPaperCorpusId": "52290108"
                        },
                        {
                            "start": 403,
                            "end": 419,
                            "matchedPaperCorpusId": "221559239"
                        },
                        {
                            "start": 438,
                            "end": 456,
                            "matchedPaperCorpusId": "29308926"
                        },
                        {
                            "start": 633,
                            "end": 657,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1081,
                            "end": 1105,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1705,
                            "end": 1728,
                            "matchedPaperCorpusId": "256416199"
                        },
                        {
                            "start": 1940,
                            "end": 1960,
                            "matchedPaperCorpusId": "271039191"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64404296875
                }
            ],
            "relevance_judgement": 0.64404296875,
            "relevance_judgment_input_expanded": "# Title: The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model\n# Venue: arXiv.org\n# Authors: Kaito Takanami, Takashi Takahashi, Ayaka Sakata\n## Abstract\nSelf-distillation (SD), a technique where a model improves itself using its own predictions, has attracted attention as a simple yet powerful approach in machine learning. Despite its widespread use, the mechanisms underlying its effectiveness remain unclear. In this study, we investigate the efficacy of hyperparameter-tuned multi-stage SD with a linear classifier for binary classification on noisy Gaussian mixture data. For the analysis, we employ the replica method from statistical physics. Our findings reveal that the primary driver of SD's performance improvement is denoising through hard pseudo-labels, with the most notable gains observed in moderately sized datasets. We also identify two practical heuristics to enhance SD: early stopping that limits the number of stages, which is broadly effective, and bias parameter fixing, which helps under label imbalance. To empirically validate our theoretical findings derived from our toy model, we conduct additional experiments on CIFAR-10 classification using pretrained ResNet backbone. These results provide both theoretical and practical insights, advancing our understanding and application of SD in noisy settings.\n## Introduction\nKnowledge distillation (KD) Hinton et al. [2015] is a technique in machine learning that transfers the learned information from a complex model (often referred to as the teacher) to a simpler model (the student). This method attracted attention for achieving model compression with minimal performance loss, and has been applied across various domains, including image classification Liu et al. [2018], Xu et al. [2020], object detection Chen et al. [2017], and natural language processing Calderon et al. [2023], Gu et al. [2024]. \n\nAmong the various forms of KD, self-distillation (SD), originally termed born again neural network Furlanello et al. [2018] is particularly intriguing. In SD, the teacher and student models share Preprint. Under review. arXiv:2501.16226v3 [stat.ML] 17 May 2025 identical architectures. This means that SD does not attempt the model compression; rather, it retrains the student model using the teacher's output. SD presents a intriguing paradox: despite training an identical model on the same dataset, the student model can outperform the teacher Furlanello et al. [2018], Hahn and Choi [2019], Clark et al. [2019]. \n\nTwo main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data. The second hypothesis attributes the improvement to a denoising effect Das and Sanghavi [2023], Das et al. [2024] where the teacher model reduces the influence of the incorrect noisy labels in the training data, enabling the student model to learn a more reliable representation of the underlying patterns Pareek et al. [2024]. \n\nAlthough these hypotheses offer plausible explanations, the optimal behavior of SD, achieved through hyperparameter optimization and repeated iterations Pareek et al. [2024], remains poorly understood.",
            "reference_string": "[275920765 | Takanami et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 58,
            "citation_count": 41,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.01255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145496882",
                    "name": "Bin Dong"
                },
                {
                    "authorId": "104162954",
                    "name": "Jikai Hou"
                },
                {
                    "authorId": "48518029",
                    "name": "Yiping Lu"
                },
                {
                    "authorId": "47294286",
                    "name": "Zhihua Zhang"
                }
            ],
            "abstract": "Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is \"early stopping\". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, {Anisotropic Information Retrieval (AIR)}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of $\\ell_2$ distance, while the previous result was on convergence in $0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.",
            "corpus_id": 203642142,
            "sentences": [
                {
                    "corpus_id": "203642142",
                    "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
                    "text": "Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
                    "score": 0.5292363950836432,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1149
                        },
                        {
                            "start": 1152,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1958
                        },
                        {
                            "start": 1961,
                            "end": 2123
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 117,
                            "matchedPaperCorpusId": "1779661"
                        },
                        {
                            "start": 909,
                            "end": 911,
                            "matchedPaperCorpusId": "21596346"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64013671875
                }
            ],
            "relevance_judgement": 0.64013671875,
            "relevance_judgment_input_expanded": "# Title: Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network\n# Venue: arXiv.org\n# Authors: Bin Dong, Jikai Hou, Yiping Lu, Zhihua Zhang\n## Abstract\nDistillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is \"early stopping\". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, {Anisotropic Information Retrieval (AIR)}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of $\\ell_2$ distance, while the previous result was on convergence in $0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.\n## Introduction\nDeep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
            "reference_string": "[203642142 | Dong et al. | 2019 | Citations: 41]"
        },
        {
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 197,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.04268",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.04268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "11577774",
                    "name": "Chengming Hu"
                },
                {
                    "authorId": "1870442533",
                    "name": "Xuan Li"
                },
                {
                    "authorId": "2822590",
                    "name": "Danyang Liu"
                },
                {
                    "authorId": "107747459",
                    "name": "Haolun Wu"
                },
                {
                    "authorId": "2145308240",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2125037263",
                    "name": "Ju Wang"
                },
                {
                    "authorId": "2151061048",
                    "name": "Xue Liu"
                }
            ],
            "abstract": "Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.",
            "corpus_id": 260704230,
            "sentences": [
                {
                    "corpus_id": "260704230",
                    "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
                    "text": "Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks. Moreover, more advanced training schemes and architecture designs are introduced to improve the efficiency of the distillation process. Although several works [193,194,195,196] focus on regression tasks, one promising research direction can be investigated in the theoretical studies of regression-based knowledge learning, such as the representation of dark knowledge on regression problems. The final predictions of teacher networks are represented as knowledge to be transferred to student networks [204,205], and student networks also aim to mimic the extracted feature representations from teacher networks [206,207]. With deeper theoretical studies on regression-based knowledge learning, teacherstudent architectures will be further effectively employed in practical applications.",
                    "score": 0.6464851049463406,
                    "section_title": "Theoretical Understandings of Regression-Based Knowledge Learning",
                    "char_start_offset": 72996,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1004
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 376,
                            "end": 381,
                            "matchedPaperCorpusId": "211572450"
                        },
                        {
                            "start": 381,
                            "end": 385,
                            "matchedPaperCorpusId": "233582109"
                        },
                        {
                            "start": 389,
                            "end": 393,
                            "matchedPaperCorpusId": "243791573"
                        },
                        {
                            "start": 724,
                            "end": 728,
                            "matchedPaperCorpusId": "233582109"
                        },
                        {
                            "start": 829,
                            "end": 834,
                            "matchedPaperCorpusId": "243791573"
                        },
                        {
                            "start": 834,
                            "end": 838,
                            "matchedPaperCorpusId": "199405345"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6376953125
                }
            ],
            "relevance_judgement": 0.6376953125,
            "relevance_judgment_input_expanded": "# Title: Teacher-Student Architecture for Knowledge Distillation: A Survey\n# Venue: arXiv.org\n# Authors: Chengming Hu, Xuan Li, Danyang Liu, Haolun Wu, Xi Chen, Ju Wang, Xue Liu\n## Abstract\nAlthough Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.\n## Theoretical Understandings of Regression-Based Knowledge Learning\nCurrently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks. Moreover, more advanced training schemes and architecture designs are introduced to improve the efficiency of the distillation process. Although several works [193,194,195,196] focus on regression tasks, one promising research direction can be investigated in the theoretical studies of regression-based knowledge learning, such as the representation of dark knowledge on regression problems. The final predictions of teacher networks are represented as knowledge to be transferred to student networks [204,205], and student networks also aim to mimic the extracted feature representations from teacher networks [206,207]. With deeper theoretical studies on regression-based knowledge learning, teacherstudent architectures will be further effectively employed in practical applications.",
            "reference_string": "[260704230 | Hu et al. | 2023 | Citations: 19]"
        },
        {
            "title": "Shared Knowledge Distillation Network for Object Detection",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/13/8/1595/pdf?version=1713782440",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13081595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13081595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158003499",
                    "name": "Zhen Guo"
                },
                {
                    "authorId": "2297246012",
                    "name": "Pengzhou Zhang"
                },
                {
                    "authorId": "2297096848",
                    "name": "Peng Liang"
                }
            ],
            "abstract": "Object detection based on Knowledge Distillation can enhance the capabilities and performance of 5G and 6G networks in various domains, such as autonomous vehicles, smart surveillance, and augmented reality. The integration of object detection with Knowledge Distillation techniques is expected to play a pivotal role in realizing the full potential of these networks. This study presents Shared Knowledge Distillation (Shared-KD) as a solution to overcome optimization challenges caused by disparities in cross-layer features between teacher\u2013student networks. The significant gaps in intermediate-level features between teachers and students present a considerable obstacle to the efficacy of distillation. To tackle this issue, we draw inspiration from collaborative learning in real-world education, where teachers work together to prepare lessons and students engage in peer learning. Building upon this concept, our innovative contributions in model construction are highlighted as follows: (1) A teacher knowledge augmentation module: this module is proposed to combine lower-level teacher features, facilitating the knowledge transfer from the teacher to the student. (2) A student mutual learning module is introduced to enable students to learn from each other, mimicking the peer learning concept in collaborative learning. (3) The Teacher Share Module combines lower-level teacher features: the specific functionality of the teacher knowledge augmentation module is described, which involves combining lower-level teacher features. (4) The multi-step transfer process can be easily optimized due to the minimal gap between the features: the proposed approach breaks down the knowledge transfer process into multiple steps, which can be easily optimized due to the minimal gap between the features involved in each step. Shared-KD uses simple feature losses without additional weights in transformation, resulting in an efficient distillation process that can be easily combined with other methods for further improvement. The effectiveness of our approach is validated through experiments on popular tasks such as object detection and instance segmentation.",
            "corpus_id": 269317596,
            "sentences": [
                {
                    "corpus_id": "269317596",
                    "title": "Shared Knowledge Distillation Network for Object Detection",
                    "text": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
                    "score": 0.5680589431467109,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 11431,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 440
                        },
                        {
                            "start": 443,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 925
                        },
                        {
                            "start": 928,
                            "end": 1032
                        },
                        {
                            "start": 1033,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1181
                        },
                        {
                            "start": 1184,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1916
                        },
                        {
                            "start": 1917,
                            "end": 1966
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63720703125
                }
            ],
            "relevance_judgement": 0.63720703125,
            "relevance_judgment_input_expanded": "# Title: Shared Knowledge Distillation Network for Object Detection\n# Venue: Electronics\n# Authors: Zhen Guo, Pengzhou Zhang, Peng Liang\n## Abstract\nObject detection based on Knowledge Distillation can enhance the capabilities and performance of 5G and 6G networks in various domains, such as autonomous vehicles, smart surveillance, and augmented reality. The integration of object detection with Knowledge Distillation techniques is expected to play a pivotal role in realizing the full potential of these networks. This study presents Shared Knowledge Distillation (Shared-KD) as a solution to overcome optimization challenges caused by disparities in cross-layer features between teacher\u2013student networks. The significant gaps in intermediate-level features between teachers and students present a considerable obstacle to the efficacy of distillation. To tackle this issue, we draw inspiration from collaborative learning in real-world education, where teachers work together to prepare lessons and students engage in peer learning. Building upon this concept, our innovative contributions in model construction are highlighted as follows: (1) A teacher knowledge augmentation module: this module is proposed to combine lower-level teacher features, facilitating the knowledge transfer from the teacher to the student. (2) A student mutual learning module is introduced to enable students to learn from each other, mimicking the peer learning concept in collaborative learning. (3) The Teacher Share Module combines lower-level teacher features: the specific functionality of the teacher knowledge augmentation module is described, which involves combining lower-level teacher features. (4) The multi-step transfer process can be easily optimized due to the minimal gap between the features: the proposed approach breaks down the knowledge transfer process into multiple steps, which can be easily optimized due to the minimal gap between the features involved in each step. Shared-KD uses simple feature losses without additional weights in transformation, resulting in an efficient distillation process that can be easily combined with other methods for further improvement. The effectiveness of our approach is validated through experiments on popular tasks such as object detection and instance segmentation.\n## Knowledge Distillation\nFirstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
            "reference_string": "[269317596 | Guo et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02793, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188978724",
                    "name": "Mingcheng Li"
                },
                {
                    "authorId": "2143920085",
                    "name": "Dingkang Yang"
                },
                {
                    "authorId": "2290474772",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2108655879",
                    "name": "Shunli Wang"
                },
                {
                    "authorId": "2279095231",
                    "name": "Jiawei Chen"
                },
                {
                    "authorId": "2186872968",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2290653228",
                    "name": "Jinjie Wei"
                },
                {
                    "authorId": "2278886236",
                    "name": "Yue Jiang"
                },
                {
                    "authorId": "2303841599",
                    "name": "Qingyao Xu"
                },
                {
                    "authorId": "2298268194",
                    "name": "Xiaolu Hou"
                },
                {
                    "authorId": "2216487730",
                    "name": "Mingyang Sun"
                },
                {
                    "authorId": "2202592845",
                    "name": "Ziyun Qian"
                },
                {
                    "authorId": "2218981837",
                    "name": "Dongliang Kou"
                },
                {
                    "authorId": "2278978362",
                    "name": "Lihua Zhang"
                }
            ],
            "abstract": "Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model's performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.",
            "corpus_id": 273821996,
            "sentences": [
                {
                    "corpus_id": "273821996",
                    "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
                    "text": "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features [13,14,19,33,35,39,45,43,65,73] and distillation from logits [6,10,30,52,75]. Many studies [5,17,38,21,47,51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities. The teacher network typically provides richer and more comprehensive feature representations than the student network. For instance, KD-Net [17] utilizes a teacher network with complete modalities to supervise the unimodal student network at both the feature and logits levels. Despite their promising results, these methods neglect precise supervision of representations, resulting in low-quality knowledge transfer. To this end, we implement hierarchical semantic and distributional alignment of the multi-scale representations of both networks to transfer knowledge effectively.",
                    "score": 0.5383339259766358,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6680,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1165
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 252,
                            "end": 256,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 256,
                            "end": 259,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 259,
                            "end": 262,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 262,
                            "end": 265,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 265,
                            "end": 268,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 271,
                            "end": 274,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 277,
                            "end": 280,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 313,
                            "end": 316,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 316,
                            "end": 319,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 319,
                            "end": 322,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 322,
                            "end": 325,
                            "matchedPaperCorpusId": "54436113"
                        },
                        {
                            "start": 325,
                            "end": 328,
                            "matchedPaperCorpusId": "5299559"
                        },
                        {
                            "start": 343,
                            "end": 346,
                            "matchedPaperCorpusId": "233219684"
                        },
                        {
                            "start": 346,
                            "end": 349,
                            "matchedPaperCorpusId": "221543802"
                        },
                        {
                            "start": 349,
                            "end": 352,
                            "matchedPaperCorpusId": "245445463"
                        },
                        {
                            "start": 355,
                            "end": 358,
                            "matchedPaperCorpusId": "263605398"
                        },
                        {
                            "start": 724,
                            "end": 728,
                            "matchedPaperCorpusId": "221543802"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6357421875
                }
            ],
            "relevance_judgement": 0.6357421875,
            "relevance_judgment_input_expanded": "# Title: Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning\n# Venue: Neural Information Processing Systems\n# Authors: Mingcheng Li, Dingkang Yang, Yang Liu, Shunli Wang, Jiawei Chen, Shuai Wang, Jinjie Wei, Yue Jiang, Qingyao Xu, Xiaolu Hou, Mingyang Sun, Ziyun Qian, Dongliang Kou, Lihua Zhang\n## Abstract\nMultimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model's performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.\n## Knowledge Distillation\nKnowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features [13,14,19,33,35,39,45,43,65,73] and distillation from logits [6,10,30,52,75]. Many studies [5,17,38,21,47,51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities. The teacher network typically provides richer and more comprehensive feature representations than the student network. For instance, KD-Net [17] utilizes a teacher network with complete modalities to supervise the unimodal student network at both the feature and logits levels. Despite their promising results, these methods neglect precise supervision of representations, resulting in low-quality knowledge transfer. To this end, we implement hierarchical semantic and distributional alignment of the multi-scale representations of both networks to transfer knowledge effectively.",
            "reference_string": "[273821996 | Li et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "venue": "Fire",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2571-6255/6/12/446/pdf?version=1700635738",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fire6120446?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fire6120446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268047558",
                    "name": "Jinzhou Xie"
                },
                {
                    "authorId": "2268031861",
                    "name": "Hongmin Zhao"
                }
            ],
            "abstract": "This paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.",
            "corpus_id": 265384964,
            "sentences": [
                {
                    "corpus_id": "265384964",
                    "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                    "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
                    "score": 0.5528205354748499,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 9617,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2255
                        },
                        {
                            "start": 2256,
                            "end": 2403
                        },
                        {
                            "start": 2404,
                            "end": 2463
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.630859375
                }
            ],
            "relevance_judgement": 0.630859375,
            "relevance_judgment_input_expanded": "# Title: Forest Fire Object Detection Analysis Based on Knowledge Distillation\n# Venue: Fire\n# Authors: Jinzhou Xie, Hongmin Zhao\n## Abstract\nThis paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.\n## Knowledge Distillation\nKnowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "reference_string": "[265384964 | Xie et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "reference_count": 70,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2404.16456",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188978724",
                    "name": "Mingcheng Li"
                },
                {
                    "authorId": "2143920085",
                    "name": "Dingkang Yang"
                },
                {
                    "authorId": "2212046663",
                    "name": "Xiao Zhao"
                },
                {
                    "authorId": "2186872968",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2298314652",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2288061973",
                    "name": "Kun Yang"
                },
                {
                    "authorId": "2216487730",
                    "name": "Mingyang Sun"
                },
                {
                    "authorId": "2218981837",
                    "name": "Dongliang Kou"
                },
                {
                    "authorId": "2202592845",
                    "name": "Ziyun Qian"
                },
                {
                    "authorId": "2278978362",
                    "name": "Lihua Zhang"
                }
            ],
            "abstract": "Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. Most MSA efforts are based on the assumption of modality completeness. However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance. To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities. Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics. Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations. Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization. Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines.",
            "corpus_id": 269362788,
            "sentences": [
                {
                    "corpus_id": "269362788",
                    "title": "Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities",
                    "text": "Knowledge distillation utilizes additional supervisory information from the pre-trained teacher's network to assist in the training of the student's network [11].Knowledge distillation methods can be roughly categorized into two types, distillation from intermediate features [15,29,38,61] and responses [4,8,27,48,68].Many studies [13,18,33,40,47] employ knowledge distillation for MSA tasks with missing modalities.The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model.For instance, [13] utilizes the complete-modality teacher network to implement supervision on the unimodal student network at both feature and response levels.Despite promising outcomes, they are subject to several significant limitations: (i) Knowledge transfer is limited to individual samples, overlooking the exploitation of clear correlations among samples and among categories.(ii) Supervision on student networks is coarse-grained and inadequate, without considering the potential alignment of feature distributions.To this end, we propose a correlation-decoupled knowledge distillation framework that facilitates the learning of robust joint representations by refining and transferring the crosssample, cross-category, and cross-target correlations.",
                    "score": 0.5805118672159589,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5209,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 162,
                            "end": 319
                        },
                        {
                            "start": 319,
                            "end": 417
                        },
                        {
                            "start": 417,
                            "end": 586
                        },
                        {
                            "start": 586,
                            "end": 682
                        },
                        {
                            "start": 682,
                            "end": 841
                        },
                        {
                            "start": 841,
                            "end": 1065
                        },
                        {
                            "start": 1065,
                            "end": 1205
                        },
                        {
                            "start": 1205,
                            "end": 1440
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 276,
                            "end": 280,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 280,
                            "end": 283,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 286,
                            "end": 289,
                            "matchedPaperCorpusId": "231855771"
                        },
                        {
                            "start": 304,
                            "end": 307,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 307,
                            "end": 309,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 309,
                            "end": 312,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 312,
                            "end": 315,
                            "matchedPaperCorpusId": "54436113"
                        },
                        {
                            "start": 315,
                            "end": 318,
                            "matchedPaperCorpusId": "5299559"
                        },
                        {
                            "start": 332,
                            "end": 336,
                            "matchedPaperCorpusId": "221543802"
                        },
                        {
                            "start": 339,
                            "end": 342,
                            "matchedPaperCorpusId": "245445463"
                        },
                        {
                            "start": 342,
                            "end": 345,
                            "matchedPaperCorpusId": "263605398"
                        },
                        {
                            "start": 696,
                            "end": 700,
                            "matchedPaperCorpusId": "221543802"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62451171875
                }
            ],
            "relevance_judgement": 0.62451171875,
            "relevance_judgment_input_expanded": "# Title: Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Mingcheng Li, Dingkang Yang, Xiao Zhao, Shuai Wang, Yan Wang, Kun Yang, Mingyang Sun, Dongliang Kou, Ziyun Qian, Lihua Zhang\n## Abstract\nMultimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. Most MSA efforts are based on the assumption of modality completeness. However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance. To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities. Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics. Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations. Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization. Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines.\n## Knowledge Distillation\nKnowledge distillation utilizes additional supervisory information from the pre-trained teacher's network to assist in the training of the student's network [11].Knowledge distillation methods can be roughly categorized into two types, distillation from intermediate features [15,29,38,61] and responses [4,8,27,48,68].Many studies [13,18,33,40,47] employ knowledge distillation for MSA tasks with missing modalities.The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model.For instance, [13] utilizes the complete-modality teacher network to implement supervision on the unimodal student network at both feature and response levels.Despite promising outcomes, they are subject to several significant limitations: (i) Knowledge transfer is limited to individual samples, overlooking the exploitation of clear correlations among samples and among categories.(ii) Supervision on student networks is coarse-grained and inadequate, without considering the potential alignment of feature distributions.To this end, we propose a correlation-decoupled knowledge distillation framework that facilitates the learning of robust joint representations by refining and transferring the crosssample, cross-category, and cross-target correlations.",
            "reference_string": "[269362788 | Li et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
            "venue": "International Conference on Machine Learning",
            "year": 2019,
            "reference_count": 28,
            "citation_count": 245,
            "influential_citation_count": 28,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.08114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143747407",
                    "name": "Gaurav Kumar Nayak"
                },
                {
                    "authorId": "2217000",
                    "name": "Konda Reddy Mopuri"
                },
                {
                    "authorId": "7155274",
                    "name": "Vaisakh Shaj"
                },
                {
                    "authorId": "144682140",
                    "name": "R. Venkatesh Babu"
                },
                {
                    "authorId": "1429640900",
                    "name": "Anirban Chakraborty"
                }
            ],
            "abstract": "Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method \"Zero-Shot Knowledge Distillation\" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.",
            "corpus_id": 159041346,
            "sentences": [
                {
                    "corpus_id": "159041346",
                    "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                    "text": "Knowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
                    "score": 0.5097244147033593,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1731
                        },
                        {
                            "start": 1734,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2173
                        },
                        {
                            "start": 2174,
                            "end": 2272
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 297,
                            "end": 322,
                            "matchedPaperCorpusId": "6844431"
                        },
                        {
                            "start": 334,
                            "end": 357,
                            "matchedPaperCorpusId": "5808102"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6123046875
                }
            ],
            "relevance_judgement": 0.6123046875,
            "relevance_judgment_input_expanded": "# Title: Zero-Shot Knowledge Distillation in Deep Networks\n# Venue: International Conference on Machine Learning\n# Authors: Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, R. Venkatesh Babu, Anirban Chakraborty\n## Abstract\nKnowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method \"Zero-Shot Knowledge Distillation\" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.\n## Introduction\nKnowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
            "reference_string": "[159041346 | Nayak et al. | 2019 | Citations: 245]"
        },
        {
            "title": "Improving Knowledge Distillation via Head and Tail Categories",
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "year": 2024,
            "reference_count": 71,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325814?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215812572",
                    "name": "Liuchi Xu"
                },
                {
                    "authorId": "2157467240",
                    "name": "Jin Ren"
                },
                {
                    "authorId": "2151325820",
                    "name": "Zhenhua Huang"
                },
                {
                    "authorId": "2265252477",
                    "name": "Weishi Zheng"
                },
                {
                    "authorId": "2261262093",
                    "name": "Yunwen Chen"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a technique that transfers \u201cdark knowledge\u201d from a deep teacher network (teacher) to a shallow student network (student). Despite significant advances in KD, existing work has not adequately mined two crucial types of knowledge: 1) the knowledge of head categories, which represents the relationship between the target category and its similar categories. Our findings reveal that this highly similar (complex) knowledge is essential for improving student\u2019s performance; and 2) the effectively utilized knowledge of tail categories. Existing studies often treat the non-target categories collectively without sufficiently considering the effectiveness of knowledge from tail categories. To tackle these challenges, we reformulate classical KD (ReKD) into two components: Top- $K$ Inter-class Similar Distillation (TISD) and Non-Top- $K$ Inter-class Discriminability (NTID). Firstly, TISD captures and imparts the knowledge of head categories to the student. Our experimental results have verified that TISD is particularly effective in transferring the knowledge of head categories, even in fine-grained dataset classification. Secondly, we theoretically show that the weighting coefficient of NTID increases with the probability of Top- $K$ , leading to stronger suppression of knowledge transfer for tail categories. This observation explains why difficult samples are more informative than simple ones. To better utilize both types of knowledge, we optimize both TISD and NTID using different weighting coefficients, thereby enhancing the student\u2019s ability to learn this valuable knowledge from both head and tail categories. Furthermore, our extensive experimental results demonstrate that ReKD achieves state-of-the-art performance on various image classification datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K, as well as object detection and instance segmentation using the MS-COCO dataset.",
            "corpus_id": 264372297,
            "sentences": [],
            "relevance_judgement": 0.61083984375,
            "relevance_judgment_input_expanded": "# Title: Improving Knowledge Distillation via Head and Tail Categories\n# Venue: IEEE transactions on circuits and systems for video technology (Print)\n# Authors: Liuchi Xu, Jin Ren, Zhenhua Huang, Weishi Zheng, Yunwen Chen\n## Abstract\nKnowledge distillation (KD) is a technique that transfers \u201cdark knowledge\u201d from a deep teacher network (teacher) to a shallow student network (student). Despite significant advances in KD, existing work has not adequately mined two crucial types of knowledge: 1) the knowledge of head categories, which represents the relationship between the target category and its similar categories. Our findings reveal that this highly similar (complex) knowledge is essential for improving student\u2019s performance; and 2) the effectively utilized knowledge of tail categories. Existing studies often treat the non-target categories collectively without sufficiently considering the effectiveness of knowledge from tail categories. To tackle these challenges, we reformulate classical KD (ReKD) into two components: Top- $K$ Inter-class Similar Distillation (TISD) and Non-Top- $K$ Inter-class Discriminability (NTID). Firstly, TISD captures and imparts the knowledge of head categories to the student. Our experimental results have verified that TISD is particularly effective in transferring the knowledge of head categories, even in fine-grained dataset classification. Secondly, we theoretically show that the weighting coefficient of NTID increases with the probability of Top- $K$ , leading to stronger suppression of knowledge transfer for tail categories. This observation explains why difficult samples are more informative than simple ones. To better utilize both types of knowledge, we optimize both TISD and NTID using different weighting coefficients, thereby enhancing the student\u2019s ability to learn this valuable knowledge from both head and tail categories. Furthermore, our extensive experimental results demonstrate that ReKD achieves state-of-the-art performance on various image classification datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K, as well as object detection and instance segmentation using the MS-COCO dataset.\n",
            "reference_string": "[264372297 | Xu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
            "venue": "Inf.",
            "year": 2021,
            "reference_count": 25,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/12/7/264/pdf?version=1626255483",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/info12070264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/info12070264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109643902",
                    "name": "Jinghan Wang"
                },
                {
                    "authorId": "2151302904",
                    "name": "Guangyue Li"
                },
                {
                    "authorId": "2107940856",
                    "name": "Wenzhao Zhang"
                }
            ],
            "abstract": "The powerful performance of deep learning is evident to all. With the deepening of research, neural networks have become more complex and not easily generalized to resource-constrained devices. The emergence of a series of model compression algorithms makes artificial intelligence on edge possible. Among them, structured model pruning is widely utilized because of its versatility. Structured pruning prunes the neural network itself and discards some relatively unimportant structures to compress the model\u2019s size. However, in the previous pruning work, problems such as evaluation errors of networks, empirical determination of pruning rate, and low retraining efficiency remain. Therefore, we propose an accurate, objective, and efficient pruning algorithm\u2014Combine-Net, introducing Adaptive BN to eliminate evaluation errors, the Kneedle algorithm to determine the pruning rate objectively, and knowledge distillation to improve the efficiency of retraining. Results show that, without precision loss, Combine-Net achieves 95% parameter compression and 83% computation compression on VGG16 on CIFAR10, 71% of parameter compression and 41% computation compression on ResNet50 on CIFAR100. Experiments on different datasets and models have proved that Combine-Net can efficiently compress the neural network\u2019s parameters and computation.",
            "corpus_id": 236984375,
            "sentences": [
                {
                    "corpus_id": "236984375",
                    "title": "Combine-Net: An Improved Filter Pruning Algorithm",
                    "text": "Knowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
                    "score": 0.5855412793615322,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 15408,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 71,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1159
                        },
                        {
                            "start": 1162,
                            "end": 1479
                        },
                        {
                            "start": 1480,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1703
                        },
                        {
                            "start": 1704,
                            "end": 1852
                        },
                        {
                            "start": 1855,
                            "end": 2017
                        },
                        {
                            "start": 2020,
                            "end": 2167
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 66,
                            "end": 69,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60888671875
                }
            ],
            "relevance_judgement": 0.60888671875,
            "relevance_judgment_input_expanded": "# Title: Combine-Net: An Improved Filter Pruning Algorithm\n# Venue: Inf.\n# Authors: Jinghan Wang, Guangyue Li, Wenzhao Zhang\n## Abstract\nThe powerful performance of deep learning is evident to all. With the deepening of research, neural networks have become more complex and not easily generalized to resource-constrained devices. The emergence of a series of model compression algorithms makes artificial intelligence on edge possible. Among them, structured model pruning is widely utilized because of its versatility. Structured pruning prunes the neural network itself and discards some relatively unimportant structures to compress the model\u2019s size. However, in the previous pruning work, problems such as evaluation errors of networks, empirical determination of pruning rate, and low retraining efficiency remain. Therefore, we propose an accurate, objective, and efficient pruning algorithm\u2014Combine-Net, introducing Adaptive BN to eliminate evaluation errors, the Kneedle algorithm to determine the pruning rate objectively, and knowledge distillation to improve the efficiency of retraining. Results show that, without precision loss, Combine-Net achieves 95% parameter compression and 83% computation compression on VGG16 on CIFAR10, 71% of parameter compression and 41% computation compression on ResNet50 on CIFAR100. Experiments on different datasets and models have proved that Combine-Net can efficiently compress the neural network\u2019s parameters and computation.\n## Knowledge Distillation\nKnowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
            "reference_string": "[236984375 | Wang et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "venue": "Neural Information Processing Systems",
            "year": 2021,
            "reference_count": 44,
            "citation_count": 101,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.07650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2122903402",
                    "name": "D. Park"
                },
                {
                    "authorId": "9959922",
                    "name": "Moonsu Cha"
                },
                {
                    "authorId": "48366572",
                    "name": "C. Jeong"
                },
                {
                    "authorId": "122204255",
                    "name": "Daesin Kim"
                },
                {
                    "authorId": "40030651",
                    "name": "Bohyung Han"
                }
            ],
            "abstract": "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.",
            "corpus_id": 231925118,
            "sentences": [
                {
                    "corpus_id": "231925118",
                    "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
                    "text": "Since Hinton et al. [1] introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students. FitNets [18] distills intermediate features of a teacher network, where the student network transforms the intermediate features using guided layers and then calculates the difference between the guided layers and the intermediate features of teacher network. The position of distillation is shifted to the layers before the ReLU operations in [19], which also proposes the novel activation function and the partial L 2 loss function for effective knowledge transfer. Zagoruyko and Komodakis [20] argue importance of attention and propose an attention transfer (AT) method from teachers to students while Kim et al. [21] compute the factor information of the teacher representations using an autoencoder, which is decoded by students for knowledge transfer. Relational knowledge distillation (RKD) [22] introduces a technique to transfer relational information such as distances and angles of features. \n\nCRD [23] maximizes mutual information between a teacher and a student via contrastive learning. \n\nThere exist a couple of methods to perform knowledge distillation without teacher models. For example, ONE [24] distills knowledge from an ensemble of multiple students while BYOT [25] transfers knowledge from deeper layers to shallower ones. Besides, SSKD [26] distills self-supervised features of teachers to students for transferring richer knowledge.",
                    "score": 0.5961504429209953,
                    "section_title": "What to distill",
                    "char_start_offset": 4237,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1220
                        },
                        {
                            "start": 1223,
                            "end": 1318
                        },
                        {
                            "start": 1321,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1675
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 23,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 662,
                            "end": 666,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 1116,
                            "end": 1120,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1227,
                            "end": 1231,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1428,
                            "end": 1432,
                            "matchedPaperCorpusId": "48352434"
                        },
                        {
                            "start": 1501,
                            "end": 1505,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1578,
                            "end": 1582,
                            "matchedPaperCorpusId": "219636179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59912109375
                }
            ],
            "relevance_judgement": 0.59912109375,
            "relevance_judgment_input_expanded": "# Title: Learning Student-Friendly Teacher Networks for Knowledge Distillation\n# Venue: Neural Information Processing Systems\n# Authors: D. Park, Moonsu Cha, C. Jeong, Daesin Kim, Bohyung Han\n## Abstract\nWe propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.\n## What to distill\nSince Hinton et al. [1] introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students. FitNets [18] distills intermediate features of a teacher network, where the student network transforms the intermediate features using guided layers and then calculates the difference between the guided layers and the intermediate features of teacher network. The position of distillation is shifted to the layers before the ReLU operations in [19], which also proposes the novel activation function and the partial L 2 loss function for effective knowledge transfer. Zagoruyko and Komodakis [20] argue importance of attention and propose an attention transfer (AT) method from teachers to students while Kim et al. [21] compute the factor information of the teacher representations using an autoencoder, which is decoded by students for knowledge transfer. Relational knowledge distillation (RKD) [22] introduces a technique to transfer relational information such as distances and angles of features. \n\nCRD [23] maximizes mutual information between a teacher and a student via contrastive learning. \n\nThere exist a couple of methods to perform knowledge distillation without teacher models. For example, ONE [24] distills knowledge from an ensemble of multiple students while BYOT [25] transfers knowledge from deeper layers to shallower ones. Besides, SSKD [26] distills self-supervised features of teachers to students for transferring richer knowledge.",
            "reference_string": "[231925118 | Park et al. | 2021 | Citations: 101]"
        },
        {
            "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
            "venue": "",
            "year": 2020,
            "reference_count": 46,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.00573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064424445",
                    "name": "Fei Ding"
                },
                {
                    "authorId": "2257087597",
                    "name": "Yin Yang"
                },
                {
                    "authorId": "2256778126",
                    "name": "Hongxin Hu"
                },
                {
                    "authorId": "2095713717",
                    "name": "V. Krovi"
                },
                {
                    "authorId": "2140495064",
                    "name": "Feng Luo"
                }
            ],
            "abstract": "Knowledge distillation (KD) has become an important technique for model compression and knowledge transfer. In this work, we first perform a comprehensive analysis of the knowledge transferred by different KD methods. We demonstrate that traditional KD methods, which minimize the KL divergence of softmax outputs between networks, are related to the knowledge alignment of an individual sample only. Meanwhile, recent contrastive learning-based KD methods mainly transfer relational knowledge between different samples, namely, knowledge correlation. While it is important to transfer the full knowledge from teacher to student, we introduce the Multi-level Knowledge Distillation (MLKD) by effectively considering both knowledge alignment and correlation. MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge from supervised or self-supervised pretrained teachers. We show that MLKD can improve the reliability and transferability of learned representations. Experiments demonstrate that MLKD outperforms other state-of-the-art methods on a large number of experimental settings including different (a) pretraining strategies (b) network architectures (c) datasets (d) tasks.",
            "corpus_id": 263789814,
            "sentences": [
                {
                    "corpus_id": "263789814",
                    "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
                    "text": "Knowledge Distillation. Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals. SRRL [41] performs knowledge distillation by leveraging the teacher's projection matrix to train the student's representation via L2 loss. However, these works rely on a supervised pretrained teacher (with logits), and they may be not suitable for self-supervised pretrained teachers. SSKD [40] is proposed to combine the self-supervised auxiliary task and KD to transfer richer dark knowledge, but it cannot be trained in an end-to-end training way. Similar to logits matching, intermediate representation [32,43,42,36,22] are widely used for KD. FitNet [32] proposes to match the whole feature maps, which is difficult and may affect the convergence of the student in some cases. Attention transfer [43] utilizes spatial attention maps as the supervisory signal. AB [22] proposes to learn the activation boundaries of the hidden neurons in the teacher. SP [36] focuses on transferring the similar (dissimilar) activations between the teacher and student. However, most of these works depend on certain architectures, such as convolutional networks. Since these distillation methods involve knowledge matching in an individual sample, they are related to knowledge alignment. Our work also includes the knowledge alignment objective, and it doesn't rely on pretraining strategies or network architectures. \n\nKnowledge distillation and self-supervised learning. Self-supervised learning [30,2,8,20,6] focuses on learning low-dimensional representations by the instance discrimination, which usually requires a large number of negative samples. Recently, BYOL [18] and DINO [7] utilize the momentum encoder to avoid collapse without negatives. The momentum encoder can be considered as the mean teacher [34], which is built dynamically during the student training. For KD, the teacher is pretrained and fixed during distillation. Although different views (augmented images) are passed through networks in self-supervised learning, they are from the same original sample, and have the same semantic meaning.",
                    "score": 0.5114456651582487,
                    "section_title": "Related Work",
                    "char_start_offset": 4102,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 23
                        },
                        {
                            "start": 24,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1533
                        },
                        {
                            "start": 1536,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2055
                        },
                        {
                            "start": 2056,
                            "end": 2232
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 236,
                            "matchedPaperCorpusId": "235613564"
                        },
                        {
                            "start": 741,
                            "end": 744,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 744,
                            "end": 747,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 747,
                            "end": 750,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 995,
                            "end": 999,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1085,
                            "end": 1089,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1622,
                            "end": 1625,
                            "matchedPaperCorpusId": "207930212"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58935546875
                }
            ],
            "relevance_judgement": 0.58935546875,
            "relevance_judgment_input_expanded": "# Title: Multi-level Knowledge Distillation via Knowledge Alignment and Correlation\n# Venue: \n# Authors: Fei Ding, Yin Yang, Hongxin Hu, V. Krovi, Feng Luo\n## Abstract\nKnowledge distillation (KD) has become an important technique for model compression and knowledge transfer. In this work, we first perform a comprehensive analysis of the knowledge transferred by different KD methods. We demonstrate that traditional KD methods, which minimize the KL divergence of softmax outputs between networks, are related to the knowledge alignment of an individual sample only. Meanwhile, recent contrastive learning-based KD methods mainly transfer relational knowledge between different samples, namely, knowledge correlation. While it is important to transfer the full knowledge from teacher to student, we introduce the Multi-level Knowledge Distillation (MLKD) by effectively considering both knowledge alignment and correlation. MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge from supervised or self-supervised pretrained teachers. We show that MLKD can improve the reliability and transferability of learned representations. Experiments demonstrate that MLKD outperforms other state-of-the-art methods on a large number of experimental settings including different (a) pretraining strategies (b) network architectures (c) datasets (d) tasks.\n## Related Work\nKnowledge Distillation. Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals. SRRL [41] performs knowledge distillation by leveraging the teacher's projection matrix to train the student's representation via L2 loss. However, these works rely on a supervised pretrained teacher (with logits), and they may be not suitable for self-supervised pretrained teachers. SSKD [40] is proposed to combine the self-supervised auxiliary task and KD to transfer richer dark knowledge, but it cannot be trained in an end-to-end training way. Similar to logits matching, intermediate representation [32,43,42,36,22] are widely used for KD. FitNet [32] proposes to match the whole feature maps, which is difficult and may affect the convergence of the student in some cases. Attention transfer [43] utilizes spatial attention maps as the supervisory signal. AB [22] proposes to learn the activation boundaries of the hidden neurons in the teacher. SP [36] focuses on transferring the similar (dissimilar) activations between the teacher and student. However, most of these works depend on certain architectures, such as convolutional networks. Since these distillation methods involve knowledge matching in an individual sample, they are related to knowledge alignment. Our work also includes the knowledge alignment objective, and it doesn't rely on pretraining strategies or network architectures. \n\nKnowledge distillation and self-supervised learning. Self-supervised learning [30,2,8,20,6] focuses on learning low-dimensional representations by the instance discrimination, which usually requires a large number of negative samples. Recently, BYOL [18] and DINO [7] utilize the momentum encoder to avoid collapse without negatives. The momentum encoder can be considered as the mean teacher [34], which is built dynamically during the student training. For KD, the teacher is pretrained and fixed during distillation. Although different views (augmented images) are passed through networks in self-supervised learning, they are from the same original sample, and have the same semantic meaning.",
            "reference_string": "[263789814 | Ding et al. | 2020 | Citations: 4]"
        },
        {
            "title": "Dual discriminator adversarial distillation for data-free model compression",
            "venue": "International Journal of Machine Learning and Cybernetics",
            "year": 2021,
            "reference_count": 72,
            "citation_count": 20,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.05382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "2185595070",
                    "name": "Milos Manic"
                },
                {
                    "authorId": "46544755",
                    "name": "Huiyu Zhou"
                },
                {
                    "authorId": "145429878",
                    "name": "Hui Yu"
                }
            ],
            "abstract": "Knowledge distillation has been widely used to produce portable and efficient neural networks which can be well applied on edge devices for computer vision tasks. However, almost all top-performing knowledge distillation methods need to access the original training data, which usually has a huge size and is often unavailable. To tackle this problem, we propose a novel data-free approach in this paper, named Dual Discriminator Adversarial Distillation (DDAD) to distill a neural network without the need of any training data or meta-data. To be specific, we use a generator to create samples through dual discriminator adversarial distillation, which mimics the original training data. The generator not only uses the pre-trained teacher\u2019s intrinsic statistics in existing batch normalization layers but also obtains the maximum discrepancy from the student model. Then the generated samples are used to train the compact student network under the supervision of the teacher. The proposed method obtains an efficient student network which closely approximates its teacher network, without using the original training data. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach on CIFAR, Caltech101 and ImageNet datasets for classification tasks. Moreover, we extend our method to semantic segmentation tasks on several public datasets such as CamVid, NYUv2, Cityscapes and VOC 2012. To the best of our knowledge, this is the first work on generative model based data-free knowledge distillation on large-scale datasets such as ImageNet, Cityscapes and VOC 2012. Experiments show that our method outperforms all baselines for data-free knowledge distillation.",
            "corpus_id": 233210081,
            "sentences": [
                {
                    "corpus_id": "233210081",
                    "title": "Dual discriminator adversarial distillation for data-free model compression",
                    "text": "Caruana et al. [41] first confirm that one ensemble of networks could transfer the knowledge to the single network. Then Ba et al. [42] propose to teach the student network by penalizing the difference of logits between the teacher and student. Later, the concept of Knowledge Distillation (KD) is introduced by Hinton et al. [24] to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher. Note that, KD requires original training data to capture the valuable knowledge from the teacher network. However, the knowledge contained in the soft-labels is insufficient when the teacher network goes deeper. To tackle this issue, some improvements have been made, which extend KD by utilizing the intermediate representation as supervision. For example, Fitnets [27] forces the student to learn the similar intermediate features as teacher's which are defined as hints. Zagoruyko et al. [28] define the attention maps from the intermediate features as the knowledge and then obtain a better performance compared to the one using original feature itself. Moreover, FSP [29] designs the flow distillation loss to force the student to mimic flow matrices of teacher among the feature maps between two layers. RKD [30] transfers mutual relations of data examples by the distance-wise and angle-wise distillation losses. SP [31] preserves the pairwise similarities in student's representation space instead to mimic the representation space of the teacher. CTKD [43] combines the knowledge from different teacher models to improve the student's performance in KD. Due to excellent performance, knowledge distillation has been used to solve a variety of complex applications such as object detection [26,44], semantic segmentation [45], lane detection [46], face recognition [47][48][49] and action recognition [50]. Nevertheless, the above traditional data-driven knowledge distillation methods need full of original training data, which are difficult to be obtained in real world. Thus, several few-shot knowledge distillation approaches are proposed to di",
                    "score": 0.5180468187640946,
                    "section_title": "Data-driven knowledge distillation",
                    "char_start_offset": 7759,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 15,
                            "end": 19,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 131,
                            "end": 135,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 326,
                            "end": 330,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1052,
                            "end": 1056,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1177,
                            "end": 1181,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1358,
                            "end": 1362,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1500,
                            "end": 1504,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1609,
                            "end": 1613,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1747,
                            "end": 1751,
                            "matchedPaperCorpusId": "198179767"
                        },
                        {
                            "start": 1988,
                            "end": 1991,
                            "matchedPaperCorpusId": "201666186"
                        },
                        {
                            "start": 2015,
                            "end": 2019,
                            "matchedPaperCorpusId": "73729180"
                        },
                        {
                            "start": 2036,
                            "end": 2040,
                            "matchedPaperCorpusId": "199405591"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57421875
                }
            ],
            "relevance_judgement": 0.57421875,
            "relevance_judgment_input_expanded": "# Title: Dual discriminator adversarial distillation for data-free model compression\n# Venue: International Journal of Machine Learning and Cybernetics\n# Authors: Haoran Zhao, Xin Sun, Junyu Dong, Milos Manic, Huiyu Zhou, Hui Yu\n## Abstract\nKnowledge distillation has been widely used to produce portable and efficient neural networks which can be well applied on edge devices for computer vision tasks. However, almost all top-performing knowledge distillation methods need to access the original training data, which usually has a huge size and is often unavailable. To tackle this problem, we propose a novel data-free approach in this paper, named Dual Discriminator Adversarial Distillation (DDAD) to distill a neural network without the need of any training data or meta-data. To be specific, we use a generator to create samples through dual discriminator adversarial distillation, which mimics the original training data. The generator not only uses the pre-trained teacher\u2019s intrinsic statistics in existing batch normalization layers but also obtains the maximum discrepancy from the student model. Then the generated samples are used to train the compact student network under the supervision of the teacher. The proposed method obtains an efficient student network which closely approximates its teacher network, without using the original training data. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach on CIFAR, Caltech101 and ImageNet datasets for classification tasks. Moreover, we extend our method to semantic segmentation tasks on several public datasets such as CamVid, NYUv2, Cityscapes and VOC 2012. To the best of our knowledge, this is the first work on generative model based data-free knowledge distillation on large-scale datasets such as ImageNet, Cityscapes and VOC 2012. Experiments show that our method outperforms all baselines for data-free knowledge distillation.\n## Data-driven knowledge distillation\nCaruana et al. [41] first confirm that one ensemble of networks could transfer the knowledge to the single network. Then Ba et al. [42] propose to teach the student network by penalizing the difference of logits between the teacher and student. Later, the concept of Knowledge Distillation (KD) is introduced by Hinton et al. [24] to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher. Note that, KD requires original training data to capture the valuable knowledge from the teacher network. However, the knowledge contained in the soft-labels is insufficient when the teacher network goes deeper. To tackle this issue, some improvements have been made, which extend KD by utilizing the intermediate representation as supervision. For example, Fitnets [27] forces the student to learn the similar intermediate features as teacher's which are defined as hints. Zagoruyko et al. [28] define the attention maps from the intermediate features as the knowledge and then obtain a better performance compared to the one using original feature itself. Moreover, FSP [29] designs the flow distillation loss to force the student to mimic flow matrices of teacher among the feature maps between two layers. RKD [30] transfers mutual relations of data examples by the distance-wise and angle-wise distillation losses. SP [31] preserves the pairwise similarities in student's representation space instead to mimic the representation space of the teacher. CTKD [43] combines the knowledge from different teacher models to improve the student's performance in KD. Due to excellent performance, knowledge distillation has been used to solve a variety of complex applications such as object detection [26,44], semantic segmentation [45], lane detection [46], face recognition [47][48][49] and action recognition [50]. Nevertheless, the above traditional data-driven knowledge distillation methods need full of original training data, which are difficult to be obtained in real world. Thus, several few-shot knowledge distillation approaches are proposed to di",
            "reference_string": "[233210081 | Zhao et al. | 2021 | Citations: 20]"
        },
        {
            "title": "Attention and feature transfer based knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-023-43986-y.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10603170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243407272",
                    "name": "Guoliang Yang"
                },
                {
                    "authorId": "2220900849",
                    "name": "Shuaiying Yu"
                },
                {
                    "authorId": "2261922204",
                    "name": "Yangyang Sheng"
                },
                {
                    "authorId": "2257352809",
                    "name": "Hao Yang"
                }
            ],
            "abstract": "Existing knowledge distillation (KD) methods are mainly based on features, logic, or attention, where features and logic represent the results of reasoning at different stages of a convolutional neural network, and attention maps symbolize the reasoning process. Because of the continuity of the two in time, transferring only one of them to the student network will lead to unsatisfactory results. We study the knowledge transfer between the teacher-student network to different degrees, revealing the importance of simultaneously transferring knowledge related to the reasoning process and reasoning results to the student network, providing a new perspective for the study of KD. On this basis, we proposed the knowledge distillation method based on attention and feature transfer (AFT-KD). First, we use transformation structures to transform intermediate features into attentional and feature block (AFB) that contain both inference process information and inference outcome information, and force students to learn the knowledge in AFBs. To save computation in the learning process, we use block operations to align the teacher-student network. In addition, in order to balance the attenuation ratio between different losses, we design an adaptive loss function based on the loss optimization rate. Experiments have shown that AFT-KD achieves state-of-the-art performance in multiple benchmark tests.",
            "corpus_id": 264516404,
            "sentences": [
                {
                    "corpus_id": "264516404",
                    "title": "Attention and feature transfer based knowledge distillation",
                    "text": "The concept of knowledge distillation (KD) was proposed by Hinton et al. 21 , which forced the student network to extract knowledge from the soft labels and ground truth labels provided by teachers. In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced. The existing KD methods can be mainly divided into three types: logic-based 20,21,[31][32][33][34] , feature-based 18,[22][23][24][25][26][27][28][29]35 , and attention maps-based 19,30 . \n\nLogic distillation transfers the knowledge implicit in the output logic of the teacher model to the student network. BAN 32 obtained superior performance to the teacher model by directing the same parameterized network as the teacher. DKD 20 reformulates KD loss into target-class knowledge distillation (TCKD) and non-target-class knowledge distillation (NCKD), revealing that KD's coupling formula limits the effectiveness and flexibility of knowledge transfer. CrossKD 34 passes intermediate features of the student network to the teacher's detection head, resulting in cross predictions, which are then forced to mimic the teacher's predictions. In addition, there are several articles on logical distillation methods 21,33,34 . \n\nFeature-based KD methods tend to have better performance, forcing students to extract valid content from intermediate features of the teacher network at the cost of requiring more computation than logical distillation. RKD 25 can transform the relationship of data examples to punish differences in teacher and student relevance, similar to the transfer of sample relevance studies from teacher and student networks 26,27 . PKT 35 models the teacher's knowledge as a probability distribution and uses KL divergence to measure distance. RKD 25 uses multicase relationships to guide students' learning. CRD 22 combines comparative learning with knowledge distillation, and uses comparative objectives to carry out knowledge transfer. ReviewKD 18 uses cross-layer connection paths to integrate the knowledge implied by features at different levels. \n\nKD method based on attention diagram instructs students what information the network should pay attention to in reasoning.",
                    "score": 0.5470801748063799,
                    "section_title": "Related work",
                    "char_start_offset": 2593,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 505
                        },
                        {
                            "start": 508,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1240
                        },
                        {
                            "start": 1243,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2088
                        },
                        {
                            "start": 2091,
                            "end": 2213
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 394,
                            "end": 397,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 400,
                            "end": 404,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 404,
                            "end": 408,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 408,
                            "end": 412,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 433,
                            "end": 436,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 440,
                            "end": 444,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 448,
                            "end": 452,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 452,
                            "end": 456,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 456,
                            "end": 460,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 460,
                            "end": 464,
                            "matchedPaperCorpusId": "232232777"
                        },
                        {
                            "start": 464,
                            "end": 468,
                            "matchedPaperCorpusId": "229220499"
                        },
                        {
                            "start": 498,
                            "end": 501,
                            "matchedPaperCorpusId": "258309453"
                        },
                        {
                            "start": 629,
                            "end": 631,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 747,
                            "end": 749,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1233,
                            "end": 1236,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 1466,
                            "end": 1468,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1659,
                            "end": 1662,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1662,
                            "end": 1664,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1783,
                            "end": 1785,
                            "matchedPaperCorpusId": "131765296"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56689453125
                }
            ],
            "relevance_judgement": 0.56689453125,
            "relevance_judgment_input_expanded": "# Title: Attention and feature transfer based knowledge distillation\n# Venue: Scientific Reports\n# Authors: Guoliang Yang, Shuaiying Yu, Yangyang Sheng, Hao Yang\n## Abstract\nExisting knowledge distillation (KD) methods are mainly based on features, logic, or attention, where features and logic represent the results of reasoning at different stages of a convolutional neural network, and attention maps symbolize the reasoning process. Because of the continuity of the two in time, transferring only one of them to the student network will lead to unsatisfactory results. We study the knowledge transfer between the teacher-student network to different degrees, revealing the importance of simultaneously transferring knowledge related to the reasoning process and reasoning results to the student network, providing a new perspective for the study of KD. On this basis, we proposed the knowledge distillation method based on attention and feature transfer (AFT-KD). First, we use transformation structures to transform intermediate features into attentional and feature block (AFB) that contain both inference process information and inference outcome information, and force students to learn the knowledge in AFBs. To save computation in the learning process, we use block operations to align the teacher-student network. In addition, in order to balance the attenuation ratio between different losses, we design an adaptive loss function based on the loss optimization rate. Experiments have shown that AFT-KD achieves state-of-the-art performance in multiple benchmark tests.\n## Related work\nThe concept of knowledge distillation (KD) was proposed by Hinton et al. 21 , which forced the student network to extract knowledge from the soft labels and ground truth labels provided by teachers. In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced. The existing KD methods can be mainly divided into three types: logic-based 20,21,[31][32][33][34] , feature-based 18,[22][23][24][25][26][27][28][29]35 , and attention maps-based 19,30 . \n\nLogic distillation transfers the knowledge implicit in the output logic of the teacher model to the student network. BAN 32 obtained superior performance to the teacher model by directing the same parameterized network as the teacher. DKD 20 reformulates KD loss into target-class knowledge distillation (TCKD) and non-target-class knowledge distillation (NCKD), revealing that KD's coupling formula limits the effectiveness and flexibility of knowledge transfer. CrossKD 34 passes intermediate features of the student network to the teacher's detection head, resulting in cross predictions, which are then forced to mimic the teacher's predictions. In addition, there are several articles on logical distillation methods 21,33,34 . \n\nFeature-based KD methods tend to have better performance, forcing students to extract valid content from intermediate features of the teacher network at the cost of requiring more computation than logical distillation. RKD 25 can transform the relationship of data examples to punish differences in teacher and student relevance, similar to the transfer of sample relevance studies from teacher and student networks 26,27 . PKT 35 models the teacher's knowledge as a probability distribution and uses KL divergence to measure distance. RKD 25 uses multicase relationships to guide students' learning. CRD 22 combines comparative learning with knowledge distillation, and uses comparative objectives to carry out knowledge transfer. ReviewKD 18 uses cross-layer connection paths to integrate the knowledge implied by features at different levels. \n\nKD method based on attention diagram instructs students what information the network should pay attention to in reasoning.",
            "reference_string": "[264516404 | Yang et al. | 2023 | Citations: 4]"
        },
        {
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "venue": "",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268314644",
                    "name": "Chi-Ping Su"
                },
                {
                    "authorId": "1569686364",
                    "name": "Ching-Hsun Tseng"
                },
                {
                    "authorId": "2338265886",
                    "name": "Bin Pu"
                },
                {
                    "authorId": "2338506240",
                    "name": "Lei Zhao"
                },
                {
                    "authorId": "2328588941",
                    "name": "Zhuangzhuang Chen"
                },
                {
                    "authorId": "2116351339",
                    "name": "Shin-Jye Lee"
                }
            ],
            "abstract": "Knowledge distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-value samples. Extensive experiments across diverse KD frameworks and tasks$\\unicode{x2014}$including image classification, object detection, and large language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code will be publicly available.",
            "corpus_id": 265444951,
            "sentences": [
                {
                    "corpus_id": "265444951",
                    "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                    "text": "As the growing size of state-of-the-art deep learning models in computer vision (CV) [6,7,19,33] and natural language processing (NLP) [1,5], their deployment in resourcelimited settings becomes more challenging. Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form. *Corresponding author: camhero@gmail.com Previous works have introduced various forms of dark knowledge and refined the knowledge transfer process through structural modifications [2,9,17,22,26,35]. The effectiveness of these methods emphasizes the pivotal role dark knowledge plays within the KD framework. While these methods mostly employ a fixed training paradigm, adaptive distillation approaches have brought forth a more dynamic transfer process [3,13,18,20,31,37,38]. These approaches dynamically modulate the knowledge transfer, typically based on the teacher-student performance gap, ensuring a more tailored knowledge transfer. Despite their effectiveness, these methods often come with limitations such as being confined to specific frameworks [3,31,37,38], computationally intensives [18,20], or yielding marginal improvements [13]. Furthermore, we identify these methods may overlook the inherent student's bias toward specific knowledge in KD, leading to imbalanced learning. \n\nWhile KD achieves promising results by using Kullback-Leibler (KL) divergence to align the student's predictions with the teacher's, a notable performance gap between the models remains. We hypothesize that this gap may be attributed to the student's overconfident predictions. To investigate this, we employ entropy, a concept from information theory that quantifies the unpredictability or information of a random variable [28], to measure the confidence of predictions. We then utilize the kernel density estimation (KDE) to visualize and compare the entropy distributions of the teacher and student.",
                    "score": 0.5046336387057245,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1553
                        },
                        {
                            "start": 1556,
                            "end": 1742
                        },
                        {
                            "start": 1743,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 2028
                        },
                        {
                            "start": 2029,
                            "end": 2159
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 85,
                            "end": 88,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 88,
                            "end": 90,
                            "matchedPaperCorpusId": "206594692"
                        },
                        {
                            "start": 90,
                            "end": 93,
                            "matchedPaperCorpusId": "232352874"
                        },
                        {
                            "start": 93,
                            "end": 96,
                            "matchedPaperCorpusId": "229363322"
                        },
                        {
                            "start": 135,
                            "end": 138,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 138,
                            "end": 140,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 743,
                            "end": 746,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 746,
                            "end": 748,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 748,
                            "end": 751,
                            "matchedPaperCorpusId": "258298441"
                        },
                        {
                            "start": 751,
                            "end": 754,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 757,
                            "end": 760,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1016,
                            "end": 1019,
                            "matchedPaperCorpusId": "239024317"
                        },
                        {
                            "start": 1019,
                            "end": 1022,
                            "matchedPaperCorpusId": "254069919"
                        },
                        {
                            "start": 1025,
                            "end": 1028,
                            "matchedPaperCorpusId": "244119770"
                        },
                        {
                            "start": 1031,
                            "end": 1034,
                            "matchedPaperCorpusId": "244680427"
                        },
                        {
                            "start": 1034,
                            "end": 1037,
                            "matchedPaperCorpusId": "249626454"
                        },
                        {
                            "start": 1319,
                            "end": 1322,
                            "matchedPaperCorpusId": "239024317"
                        },
                        {
                            "start": 1325,
                            "end": 1328,
                            "matchedPaperCorpusId": "244680427"
                        },
                        {
                            "start": 1328,
                            "end": 1331,
                            "matchedPaperCorpusId": "249626454"
                        },
                        {
                            "start": 1364,
                            "end": 1367,
                            "matchedPaperCorpusId": "244119770"
                        },
                        {
                            "start": 1403,
                            "end": 1407,
                            "matchedPaperCorpusId": "254069919"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56494140625
                }
            ],
            "relevance_judgement": 0.56494140625,
            "relevance_judgment_input_expanded": "# Title: EA-KD: Entropy-based Adaptive Knowledge Distillation\n# Venue: \n# Authors: Chi-Ping Su, Ching-Hsun Tseng, Bin Pu, Lei Zhao, Zhuangzhuang Chen, Shin-Jye Lee\n## Abstract\nKnowledge distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-value samples. Extensive experiments across diverse KD frameworks and tasks$\\unicode{x2014}$including image classification, object detection, and large language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code will be publicly available.\n## Introduction\nAs the growing size of state-of-the-art deep learning models in computer vision (CV) [6,7,19,33] and natural language processing (NLP) [1,5], their deployment in resourcelimited settings becomes more challenging. Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form. *Corresponding author: camhero@gmail.com Previous works have introduced various forms of dark knowledge and refined the knowledge transfer process through structural modifications [2,9,17,22,26,35]. The effectiveness of these methods emphasizes the pivotal role dark knowledge plays within the KD framework. While these methods mostly employ a fixed training paradigm, adaptive distillation approaches have brought forth a more dynamic transfer process [3,13,18,20,31,37,38]. These approaches dynamically modulate the knowledge transfer, typically based on the teacher-student performance gap, ensuring a more tailored knowledge transfer. Despite their effectiveness, these methods often come with limitations such as being confined to specific frameworks [3,31,37,38], computationally intensives [18,20], or yielding marginal improvements [13]. Furthermore, we identify these methods may overlook the inherent student's bias toward specific knowledge in KD, leading to imbalanced learning. \n\nWhile KD achieves promising results by using Kullback-Leibler (KL) divergence to align the student's predictions with the teacher's, a notable performance gap between the models remains. We hypothesize that this gap may be attributed to the student's overconfident predictions. To investigate this, we employ entropy, a concept from information theory that quantifies the unpredictability or information of a random variable [28], to measure the confidence of predictions. We then utilize the kernel density estimation (KDE) to visualize and compare the entropy distributions of the teacher and student.",
            "reference_string": "[265444951 | Su et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.11704",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.11704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298916636",
                    "name": "Taiyuan Mei"
                },
                {
                    "authorId": "2298918720",
                    "name": "Yun Zi"
                },
                {
                    "authorId": "2222987403",
                    "name": "X. Cheng"
                },
                {
                    "authorId": "2297725659",
                    "name": "Zijun Gao"
                },
                {
                    "authorId": "2297735971",
                    "name": "Qi Wang"
                },
                {
                    "authorId": "2302372513",
                    "name": "Haowei Yang"
                }
            ],
            "abstract": "The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.",
            "corpus_id": 269921267,
            "sentences": [
                {
                    "corpus_id": "269921267",
                    "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
                    "text": "This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
                    "score": 0.5568053227737461,
                    "section_title": "B. Knowledge distillation",
                    "char_start_offset": 14355,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 182,
                            "end": 419
                        },
                        {
                            "start": 419,
                            "end": 894
                        },
                        {
                            "start": 894,
                            "end": 1033
                        },
                        {
                            "start": 1033,
                            "end": 1183
                        },
                        {
                            "start": 1183,
                            "end": 1393
                        },
                        {
                            "start": 1393,
                            "end": 1762
                        },
                        {
                            "start": 1764,
                            "end": 1877
                        },
                        {
                            "start": 1877,
                            "end": 2405
                        },
                        {
                            "start": 2407,
                            "end": 2516
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56396484375
                }
            ],
            "relevance_judgement": 0.56396484375,
            "relevance_judgment_input_expanded": "# Title: Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks\n# Venue: 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)\n# Authors: Taiyuan Mei, Yun Zi, X. Cheng, Zijun Gao, Qi Wang, Haowei Yang\n## Abstract\nThe internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.\n## B. Knowledge distillation\nThis process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
            "reference_string": "[269921267 | Mei et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 64,
            "citation_count": 26,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.13298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2087535968",
                    "name": "Ching Lam Choi"
                },
                {
                    "authorId": "2115477215",
                    "name": "Xiao Zhang"
                },
                {
                    "authorId": "46737362",
                    "name": "Peipei Zhao"
                },
                {
                    "authorId": "2075369514",
                    "name": "Feng Zhu"
                },
                {
                    "authorId": "145638781",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "47893312",
                    "name": "Hongsheng Li"
                }
            ],
            "abstract": "The recent studies of knowledge distillation have discovered that ensembling the\"dark knowledge\"from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +0.7% gain of Swin-T on ImageNet with only +1.5% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.",
            "corpus_id": 233407431,
            "sentences": [
                {
                    "corpus_id": "233407431",
                    "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
                    "text": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
                    "score": 0.5239663277645721,
                    "section_title": "Revisit of Knowledge Distillation",
                    "char_start_offset": 8697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 175,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 502
                        },
                        {
                            "start": 505,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 831
                        },
                        {
                            "start": 834,
                            "end": 931
                        },
                        {
                            "start": 934,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 947,
                            "end": 951,
                            "matchedPaperCorpusId": "219965421"
                        },
                        {
                            "start": 951,
                            "end": 954,
                            "matchedPaperCorpusId": "48352434"
                        },
                        {
                            "start": 954,
                            "end": 957,
                            "matchedPaperCorpusId": "54447578"
                        },
                        {
                            "start": 957,
                            "end": 960,
                            "matchedPaperCorpusId": "204838340"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56103515625
                }
            ],
            "relevance_judgement": 0.56103515625,
            "relevance_judgment_input_expanded": "# Title: Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification\n# Venue: arXiv.org\n# Authors: Yixiao Ge, Ching Lam Choi, Xiao Zhang, Peipei Zhao, Feng Zhu, Rui Zhao, Hongsheng Li\n## Abstract\nThe recent studies of knowledge distillation have discovered that ensembling the\"dark knowledge\"from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +0.7% gain of Swin-T on ImageNet with only +1.5% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.\n## Revisit of Knowledge Distillation\nKnowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
            "reference_string": "[233407431 | Ge et al. | 2021 | Citations: 26]"
        },
        {
            "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.07951",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.07951, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2041323382",
                    "name": "Maorong Wang"
                },
                {
                    "authorId": "49948838",
                    "name": "L. Xiao"
                },
                {
                    "authorId": "145572097",
                    "name": "T. Yamasaki"
                }
            ],
            "abstract": "Online knowledge distillation (KD) has received increasing attention in recent years. However, while most existing online KD methods focus on developing complicated model structures and training strategies to improve the distillation of high-level knowledge like probability distribution, the effects of the multi-level knowledge in the online KD are greatly overlooked, especially the low-level knowledge. Thus, to provide a novel viewpoint to online KD, we propose MetaMixer, a regularization strategy that can strengthen the distillation by combining the low-level knowledge that impacts the localization capability of the networks, and high-level knowledge that focuses on the whole image. Experiments under different conditions show that MetaMixer can achieve significant performance gains over state-of-the-art methods.",
            "corpus_id": 257504799,
            "sentences": [
                {
                    "corpus_id": "257504799",
                    "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
                    "text": "The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in [1]. However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network. Besides the vanilla KD, researchers have explored more methods to extract more salient dark knowledge from the pre-trained teacher network to the student [28,15,26,27,31,30]. For example, Fitnets [28] proposed to use intermediate feature representation as a hint to guide the training of the student network. Tian et al. [30] first used the contrastive learning mechanism to extract the structural knowledge of the teacher network to supervise the training of the student.",
                    "score": 0.6080273361095455,
                    "section_title": "Conventional Knowledge Distillation",
                    "char_start_offset": 3200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 922
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 127,
                            "end": 130,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 611,
                            "end": 614,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 614,
                            "end": 617,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 617,
                            "end": 620,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55712890625
                }
            ],
            "relevance_judgement": 0.55712890625,
            "relevance_judgment_input_expanded": "# Title: MetaMixer: A Regularization Strategy for Online Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Maorong Wang, L. Xiao, T. Yamasaki\n## Abstract\nOnline knowledge distillation (KD) has received increasing attention in recent years. However, while most existing online KD methods focus on developing complicated model structures and training strategies to improve the distillation of high-level knowledge like probability distribution, the effects of the multi-level knowledge in the online KD are greatly overlooked, especially the low-level knowledge. Thus, to provide a novel viewpoint to online KD, we propose MetaMixer, a regularization strategy that can strengthen the distillation by combining the low-level knowledge that impacts the localization capability of the networks, and high-level knowledge that focuses on the whole image. Experiments under different conditions show that MetaMixer can achieve significant performance gains over state-of-the-art methods.\n## Conventional Knowledge Distillation\nThe idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in [1]. However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network. Besides the vanilla KD, researchers have explored more methods to extract more salient dark knowledge from the pre-trained teacher network to the student [28,15,26,27,31,30]. For example, Fitnets [28] proposed to use intermediate feature representation as a hint to guide the training of the student network. Tian et al. [30] first used the contrastive learning mechanism to extract the structural knowledge of the teacher network to supervise the training of the student.",
            "reference_string": "[257504799 | Wang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
            "venue": "IEEE Transactions on Cybernetics",
            "year": 2019,
            "reference_count": 92,
            "citation_count": 59,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.09643",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.09643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "10944885",
                    "name": "Changrui Chen"
                },
                {
                    "authorId": "2087106420",
                    "name": "Zihe Dong"
                }
            ],
            "abstract": "High storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance.",
            "corpus_id": 198179767,
            "sentences": [
                {
                    "corpus_id": "198179767",
                    "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                    "text": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
                    "score": 0.5155854766336335,
                    "section_title": "B. Formulation",
                    "char_start_offset": 17544,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 59
                        },
                        {
                            "start": 60,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 637
                        },
                        {
                            "start": 640,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1783
                        },
                        {
                            "start": 1786,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2079
                        },
                        {
                            "start": 2082,
                            "end": 2156
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1257,
                            "end": 1261,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1325,
                            "end": 1329,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "3643430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55322265625
                }
            ],
            "relevance_judgement": 0.55322265625,
            "relevance_judgment_input_expanded": "# Title: Highlight Every Step: Knowledge Distillation via Collaborative Teaching\n# Venue: IEEE Transactions on Cybernetics\n# Authors: Haoran Zhao, Xin Sun, Junyu Dong, Changrui Chen, Zihe Dong\n## Abstract\nHigh storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance.\n## B. Formulation\nDeep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
            "reference_string": "[198179767 | Zhao et al. | 2019 | Citations: 59]"
        },
        {
            "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 120,
            "citation_count": 47,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308012469",
                    "name": "Can Jin"
                },
                {
                    "authorId": "2282541442",
                    "name": "Tong Che"
                },
                {
                    "authorId": "2282596442",
                    "name": "Hongwu Peng"
                },
                {
                    "authorId": "2282543805",
                    "name": "Yiyuan Li"
                },
                {
                    "authorId": "2237790577",
                    "name": "Marco Pavone"
                }
            ],
            "abstract": "Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and, in turn, provide feedback to help the main model capture more generalizable and imitable correlations. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and methodologies like Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to training models on the original dataset. The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning. Code is available at https://github.com/jincan333/LoT.",
            "corpus_id": 267413204,
            "sentences": [
                {
                    "corpus_id": "267413204",
                    "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
                    "text": "Knowledge distillation (KD) is a technique that transfers knowledge from a teacher model to a student model by training the student to imitate the teacher's outputs [36]. This approach is widely applied in areas such as model compression, transparency, and interpretability [8,10,27,36,60,91]. Model compression is often motivated by resource constraints. Pioneering works include Bucilu\u01ce et al. [10], which compresses ensemble neural networks into a single network, and Ba and Caruana [3], which improves shallow neural network accuracy by mimicking deep networks. KD is also applied in various domains, including deep reinforcement learning [80], continual learning [28,59,85], and learning privileged information theory [62,76]. The dark knowledge method [36] further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label. Additionally, in born-again networks (BAN) [29], the teacher and student have identical neural architecture and model sizes, but the student can surprisingly surpass the teacher's accuracy.",
                    "score": 0.6375518336591193,
                    "section_title": "B.2.1 Knowledge Distillation",
                    "char_start_offset": 29576,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1215
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 165,
                            "end": 169,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 277,
                            "end": 280,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 280,
                            "end": 283,
                            "matchedPaperCorpusId": "3976789"
                        },
                        {
                            "start": 283,
                            "end": 286,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 289,
                            "end": 292,
                            "matchedPaperCorpusId": "21713934"
                        },
                        {
                            "start": 396,
                            "end": 400,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 672,
                            "end": 675,
                            "matchedPaperCorpusId": "4853851"
                        },
                        {
                            "start": 727,
                            "end": 730,
                            "matchedPaperCorpusId": "41866457"
                        },
                        {
                            "start": 758,
                            "end": 762,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1069,
                            "end": 1073,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53662109375
                }
            ],
            "relevance_judgement": 0.53662109375,
            "relevance_judgment_input_expanded": "# Title: Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate\n# Venue: Neural Information Processing Systems\n# Authors: Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, Marco Pavone\n## Abstract\nGeneralization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and, in turn, provide feedback to help the main model capture more generalizable and imitable correlations. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and methodologies like Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to training models on the original dataset. The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning. Code is available at https://github.com/jincan333/LoT.\n## B.2.1 Knowledge Distillation\nKnowledge distillation (KD) is a technique that transfers knowledge from a teacher model to a student model by training the student to imitate the teacher's outputs [36]. This approach is widely applied in areas such as model compression, transparency, and interpretability [8,10,27,36,60,91]. Model compression is often motivated by resource constraints. Pioneering works include Bucilu\u01ce et al. [10], which compresses ensemble neural networks into a single network, and Ba and Caruana [3], which improves shallow neural network accuracy by mimicking deep networks. KD is also applied in various domains, including deep reinforcement learning [80], continual learning [28,59,85], and learning privileged information theory [62,76]. The dark knowledge method [36] further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label. Additionally, in born-again networks (BAN) [29], the teacher and student have identical neural architecture and model sizes, but the student can surprisingly surpass the teacher's accuracy.",
            "reference_string": "[267413204 | Jin et al. | 2024 | Citations: 47]"
        },
        {
            "title": "Online Multi-Granularity Distillation for GAN Compression",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2021,
            "reference_count": 76,
            "citation_count": 39,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2108.06908",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.06908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Yuxi Ren"
                },
                {
                    "authorId": "2118432533",
                    "name": "Jie Wu"
                },
                {
                    "authorId": "2118724465",
                    "name": "Xuefeng Xiao"
                },
                {
                    "authorId": "1706007",
                    "name": "Jianchao Yang"
                }
            ],
            "abstract": "Generative Adversarial Networks (GANs) have witnessed prevailing success in yielding outstanding images, however, they are burdensome to deploy on resource-constrained devices due to ponderous computational costs and hulking memory usage. Although recent efforts on compressing GANs have acquired remarkable results, they still exist potential model redundancies and can be further compressed. To solve this issue, we propose a novel online multi-granularity distillation (OMGD) scheme to obtain lightweight GANs, which contributes to generating highfidelity images with low computational demands. We offer the first attempt to popularize single-stage online distillation for GAN-oriented compression, where the progressively promoted teacher generator helps to refine the discriminator-free based student generator. Complementary teacher generators and network layers provide comprehensive and multi-granularity concepts to enhance visual fidelity from diverse dimensions. Experimental results on four benchmark datasets demonstrate that OMGD successes to compress 40\u00d7 MACs and 82.5\u00d7 parameters on Pix2Pix and CycleGAN, without loss of image quality. It reveals that OMGD provides a feasible solution for the deployment of real-time image translation on resource-constrained devices. Our code and models are made public at: https://github.com/bytedance/OMGD",
            "corpus_id": 237091534,
            "sentences": [
                {
                    "corpus_id": "237091534",
                    "title": "Online Multi-Granularity Distillation for GAN Compression",
                    "text": "Knowledge Distillation (KD) [19] is a fundamental compression technique, where a smaller student model is optimized under the effective information transfer and supervision of a larger teacher model or ensembles [6]. Hinton [19] performed knowledge distillation via minimizing the distance between the output distribution statistics between student and teacher network. In this way, the student network attempts to learn dark knowledge [19] that contains the similarities between different classes, which can not be provided by the ground truth labels. Romero et al. [41] further took advantage of the concepts of feature maps from the intermediate layers to enhance the performance of the student network. Zhou et al. [65] presented that each channel of the feature map corresponds to a visual pattern, so they focus on transferring attention concepts [53,54,55] of feature map from each channel in intermediate layers. \n\nMoreover, You et al. [59] revealed that multiple teacher networks can provide more comprehensive knowledge for learning a more effective student network. MEAL [44] compressed large and complex trained ensembles into a sin-gle network, which employs an adversarial-based learning strategy to guide the pre-defined student network to transfer knowledge from teacher models. Offline knowledge distillation requires a pre-trained teacher model in the stage of optimizing, while online KD simultaneously optimizes the teacher and student network or just a group of student peers [51]. Anil et al. [2] trained two networks with the identical architecture parallelly and these two networks play the role of student and teacher iteratively. In this paper, we employ the multi-granularity based online distillation scheme, which aims to learn an effective student model from complementary structure of the teacher generators and the knowledge from diverse layers.",
                    "score": 0.638343488625316,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6919,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 920
                        },
                        {
                            "start": 923,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1294
                        },
                        {
                            "start": 1295,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1877
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 32,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 212,
                            "end": 215,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 224,
                            "end": 228,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 436,
                            "end": 440,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 853,
                            "end": 857,
                            "matchedPaperCorpusId": "56594619"
                        },
                        {
                            "start": 857,
                            "end": 860,
                            "matchedPaperCorpusId": "201809759"
                        },
                        {
                            "start": 860,
                            "end": 863,
                            "matchedPaperCorpusId": "39802142"
                        },
                        {
                            "start": 944,
                            "end": 948,
                            "matchedPaperCorpusId": "26021416"
                        },
                        {
                            "start": 1082,
                            "end": 1086,
                            "matchedPaperCorpusId": "54447578"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53564453125
                }
            ],
            "relevance_judgement": 0.53564453125,
            "relevance_judgment_input_expanded": "# Title: Online Multi-Granularity Distillation for GAN Compression\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Yuxi Ren, Jie Wu, Xuefeng Xiao, Jianchao Yang\n## Abstract\nGenerative Adversarial Networks (GANs) have witnessed prevailing success in yielding outstanding images, however, they are burdensome to deploy on resource-constrained devices due to ponderous computational costs and hulking memory usage. Although recent efforts on compressing GANs have acquired remarkable results, they still exist potential model redundancies and can be further compressed. To solve this issue, we propose a novel online multi-granularity distillation (OMGD) scheme to obtain lightweight GANs, which contributes to generating highfidelity images with low computational demands. We offer the first attempt to popularize single-stage online distillation for GAN-oriented compression, where the progressively promoted teacher generator helps to refine the discriminator-free based student generator. Complementary teacher generators and network layers provide comprehensive and multi-granularity concepts to enhance visual fidelity from diverse dimensions. Experimental results on four benchmark datasets demonstrate that OMGD successes to compress 40\u00d7 MACs and 82.5\u00d7 parameters on Pix2Pix and CycleGAN, without loss of image quality. It reveals that OMGD provides a feasible solution for the deployment of real-time image translation on resource-constrained devices. Our code and models are made public at: https://github.com/bytedance/OMGD\n## Knowledge Distillation\nKnowledge Distillation (KD) [19] is a fundamental compression technique, where a smaller student model is optimized under the effective information transfer and supervision of a larger teacher model or ensembles [6]. Hinton [19] performed knowledge distillation via minimizing the distance between the output distribution statistics between student and teacher network. In this way, the student network attempts to learn dark knowledge [19] that contains the similarities between different classes, which can not be provided by the ground truth labels. Romero et al. [41] further took advantage of the concepts of feature maps from the intermediate layers to enhance the performance of the student network. Zhou et al. [65] presented that each channel of the feature map corresponds to a visual pattern, so they focus on transferring attention concepts [53,54,55] of feature map from each channel in intermediate layers. \n\nMoreover, You et al. [59] revealed that multiple teacher networks can provide more comprehensive knowledge for learning a more effective student network. MEAL [44] compressed large and complex trained ensembles into a sin-gle network, which employs an adversarial-based learning strategy to guide the pre-defined student network to transfer knowledge from teacher models. Offline knowledge distillation requires a pre-trained teacher model in the stage of optimizing, while online KD simultaneously optimizes the teacher and student network or just a group of student peers [51]. Anil et al. [2] trained two networks with the identical architecture parallelly and these two networks play the role of student and teacher iteratively. In this paper, we employ the multi-granularity based online distillation scheme, which aims to learn an effective student model from complementary structure of the teacher generators and the knowledge from diverse layers.",
            "reference_string": "[237091534 | Ren et al. | 2021 | Citations: 39]"
        },
        {
            "title": "Multistage feature fusion knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-64041-4.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11166915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307186661",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2307436738",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2305765306",
                    "name": "Pengfei Lv"
                },
                {
                    "authorId": "2305752302",
                    "name": "Pan He"
                },
                {
                    "authorId": "2287501699",
                    "name": "Zheng Zhou"
                },
                {
                    "authorId": "2817613",
                    "name": "Chuanyun Xu"
                }
            ],
            "abstract": "Generally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.",
            "corpus_id": 270389751,
            "sentences": [
                {
                    "corpus_id": "270389751",
                    "title": "Multistage feature fusion knowledge distillation",
                    "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
                    "score": 0.5720888037592825,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 2815,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 194,
                            "end": 356
                        },
                        {
                            "start": 356,
                            "end": 489
                        },
                        {
                            "start": 491,
                            "end": 642
                        },
                        {
                            "start": 642,
                            "end": 783
                        },
                        {
                            "start": 783,
                            "end": 952
                        },
                        {
                            "start": 952,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1579
                        },
                        {
                            "start": 1579,
                            "end": 1780
                        },
                        {
                            "start": 1780,
                            "end": 1981
                        },
                        {
                            "start": 1981,
                            "end": 2204
                        },
                        {
                            "start": 2204,
                            "end": 2387
                        },
                        {
                            "start": 2387,
                            "end": 2522
                        },
                        {
                            "start": 2522,
                            "end": 2673
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 68,
                            "end": 70,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 955,
                            "end": 957,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1251,
                            "end": 1253,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1582,
                            "end": 1584,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 2208,
                            "end": 2210,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 2391,
                            "end": 2393,
                            "matchedPaperCorpusId": "102483181"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53515625
                }
            ],
            "relevance_judgement": 0.53515625,
            "relevance_judgment_input_expanded": "# Title: Multistage feature fusion knowledge distillation\n# Venue: Scientific Reports\n# Authors: Gang Li, Kun Wang, Pengfei Lv, Pan He, Zheng Zhou, Chuanyun Xu\n## Abstract\nGenerally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.\n## Knowledge distillation\nKnowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "reference_string": "[270389751 | Li et al. | 2024 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "237478571",
            "title": "BERTtoCNN: Similarity-preserving enhanced knowledge distillation for stance detection",
            "text": "Knowledge distillation is a common model compression and transfer learning method. Through the \"teacher\" model and the \"student\" model, the implicit knowledge(dark knowledge) learned from the \"teacher\" model of complex network is distilled into the simple",
            "score": 0.8061479924295386,
            "section_title": "Knowledge distillation",
            "char_start_offset": 5594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4658203125
        },
        {
            "corpus_id": "264555654",
            "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
            "text": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
            "score": 0.7872691246349993,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.",
            "score": 0.7653766700550831,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "226262351",
            "title": "Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training",
            "text": "In parallel, student-teacher learning has received intensive attention because of its excellent performance on various tasks. A typical application is to transfer knowledge from a large, powerful teacher network to a compact yet accurate student network, so as to boost the training process and the resulting performance (Watanabe et al., 2017;Bucilu and Niculescu-Mizil, 2006;Wang et al., 2018). For example, in (Bucilu and Niculescu-Mizil, 2006), a student network was encouraged to mimic the output of a teacher network via mean squared error. (You et al., 2017) proposed a dark knowledge distillation method, in which the student network accommodated the true labels and captured the structures among the labels. Instead of considering one single teacher network, several studies trained a student network by incorporating multiple teacher networks in the output layer or the hidden layers (Park and Kwak, 2019;You et al., 2017).",
            "score": 0.7006838189680337,
            "section_title": "Student-teacher Learning Paradigm",
            "char_start_offset": 7533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 344,
                    "matchedPaperCorpusId": "9154797"
                },
                {
                    "start": 344,
                    "end": 377,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 377,
                    "end": 395,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 413,
                    "end": 447,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 547,
                    "end": 565,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 915,
                    "end": 932,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2783203125
        },
        {
            "corpus_id": "251066725",
            "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
            "text": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,5,18,34], designing student-friendly architecture [16,20], improving the distillation efficiency [7,14,27,29] and explaining the distillation's working mechanism [1,23]. \n\nIn this work, we focus on how to transfer the \"dark knowledge\" in an almost free manner. Furlanello et al. [7] proposed to improve the performance of the student network by distilling a teacher network with the same architecture. However, it is still a two-stage approach, which first trains the teacher and then distills knowledge to the student. To reduce the training time, many selfdistillation methods were proposed. They gain soft label supervision on the fly without the pretraining step.",
            "score": 0.681922972997853,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1176
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 511,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 606,
                    "end": 609,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 674,
                    "end": 677,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "203593636",
            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
            "text": "After that, Hinton et al. [30] proposed dark knowledge (DK) to transfer knowledge from a large pre-trained model to a small student network, which uses an extra supervision provided by the soft final outputs of the pre-trained teacher. The extra supervision extracted from a pre-trained teacher model is often in form of class posterior probabilities [30], feature representations [31,32,40,4], distribution of intermediate activations [41,42], or inter-layer flow [43]). However, these distillation methods require at least two-stage training, including pre-training the teacher network and training the student network with an extra supervision, which is computation expensive during training. The more recently proposed deep mutual learning [44] overcomes this limitation by conducting an online distillation in one stage training between two peer student networks. Anil et al. [45] further extended this idea to accelerate the training of large-scale distributed neural networks. However, these online distillation methods use each student as the opposing teacher, which is not powerful and even limits the efficacy of knowledge discovery. \n\nActually, the above knowledge distillation methods redesign the new student network, which is thinner or more shallow than the teacher network. Different from the previous knowledge distillation schemes, we produce a student network by directly replacing the standard convolutions with different cheap convolutions without a complicated redesign. Moreover, to take the place of peer student as a teacher, we design a new online distillation method to online construct the strong teacher network, in which mutual learning between the teacher and the student model is used to further improve their performance. \n\nOther orthogonal methods. There are other four kinds of CNN compression methods, i.e., network architecture search [46,47,48,49,50,51], parameter quantization [14,15,13,52,53], network pruning [9,10,54,11,12,55,56,57,58] and low-rank decomposition [59,4,7,6,5,8]. Our method can be integrated with these orthogonal methods to further improve the performance, which are however orthogonal to the core contribution of this paper.",
            "score": 0.6521999110571807,
            "section_title": "Related Work",
            "char_start_offset": 9451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 388,
                    "end": 391,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 391,
                    "end": 393,
                    "matchedPaperCorpusId": "52915624"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "149834243"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "2331610"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "12713052"
                },
                {
                    "start": 1876,
                    "end": 1879,
                    "matchedPaperCorpusId": "12227989"
                },
                {
                    "start": 1879,
                    "end": 1882,
                    "matchedPaperCorpusId": "1740355"
                },
                {
                    "start": 1882,
                    "end": 1885,
                    "matchedPaperCorpusId": "743641"
                },
                {
                    "start": 1885,
                    "end": 1888,
                    "matchedPaperCorpusId": "206770867"
                },
                {
                    "start": 1929,
                    "end": 1932,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1953,
                    "end": 1956,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1956,
                    "end": 1959,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1962,
                    "end": 1965,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 1968,
                    "end": 1971,
                    "matchedPaperCorpusId": "85459412"
                },
                {
                    "start": 1971,
                    "end": 1974,
                    "matchedPaperCorpusId": "51607445"
                },
                {
                    "start": 1974,
                    "end": 1977,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 2005,
                    "end": 2009,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 2009,
                    "end": 2011,
                    "matchedPaperCorpusId": "52915624"
                },
                {
                    "start": 2011,
                    "end": 2013,
                    "matchedPaperCorpusId": "1437449"
                },
                {
                    "start": 2013,
                    "end": 2015,
                    "matchedPaperCorpusId": "15602035"
                },
                {
                    "start": 2017,
                    "end": 2019,
                    "matchedPaperCorpusId": "5748557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "231986112",
            "title": "Exploring Knowledge Distillation of a Deep Neural Network for Multi-Script identification",
            "text": "Earlier papers used ensemble methods for model compression [8,9]. Distillation of knowledge from a teacher network and transferring it to a student network to mimic the teacher network is a basic fundamental concept of knowledge distillation. The first proposed concept of knowledge distillation [7] introduces the concept of compressing the knowledge of a more in-depth or larger model to a single computational efficient neural network. It has introduced the concept of dark knowledge transfer from a deep teacher network to a smaller student network by taking the softmax of the results of the teacher network with a specific temperature value and calculating loss between it and the predicted outputs of the student network. They validated their findings by running on MNIST dataset and, JFT dataset by google and other speech recognition tasks. Since then, knowledge distillation has progressed a lot, and adversarial methods [17,18] also have utilized for modelling knowledge transfer between teacher and student. After this study, extensive research has conducted on knowledge distillation. In the paper [11] has introduced the transfer of a hidden activation output and other has proposed transferring attention information as knowledge [20]. \n\nArticle [19] has briefly described the advantages and efficiency of the knowledge distillation. It has described importance of knowledge transfer from teacher to student model using distilled knowledge. They have compared two student deep neural networks trained with teacher network and without teacher model with same size.They have proposed a method of transferring the distilled knowledge between two layers to shows three important points. The student model is more efficient than the original model and it also outperform the original model which is trained from scratch. The student network understand the flow of solving the problem and it start learning with good initial weights. It can learnt and optimized faster than original or normal deep neural network.This paper proves that, the student model reports better efficiency than a normal network without a teacher model.They have compared various knowledge transfer techniques with a normal network without any teacher model for knowledge transfer.They have learned their model with two main condition. First, the teacher model must pretrained with some different dataset and second condition is the teacher model is shallower or deeper than the student model. Their approach contains two step training.",
            "score": 0.6490348788681366,
            "section_title": "Related Study",
            "char_start_offset": 4015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2476
                },
                {
                    "start": 2477,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 62,
                    "end": 64,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304443359375
        },
        {
            "corpus_id": "225040647",
            "title": "Robustness-aware 2-bit quantization with real-time performance for neural network",
            "text": "Knowledge distillation is a technique to transfer knowledge from a complex network (teacher network) to a compact network (student network), including data knowledge distillation and structure knowledge distillation. Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss [14]. Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer [15]. For the first time, we use knowledge distillation to extract the structural information of the network and construct new loss function to guide the quantization process of the network.",
            "score": 0.6478703228251189,
            "section_title": "Knowledge distillation",
            "char_start_offset": 6179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 815
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 625,
                    "end": 629,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1993408203125
        },
        {
            "corpus_id": "260704230",
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "text": "Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks. Moreover, more advanced training schemes and architecture designs are introduced to improve the efficiency of the distillation process. Although several works [193,194,195,196] focus on regression tasks, one promising research direction can be investigated in the theoretical studies of regression-based knowledge learning, such as the representation of dark knowledge on regression problems. The final predictions of teacher networks are represented as knowledge to be transferred to student networks [204,205], and student networks also aim to mimic the extracted feature representations from teacher networks [206,207]. With deeper theoretical studies on regression-based knowledge learning, teacherstudent architectures will be further effectively employed in practical applications.",
            "score": 0.6464851049463406,
            "section_title": "Theoretical Understandings of Regression-Based Knowledge Learning",
            "char_start_offset": 72996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1004
                }
            ],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 381,
                    "matchedPaperCorpusId": "211572450"
                },
                {
                    "start": 381,
                    "end": 385,
                    "matchedPaperCorpusId": "233582109"
                },
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "243791573"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "233582109"
                },
                {
                    "start": 829,
                    "end": 834,
                    "matchedPaperCorpusId": "243791573"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "199405345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "In the perspective of a Single Teacher-Single Student learning paradigm, knowledge distillation is a simple way to transfer knowledge of a teacher to improve the performance of small deep learning model called a student. More specifically, knowledge distillation refers to the method that helps the training process of a small network (student) under the supervision of a large network (teacher). The additional supervision about the relative probabilities of secondary class and relational information between data points at the output of Teacher network can be useful in increasing the efficacy of the Student network. \n\nWe can downsize a student network regardless of the structural difference between teacher and student. Allowing this architectural flexibility, knowledge distillation is emerging as a next generation approach of network compression. However, too excessive gap of the capacity between a teacher network and a student network is a critical obstacle for knowledge transfer performance. This is empirically shown and analyzed in [17]. \n\n2) Multi-Step Learning Seyed-Iman Mirzadeh et al. [17] showed that the student network performance degrades when the gap between student and teacher networks is large. So, given a fixed student network, one cannot employ an arbitrarily large teacher network; in other words, a teacher network can effectively transfer its knowledge to student networks having up to a certain capacity. \n\nTo alleviate this shortcoming, multi-step knowledge distillation was introduced. It is a new distillation framework called Teacher Assistant Knowledge Distillation (TAKD), which introduces intermediate-sized network known as teacher assistants (TAs) between the teacher and the student to fill in the gap. TA models are distilled from the teacher, and the student is then only distilled from the TAs. Through extensive empirical evaluations and a theoretical justification, they showed that introducing intermediate TA networks improve the distillation performance and concluded that the size (capacity) gap difference between a teacher-TA and a TA-student is important. \n\nFig. 1. A teacher assistant network fills the gap between student and teacher networks [17]. \n\nFig. 1 shows the overall knowledge distillation structure incorporating teacher assistant. \n\n3) Multiple-Teacher Learning Shan You et al. [15] proposed a new method which uses multiple teacher networks to train a thin and deep student network. Fig. 2 shows overall knowledge distillation incorporating multiple teachers.",
            "score": 0.6434493140698129,
            "section_title": "1) Single Teacher-Single Student",
            "char_start_offset": 8203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2113
                },
                {
                    "start": 2116,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2208
                },
                {
                    "start": 2211,
                    "end": 2301
                },
                {
                    "start": 2304,
                    "end": 2454
                },
                {
                    "start": 2455,
                    "end": 2531
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "237091534",
            "title": "Online Multi-Granularity Distillation for GAN Compression",
            "text": "Knowledge Distillation (KD) [19] is a fundamental compression technique, where a smaller student model is optimized under the effective information transfer and supervision of a larger teacher model or ensembles [6]. Hinton [19] performed knowledge distillation via minimizing the distance between the output distribution statistics between student and teacher network. In this way, the student network attempts to learn dark knowledge [19] that contains the similarities between different classes, which can not be provided by the ground truth labels. Romero et al. [41] further took advantage of the concepts of feature maps from the intermediate layers to enhance the performance of the student network. Zhou et al. [65] presented that each channel of the feature map corresponds to a visual pattern, so they focus on transferring attention concepts [53,54,55] of feature map from each channel in intermediate layers. \n\nMoreover, You et al. [59] revealed that multiple teacher networks can provide more comprehensive knowledge for learning a more effective student network. MEAL [44] compressed large and complex trained ensembles into a sin-gle network, which employs an adversarial-based learning strategy to guide the pre-defined student network to transfer knowledge from teacher models. Offline knowledge distillation requires a pre-trained teacher model in the stage of optimizing, while online KD simultaneously optimizes the teacher and student network or just a group of student peers [51]. Anil et al. [2] trained two networks with the identical architecture parallelly and these two networks play the role of student and teacher iteratively. In this paper, we employ the multi-granularity based online distillation scheme, which aims to learn an effective student model from complementary structure of the teacher generators and the knowledge from diverse layers.",
            "score": 0.638343488625316,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "56594619"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "201809759"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "39802142"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "54447578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "267413204",
            "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
            "text": "Knowledge distillation (KD) is a technique that transfers knowledge from a teacher model to a student model by training the student to imitate the teacher's outputs [36]. This approach is widely applied in areas such as model compression, transparency, and interpretability [8,10,27,36,60,91]. Model compression is often motivated by resource constraints. Pioneering works include Bucilu\u01ce et al. [10], which compresses ensemble neural networks into a single network, and Ba and Caruana [3], which improves shallow neural network accuracy by mimicking deep networks. KD is also applied in various domains, including deep reinforcement learning [80], continual learning [28,59,85], and learning privileged information theory [62,76]. The dark knowledge method [36] further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label. Additionally, in born-again networks (BAN) [29], the teacher and student have identical neural architecture and model sizes, but the student can surprisingly surpass the teacher's accuracy.",
            "score": 0.6375518336591193,
            "section_title": "B.2.1 Knowledge Distillation",
            "char_start_offset": 29576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1215
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "3976789"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "21713934"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "41866457"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "229340414",
            "title": "Knowledge Transfer Based Fine-Grained Visual Classification",
            "text": "Knowledge distillation Learning has been widely studied and applied in machine learning. It is a technology to distill the knowledge of one model to another. Hinton et al. [16] firstly propose the concept of dark knowledge based on teacherstudent network, where teacher network is often a more complex network with desirable performance and fine generalization. With teacher's help the simpler student model with less parameter also has similar performance with teacher. Remero et al. [17] convey knowledge by feature maps, guiding student to acquire the ability to extract features. Zaforuyko et al. [18] propose the method that the student model is trained with the guidance of the teacher's attention map. \n\nAll aforementioned methods aim to make the student model has similar performance and fine generalization to the teacher model. However, in this paper, we guide the student model learn complementary knowledge with the teacher. We replace the KL divergence with the proposed OR-Loss and take spatial attention map as knowledge carrier.",
            "score": 0.6326098291998368,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154052734375
        },
        {
            "corpus_id": "264590688",
            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
            "text": "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT [30] uses convolutional operations to transfer dark knowledge; 2) DeiT [31] proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP [32] exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD [34] exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes [36]. With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge [36]. This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks. \n\nIn the next section, we consider this observation and leverage knowledge self-distillation in Multi-stage MVC.",
            "score": 0.6231205753332721,
            "section_title": "B. Contrastive Learning",
            "char_start_offset": 10461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1651
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "245006036"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "252918735"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "240544883",
            "title": "Deep Neural Network Compression for Plant Disease Recognition",
            "text": "Knowledge distillation, also known as teacher-student training, was proposed in [37]. An experiment demonstrated that a well-trained network uses its output soft labels to guide a simple, small student network through training, and dark knowledge can be easily transferred to the student network without changing its structure. In [38], authors proposed an ensemble of teacher networks to improve the generalization ability of a student network. \n\nIn [39], the authors used the attention maps of the middle layers of a teacher network to guide the training of the student network, aiming at enabling the student network to learn the feature maps of the teacher more effectively. Researchers in [40] found that if the given teacher and student networks are similar in structure, the student network more easily learns the knowledge of the teacher network, thus saving time when training a complex model.",
            "score": 0.6217186906965138,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 902
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390625
        },
        {
            "corpus_id": "238223320",
            "title": "Determining Top Fully Connected Layer\u2019s Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19",
            "text": "Neural networks use distributed representation of feature map of input to interpret and processing in hidden layers [26]. These representations are hard to understand and represent in general form. Knowledge distillation (KD) was first applied on deep neural networks by Hinton et al. [27]. KD was first proposed by Bucila et al. [28]. \n\nKnowledge distillation is the process to transfer the representations and abilities learned by a large network (teacher network) to any smaller network (student network). Main drawback of knowledge distillation is that it can only be applied for classification tasks, not for regression tasks [29]. KD uses the \"dark knowledge\" (softened logit output of the bottom output layer of teacher network) that is transferred to student network. This dark knowledge is more than interlabel correlations and one-hot encoding of labels. In case of regression, the deep network interprets the continuous values, which has a tendency of unknown error distribution; as a result, there is no dark knowledge for deep networks trained for regression. But later [30] present their knowledge distillation process for pose regression. Multiple papers try to propose a framework to train the teacher and student network in parallel. Yim et al. [31] and You et al. [32] proposed a shared layer-representation for it. Czarnecki et al. [33] narrowed down the variance between teacher and student derivatives of the loss shared with the discrepancy from teacher predictions. Tarvainen and Valpola [34] proposed an averaging procedure for model weights for the same purpose. Urban et al. [35] trained a network of convolutional neural networks and teach shallow multilayer perceptron as student networks. Sau and Balasubramanian [36] injected noise into teacher logits for making the student network more robust. Employing several teacher networks is always a way to decrease the accuracy difference between teacher and student network. Zhang et al. [37] proposed deep mutual learning so that teacher and student networks can learn side by side during training. \n\nHinton et al. [27] claim that the success of knowledge distillation is credited to the logit distribution of the incorrect outputs.",
            "score": 0.6181973133641059,
            "section_title": "Distilling Neural Networks",
            "char_start_offset": 5608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "32588614"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "16550689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "267290948",
            "title": "A long-short dual-mode knowledge distillation framework for empirical asset pricing models in digital financial networks",
            "text": "Knowledge distillation (L. Wang & Yoon, 2021;Zhao et al., 2022) is one type of model compression method based on the teacher-student network training manner, which has been widely studied and applied to various machine learning tasks. Specifically, the knowledge distillation system mainly consists of domain knowledge attribute, distillation algorithm and teacher-student structure, which explores how to transfer knowledge from teacher model to student model. To boost the generalisation performance of data-driven models, the key of knowledge distillation can effectively utilise the additional knowledge to assist in training traditional neural networks. \n\nAiming to solve the issues of online deployment of large integrated models, Hinton et al. (2015) first proposed a knowledge distillation framework for converting information in the large models into training small models. Generally, according to whether the teacher network is updated or not, knowledge distillation can be roughly divided into three architectures: offline distillation, online distillation and self-distillation. The offline distillation process consists of two stages, including the teacher model pre-training and knowledge transfer. For example, Ma et al. (2023) designed a multimodal contrastive knowledge distillation framework, which teacher network adaptively refine the knowledge to during multimodal contrastive learning. However, online distillation adopts an end-to-end training manner, which teacher model and student model can be updated at the same time. For example, Li et al. (2022) introduced a novel online distillation scheme based on feature fusion and self-distillation. This approach can jointly train student networks to learn diverse information and reduce the computational complexity of model during deployment. The self-distillation framework means that both the teacher and student networks adopt the same network structure, in which the student network learns knowledge by itself. In particular, to explore the instructive knowledge unavailable to teacher networks, Zhang et al. (2022) proposed a self-distillation method to adaptively generate internal and external relationships between its targets as instructive knowledge. To address the most important sample regions, Gou et al. (2022) designed multilevel attention-based sample correlations for knowledge distillation for image classification and and person reidentification.",
            "score": 0.616771886469889,
            "section_title": "Knowledge distillation",
            "char_start_offset": 10764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2232
                },
                {
                    "start": 2233,
                    "end": 2437
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 45,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 45,
                    "end": 63,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1226,
                    "end": 1242,
                    "matchedPaperCorpusId": "257555248"
                },
                {
                    "start": 1559,
                    "end": 1575,
                    "matchedPaperCorpusId": "232380330"
                },
                {
                    "start": 2072,
                    "end": 2091,
                    "matchedPaperCorpusId": "237605150"
                },
                {
                    "start": 2279,
                    "end": 2296,
                    "matchedPaperCorpusId": "252552769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05242919921875
        },
        {
            "corpus_id": "276249293",
            "title": "Contrastive Representation Distillation via Multi-Scale Feature Decoupling",
            "text": "Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network. \n\nDepending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits (Hinton, 2015;Luo, 2024;Zhao et al., 2022;Sun et al., 2024;Jin et al., 2023;Li et al., 2023), features (Romero et al., 2014;Tian et al., 2019;Chen et al., 2022;2021;Heo et al., 2019;Park et al., 2019;Ahn et al., 2019), and attention (Zagoruyko & Komodakis, 2016;Guo et al., 2023). \n\nMany transferring features methods followed Fit-Net (Romero et al., 2014) by utilizing single-stage features for knowledge distillation. PKT (Passalis et al., 2020) aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. SimKD (Chen et al., 2022) decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. CRD (Tian et al., 2019) combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives. \n\nIn the feature-based distillation methods, many works proposed to utilize multi-level feature distillation. OFD (Heo et al., 2019) enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric. ReviewKD (Chen et al., 2021) improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student. \n\nHowever, previous methods primarily focused on global feature information, without addressing the decoupling of rela-tionships between global and local features.",
            "score": 0.6117875378820371,
            "section_title": "Related Work",
            "char_start_offset": 6016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 376,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 376,
                    "end": 393,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 393,
                    "end": 410,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 410,
                    "end": 426,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 476,
                    "end": 494,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 494,
                    "end": 499,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 499,
                    "end": 516,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 534,
                    "end": 551,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 596,
                    "end": 613,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 758,
                    "end": 781,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 914,
                    "end": 933,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 1382,
                    "end": 1400,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1633,
                    "end": 1652,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "221802641",
            "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants",
            "text": "Knowledge Distillation: Knowledge distillation is a popular research topic in the field of model compression [9,37]. We can extract distilled knowledge from a teacher network and transfer it to a student network to mimic the teacher network. The basic concept of knowledge distillation [14] is to compress the knowledge of a deeper or larger model to a single computational efficient neural network. After this study, extensive research was conducted on knowledge distillation. Remero et al. [28] introduced the transfer of a hidden activation output and Zagoruyko et al. [40] proposed transferring attention information as knowledge. Yim et al. [36] defined distilled knowledge from the teacher network as the flow of the solution process (FSP), which is calculated as the inner product between feature maps from two selected layers.\n\nRecently, Tung et al. [33] introduced similaritypreserving knowledge distillation guided training of a student network such that input pairs that produce similar activations in the teacher network produce similar activations in the student network. Zhang et al. [41] proposed selfdistillation in which student networks train the knowledge by themselves from deeper to shallower layers so that a teacher network is not required. Because the training of the pre-trained teacher model is not required, the time for training the student model can be reduced. Contrarily, Shen et al. [29] believed that a student network can learn knowledge efficiently from an ensemble of teacher networks; they proposed the use of an adversarial-based learning strategy with a block-wise training loss. For an online distillation framework, Zhang et al. [42] suggested that peer students learn from each other through the cross-entropy loss between each pair of students. Chen et al. [4] also suggested using peers where multiple student models train each student model based on auxiliary peers and one group leader.\n\nGuo et al. [8] proposed collaborative learning-based online knowledge distillation that trained students without a teacher where knowledge is transferred among arbitrary students during collaborative training.\n\nThere have also been recent attempts to break away from the traditional method. Xu et al. [35] showed that contrastive learning as a self-supervision task helps to gain more rounded knowledge from a teacher network. Yuan et al. [38] addressed the time",
            "score": 0.6105134399989927,
            "section_title": "Related Work",
            "char_start_offset": 5585,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "4142619"
                },
                {
                    "start": 572,
                    "end": 576,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1098,
                    "end": 1102,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1415,
                    "end": 1419,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 1670,
                    "end": 1674,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1945,
                    "end": 1948,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057281494140625
        },
        {
            "corpus_id": "257504799",
            "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
            "text": "The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in [1]. However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network. Besides the vanilla KD, researchers have explored more methods to extract more salient dark knowledge from the pre-trained teacher network to the student [28,15,26,27,31,30]. For example, Fitnets [28] proposed to use intermediate feature representation as a hint to guide the training of the student network. Tian et al. [30] first used the contrastive learning mechanism to extract the structural knowledge of the teacher network to supervise the training of the student.",
            "score": 0.6080273361095455,
            "section_title": "Conventional Knowledge Distillation",
            "char_start_offset": 3200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 922
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "236782270",
            "title": "Adaptive Teacher Finetune: Towards high-performance knowledge distillation through adaptive fine-tuning",
            "text": "Knowledge distillation [1] is a well-known technique to learn compact deep neural network models with competitive accuracy, where a smaller network (student) is trained to simulate the representations of a larger one (teacher) with higher accuracy. The popularity of knowledge distillation is mainly due to its simplicity and generality; it is straightforward to learn a student model based on a teacher and there is no restriction about the network architectures of both models. The main goal of most approaches is how to transfer dark knowledge to student models effectively, given predefifined or pretrained teacher networks. Parameter quantification or character recognition, pruning, and knowledge distillation (KD) are some well-known methods in this subject [1]. KD defines the class probability of the teacher system as the aim that the tiny contactors strives to replicate by transferring the knowledge of big pre-training networks (or integration of tiny networks) as a teacher network. Students can enhance their performance by matching their predictions with the teacher's predictions. There is no unique teacher-student function in provided to users refinement. Starting with the initial training, all systems learn at the same time by teaching each other. It uses ground truth labels to train traditional cross-entropy loss and imitation loss to learn from peers. The result obtained by the network trained in this online distillation method is not only better than the network trained only by cross-entropy loss, but it performs better than a network taught in the typical offline distillation method through an also before the teacher network Hinton et al. developed many prospective knowledge distillation approaches to promote the optimization process of distilling by using diverse \"information,\" such as intermediate representation, in [2], inter-layer flow [3], pay attention to figure [5], structure Relationship [6] and activation similarity [7]. While these 's system well in compressed deployment models, they often use a two-stage training approach, which involves before the a highly capable previous teachers before transferring knowledge to a compact student model, which takes more time and money.",
            "score": 0.6055580516378004,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 1907,
                    "end": 1910,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1935,
                    "end": 1938,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3681640625
        },
        {
            "corpus_id": "231648215",
            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
            "text": "Before we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
            "score": 0.6011706705271493,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7626,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 496,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 951,
                    "end": 972,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71337890625
        },
        {
            "corpus_id": "269327338",
            "title": "Transfer learning and self-distillation for automated detection of schizophrenia using single-channel EEG and scalogram images.",
            "text": "Knowledge Distillation denotes the procedure of transferring knowledge from a sophisticated model to a more straightforward one. It involves training smaller models to achieve similar accuracy as larger models by leveraging the knowledge gained from the larger models. Within the context of knowledge distillation, the term \"teacher network\" is used to describe the larger model, whereas the \"student network\" refers to the smaller network. The fundamental concept behind Knowledge Distillation is to train a smaller and less complex model to imitate the behavior and generalization capabilities of a larger and more complex model. The process of knowledge distillation involves transferring knowledge from the teacher network to the student network by optimizing a loss function [32]. Figure 6 [33] illustrates a typical framework for knowledge distillation, where a teacher-student relationship is established.",
            "score": 0.6006983108729769,
            "section_title": "Knowledge distillation",
            "char_start_offset": 17184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04931640625
        },
        {
            "corpus_id": "256900863",
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "text": "Knowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
            "score": 0.5965737149579841,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 8754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1583
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "233033712"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "209405263"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1188,
                    "end": 1192,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Since Hinton et al. [1] introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students. FitNets [18] distills intermediate features of a teacher network, where the student network transforms the intermediate features using guided layers and then calculates the difference between the guided layers and the intermediate features of teacher network. The position of distillation is shifted to the layers before the ReLU operations in [19], which also proposes the novel activation function and the partial L 2 loss function for effective knowledge transfer. Zagoruyko and Komodakis [20] argue importance of attention and propose an attention transfer (AT) method from teachers to students while Kim et al. [21] compute the factor information of the teacher representations using an autoencoder, which is decoded by students for knowledge transfer. Relational knowledge distillation (RKD) [22] introduces a technique to transfer relational information such as distances and angles of features. \n\nCRD [23] maximizes mutual information between a teacher and a student via contrastive learning. \n\nThere exist a couple of methods to perform knowledge distillation without teacher models. For example, ONE [24] distills knowledge from an ensemble of multiple students while BYOT [25] transfers knowledge from deeper layers to shallower ones. Besides, SSKD [26] distills self-supervised features of teachers to students for transferring richer knowledge.",
            "score": 0.5961504429209953,
            "section_title": "What to distill",
            "char_start_offset": 4237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "219636179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "254044469",
            "title": "Class-aware Information for Logit-based Knowledge Distillation",
            "text": "The concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits. Therefore, the studies on KD can be divided into two categories, i.e., logit-based distillation and feature-based distillation.",
            "score": 0.5950559247251486,
            "section_title": "Related Work",
            "char_start_offset": 4417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 533
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "Knowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
            "score": 0.5939056397517228,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2518
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 73,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "32588614"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "222310537"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 862,
                    "end": 866,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2091,
                    "end": 2095,
                    "matchedPaperCorpusId": "237734820"
                },
                {
                    "start": 2179,
                    "end": 2183,
                    "matchedPaperCorpusId": "225257208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6728515625
        },
        {
            "corpus_id": "251460030",
            "title": "Improving Deep Mutual Learning via Knowledge Distillation",
            "text": "Knowledge distillation in [8] is one of the most popular knowledge transfer methods today, and it uses a teacher-student framework. The basic idea of this method is that a pretrained teacher network (i.e., a cumbersome network or the biggest network) using certain hyperparameters are then used to train an untrained student network (i.e., small network) for the purpose of transferring knowledge. This process uses a distillation knowledge equation where a temperature (T) is involved and can be varied to obtain a soft probability output from a class C image which can be calculated as: \n\nSuppose the teacher network is marked as G t and the student network is G s , then the distillation loss can be defined as: \n\nAs a result, the student loss function contained in Figure 2 is minimized during the training process based on ( 6) and (7) as: \n\nwhere \u03bb is a balancing value between the two losses. The main purpose of the teacherstudent framework is to force the student output probability to imitate or match the pre-trained teacher network's probability output.",
            "score": 0.5937166135512917,
            "section_title": "DML and KD",
            "char_start_offset": 10613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09515380859375
        },
        {
            "corpus_id": "232147231",
            "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation",
            "text": "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data. The objective function of learning student M S is defined as below. \n\nwhere \u03bb kl and \u03bb ce are the weights of KL-divergence loss and cross entropy loss respectively, E denotes the ensemble operation of two models. In experiments, the ensemble operation is implemented by averaging the outputs of two complementary teachers. By integrating knowledge from two views and making full use of unlabeled data, we can obtain one student with even superior performance than any one of its teachers.",
            "score": 0.5862820359636806,
            "section_title": "Multi-teacher Knowledge Distillation",
            "char_start_offset": 13820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44580078125
        },
        {
            "corpus_id": "212855595",
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "text": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
            "score": 0.5855641613997551,
            "section_title": "Objective Functions",
            "char_start_offset": 15526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 524,
                    "end": 542,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1267,
                    "end": 1285,
                    "matchedPaperCorpusId": "29308926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "236984375",
            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
            "text": "Knowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
            "score": 0.5855412793615322,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 15408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "225066713",
            "title": "Empowering Knowledge Distillation via Open Set Recognition for Robust 3D Point Cloud Classification",
            "text": "Interestingly, there is also a slight improvement in the open set capabilities of the distilled student model, which we attribute to the supervision recieved from the teacher network during the proposed distillation training. This finding implies that the along with the transfer of dark knowledge from teacher to student as proposed by [6], distillation process also transfers open set capability to the distilled student.",
            "score": 0.5820293822370226,
            "section_title": "Knowledge Distillation Performance",
            "char_start_offset": 23750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 423
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152587890625
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "The concept of knowledge distillation (KD) was first introduced by Hinton and Dean [19] as a model compression framework which releases the parameter size of the deep learning model by constructing a teacher-student paradigm where the student network is trained to capture the information contained not only in the hard version of the true label, but also in the softer version of the teacher's output. Different from the ordinary transfer learning, the knowledge distillation accomplishes the knowledge transference tasks by altering the loss function of the student model to follow the output of the teacher model [20]. The traditional KD framework compresses one or several cumbersome networks (teachers) into a student network with a shallow structure. The framework of the conventional knowledge distillation can be categorized into two types: single teacherbased knowledge distillation and multi-teacher-based knowledge distillation [19,[21][22][23][24].",
            "score": 0.5813386451109892,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 960
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "209078813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05987548828125
        },
        {
            "corpus_id": "269362788",
            "title": "Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities",
            "text": "Knowledge distillation utilizes additional supervisory information from the pre-trained teacher's network to assist in the training of the student's network [11].Knowledge distillation methods can be roughly categorized into two types, distillation from intermediate features [15,29,38,61] and responses [4,8,27,48,68].Many studies [13,18,33,40,47] employ knowledge distillation for MSA tasks with missing modalities.The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model.For instance, [13] utilizes the complete-modality teacher network to implement supervision on the unimodal student network at both feature and response levels.Despite promising outcomes, they are subject to several significant limitations: (i) Knowledge transfer is limited to individual samples, overlooking the exploitation of clear correlations among samples and among categories.(ii) Supervision on student networks is coarse-grained and inadequate, without considering the potential alignment of feature distributions.To this end, we propose a correlation-decoupled knowledge distillation framework that facilitates the learning of robust joint representations by refining and transferring the crosssample, cross-category, and cross-target correlations.",
            "score": 0.5805118672159589,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 286,
                    "end": 289,
                    "matchedPaperCorpusId": "231855771"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 307,
                    "end": 309,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 312,
                    "end": 315,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "5299559"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "221543802"
                },
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "245445463"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "263605398"
                },
                {
                    "start": 696,
                    "end": 700,
                    "matchedPaperCorpusId": "221543802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "271544434",
            "title": "Accelerating the Low-Rank Decomposed Models",
            "text": "Knowledge Distillation (KD) uses a teacher-student framework in order to transfer the knowledge from a larger network (teacher) into a compact and efficient one (student) by adding and auxiliary loss to imitate softmax outputs or logits from the teacher as a representation of class distributions (Hinton et al., 2015;Rashid et al., 2021). In this method, the architecture of the student could be totally different from that of the teacher. The student model tries to mimic the behaviour of the teacher through distilling knowledge from the teachers output or intermediate layers (Mirzadeh et al., 2020). \n\nThe theory behind how KD works is still an open question in the literature, however, there are some works explaining the contribution of adding class similarity information in the output of the teacher (which is reffered to as the dark knowledge as well), or regularization effects of the KD loss as potential reasons. However, these methods have poor mathematical support and could face some serious limitations in high compression ratios.",
            "score": 0.5790701007056989,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1047
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 603,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304931640625
        },
        {
            "corpus_id": "189762324",
            "title": "Linear Distillation Learning",
            "text": "Knowledge distillation (KD) is a method of transferring \"knowledge\" from one machine learning model called teacher to another one called student. The idea behind KD is that a teacher network is a high-capacity model with desired high performance and a student network is a more lightweight model [1,19,27]. A student cannot match the teacher, but the distillation process brings the student closer to the predictive power of the teacher. Distillation idea was brought to the neural network community by Hinton et. al. [7]. \n\nIn distillation learning, knowledge is transferred by training a student model, using a soft target distribution for comparison with the output layer. This distribution is produced by a cumbersome model with a high temperature in its output softmax \n\nwith z i are logits and T is temperature. Another scenario of knowledge distillation training is transferring knowledge from an ensemble of highly regularized models to a smaller model [7]. \n\nDistillation can also be applied for adversarial permutation [15], born-again neural networks [6] and Global Additive Explanations [26]. Furthermore, Sau and Balasubramanian [20] proposed to add random perturbations into soft labels for simulating learning from multiple teachers. \n\nSurprisingly, a distillation method often allows smaller student network to be trained to mimic the larger and deeper models very accurately, while the student trained on the one-hot hard targets cannot achieve the same results. The clear reason for this awaits to be discovered.",
            "score": 0.5771217275334216,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1530
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "1923568"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "2672720"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "53531803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04742431640625
        },
        {
            "corpus_id": "236987020",
            "title": "Learning from Matured Dumb Teacher for Fine Generalization",
            "text": "Knowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- [16]. The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], [17] To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth.",
            "score": 0.5739544318141127,
            "section_title": "II. RELATED WORK A. Knowledge Distillation",
            "char_start_offset": 3835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4423828125
        },
        {
            "corpus_id": "257346971",
            "title": "An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment",
            "text": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the information is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively. In total, 9 sizes were obtained from clustering, e.g., (16,23), (32,45), (34,26), (39, 68), (74, 48), (82, 123), (136, 98), (187, 231) and (386, 334).",
            "score": 0.5735898203318188,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 966
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 884,
                    "end": 887,
                    "matchedPaperCorpusId": "237825585"
                },
                {
                    "start": 893,
                    "end": 896,
                    "matchedPaperCorpusId": "29535948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "234292017",
            "title": "Knowledge from the original network: restore a better pruned network with knowledge distillation",
            "text": "Knowledge distillation is a knowledge transfer technology widely used in computer vision [23], natural language processing [24], and other deep learning fields. The vanilla knowledge distillation strategy was proposed by Hinton in 2015 [14]. In the vanilla method, the softened outputs of the logits layer of a robust, high-accuracy, and well pretrained network, are used to guide and supervise the outputs of the student network (often a smaller network). It is considered that the dark knowledge hidden in the output of the teacher network's logits layers is used to improve the student network's performance. Knowledge distillation has achieved outstanding results. In the continuous development, response-based knowledge, feature-based knowledge, relation-based knowledge [6], and other knowledge distillation methods based on different knowledge have been gradually proposed. Despite the different knowledge definitions and distillation methods, the goal is similarly to approximate the representation of the student network to the teacher network. When it comes to the effects of knowledge definition, the structural differences between the networks are very important. [25] also finds that networks with similar structures are easier to transfer knowledge. Therefore, in this paper, the knowledge distillation between the sub-networks of the original network is used to minimize the structural differences.",
            "score": 0.5730973251008145,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7839,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 93,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "70346512"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478271484375
        },
        {
            "corpus_id": "270389751",
            "title": "Multistage feature fusion knowledge distillation",
            "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "score": 0.5720888037592825,
            "section_title": "Knowledge distillation",
            "char_start_offset": 2815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 489
                },
                {
                    "start": 491,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2204
                },
                {
                    "start": 2204,
                    "end": 2387
                },
                {
                    "start": 2387,
                    "end": 2522
                },
                {
                    "start": 2522,
                    "end": 2673
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 70,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 955,
                    "end": 957,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1251,
                    "end": 1253,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1582,
                    "end": 1584,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 2208,
                    "end": 2210,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2391,
                    "end": 2393,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "270389751",
            "title": "Multistage feature fusion knowledge distillation",
            "text": "ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.In the realm of multistage knowledge distillation methods, TSKD 23 effectively enhances the testing accuracy of student networks through multistage guidance from teacher networks.OtO 24 employs a joint multistage to multistage training approach between teacher and student networks, achieving significant improvements in multistage knowledge distillation.\n\nIn contrast to the above methods, our experiment utilizes a multistage feature fusion knowledge distillation approach.This approach effectively addresses the challenges of feature distribution disparities in knowledge transfer, resulting in a substantial enhancement of recognition accuracy for lightweight models.",
            "score": 0.5703090228839366,
            "section_title": "Knowledge distillation",
            "char_start_offset": 5337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 506
                },
                {
                    "start": 508,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 822
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 217,
                    "matchedPaperCorpusId": "248683566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1849365234375
        },
        {
            "corpus_id": "255942245",
            "title": "Dataset Distillation: A Comprehensive Review",
            "text": "Knowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
            "score": 0.5696678933978105,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "227232038"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1218,
                    "end": 1222,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 1455,
                    "end": 1459,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1857,
                    "end": 1861,
                    "matchedPaperCorpusId": "159041346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1263427734375
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "The concept of knowledge distillation (KD) was first introduced by Hinton and Dean [19] as a model compression framework which releases the parameter size of the deep learning model by constructing a teacher-student paradigm where the student network is trained to capture the information contained not only in the hard version of the true label, but also in the softer version of the teacher's output. Different from the ordinary transfer learning, the knowledge distillation accomplishes the knowledge transference tasks by altering the loss function of the student model to follow the output of the teacher model [20]. The traditional KD framework compresses one or several cumbersome networks (teachers) into a student network with a shallow structure. The framework of the conventional knowledge distillation can be categorized into two types: single teacher-based knowledge distillation and multi-teacher-based knowledge distillation [19,[21][22][23][24].",
            "score": 0.568930310049056,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 961
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "209078813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06268310546875
        },
        {
            "corpus_id": "269317596",
            "title": "Shared Knowledge Distillation Network for Object Detection",
            "text": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
            "score": 0.5680589431467109,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "265351966",
            "title": "Leveraging different learning styles for improved knowledge distillation in biomedical imaging",
            "text": "Knowledge Distillation [17] is an approach introduced to transfer the knowledge in terms of probability outputs, p i , from a complex, highly parameterized pre-trained teacher network f (X, \u03d5) to a simple and compact student network g(X, \u03b8) to achieve model compression while retaining the high performance of the teacher.Given a training set with N samples X = {x i } N i=1 with corresponding labels Y = {y i } N i=1 , the teacher network f (X, \u03d5), is trained on the ground truth labels.The probabilistic output of a teacher network for a sample x i is defined as p i given by the extended softmax as:\n\nwhere z c corresponds to the logits, C is the number of classes, and T is the temperature parameter to get a smoother output\n\nPredictions, Predictions Features, Features Features, Predictions T \u2192 S 2 , S 1 \u2192 S 2 Predictions, Predictions Features, Features Predictions, Features probability distribution of the classes.Generally, the objective function for the teacher network is the standard Cross-Entropy (CE) error defined as:\n\nNow, the student networks are trained on the combined loss of Cross-Entropy (CE), and Knowledge Distillation (KD), where the CE helps the student networks to adhere to the ground truth labels and KD assists them to align their learning with that of the teacher.Here, Kullback Leibler (KL) divergence [28] is used for L KD p to measure the correspondence between the teacher and student predictions p i and s i respectively as:\n\nFinally, the loss function for the student network is the weighted (\u03b1) summation of the cross entropy (L CE ) and knowledge distillation (L KD p ) terms:\n\nwhere hyperparameter \u03b1 is used to balance the contributions of the hard target loss (L CE ) and soft target loss (L KD p ) during the distillation process for each student.The knowledge can be transferred in an online or offline manner from the teacher to the student networks.",
            "score": 0.5676311168926418,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 602
                },
                {
                    "start": 604,
                    "end": 728
                },
                {
                    "start": 730,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1032
                },
                {
                    "start": 1034,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1460
                },
                {
                    "start": 1462,
                    "end": 1615
                },
                {
                    "start": 1617,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1463623046875
        },
        {
            "corpus_id": "204955470",
            "title": "MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization",
            "text": "Knowledge distillation [10] is an effective and popular approach for model compression by distilling a complex teacher model to a simpler student model. The success of transferring the dark knowledge between networks has inspired many novel research work in com-puter vision [15][20] [7]. Recently, researchers find that, rather than the one-way knowledge transfer, enabling collaborative learning of several simple student models with a two-way knowledge sharing can achieve superior results [30][29] and can be efficiently trained within a distributed training system [2].",
            "score": 0.5658435573702663,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 574
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "6702706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1929931640625
        },
        {
            "corpus_id": "238227092",
            "title": "Prune Your Model Before Distill It",
            "text": "Knowledge distillation (KD) [20] transfers the knowledge from the strong teacher network to a smaller student network. The student network is trained with soft targets provided by the teacher network and some intermediate features [40,53,57]. There are variations of KD such as KD using GAN [52], Jacobian matching KD [5,45], distillation of activation boundaries [19], contrastive distillation [49], and distillation from graph neural networks [23,53]. \n\nRecently, many works have reported that the large gap between student and teacher causes degradation in student network performance [36]. Cho and Hariharan showed that the less-trained network transfers better knowledge to a small network [4]. Park et al. [37] proposed a student-aware teacher learning to transfer the teacher's knowledge effectively. In this paper, we provide an extremely simple way to generate a student-friendly teacher using unstructured pruning.",
            "score": 0.5645276306281236,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 924
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "4916078"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "231925118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047882080078125
        },
        {
            "corpus_id": "227247945",
            "title": "An Once-for-All Budgeted Pruning Framework for ConvNets Considering Input Resolution",
            "text": "Full Model [26] 314M 72.2% GFS [32] 258M 71.9% GOPWL [33] 245M 72.2% OFARPruning(Ours) 253M 74.0% Inplace Distillation with Teaching Assistant. Knowledge distillation is regarded as an effective method for improving the accuracy of compact network by transferring teacher network information to the student network which is widely used in practice. However, it is found that when the teacher network and the student network have a large gap in information extraction ability, the student network cannot effectively learn the large amount of detailed knowledge of the teacher network. Therefore, a medium size network is used as a teaching assistant network between the teacher network and the student network to process the information. Therefore, in our proposed framework, we form a teacher-teaching assistant-student structure to make up for the gap. According to the ablation experiment, such distillation structure is better than ordinary distillation methods. Batch Normalization Statics. Since OFARPruning can customize the number of compact networks, we can use the BN storage method in S-Net or the post-statistics method of BN in US-Net. When the edge devices resources do not change much dynamically, in other words, once train-ing does not require many compact networks, the former statistical method can be used. The latter is more suitable when the resource range of the edge devices is large and many compact networks are required for once training.",
            "score": 0.5615165917228758,
            "section_title": "Network FLOPs Top-1 Accuracy",
            "char_start_offset": 9185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1464
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 15,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "226222209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01473236083984375
        },
        {
            "corpus_id": "235186864",
            "title": "KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent Reinforcement Learning",
            "text": "As mentioned earlier, KD has achieved great success in the field of computer vision though extracting dark knowledge. The key to extracting dark knowledge is to get the soft probability output of trained teachers with distillation. Here, we let a t be the input log of the final softmax layer of the teacher network, where a t = [a 1 , a 2 , ...., a j ]. The logits are converted into probabilities q t = [q 1 , q 2 , ...., q j ] using the following softmax function: q i = e a i \u03a3je a j where i represents the i-th neuron. [3] proposed softening the teacher probabilities with temperature T to extract more knowledge: \n\nCompared to true labels from datasets, the soft output of the teacher provides more information. Based on the same input x, the teacher and student networks produce probabilities q t (x) and q s (x) with Equation ( 4) respectively. The gap between q t (x) and q s (x) is usually penalised by the Kullback-Leibler (KL) divergence (Equation ( 6)): \n\n(5) \n\nwhere P(x) and Q(x) are two probability distributions of the random variable x. KD inspires us to believe that minimizing the gap between agent models through the skill of distillation is the essence of knowledge sharing. We then draw upon the KD thought in our MARL research to share knowledge and verify the feasibility of KnowSR in Section 3.3.",
            "score": 0.5605874449914026,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "250627348",
            "title": "Learning with Recoverable Forgetting",
            "text": "Knowledge transfer aims at transferring knowledge from networks to networks. Here, we mainly discuss the related works in knowledge distillation [15,14,54], which trains a student model of a compact size by learning from a larger teacher model or a set of teachers handling the same task. It has been successfully conducted in deep model compression [61], incremental learning [37], continual learning [25,57] and other tasks other than classification [5,53,22,50,36,60,51,55,23]. \n\nIn addition to the above methods that transfer knowledge from one network to another, it could happen in plenty forms. Such as for combining or amalgamating multi-source knowledge, Gao et al. [12] introduce a multi-teacher and single-student knowledge concentration approach. And in order to handle multitask problems in one single network, knowledge amalgamation [56] is proposed to train the student network on multiple scene understanding tasks, leading to better performance than the teachers. To make it further, Ye et al. [58] apply a two-step filter strategy to customize the arbitrary task set on TargetNet. Besides, the multi-stage knowledge transfer is enabled by Yuan et al. [62] to design a multi-stage knowledge distillation paradigm to decompose the distillation process. \n\nKnowledge distillation could also be a reliable method to transfer knowledge from old data to new data, and there are also some distillation-based works [8,21,45,11] for solving the coming new data in life-long learning setting. Cheraghian et al. [8] address the problem of few-shot class incremental learning by utilizing the semantic information. Hu et al. [21] derive a distillation method to retain the old effect overwhelmed by the new data effect, and thus alleviate the forgetting of the old class in testing. \n\nThese knowledge transfer methods transfer knowledge from networks to networks, we make the first work to filter and deposit the knowledge.",
            "score": 0.5569608355313028,
            "section_title": "Knowledge Transfer",
            "char_start_offset": 5055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "201805763"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "214713848"
                },
                {
                    "start": 377,
                    "end": 381,
                    "matchedPaperCorpusId": "3285974"
                },
                {
                    "start": 406,
                    "end": 409,
                    "matchedPaperCorpusId": "244920778"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "222291105"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "244729033"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "244478080"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "247748676"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "214605947"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "128358417"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "167217419"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "209078813"
                },
                {
                    "start": 1424,
                    "end": 1427,
                    "matchedPaperCorpusId": "232147521"
                },
                {
                    "start": 1427,
                    "end": 1430,
                    "matchedPaperCorpusId": "232092679"
                },
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "235306374"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "232147521"
                },
                {
                    "start": 1630,
                    "end": 1634,
                    "matchedPaperCorpusId": "232092679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018829345703125
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Knowledge distillation [1] is a well-known technique to learn compact deep neural network models with competitive accuracy, where a smaller network (student) is trained to simulate the representations of a larger one (teacher). The popularity of knowledge distillation is mainly due to its simplicity and generality; it is straightforward to learn a student model based on a teacher and there is no restriction about the network architectures of both models. The main goal of most approaches is how to transfer dark knowledge to student models effectively, given predefined and pretrained teacher networks. \n\nAlthough knowledge distillation is a promising and convenient method, it sometimes fails to achieve satisfactory performance in terms of accuracy. This is partly because the model capacity of a student is too limited compared to that of a teacher and knowledge distillation algorithms are suboptimal [2,3]. In addition to this reason, we claim that the consistency of teacher and student features is critical to knowledge transfer and the inappropriate representation learning of a teacher often leads to the suboptimality of knowledge distillation. \n\nWe are interested in making a teacher network hold better transferable knowledge by providing the teacher with a snapshot of the student model at the time of its training. We take advantage of the typical structures of convolutional neural networks with multiple blocks and make the representations of each block in teachers easy to be transferred to students. The proposed approach aims to train  teacher models friendly to students for facilitating knowledge distillation; we call the teacher model trained by this strategy student-friendly teacher network (SFTN). SFTN is deployed in arbitrary distillation algorithms easily due to its generality for training models and transferring knowledge. \n\nSFTN is partly related to collaborative learning methods [4,5,6], which may suffer from the correlation between the models trained jointly and fail to fully exploit knowledge in teacher models. On the other hand, SFTN is free from the limitation since it performs knowledge transfer from a teacher to a student in one direction via a two-stage learning procedure-student-aware training of teacher network followed by knowledge distillation from a teacher to a student. Although the structure of a teacher network depends on target student models, it is sufficiently generic to be adopted by students with various architectures.",
            "score": 0.5568989110350908,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2488
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 912,
                    "end": 914,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 1918,
                    "end": 1921,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1921,
                    "end": 1923,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
            "score": 0.5568053227737461,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 14355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 182,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 894
                },
                {
                    "start": 894,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 2405
                },
                {
                    "start": 2407,
                    "end": 2516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "Despite its huge popularity, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training. The so-called dark knowledge transferred in the process helps the student learn the finer structure of teacher network. Hinton, Vinyals, and Dean (2015) argues that the success of knowledge distillation is attributed to the logit distribution of the incorrect outputs, which provides information on the similarity between output categories. Furlanello et al. (2018) investigated the success of knowledge distillation via gradients of the loss where the soft-target part acts as an importance sampling weight based on the teachers confidence in its maximum value. Zhang et al. (2017) analyzed knowledge distillation from the posterior entropy viewpoint claiming that soft-targets bring robustness by regularizing a much more informed choice of alternatives than blind entropy regularization. Last but not least, Lopez-Paz et al. (2015) studied the effectiveness of knowledge distillation from the perspective of learning theory (Vapnik 1998) by studying the estimation error in empirical risk minimization framework. \n\nIn this paper, we take this last approach to support our claim on the effectiveness of introducing an intermediate network between student and teacher. Moreover, we empirically analyze it via visualizing the loss function.",
            "score": 0.5553372498824865,
            "section_title": "Related Work",
            "char_start_offset": 6977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1390
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 302,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 713,
                    "end": 732,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 961,
                    "end": 984,
                    "matchedPaperCorpusId": "3323727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377197265625
        },
        {
            "corpus_id": "224801420",
            "title": "Locally Linear Region Knowledge Distillation",
            "text": "Logit-based approaches [6] construct the distillation objective based on output logits. Hinton et al. [6] propose KD which uses the softened logits of a teacher network as the targets to train a student network. The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance. Park et al. [11] point out that KD only considers knowledge transfer on individual samples and thus they propose to transfer mutual relations of data examples from a teacher to a student by penalizing logit-based structural differences between them. Zhao et al. [12] consider the information in training process for knowledge distillation by employing two teachers. One teacher uses its temporary output logits during the training process to supervise the student step by step, which assists the student to find the optimal path towards the final logits. The other teacher provides the information about a critical region that is more useful for the task.",
            "score": 0.5538353148245415,
            "section_title": "A. Logit-based Distillation Approaches",
            "char_start_offset": 6947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1010
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "198179767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "265384964",
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "score": 0.5528205354748499,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2403
                },
                {
                    "start": 2404,
                    "end": 2463
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "257427514",
            "title": "Learn More for Food Recognition via Progressive Self-Distillation",
            "text": "The concept of knowledge distillation was firstly proposed in (Hinton et al. 2015). In a learning paradigm of knowledge distillation, a bigger teacher network guides the training of a smaller student network to transfer its knowledge to  To enhance efficiency and effectiveness in knowledge transferring, self knowledge distillation is proposed to utilize knowledge from itself, without the involvement of extra networks. For example, aggregation of various distortion data is used to achieve self-distillation (Lee, Hwang, and Shin 2020). Motivated by this, we propose a progressive self-distillation method to mine more details for food recognition.",
            "score": 0.5523475549707305,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7733,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 538,
                    "matchedPaperCorpusId": "220249782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0119171142578125
        },
        {
            "corpus_id": "269982135",
            "title": "Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch",
            "text": "D EPLOYING large-scale neural networks on portable de- vices with limited computation and storage resources is challenging [1], and efficient architectures such as Mo-bileNets [2], [3] and ShuffleNets [4], [5] have been designed for lightweight deployment.However, the performances of these lightweight networks are usually not comparable to the larger ones.Commonly, second learning [6], [7] or knowledge distillation (KD) [8]- [10] could be utilized to transfer the knowledge of a more complex and well-performed network (i.e., the teacher) to the smaller ones (i.e., the student).The dark knowledge in KD is still a mystery that has attracted lots of studies [9], [11]- [13], and their goal is to answer the following question: what's the knowledge that the teacher provides and why they are effective in KD?\n\nIn the original KD method [9], the student aims to mimic the teacher's behavior by minimizing the Kullback-Leibler (KL) divergence between their output probabilities.That is, the logits and softened probabilities, i.e., the inputs to the final softmax operator and the corresponding outputs, are the specific knowledge transferred in KD.With the development of KD methods, the output-level knowledge has been extended to various types [14], including the intermediate features [15]- [20], the sample relationships [21]- [27], the parameters [28], [29], and the collaborative or online knowledge [10], [30] etc.However, the outputs of neural networks are much easier to visualize, analyze, and understand.Therefore, we focus on the original KD [9] and aim to understand the dark knowledge (i.e., the logits and softened probabilities) provided by the teachers.Unlike previous studies, we majorly study the output-level dark knowledge provided by teachers with various capacities, which receives little attention in previous studies.",
            "score": 0.5520874456663969,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1150
                },
                {
                    "start": 1150,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "12670695"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "51880435"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "3039694"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "1024861"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 662,
                    "end": 665,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 839,
                    "end": 842,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1290,
                    "end": 1294,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "243860797"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 1354,
                    "end": 1358,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "3603048"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "232380330"
                },
                {
                    "start": 1556,
                    "end": 1559,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3310546875
        },
        {
            "corpus_id": "267657497",
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "score": 0.5517006978554995,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "258298441"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "269167845"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "269206209"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "260933721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68896484375
        },
        {
            "corpus_id": "234032397",
            "title": "Robust CNN Compression Framework for Security-Sensitive Embedded Systems",
            "text": "Knowledge distillation is a technique to transfer the information of a network (teacher) to another network (student) by minimizing the gap between the SoftMax outputs of the two networks. In our framework, we use a pretrained original network as the teacher and provide its SoftMax output to a student network being compressed. We summarize our contribution as follows:",
            "score": 0.5495655954501208,
            "section_title": "Introduction",
            "char_start_offset": 3919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 370
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "53165244",
            "title": "Cogni-Net: Cognitive Feature Learning Through Deep Visual Perception",
            "text": "Consequently, another driving idea on which our work is based on is that of teacher-student networks [16,17] and the transfer of dark knowledge [21] between two different models. The dark knowledge transfer successfully compresses the required discriminative knowledge from a cumbersome, complex and deep model to a comparatively lighter model without the considerable degradation in performance but with the added benefit of being able to deploy it more efficiently under realtime circumstances. Though previous works [16,21] have used this technique for the transfer of knowledge within the same modality, the authors of [9] has applied the same technique for cross-modal knowledge transfer with considerable success. Here, we have also applied the concept of crossmodal transfer of knowledge between the teacher and student networks where the teacher network operates on vision to Our contribution through this work is three fold. First, to the best of our knowledge, we make a novel contribution using knowledge distillation for brain signal classification. Second, the proposed brain signal network, CogniNet, when trained under unsupervised settings acts as a novel feature extractor and shows competitive performance while handling unseen signal categories. Thirdly, we also show that when trained under supervised settings, our model easily beats the current state-of-the-art algorithm for brain signal classification.",
            "score": 0.5487778883935328,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1426
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "6832420"
                },
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "2915490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418701171875
        },
        {
            "corpus_id": "247521335",
            "title": "Generalized Knowledge Distillation via Relationship Matching",
            "text": "Knowledge Distillation (KD). Rich supervision plays a crucial role in building a machine learning or visual recognition system, where taking advantage of the learning experience from related pre-trained models becomes a shortcut to facilitate the model training in the current task [29]. Different from fine-tuning [30] or weights matching [31], [32], [33], [34], [35], [36] that regularize the model from the \"parameters\" perspective, we can reuse the dark knowledge/privileged information [37], [38], [39] to explain or assist the training process of the model from the \"data\" aspect [40], [3]. Denote a fixed well-trained model from a related task and the model in the current task as the \"teacher\" and the \"student\", respectively. KD matches the behaviors of two models on the current task's data [41], [42], [19]. The teacher could be a highcapacity deep neural network trained on the same task [13], [9], [43] or a previous generation of the model along the training progress [44], [7], [8]. The dark knowledge in KD can be implemented as the soft label, i.e., the posterior probability of an instance [3], [45], [17], hidden layer activation [5], [46], [47], [48], parameter flows [4], transformations [49], and pathwise statistics [50]. Distilling the knowledge from one model to another has been investigated for model compression [2], [11], [12], [51] and incremental learning [52], [9], [43]. Cross-task knowledge transfer. In practical applications, the knowledge from teachers of other tasks, i.e., teachers trained on non-overlapping sets of labels, also assists the training of a target task student. Heterogeneous transfer learning updates both student and teacher on the current and related domains (resp. tasks) to close distribution (resp. label) divergence [22], [53].",
            "score": 0.547992800801794,
            "section_title": "RELATED WORK",
            "char_start_offset": 6355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1788
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "3444548"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "13740328"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "8628903"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "3603048"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "46972534"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "219489919"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "12342641"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "43829067"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "226841742"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 1108,
                    "end": 1111,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "16391544"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "222178995"
                },
                {
                    "start": 1340,
                    "end": 1343,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1387,
                    "end": 1391,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "226841742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474853515625
        },
        {
            "corpus_id": "264516404",
            "title": "Attention and feature transfer based knowledge distillation",
            "text": "The concept of knowledge distillation (KD) was proposed by Hinton et al. 21 , which forced the student network to extract knowledge from the soft labels and ground truth labels provided by teachers. In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced. The existing KD methods can be mainly divided into three types: logic-based 20,21,[31][32][33][34] , feature-based 18,[22][23][24][25][26][27][28][29]35 , and attention maps-based 19,30 . \n\nLogic distillation transfers the knowledge implicit in the output logic of the teacher model to the student network. BAN 32 obtained superior performance to the teacher model by directing the same parameterized network as the teacher. DKD 20 reformulates KD loss into target-class knowledge distillation (TCKD) and non-target-class knowledge distillation (NCKD), revealing that KD's coupling formula limits the effectiveness and flexibility of knowledge transfer. CrossKD 34 passes intermediate features of the student network to the teacher's detection head, resulting in cross predictions, which are then forced to mimic the teacher's predictions. In addition, there are several articles on logical distillation methods 21,33,34 . \n\nFeature-based KD methods tend to have better performance, forcing students to extract valid content from intermediate features of the teacher network at the cost of requiring more computation than logical distillation. RKD 25 can transform the relationship of data examples to punish differences in teacher and student relevance, similar to the transfer of sample relevance studies from teacher and student networks 26,27 . PKT 35 models the teacher's knowledge as a probability distribution and uses KL divergence to measure distance. RKD 25 uses multicase relationships to guide students' learning. CRD 22 combines comparative learning with knowledge distillation, and uses comparative objectives to carry out knowledge transfer. ReviewKD 18 uses cross-layer connection paths to integrate the knowledge implied by features at different levels. \n\nKD method based on attention diagram instructs students what information the network should pay attention to in reasoning.",
            "score": 0.5470801748063799,
            "section_title": "Related work",
            "char_start_offset": 2593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "229220499"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 629,
                    "end": 631,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 747,
                    "end": 749,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1466,
                    "end": 1468,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1659,
                    "end": 1662,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1662,
                    "end": 1664,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1783,
                    "end": 1785,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "244488325",
            "title": "Semi-Online Knowledge Distillation",
            "text": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance.",
            "score": 0.5454814634962041,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2489013671875
        },
        {
            "corpus_id": "259228825",
            "title": "Research on Safety Helmet Detection Algorithm Based on Improved YOLOv5s",
            "text": "Knowledge distillation is a technique utilized to extract the knowledge of a large teacher model and condense it into a small student model. It can be understood as a large teacher neural network teaching his knowledge to a small student network [35][36][37]. \n\nThe process is transferred from the teacher network to the student network. The teacher network is generally bloated; therefore, the teacher network provides knowledge to the student network. The student network is a relatively small network and can thus obtain a lightweight network model. Knowledge distillation adopts the teacher-student mode. In this mode, the teacher is the output party of \"knowledge\", and the student is the receiver of \"knowledge\" [38]. \n\nThe teacher has a strong learning ability and can transfer the learned knowledge to the student model with a lower learning ability, so as to improve the generalization ability of the student model. The complicated and cumbersome but easy-to-use teacher model has no upper limit; it is purely a tutor, and in reality, a simple and flexible student model is deployed. The knowledge distillation process is shown in Figure 9   First, distill a deeper teacher network with a better extraction ability to obtain a logit, and distill it at T temperature. Then, use the classification prediction probability distribution in the Softmax layer to obtain soft targets. At the same temperature T, the logits in the student network are distilled, and then the category prediction probability distribution in Softmax is used to obtain the loss function L soft . Its expression is: \n\nwhere C j is the true label value of the j-th class. Finally, L hard and L soft are weighted and summed to obtain the final loss function L. This loss function can prevent the wrong information from the teacher network from being transmitted to the student network by comparing it with the real label. In this study, the improved YOLOv5s model was used as the teacher network, and the YOLOv5s model with the large target detection layer removed by structural pruning was used as the student model for knowledge distillation to obtain the final model and reduce the amount of calculation and parameters of the improved network model.",
            "score": 0.5447424521486605,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 38607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "250957345"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "231969372"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "244283694"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033477783203125
        },
        {
            "corpus_id": "212855595",
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "text": "knowledge distillation is an effective framework to transfer knowledge from a neural network to another, which typically consists of two branches: a teacher T , which is usually a complex model or accompanied by some extra knowledge, and a student S, which is a small network that learns from the teacher (Hinton, Vinyals, and Dean 2015). In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model: \n\nwhere L S GT is the ground-truth loss using one-hot labels, L KD is the knowledge distillation loss using teacher's soft labels and \u03bb is the coefficient to trade off such two terms. Typically, L GT is often the cross entropy loss in classification problems, and L KD is the Kullback-Leibler divergence to quantify the difference of output distribution from student to teacher: \n\nHenebry was 86 and lived in St Martinville. [0, \u2026, 0.69, \u2026, 0.17, ..., 0.14 where G is the one-hot distribution of ground-truth and G(i) is the i-th element of G. The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al. 2017).",
            "score": 0.5444055664263173,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1782
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 337,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1763,
                    "end": 1780,
                    "matchedPaperCorpusId": "29308926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "245974615",
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "text": "Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) [6] and popularized by Hinton, Vinyals, and Dean (2015) [2] knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Knowledge distillation has been commonly used in a number of learning tasks since then. Modeling knowledge transfer between teacher and student has also been done using adversarial methods. Using several teachers was still a good way to improve robustness. Some studies also proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. \n\nThe main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider. \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss and is given as, \n\nIn knowledge distillation one also tries to match the softened outputs of teacher y t =softmax(a t ) and student y s =softmax(a s ) via Kullback-Leibler divergence loss, \n\nby using a temperature parameter \u03c4 which has an additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss equation which used KD loss and cross-entropy loss, \n\nwhere \u03bb is the parameter used to trade-off between these two losses. This method is used for knowledge distillation from teacher and student models.",
            "score": 0.5438203328011032,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1610107421875
        },
        {
            "corpus_id": "21679091",
            "title": "Knowledge Distillation with Adversarial Samples Supporting Decision Boundary",
            "text": "Knowledge distillation is a method to enhance the training of a new network based on an existing, already trained network. In a teacher-student framework, the existing network is considered as a teacher and the new network becomes a student. Hinton, Vinyals, and Dean (2015), a pioneer in knowledge distillation, proposed a loss minimizing the cross-entropy between the outputs of the student and the teacher, which referred to as the knowledge distillation loss (KD loss). Due to the KD loss, the student network is trained to be a better classifier than the network trained without knowledge distillation. Although the goals of the knowledge distillation are diverse, recent studies (Yim et al. 2017;Chen et al. 2017) focus on improving a small student network using a large network as a teacher using a large teacher network. These studies aim to create a small network with the speed of a small network and the performance of a large network. This paper, too, focuses on knowledge distillation in the respect of enhancing the performance of a small network using a large network. \n\nMany of recent studies are focusing on manipulating the KD loss for various purposes. Romero et al. (2015) and Zagoruyko and Komodakis (2016) proposed new distillation losses to transfer the hidden layer response of the network and used it with the KD loss. Chen et al. (2017) and Wang and Lan (2017) designed new distillation losses for other applications based on the KD loss. In contrast to these existing approaches that concentrate on how to manipulate various parts of a network in order to improve the effect of knowledge distillation, in this paper, we investigate informative samples for an effective knowledge transfer. In general, samples near the decision boundary of a classifier have a larger impact on the performance than those far apart from it (Cortes and Vapnik 1995). Therefore, if we can generate samples close to the decision boundary, the knowledge of a teacher network would be transferred more effectively by utilizing those samples.",
            "score": 0.5436625240014206,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 274,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 685,
                    "end": 702,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1172,
                    "end": 1192,
                    "matchedPaperCorpusId": "2930547"
                },
                {
                    "start": 1848,
                    "end": 1872,
                    "matchedPaperCorpusId": "52874011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2178955078125
        },
        {
            "corpus_id": "272564024",
            "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals",
            "text": "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning [102]. The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model. The distillation algorithms can be categorized as offline, online, or self-distillation. \n\nOffline distillation [101,[103][104][105] extracts knowledge from a pre-trained teacher model and uses the soft label output of the teacher model to train the student network. The authors of [103] used data augmentation to exploit the output distributions of multiple teacher networks. The authors of [101] introduced a tailored distillation approach for quantized models, demonstrating that quantized student networks can closely match the accuracy of full-precision teacher networks while achieving high compression rates and inference acceleration. In contrast, [104] pioneered a data-free technique, training the student network using synthetic data responses from the complex teacher network. Online distillation [106][107][108] occurs during the simultaneous training of both the teacher and student models. It employs online knowledge distillation using the soft-label outputs of the teacher network. Ref. [106] proposed training the student model at different checkpoints of the teacher model until convergence is achieved. Meanwhile, Collaborative Learning Knowledge Distillation (KDCL) [107] dynamically generates high-quality soft targets through an ensemble approach for one-stage online training. Furthermore, as observed in [105,108], knowledge distillation extends its influence to the intermediate layers of the teacher network.",
            "score": 0.5433671329802554,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 46568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 399,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 987,
                    "end": 992,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 992,
                    "end": 997,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 997,
                    "end": 1002,
                    "matchedPaperCorpusId": "182183296"
                },
                {
                    "start": 1152,
                    "end": 1157,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 1526,
                    "end": 1531,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "226841849"
                },
                {
                    "start": 1874,
                    "end": 1879,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 2057,
                    "end": 2062,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1539306640625
        },
        {
            "corpus_id": "118649278",
            "title": "Variational Information Distillation for Knowledge Transfer",
            "text": "i.e., the teacher network. While the framework was originally designed for knowledge transfer between DNNs on the same dataset, recent works [30,31] started exploiting its potential for more general transfer learning tasks, i.e., when the source data and the target data are different.\n\nMany knowledge transfer methods have been proposed with various intuitions. Hinton et al. [12] and Ba and Caruana [2] propose to match the final layers of the teacher and the student network, as the outputs from the final layer of the teacher network provide more information than raw labels. Romero et al. [22] proposes to match intermediate layers of the student network to the corresponding layers of the teacher network. Recent works [3,6,13,30,31] relax the regularization of matching the entire layer by matching carefully designed features/statistics extracted from intermediate layers of the teacher and the student networks, e.g., attention maps [31] and maximum mean discrepancy [13].\n\nEvidently, there is no commonly agreed theory behind knowledge transfer. This causes difficulty in understanding empirical results and in developing new methods in a more principled way. In this paper, we propose variational information distillation (VID) as an attempt towards this direction in which we formulate the knowledge transfer as maximization of the mutual information between the teacher and the student networks. This framework proposes an actionable objective for knowledge transfer and allows us to quantify the amount of information that is transferred from a teacher network to a student network. Since the mutual information is computationally intractable, we employ a variational information maximization [1] scheme to maximize the variational lower bound instead. See Figure 1 for the conceptual diagram of the proposed knowledge transfer method. We further show that several existing knowledge transfer methods [16,22] can be derived as specific implementations of our framework by choosing different forms of the variational lower bound. We empirically validate the VID framework, which significantly outperforms existing methods. We observe the gap is especially large in the cases of small data and heterogeneous architectures.\n\nIn summary, the overall contributions of our paper are as follows:\n\n\u2022 We propose variational information distillation, a prin-cipled knowledge transfer framework through maximizing mutual information between two networks based",
            "score": 0.5430201839279346,
            "section_title": "Introduction",
            "char_start_offset": 2018,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "4375646"
                },
                {
                    "start": 728,
                    "end": 730,
                    "matchedPaperCorpusId": "49542008"
                },
                {
                    "start": 736,
                    "end": 739,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1915,
                    "end": 1919,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1856689453125
        },
        {
            "corpus_id": "237492204",
            "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks",
            "text": "Knowledge Distillation in Classification Tasks. Transferring knowledge from one model to another is a research topic that has obtained noteworthy attention during recent years. Ba and Caruana (Ba and Caruana 2014) trained a single and small neural network to imitate the logits of a large and complex neural network. Then, Hinton et al. (Hinton, Vinyals, and Dean 2015) introduced KD and dark knowledge to claim that the deeper teacher model can successfully distill its knowledge into the smaller student neural network by matching their soft targets (softmax distributions). Nowadays, a lot of successive papers have been written to propose different techniques to KD for model compression purposes. Romero et al. (Romero et al. 2015) distilled the feature representations of the teacher's intermediate layers to the student for improving the training stage of the student network. Transferring the attention maps (Zagoruyko and Komodakis 2017;Huang and Wang 2017;Tarvainen and Valpola 2017), the inner products of intermediate activation maps (Yim et al. 2017), and relational knowledge between training samples (Park et al. 2019;Tung and Mori 2019;Peng et al. 2019;Liu et al. 2019) are some other methods to promote the distillation process from one model to another. However, these approaches ignored the possibility of available subclass knowledge within the classes and therefore did not take advantage of hidden subclass knowledge to improve student performance. By contrast, in this study, we use subclass knowledge to enhance the generalization ability of the teacher network. \n\nSubclass Knowledge Distillation. The distillation of knowledge can be improved by increasing the amount of information that the teacher can transfer to the student. M\u00fcller et al. (M\u00fcller, Kornblith, and Hinton 2020) compelled the teacher to create semantically meaningful subclasses for each class during its training phase with auxiliary contrastive loss. The student is then trained to mimic the invented teacher's subclasses predictions (probabilities). When the number of training samples per class is the same, they measured the number of bits of label information about how the teacher generalizes through subclass distillation in binary classification tasks.",
            "score": 0.5429467550092686,
            "section_title": "Related Work",
            "char_start_offset": 4680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 213,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 337,
                    "end": 369,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 916,
                    "end": 946,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 966,
                    "end": 993,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1046,
                    "end": 1063,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1133,
                    "end": 1152,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1152,
                    "end": 1169,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "235489777",
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "text": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
            "score": 0.5423963456851287,
            "section_title": "B. Formulation",
            "char_start_offset": 12183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2231
                },
                {
                    "start": 2234,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1879,
                    "end": 1883,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "267759723",
            "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
            "text": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
            "score": 0.5415265663676201,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05059814453125
        },
        {
            "corpus_id": "214623002",
            "title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives",
            "text": "Our knowledge matching loss is partially inspired by the line of Knowledge Transfer (KT) research but we shift its primary focus away from model compression in the conventional KT methods. The representative Dark Knowledge Distillation [9] requires a large teacher model to aid the optimization process of a small student model via offering informative hint in the form of probabilistic prediction output as the soft label. In this framework, aiming at easing the optimization difficulty of small networks, an available strong model is required beforehand. In contrast, we concentrate on developing deeply supervised training scheme and further boosting the optimization process of state-ofthe-art CNNs instead of compact models. Moreover, unlike the teacher and student in the distillation procedure which are optimized sequentially without straightforward association during their separate training process, our training strategy drives all auxiliary branch classifiers together with the original classifier to be optimized simultaneously with a knowledge matching loss among them computed in an on-the-fly manner. Knowledge transfer process occurs in a more compact way within our proposed mechanism, which enables knowledge sharing across hierarchical layers in one single network, without the demand of an extra teacher model. Thus our knowledge integration learning scheme is ready to be deployed in the optimization process of any convolutional neural networks, both lightweight networks and heavy ones.",
            "score": 0.5406719023162029,
            "section_title": "G. Comparison to Knowledge Transfer Research",
            "char_start_offset": 40604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1510
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30712890625
        },
        {
            "corpus_id": "251107546",
            "title": "Self-Supervision and Self-Distillation with Multilayer Feature Contrast for Supervision Collapse in Few-Shot Remote Sensing Scene Classification",
            "text": "Knowledge distillation [50] can transfer the knowledge learned in one network to another. The structures of the two networks can be the same or different. Knowledge distillation usually trains a selected network as a teacher first and then uses the output of the teacher network and labels to train a network as a student, so it is also called \"teacherstudent learning\". Knowledge distillation can reduce the volume of the network and maintain a performance level close to that of the original network. Two networks with different performances can also be combined through knowledge distillation. Han [51] showed that for a given neural network, reducing the network weight by more than 85% through knowledge distillation would not significantly damage the performance of the neural network. According to the object of distillation, knowledge distillation has many forms. KD [50] realized knowledge transfer by minimizing the difference between teachers' and students' classified output labels. Fitness [52] extracted features from the middle layer of the teacher network to guide the student to learn useful knowledge, which can make the student network deeper and narrower than the teacher network. In [53], knowledge was transferred to students in the form of an attention map. IRG [54] constrained the similarity of multiple samples and proposed to constrain the instance relationship graph of students and teachers.",
            "score": 0.5392078778649741,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0214080810546875
        },
        {
            "corpus_id": "247154760",
            "title": "Learn From the Past: Experience Ensemble Knowledge Distillation",
            "text": "Traditional knowledge distillation transfers \"dark knowledge\" of a pre-trained teacher network to a student network, and ignores the knowledge in the training process of the teacher, which we call teacher\u2019s experience. However, in realistic educational scenarios, learning experience is often more important than learning results. In this work, we propose a novel knowledge distillation method by integrating the teacher\u2019s experience for knowledge transfer, named experience ensemble knowledge distillation (EEKD). We save a moderate number of intermediate models from the training process of the teacher model uniformly, and then integrate the knowledge of these intermediate models by ensemble technique. A self-attention module is used to adaptively assign weights to different intermediate models in the process of knowledge transfer. Three principles of constructing EEKD on the quality, weights and number of intermediate models are explored. A surprising conclusion is found that strong ensemble teachers do not necessarily produce strong students. The experimental results on CIFAR-100 and ImageNet show that EEKD outperforms the mainstream knowledge distillation methods and achieves the state-of-the-art. In particular, EEKD even surpasses the standard ensemble distillation on the premise of saving training cost.",
            "score": 0.538541840864013,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "273821996",
            "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
            "text": "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features [13,14,19,33,35,39,45,43,65,73] and distillation from logits [6,10,30,52,75]. Many studies [5,17,38,21,47,51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities. The teacher network typically provides richer and more comprehensive feature representations than the student network. For instance, KD-Net [17] utilizes a teacher network with complete modalities to supervise the unimodal student network at both the feature and logits levels. Despite their promising results, these methods neglect precise supervision of representations, resulting in low-quality knowledge transfer. To this end, we implement hierarchical semantic and distributional alignment of the multi-scale representations of both networks to transfer knowledge effectively.",
            "score": 0.5383339259766358,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1165
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "5299559"
                },
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "233219684"
                },
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "221543802"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "245445463"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "263605398"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "221543802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "257308816",
            "title": "A Survey on Deep-Learning-Based Real-Time SAR Ship Detection",
            "text": "Hinton put forward the concept of knowledge distillation for the first time in distilling the knowledge in a neural network. And he introduced the soft targets of teacher to induce the training of students' network. The knowledge distillation is classified into three categories, they are logits transfer, teacher assistant, and domain adaptation [109]. \n\nGenerally speaking, the teacher model has strong ability and performance, while the student model is compact. The knowledge distillation methods transfer the generalization ability of the teacher model to the compact student model to improve its performance with less complexity. The basic idea of knowledge distillation is to transfer the dark knowledge in the complex teacher model to the simple student model. These methods match or outperform the teacher's performance, while requiring notably fewer parameters and multiplications [110], [111], [112], as shown in Fig. 7. \n\nThe parameter T represents temperature. Generally, T is 1. When T is larger, a softer probability distribution will be obtained. There are two loss functions. The first loss function requires that the student model and the teacher model use the same T when calculating the softmax layer. The second loss function requires the student model T to be taken as 1, and the loss function is the weighted average of the two objective functions. Soft prediction carries more and more useful information than hard prediction. The knowledge distillation can get a lightweight CNN model with high accuracy [113]. \n\nThe softmax function is formulated as follows:",
            "score": 0.5382256452709492,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 14030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1584
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 896,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 898,
                    "end": 903,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 905,
                    "end": 910,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "102483463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.245849609375
        },
        {
            "corpus_id": "203700593",
            "title": "DiabDeep: Pervasive Diabetes Diagnosis Based on Wearable Medical Sensors and Efficient Neural Networks",
            "text": "The model can be further compressed through lowbit quantization. For example, Zhu et al. show that a ternary representation of the weights instead of full-precision (32bit) in ResNet-56 can significantly reduce memory cost while incurring only a minor accuracy loss [30]. The quantized models offer additional speedup potential for current NN accelerators [31]. Knowledge distillation: Knowledge distillation allows a compact student network to distill information (or 'dark knowledge') from a more accurate, but computationally intensive, teacher network (or group of teacher networks) by mimicking the prediction distribution, given the same data inputs. The idea was first introduced by Hinton et al. [32]. Since then, knowledge distillation has been effectively used to discover efficient networks. Romero et al. proposed FitNets that distill knowledge from the teacher's hint layers to teach compact students [33]. Passalis et al. enhanced the knowledge distillation process by introducing a concept called feature space probability distribution loss [34]. Yim et al. proposed fast minimization techniques based on intermediate feature maps that can also support transfer learning [35].",
            "score": 0.5375486657361561,
            "section_title": "Efficient neural networks",
            "char_start_offset": 8848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1191
                }
            ],
            "ref_mentions": [
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299072265625
        },
        {
            "corpus_id": "251018358",
            "title": "Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection",
            "text": "[14] introduces the soft prediction of the teacher network as dark knowledge for distillation. [28] leverages the intermediate representations learned by teacher to guide student. [19] proposes to transfer attention information of teacher. Several works [9,18,43,47,49] use the student itself as a teacher, named self-distillation. Inspired by these works, we design a novel distillation framework to distill commonalities between base classes and novel classes based on a memory bank.",
            "score": 0.5367235729518854,
            "section_title": "Related Work",
            "char_start_offset": 7443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 485
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "266693464",
            "title": "Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation",
            "text": "A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucilu\u01ce et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes. By minimizing the similarity loss between the student's predictions and the teacher's probabilities, the student network can assimilate the knowledge from the teacher, resulting in superior performance compared to training from",
            "score": 0.5366609122157945,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 797
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 225,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1685791015625
        },
        {
            "corpus_id": "269214409",
            "title": "KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated Learning",
            "text": "Knowledge Distillation (KD, for short) is an ML model compression technique, in which the knowledge from a complex model, or \"teacher\" model, is transferred to a smaller and more efficient model, known as the \"student\" model without a significant drop in accuracy [9].The general idea was first presented by Bucilua et al. in 2006 [2] and modeled in its current known form in 2014 by Hinton et al. [10] who found it easier to train a classifier using the outputs of another classifier as target values than using actual ground-truth labels.The teacher network outputs are represented by the so-called soft probabilities that contain more information about a data point than just the class label (or hard predictions) and are the input of the student network.\n\nIn practice, given an input  the teacher network produces a vector of scores\n\nthat are converted into probabilities:\n\nHinton et al. [10] proposed to modify these probabilities in soft probabilities as following:\n\nwhere  is a hyperparameter.A student network will produce a softened class probability distribution, p ().The loss for the student network is a linear combination of the cross entropy loss, namely L  and a knowledge distillation loss L  : Figure 3 shows the generic architecture of the KD using the teacher-student model.Thanks to the distillation algorithm the student mimics the teacher network learning the relationship between different classes discovered by the teacher model that contains information beyond the ground truth labels.",
            "score": 0.536639068730443,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 758
                },
                {
                    "start": 760,
                    "end": 836
                },
                {
                    "start": 838,
                    "end": 876
                },
                {
                    "start": 878,
                    "end": 971
                },
                {
                    "start": 973,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1294
                },
                {
                    "start": 1294,
                    "end": 1511
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12359619140625
        },
        {
            "corpus_id": "248022790",
            "title": "Federal SNN Distillation: A Low-Communication-Cost Federated Learning Framework for Spiking Neural Networks",
            "text": "Knowledge distillation (KD) [26] is an extraction scheme for network knowledge, which can transfer the knowledge of a trained large neural network to a small network, making the small network show a very similar effect to the large network.Small-scale networks have advantages such as fast forward computing speed and small resource consumption.In KD, the network of acquiring knowledge is called the student network, and the network of transferring knowledge is called the teacher network.\n\nThe idea behind the classical KD algorithm is that the student network and the teacher network share a dataset (also called Book in some studies) on which the student network trains.Different from the traditional training, the loss function of the student network in the distillation training not only uses the label (hard-label) of the data itself, but also needs to add the distillation items composed of soft-max labels (soft-label [27], or called logit [26]) output by the teacher network.They are combined to carry out backpropagation according to a certain weight.\n\nKD proves that the knowledge of the network not only exists in parameters but also can be reflected through output.Inspired by this, ANN researchers proposed the federal distillation (FD) framework [28]- [30], which communicates based on output instead of the gradients.This also provides ideas for our work.",
            "score": 0.5352695328361801,
            "section_title": "2.3.Knowledge Distillation",
            "char_start_offset": 5519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 490
                },
                {
                    "start": 492,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1062
                },
                {
                    "start": 1064,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04510498046875
        },
        {
            "corpus_id": "221340727",
            "title": "MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation",
            "text": "Knowledge Distillation (KD), proposed by Hinton et al. [10], aims at transferring knowledge from a deep teacher network, denoting as T, to a shallow student network, denoting as S. In order to transfer knowledge from teacher to student, the loss for training student network is modified by adding KL divergence between the teacher's and the student's output probability distributions. Formally, given labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of student network as following, \n\nwhere \u03b1 is the hyper-parameter to control the relative importance of the two terms, \u03c4 is the temperature hyper-parameter, \u03b8 T and \u03b8 S are the parameters of teacher T and student S respectively. L CE refers to the cross entropy loss and KL refers to the KL divergence which measures how one probability distribution is different from another. \n\nBy minimizing this modified loss function, the student network will try to match the one-hot ground-truth distribution (the first term) while lowering its discrepancy with output probability distribution from teacher T (x i ; \u03b8 T ) to transfer the knowledge of negative classes learned by teacher network. The probability distribution over classes can be obtained by employing softmax function, where the probability of sample s i belonging to class j can be expressed as, \n\nwhere C is number of classes, z i is the output logits of sample x i , \u03c4 is the same temperature hyper-parameter as (1). The role of \u03c4 is to control the smoothness of the output probability distribution. The higher the temperature, the higher the entropy of distribution, i.e. the smoother distribution.",
            "score": 0.5347043633476101,
            "section_title": "Background: Knowledge Distillation",
            "char_start_offset": 10547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07861328125
        },
        {
            "corpus_id": "209515527",
            "title": "Modeling Teacher-Student Techniques in Deep Neural Networks for Knowledge Distillation",
            "text": "Assume training data tuples of inputs and labels (. ) \u2208  which D is a set of training data. Let T be Teacher network with parameters   and S be Student network with parameters   . \n\nThe distillation equation can be written as Eq. ( 1) to minimize . \n\nIn Eq. ( 1),   is cross-entropy which is computed on the labels  \ufffd  that are predicted by the student and ground truth labels y with temperature =1.   is distillation loss, which is crossentropy computed on softmax output of teacher and student with temperature  . Hence, (.   . )  (.   . ) represent of softmax output of the student and the teacher, respectively.  is hyper-parameters to balance the influence of each loss [7][1]. \n\nHow the knowledge is transferred between the teacher(s) and student(s) is specified in this stage. As illustrated in Fig. 5, there are three steps in knowledge distillation playing an essential role in having an efficient TS model. These steps are including the determination of knowledge types, location of distillation, and methods of knowledge transfer. \n\nIn the base TS models, soft-labels (also known as logits) are considered as distilled knowledge. However, the knowledge can be distilled from each location of the teacher model, including the end of the model and between layers. In [14], knowledge is transferred between blocks of the teacher to student. Distillation-loss is realized through a cross-entropy function that is applied to the output of the student and softlabels of the teacher. Different knowledge types are considered in previous studies, such as soft labels, hard labels, etc. In [9], knowledge type in the form of the mutual information between intermediate layers is maximized and several functions are used to minimize the loss of intermediate layers. In [7], knowledge  types are in the form of a similarity matrix between teacher and student models. Transfer Methods is another step in the knowledge distillation stage. In [15] a multilevel approach for knowledge transfer is presented in which an information mask is provided by the teacher, and the student is trained with both of the information masks and teacher's ground truth. Also, in [16] and [17], there is more than one level of distillation.",
            "score": 0.5346261548228954,
            "section_title": "V. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "182952755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0635986328125
        },
        {
            "corpus_id": "233476572",
            "title": "Spirit Distillation: A Model Compression Method with Multi-domain Knowledge Transfer",
            "text": "Knowledge distillation researches on the technical means of training compact student network with the prompt of cumbersome teacher network. Previous works can be mainly classified into logit-based distillation [15,21,28] and feature-based distillation [14,18,19,24], which transfer the knowledge from different stages of the teacher network to improve the performance of the student network. Pioneering works on knowledge distillation bases on logits tranfer [15], which adopt a weighted average of soft and hard labels as supervisory information for student network training. Subsequent works begin to focus on transferring intermediate representation of the teacher network, like FitNet stage-wise training [24], knowledge adaptation [14], and structured knowledge distillation [18], hoping that the student network learns an effective representation based on the front part of the teacher with much fewer FLOPs.",
            "score": 0.5345332623100242,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 2969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 914
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "75137175"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "75137175"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "73729180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06256103515625
        },
        {
            "corpus_id": "260704230",
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "text": "Fig. 3 compares the architectures of student network with single and multiple teacher networks. In the multi-teacher distillation, averaging multiple teachers [24,87,88,89,90] is a commonly-used approach to incorporate the potentially diverse knowledge from teachers (i.e., each teacher with an identical importance weight); concretely, a student network aims to learn the average softened logits of multiple teacher networks. \n\nNote that multiple teacher networks can be heterogeneous since these teacher networks can be trained in various environments (e.g., different data distributions). This suggests that the transferred knowledge from various teachers can contribute differently to the student learning performance, so that the student network may learn more knowledge from similar teacher networks. As a result, averaging multiple teacher networks could be sub-optimal by assigning each teacher an identical importance weight. Thus, to learn more representative and critical knowledge from significant teacher networks, advanced teacher weighting approaches have been introduced to assign a specific importance weight to each teacher network. For exam-ple, Adaptive Multi-teacher Multi-level Knowledge Distillation (AMTML-KD) [81] includes multiple teacher networks, where each teacher network is learned an instance-level importance weight for adaptively integrating the intermediate feature representations from all teachers. Consequently, a student network can fully learn potentially diverse knowledge from multiple teachers. In addition to the logits from multiple teachers, You et al. [82] also additionally consider the relative similarity between intermediate representations of samples as one type of dark knowledge to guide the training of a student network. Concretely, the triplets are utilized to encourage the consistency of relative similarity relationships between the student and the teachers. Yuan et al. [91] formulate the teacher selection problem under an RL framework, where each teacher network is assigned an appropriate importance weight based on various training samples and the outputs of teacher networks. Then, multiple teacher networks are randomly selected based on the learned importance weights to guide the training of the student network at each epoch. Ruder et al. [92] consider multi-teacher knowledge distillation for the domain adaptation and design teacher importance weights according to the data similarity between source domains and a target domain.",
            "score": 0.5341856445327324,
            "section_title": "Multi-Teacher Distillation",
            "char_start_offset": 38440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2295
                },
                {
                    "start": 2296,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1599,
                    "end": 1603,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1931,
                    "end": 1935,
                    "matchedPaperCorpusId": "228376532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "269293061",
            "title": "CKD: Contrastive Knowledge Distillation From a Sample-Wise Perspective",
            "text": "Knowledge distillation is a representative model compression technique [49], [65], [80] that enables small models to attain strong performance of large models [1], [48], [52].As a pioneer, Hinton et al. [25] introduced the concept of knowledge distillation that transfers \"dark knowledge\" by minimizing the KL divergence between teacher logits and student logits.Over the past few years, a rich line of knowledge distillation methods has been proposed [11], [12], [30].These knowledge distillation methods come in two flavors: logit distillation and feature distillation [33], [50].\n\n1) Logit Distillation: Earlier logit distillation approaches attempt to enhance generalization performance through regularization and optimization techniques.However, there has been limited research on early-stage logit distillation due to inferior performance.DML [89] improved the generalization abilities of networks by collaboratively training an ensemble of models through mutual learning.Instead, TAKD [51] employed multistep distillation via intermediate-sized networks, effectively bridging the knowledge transfer gap between teacher and student networks.ICKD-C [44] utilized grid-level inter-channel correlation to capture the intrinsic distribution and diversity of feature representations.GLD [29] employed a local spatial pooling layer for the simultaneous extraction of finely localized knowledge and holistic representations.DIST [26] leveraged both inter-class and intra-class relations to align probabilistic distributions and reinforce semantic similarities, respectively.DKD [90] decoupled logits into a target class and all non-target classes, thereby enhancing both the effectiveness and flexibility of knowledge transfer.Auto-KD [35] firstly introduced an automated search strategy by employing Monte Carlo tree search to optimize knowledge distillation architectures.CTKD [39] modulated the task difficulty levels of a student network by using a dynamic and learnable temperature setting.MLKD [27] also proposed a multi-level logit distillation framework, which distills instance-level, batch-level, and class-level information.",
            "score": 0.5335460297633213,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 6742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 582
                },
                {
                    "start": 584,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1873
                },
                {
                    "start": 1873,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 77,
                    "end": 81,
                    "matchedPaperCorpusId": "248503415"
                },
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "251040462"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "203953149"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "253098943"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "257679074"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "257833913"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "257365467"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "260068567"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "257504921"
                },
                {
                    "start": 849,
                    "end": 853,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "244101388"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "244074368"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "267005248"
                },
                {
                    "start": 1878,
                    "end": 1882,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296142578125
        },
        {
            "corpus_id": "3643430",
            "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
            "text": "Knowledge distillation methods: The general technique in distillation based methods involves using a teacher-student strategy, where a large deep network trained for a given task teaches shallower student network(s) on the same task. The core concepts behind knowledge distillation or transfer technique have been around for a while. Bucilu\u01ce et al. (2006) show that one can compress the information in an ensemble into a single network. Ba & Caurana (2013) extend this approach to study shallow, but wide, fully connected topologies by mimicking deep neural networks. To facilitate learning, the authors introduce the concepts of learning on logits rather than the probability distribution. Hinton et al. (2015) propose a framework to transfer knowledge by introducing the concept of temperature. The key idea is to divide the logits by a temperature factor before performing a Softmax function. By using a higher temperature factor the activations of incorrect classes are boosted. This then facilitates more information flowing to the model parameters during back-propagation operation. FitNets (Romero et al., 2014) extend this work by using intermediate hidden layer outputs as target values for training a deeper, but thinner, student model. Net2Net (Chen et al., 2015a) also uses a teacher-student network system with a function-preserving transformation approach to initialize the parameters of the student network. The goal in Net2Net approach is to accelerate the training of a larger student network. Zagoruyko & Komodakis (2016) use attention as a mechanism for transferring knowledge from one network to another. In a similar theme, Yim et al. ( 2017) propose an information metric using which a teacher DNN can transfer the distilled knowledge to other student DNNs. In N2N learning work, Ashok et al. (2017) propose a reinforcement learning based approach for compressing a teacher network into an equally capable student network. They achieve a compression factor of 10x for ResNet-34 on Cifar datasets.",
            "score": 0.5332870151413516,
            "section_title": "RELATED WORK",
            "char_start_offset": 9728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 355,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1014404296875
        },
        {
            "corpus_id": "246680405",
            "title": "Conditional generative data-free knowledge distillation",
            "text": "Knowledge distillation is a general model compression technique. In KD, we can transfer knowledge by minimizing network logit difference. However, teacher logits usually have a high probability in the correct class and close to zero in other classes. It cannot provide more information than the one-hot label. To this end, Hinton et al. [9] introduced SoftMax temperature to soften model output and extract more relationship information between different classes. Furthermore, Furlanello et al. [26] trained and integrated a set of student networks as the final student model. Mirzadeh et al. [27] introduced a middle-sized model (teacher assistant) as a bridge to help the student network training. \n\nBesides distilling the logits, transferring knowledge to the student can be achieved by utilizing the feature information in hidden layers. Feature knowledge contains precise and efficient cues of the training process. In this case, Romero et al. [10] trained student networks using both logits and intermediate features. Zagoruyko et al. [11] transformed latent knowledge into attention maps and forced student networks to mimic the attention maps of the teacher. Yim et al. [28] introduced the FSP (Flow of Solution Procedure) matrix to learn the relationship between inputs and outputs. Park et al. [29] proposed a relational knowledge distillation method. They transferred the mutual relations of data according to the distance-wise loss and angle-wise loss. Bai et al. [30] designed a novel layer-wise cross distillation method and realize few-shot distillation. Li et al. [31] trained a portable objective detection model based on the supervision from proposal features. Liu et al. [32] presented a dense prediction distillation method based on holistic knowledge and pairwise similarity. Some recent works [33,34] also explored the structure effect to model distillation. Moreover, combining the knowledge distillation with other techniques (e.g., NAS [34], RL [35], or GNN [36]) can further compress and accelerate the heavy models. For other related works, [37] reviewed and looked at the development of knowledge distillation.",
            "score": 0.5328246301791597,
            "section_title": "Data-driven Knowledge Distillation",
            "char_start_offset": 5786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "208201983"
                },
                {
                    "start": 1580,
                    "end": 1584,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 1815,
                    "end": 1819,
                    "matchedPaperCorpusId": "211004051"
                },
                {
                    "start": 1819,
                    "end": 1822,
                    "matchedPaperCorpusId": "235212488"
                },
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "235212488"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "matchedPaperCorpusId": "207930254"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07147216796875
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Note that 'Section' is abbreviated as 'Sec.' in this figure (Cho and Hariharan 2019). Empirical results show that a larger model may not be a better teacher because of model capacity gap (Mirzadeh et al. 2020). Experiments also show that distillation adversely affects the student learning. The empirical evaluation of different forms of knowledge distillation about knowledge, distillation and mutual affection between teacher and student is not covered by Cho and Hariharan (2019). Knowledge distillation has also been explored for label smoothing, for assessing the accuracy of the teacher and for obtaining a prior for the optimal output layer geometry (Tang et al. 2020). \n\nKnowledge distillation for model compression is similar to the way in which human beings learn. Inspired by this, recent knowledge distillation methods have extended to teacher-student learning (Hinton et al. 2015), mutual learning (Zhang et al. 2018b), assistant teaching (Mirzadeh et al. 2020), lifelong learning (Zhai et al. 2019), and self-learning (Yuan et al. 2020). Most of the extensions of knowledge distillation concentrate on compressing deep neural networks. The resulting lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition, and natural language processing (NLP). Furthermore, the knowledge transfer from one model to another in knowledge distillation can be extended to other tasks, such as adversarial attacks (Papernot et al. 2016), data augmentation (Lee et al. 2019a;Gordon and Duh 2019), data privacy and security (Wang et al. 2019a). Motivated by knowledge distillation for model compression, the idea of knowledge transfer has been further applied in compressing the training data, i.e., dataset distillation, which transfers the knowledge from a large dataset into a small dataset to reduce the training loads of deep models (Wang et al. 2018c;Bohdal et al. 2020). \n\nIn this paper, we present a comprehensive survey on knowledge distillation.",
            "score": 0.5326356831572626,
            "section_title": "Knowledge Transfer",
            "char_start_offset": 5554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 84,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 187,
                    "end": 209,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 458,
                    "end": 482,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 952,
                    "end": 974,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 994,
                    "end": 1012,
                    "matchedPaperCorpusId": "198229709"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0095977783203125
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Despite the recent advances of knowledge distillation technique, a clear understanding of where knowledge resides in a deep neural network and an optimal method for capturing knowledge from teacher and transferring it to student remains an open question. \n\nIn recent advances of knowledge distillation, many forms of knowledge have been defined (Jiaxi Tang et al., 2020 [18]) based on the teacher-student learning paradigm and have shown dramatic success and were analyzed empirically: \n\n\u2022 Layer activation [7] \u2022 Auxiliary information [23] \u2022 Jacobian matrix of the model parameters [72], [73] \u2022 Gram matrix derived from pairs of layers [74] \u2022 Activation boundary [34] Distillation loss for knowledge distillation training is a key factor which is used to penalize the student to transfer this Knowledge from the Teacher to the Student. \n\nFahad Sarfraz et al. [75] presented broad categorization of a diverse set of knowledge distillation methods which differ from each other with respect to how knowledge is defined and transferred from the teacher. Borrowing from their categorization, we cite two groups below to demonstrate how to capture the knowledge from teacher. \n\na) Response Distillation uses only the outputs of a Teacher to train the student to mimic it. C. Bucilu\u01ce et al. [4] proposed to use the logits of a teacher network as target for the student and to minimize the squared difference. Hinton et al. [6] proposed to minimize the KL divergence between the smoother output probabilities. In the original formulation, Hinton et al. [6] introduced a knowledge distillation compression framework and proposed mimicking the softened softmax output of the teacher using a temperature parameter. It raised the temperature of the final softmax function and minimize the Kullback-Leibler (KL) divergence between the smoother output probabilities. This softened output transfers more important information which is called dark knowledge compared to the hard output. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model while using a much higher learning rate.",
            "score": 0.5320038622393096,
            "section_title": "A. Distilled Knowledge and Loss",
            "char_start_offset": 4459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 257,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 582,
                    "end": 586,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1416,
                    "end": 1419,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "258065609",
            "title": "Illumination Distillation Framework for Nighttime Person Re-Identification and a New Benchmark",
            "text": "Traditional knowledge distillation methods [49]- [52] are usually offline, which transfer a pre-trained large-scale teacher model to a small-scale student model. Current offline methods focus on different aspects of knowledge transfer, such as designing knowledge [49] and different loss functions [50]- [52]. Although these methods are simple and easy to implement, it requires a complex high-capacity teacher model, which costs a huge training time. To overcome the limitation of offline distillation, existing distillation techniques utilize different strategies to achieve online knowledge transfer between multiple models, which greatly facilitates the application of the technique in practical tasks. In the online distillation scheme, the teacher and student models are updated simultaneously in an end-to-end framework. For instance, Guo et al. [53] treat all networks as students and collaboratively train them in one stage to achieve knowledge transfer among arbitrary students. Chung et al. [54] design an online adversarial knowledge distillation method that uses discriminators to guide category probability and feature map distillation. Inspired by the one teacher vs. multiple students pattern in schools, Shen et al. [55] propose transferring knowledge from the teacher model to the student model, and between student models, which yields effective distillation results in the tracking task. In contrast to these approaches, we pursue learning different student models and then aggregating them into a teacher model.",
            "score": 0.5312622186094985,
            "section_title": "C. Knowledge Distillation Methods",
            "char_start_offset": 13259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "244040871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04046630859375
        },
        {
            "corpus_id": "257102399",
            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
            "text": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance [10]. Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance. \n\nOne promising and efficient way to distill knowledge is to mimic the logit output of last layer in the teacher model (i.e., teacher's prediction). Soft target, i.e., the probability that the input data   belongs to each class, is proposed as the logit of teacher network. For a -class classification problem, the soft target of teacher network \u0398  could be calculated by the softmax function as: \n\nWhere   denotes the -th training sample,  is the number of classes,    is logit of last layer for the th class and  denotes temperature factor which could control importance of each target. For example, a higher  produces a softer probability distribution among different classes (assign probability with less difference to each class). Soft target contains informative dark knowledge of teacher network and could enhance performance of student network. Empirically, when the student model is very small compared to the teacher model, lower temperatures work better [9]. This is because a very small model might not be able to capture all the information when we raise the temperature. Therefore, soft target could be seen as knowledge extracted from teacher network and it could be transferred to student network by matching output of two networks. To measure the output similarity between teacher network \u0398  and student network \u0398  , the Kullback Leibler (KL) divergence is often employed, which could be formulated in Eq. ( 2),",
            "score": 0.5312533294316186,
            "section_title": "Knowledge distillation",
            "char_start_offset": 15159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424560546875
        },
        {
            "corpus_id": "273811396",
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "text": "The concept of knowledge distillation [16] was first proposed as a learning strategy that employs a larger teacher network to steer the training process of a smaller student network for various tasks [27,28]. The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" [51], whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student. Recent studies examine how to select, express, and transfer the \"dark knowledge\" more practically and effectively, which can be categorized into two main types: logit-and feature-based distillations. \n\nLogits Distillation. In the wake of the earliest KD method based on temperature-regulated distillation [16], previous logit-based methods have concentrated mainly on effective regularization and optimization methods. DML [64] proposes a mutual learning method to train students and teachers simultaneously. TAKD [34] proposes using intermediary \"teacher assistants\" to transmit knowledge in a step-by-step manner in order to narrow the performance disparity between teachers and students. Additionally, several works [6,39] focus on interpreting the principles underlying KD. Recently, DKD [65] introduces an improved logit-based objective by decoupling the classical KD loss, which re-explores the potential of logitbased methods with comparable performance gains. \n\nFeature Distillation. To further enhance knowledge distillation, feature distillation is proposed to perform alignments on intermediate features as well as logit outputs, which can directly transfer teacher representations [5,14,15,41] or the correlation [36,37,47,48] from the teacher to the student. Feature methods are more likely to obtain high performance with extensive information from the teacher; however, tight feature alignment frequently relies on prior empirical observation and meticulous adjustment of hyperparameters.",
            "score": 0.5311675335419309,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 863,
                    "end": 867,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1159,
                    "end": 1162,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 1162,
                    "end": 1165,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1633,
                    "end": 1636,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1665,
                    "end": 1669,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "232269823",
            "title": "Similarity Transfer for Knowledge Distillation",
            "text": "Different from above methods, knowledge distillation enrich and get the student model by extracting kinds of knowledge from the fixed teacher model. To address the challenge of deploying CNNs in resource-constrained edge devices, Bucilua et al. [38] first propose to transfer the knowledge of an ensemble of models to a small model. Then Caruana et al. [39] propose to train student model by mimicking the teacher model's logits. Later, Hinton et al. [12] popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network. Afterward, some recent works [14] [15] extend KD by distilling knowledge from intermediate feature representations instead of soft labels. For example, FitNets [14] propose to train student network by mimicking the intermediate feature maps of teacher network, which are defined as hints. Inspired by this, Zagoruyko et al. [15] propose to match the attention maps between the teacher and the student, which are defined from the original feature maps as knowledge. Wang et al. [40] propose to improve the performance of student network by matching the distributions of spatial neuron activations between the teacher and the student. Recently, Heo et al. [41] introduce the activation boundary of the hidden neuron as knowledge for distilling the compact student network. \n\nHowever, the aforementioned knowledge distillation methods only utilize the knowledge contained in the output of specific layers of the teacher network. More richer knowledge between different layers is explored and utilized for knowledge distillation. For example, Yim et al. [16] propose to use Gram matrix between different feature layers as distilled knowledge, which named flow of solution process (FSP) that reflects the relations of different features maps.",
            "score": 0.5309611783025652,
            "section_title": "B. Knowledge Distillation.",
            "char_start_offset": 9178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "30307744"
                },
                {
                    "start": 1647,
                    "end": 1651,
                    "matchedPaperCorpusId": "53213211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "247521335",
            "title": "Generalized Knowledge Distillation via Relationship Matching",
            "text": "S UPERVISED deep learning has demonstrated success in a variety of fields [1]. Given the instances and corresponding annotations from the target task, we train a deep neural network to minimize the discrepancy between the model predictions and the ground-truth labels. Knowledge distillation (KD) [2], [3], [4] facilitates the learning efficiency of a deep neural network via taking advantage of the \"dark knowledge\" from another well-trained model. In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task. KD improves the discriminative ability of the target student model [9], [10], relieves the burden of model storage [3], [5], [4], [7], [11], [12] and enables the training of a deep neural network in low-resource environments [13], [14]. Applications of KD have been witnessed in a wide range of domains such as model/dataset compression [15], [16], [17], [18], [19], [20], multi-task learning [21], [22], and incremental image classification [23], [24]. \n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress. Although prediction matching allows knowledge to be transferred across different architectures [3], [17], its dependence on Figure 1: An illustration of strengthening a student model on the target task via distilling the knowledge from a teacher model. In standard Knowledge Distillation (KD), teacher and student share the same set of classes.",
            "score": 0.5306139987986842,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 77,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 307,
                    "end": 310,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 564,
                    "end": 567,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "226841742"
                },
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "218487294"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 910,
                    "end": 913,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 915,
                    "end": 918,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1122,
                    "end": 1126,
                    "matchedPaperCorpusId": "53763883"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1146,
                    "end": 1150,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "199543275"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "102486799"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "49655438"
                },
                {
                    "start": 1667,
                    "end": 1670,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "272986634",
            "title": "FLINT: Learning-based Flow Estimation and Temporal Interpolation for Scientific Ensemble Visualization",
            "text": "Student-Teacher Learning. In machine learning, techniques have been developed under the name of teacher-student architecture. One approach is knowledge distillation [42] to transfer the knowledge (parameters) learnt by a larger model (teacher model) and transfer it to a smaller model (student model). A separate strand of research uses the concept of privileged information [43], where the teacher provides the student during training with additional \"privileged\" information for knowledge transfer. Both approaches have been unified [44]. The learning can be offline, where student networks learn the knowledge from pre-trained teacher networks, or online, where student and teacher networks are simultaneously trained, so that the whole knowledge learning process can be end-toend trainable [45]. We have implemented an online studentteacher model and instead of a fully separate teacher network, we employ only one additional block that corresponds to the teacher network, akin to Huang et al. [8].",
            "score": 0.5305914565883051,
            "section_title": "Machine learning-based upscaling & super-resolution.",
            "char_start_offset": 12587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1002
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 998,
                    "end": 1001,
                    "matchedPaperCorpusId": "244499996"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04443359375
        },
        {
            "corpus_id": "271356843",
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "text": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
            "score": 0.5303546615513559,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09234619140625
        },
        {
            "corpus_id": "232046107",
            "title": "Risk factor identification for incident heart failure using neural network distillation and variable selection",
            "text": "Model distillation includes two components: a teacher network and a student network. The teacher network is a well-trained established deep learning model (BEHRT in our case), and the student network is a simpler model or a target model that we intend to use it to distil knowledge from the teacher network (BDLD in our case). Tang et al. [35] developed a strategy to train a student model with partial assistance from teacher models to help the student network achieve a competitive performance, and Korattikara et al. [20] proposed a Bayesian deep learning-based knowledge distillation method to transfer knowledge between probabilistic models. In this work, we combine both methods to use the probabilistic teacher network BEHRT to partially supervise the training of the probabilistic student network BDLD. As shown in Figure 2, both the teacher network and the student network use the same input information. Afterwards, the prediction from the teacher network will be used as a soft label to partially supervise the training of the student model. At the same time, the student model is also trying to predict the real label. Therefore, the loss function can be summarized as below: \n\nwhere  represents a pre-set weight to balance the training objective for mimicking the teacher and leaning for the classification task. \n\n  represents the loss for evidence lower bound, and   is the loss for knowledge distillation. We used cross entropy for negative log-likelihood in evidence lower bound. Additionally, the loss for knowledge distillation is shown as below: \n\nwhere  (|, ) represents the outputs from the student network,  (|,   ) represents the outputs from the teacher network,   are the parameters for the teacher network, and  is the weights for student network. The distillation loss can also be considered as a reframing of the standard cross entropy loss.",
            "score": 0.5302792457911273,
            "section_title": "Model distillation",
            "char_start_offset": 11160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "84176918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08831787109375
        },
        {
            "corpus_id": "269982135",
            "title": "Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch",
            "text": "Knowledge Distillation (KD) could transfer the ``dark knowledge\"of a well-performed yet large neural network to a weaker but lightweight one. From the view of output logits and softened probabilities, this paper goes deeper into the dark knowledge provided by teachers with different capacities. Two fundamental observations are: (1) a larger teacher tends to produce probability vectors that are less distinct between non-ground-truth classes; (2) teachers with different capacities are basically consistent in their cognition of relative class affinity. Abundant experimental studies verify these observations and in-depth empirical explanations are provided. The difference in dark knowledge leads to the peculiar phenomenon named ``capacity mismatch\"that a more accurate teacher does not necessarily perform as well as a smaller teacher when teaching the same student network. Enlarging the distinctness between non-ground-truth class probabilities for larger teachers could address the capacity mismatch problem. This paper explores multiple simple yet effective ways to achieve this goal and verify their success by comparing them with popular KD methods that solve the capacity mismatch.",
            "score": 0.5297349653364503,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270751953125
        },
        {
            "corpus_id": "257346971",
            "title": "An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment",
            "text": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the in-formation is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively. \n\nTo diversify the information distribution output by the teacher network, we used the temperature parameter \u03c4 to get soft prediction output by distilling logits output between the teacher network and student network. The same dataset was used because soft prediction output implied the information of the negative samples. With the help of SoftMax active function, the teacher network's class prediction probability distribution could be regarded as the soft target. Similarly, this method was used to get not only the soft prediction output but also the hard prediction output from the student network. As for the soft prediction output, soft prediction output and soft target were used to calculate loss value by loss function L so f t , which was a part of total loss. The L so f t was defined as: \n\nwhere P T i is the i-th soft target at time T, Q T i is the i-th soft prediction output at time T, N is the total number of samples and N = 27 in this paper. \n\nshown in Figure 3. KD is the process of imitating the distillation in chem softmax function with temperature parameters to \"distill\" the logit outpu and large networks, so as to generate more information in categories. This formation is called \"dark knowledge\". The additional information guides small network to learn more knowledge, and the two networks are called work and the student network, respectively. Figure 3. The KD structure.",
            "score": 0.5292616093559722,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 17198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3837890625
        },
        {
            "corpus_id": "203642142",
            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
            "text": "Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
            "score": 0.5292363950836432,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 909,
                    "end": 911,
                    "matchedPaperCorpusId": "21596346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "208175624",
            "title": "Search to Distill: Pearls Are Everywhere but Not the Eyes",
            "text": "Interest in KD increased following Hinton et al. [8], who demonstrated a method called dark knowledge distillation, in which a student model trained with the objective of matching full softmax distribution of the teacher model. Commonly, the teacher is a high-capacity model with formidable performance, while the student network is compact. By transferring knowledge, one hopes to benefit from both the student's compactness and the teacher's capacity. While this strategy has been widely adopted, especially for edge devices that require low memory or fast execution, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training. [8] suggest that the success of KD depends on the distribution of logits of the wrong responses, that carry information on the similarity between output categories. [3] argue that soft-target distribution acts as an importance sampling weight based on the teacher's confidence in its maximum value. [42] analyzed knowledge distillation from the posterior entropy viewpoint claiming that soft-targets bring robustness by regularizing a much more informed choice of alternatives than blind entropy regularization. Nevertheless, to our knowledge, no previous works attempt to explain from the perspective of network inherent architecture, as previous efforts investigate mainly from learning theory and distillation methods.",
            "score": 0.528529584538115,
            "section_title": "Na\u00efve distillation",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "In this paper, we propose a teacher-student collaborative distillation approach. Unlike traditional transfer learning, our approach fuses knowledge distillation and self-distillation, allowing the student model to learn new knowledge from the teacher network and from itself. During test stage, we vote on the different classification results of multiple sub-models in the student network. Through extensive experiments, the effectiveness of our proposed method and each component is verified, and this approach can be used to guide both knowledge distillation and multi-exit networks. Since the multiple exits in the student network can be constructed in any distillation network, we only consider a traditional distillation structure to ensure that the method is representative and can be further tested in other distillation cases in the future. Finally, the balance between model complexity and classification accuracy should be assessed in future research.",
            "score": 0.5285039968315457,
            "section_title": "Conclusion",
            "char_start_offset": 25782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 961
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047882080078125
        },
        {
            "corpus_id": "3481593",
            "title": "Fidelity-Weighted Learning",
            "text": "He proposed to use a large network or an ensemble of networks for training and a smaller network at test time. It turned out that compressing knowledge of a large system into a smaller system can improve the generalization ability. It was shown in (Lopez-Paz et al., 2016) that dark knowledge and LUPI can be unified under a single umbrella, called generalized distillation. The core idea of these models is machinesteaching-machines. As the name suggests, a machine is learning the knowledge embedded in another machine. In our case, student is correcting his knowledge by receiving privileged information about label uncertainty from teacher. \n\nOur framework extends the core idea of LUPI in the following directions: \n\n\u2022 Trainable teacher: It is often assumed that the teacher in LUPI framework has some additional true information. \n\nWe show that when this extra information is not available, one can still use the LUPI setup and define an implicit teacher whose knowledge is learned from the true data. In this approach, the performance of the final student-teacher system depends on a clever answer to the following question: which information should be considered as the privileged knowledge of teacher. \u2022 Bayesian teacher: The proposed teacher is Bayesian. It provides posterior uncertainty of the label of each sample. \u2022 Mutual representation: We introduced module \u03c8(.) which learns a mutual embedding (representation) for both student and teacher. This is in particular interesting because it defines a two-way channel between teacher and student. \u2022 Multiple teachers: We proposed a scalable method to introduce several teachers such that each teacher is specialized in a particular region of the data space.",
            "score": 0.5276931358584578,
            "section_title": "F CONNECTION WITH VAPNIK'S LEARNING USING PRIVILEGED INFORMATION",
            "char_start_offset": 48228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 272,
                    "matchedPaperCorpusId": "8125776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07318115234375
        },
        {
            "corpus_id": "262167018",
            "title": "CISO: Co-iteration semi-supervised learning for visual object detection",
            "text": "Knowledge distillation, which is essentially model compression [12,54], is proposed to be applied to classification tasks in a simple way. Unlike quantization and pruning methods, knowledge distillation proposes a teacher-student network, where the output of teacher network is knowledge, and the student network is applied to transfer knowledge for distillation. The performance and accuracy of the teacher network are higher, and the network structure is more complex than that of student network. There are two methods of knowledge acquisition in knowledge distillation; one is to use one-stage features [29,30,34], the other is to transfer knowledge through multi-stage information [11,18,51]. Knowledge distillation can lead to better model performance, reduce model latency, and compress network parameters [12]. Therefore, in this article, we take the consideration of adding a knowledge distillation method to our framework and improving the model performance.",
            "score": 0.5275590343753064,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 968
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 67,
                    "end": 70,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "195346684"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 690,
                    "end": 693,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 693,
                    "end": 696,
                    "matchedPaperCorpusId": "247244815"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05706787109375
        },
        {
            "corpus_id": "272968875",
            "title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation",
            "text": "Knowledge distillation has become widely recognized for its ability to transfer knowledge from a large teacher network to a compact and more streamlined student network. Traditional knowledge distillation methods primarily follow a teacher-oriented paradigm that imposes the task of learning the teacher's complex knowledge onto the student network. However, significant disparities in model capacity and architectural design hinder the student's comprehension of the complex knowledge imparted by the teacher, resulting in sub-optimal performance. This paper introduces a novel perspective emphasizing student-oriented and refining the teacher's knowledge to better align with the student's needs, thereby improving knowledge transfer effectiveness. Specifically, we present the Student-Oriented Knowledge Distillation (SoKD), which incorporates a learnable feature augmentation strategy during training to refine the teacher's knowledge of the student dynamically. Furthermore, we deploy the Distinctive Area Detection Module (DAM) to identify areas of mutual interest between the teacher and student, concentrating knowledge transfer within these critical areas to avoid transferring irrelevant information. This customized module ensures a more focused and effective knowledge distillation process. Our approach, functioning as a plug-in, could be integrated with various knowledge distillation methods. Extensive experimental results demonstrate the efficacy and generalizability of our method.",
            "score": 0.5272942682781241,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.139892578125
        },
        {
            "corpus_id": "232404302",
            "title": "KnowRU: Knowledge Reuse via Knowledge Distillation in Multi-Agent Reinforcement Learning",
            "text": "As mentioned above, because the teacher model provides more useful information for the student model, KD has achieved success in the field of computer vision. The soft probabilities output of trained teachers is the key of distillation. Let a t be the input logits of final softmax layer of teacher network where a t = [a 1 , a 2 , ...., a j ]. The logits are converted into probabilities q t = [q 1 , q 2 , ...., q j ] with softmax function: \n\n\u03a3j e a j . In order to extract more information compared with true labels, [7] proposes to soften the teacher probabilities with temperature T : \n\nIn KD, such dark knowledge from soft output of teacher provides more information than true labels. Based on the same image input x, the teacher network and student network produce probability q t (x) and q s (x) with Equation (6). The gap between q t (x) and q s (x) is usually penalized by Kullback-Leibler divergence (Equation ( 8)) or cross-entropy loss : \n\nwhere P(x) and Q(x) are two probability distributions on random variable x. \n\nTemperature T in Equation ( 7) also aims to soften the output of the teacher network. Then the student network could reuse knowledge the teacher network by the back propagation of L KD . Knowledge distillation inspires us that minimizing the gap between previous agents and current agents with the skill of distillation is the essence of knowledge reusing. We then draw upon such KD thought in our MARL research to reuse knowledge and verify the feasibility of KnowRU in section 3.3.",
            "score": 0.5272928961363345,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "33870153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "239016109",
            "title": "Pro-KD: Progressive Distillation by Following the Footsteps of the Teacher",
            "text": "Based on the aforementioned facts, a teacher which has stopped learning at the optimum epoch provides more informative pieces of information to the student. Our method exploits the Dark Knowledge of the best teacher by training teacher and students together; in our method, during training teacher and student together, at some point teacher will have the highest level of Dark Knowledge which leads to the best student. The important point here is that we do not need to find the optimum epoch at which this highest level of Dark Knowledge will be achieved. Since we are performing Knowledge Distillation in fixed intervals and storing intermediate checkpoints, the student trained by guidance of the best teacher will be one of these checkpoints and we have access to that. \n\nIn contrast with usual KD methods, pre-trained teacher networks are not used in our methods; in fact, we train teacher alongside with the student. Based on the aforementioned arguments, Grow-KD outperforms other KD methods as it gives us the opportunity of exploiting the experience and the Dark Knowledge provided by the best teacher, the teacher which has stopped learning at the optimum epoch.",
            "score": 0.5270238166033032,
            "section_title": "Conclusion",
            "char_start_offset": 29117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436767578125
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. Knowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. In recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. \n\nThe teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. Another variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "score": 0.5269614774108228,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 1104,
                    "end": 1106,
                    "matchedPaperCorpusId": "235677396"
                },
                {
                    "start": 1328,
                    "end": 1330,
                    "matchedPaperCorpusId": "253270253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. \n\nKnowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. \n\nIn recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. The teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. \n\nAnother variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. \n\nMask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "score": 0.5268170017756381,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2277
                },
                {
                    "start": 2280,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1106,
                    "end": 1108,
                    "matchedPaperCorpusId": "235677396"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "245218794"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "257771511"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "258841675"
                },
                {
                    "start": 1332,
                    "end": 1334,
                    "matchedPaperCorpusId": "253270253"
                },
                {
                    "start": 1862,
                    "end": 1865,
                    "matchedPaperCorpusId": "252625376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06573486328125
        },
        {
            "corpus_id": "249017724",
            "title": "Improving the Latent Space of Image Style Transfer",
            "text": "Knowledge distillation (KD) [26,27,28] is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability [28,29], which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted. This dark knowledge is mainly related to labels, so they are rarely used in low-level vision tasks (e.g., neural style transfer). Wang et al. [10] developed a collaborative knowledge distillation method to learn a much smaller model from pre-trained redundant VGG-19 for ultra-resolution style transfer. In our method, the pre-trained encoder is regarded as a regularizer to guarantee that the features extracted by the new encoder are near a suitable value.",
            "score": 0.526540929603609,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1036
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "213178672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "260494397",
            "title": "NCL++: Nested Collaborative Learning for Long-Tailed Visual Recognition",
            "text": "Knowledge distillation is a prevalent technology in knowledge transferring. One typical manner of knowledge distillation is teacher-student learning [45,46], which transfers knowledge from a large teacher model to a small student model. Current methods in knowledge distillation can be divided into three categories: offline distillation [45,47,48,49,50], online distillation [26,24,51,52,51,53] and self-distillation [54,55,56,57,58]. Early methods [45,48] often adopt an offline learning strategy, which transfers the knowledge from a pretrained teacher model to a student model. Most works [45,48,59] distill the knowledge from the output distributions, while some works achieve the knowledge transferring by matching feature representations [47] or attention maps [50]. The offline distillation is very popular in the early stage. However, the offline way only considers transferring the knowledge from the teacher to the student, and therefore, the teacher normally should be a more complex high-capacity model than the student. In recent years, knowledge distillation has been extended to an online way [26,24,51,52,51,60], where the whole knowledge distillation is conducted in an one-phase and end-to-end training scheme. For example, in Deep Mutual Learning [26], any one model can be a student and can distill knowledge from all other models. Zhu et al. [25] propose a multi-branch architecture with treating each branch as a student to further reduce computational cost. Compared with offline distillation, online distillation is more efficient by taking the learning in an one-phase end-to-end scheme. For self-distillation [54,55,56,57,58], it can be regarded as a special case in online distillation, where the teacher and the student refer to the same network. In other words, self-distillation means that the model distills the knowledge from itself. For example, Zhang et al. [55] divide the network into several sections according to their depth, and allow the low sections to distill the knowledge from high sections.",
            "score": 0.5263355510267702,
            "section_title": "Knowledge distillation.",
            "char_start_offset": 12792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "219558831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0268096923828125
        },
        {
            "corpus_id": "276885408",
            "title": "ProReflow: Progressive Reflow with Decomposed Velocity",
            "text": "Although knowledge distillation has been proven effective as a model compression technique and further extended successfully to diffusion model acceleration, the theoretical explanation for its efficacy has remained elusive. How 'dark knowledge' is effectively captured from teacher models and utilized to guide student learning remains a fundamental theoretical question [6]. Lopez-Paz et al. [19] presented a unified theoretical framework that connects distillation with privileged information, establishing a generalized framework for understanding machine-to-machine knowledge transfer. Viewing distillation as a transfer of privileged information, TAKD [25] showed that an assistant model of intermediate capacity could more effectively mediate the knowledge flow between teacher and student models.",
            "score": 0.5256879324985996,
            "section_title": "Privileged Information in Distillation",
            "char_start_offset": 7563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 804
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1290283203125
        },
        {
            "corpus_id": "251740978",
            "title": "Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications, and Open Issues",
            "text": "Knowledge distillation (KD) [65] aims to get a small light network but with good generalization capability. The basic idea is to transfer the knowledge learned from a big cumbersome network (or teacher network) with good generalization ability to a small but light network (or student network). \n\nHowever, knowledge distillation may be seriously influenced when there is a big gap in the learning capability between the teacher and student networks. In other words, if the difference is large, the student network may not be able to learn knowledge from the teacher network. Recently, several EC-based approaches have been proposed to mitigate the above issue of knowledge distillation. For example, Wu et al. [219] proposed an evolutionary embedding learning (EEL) paradigm to learn a fast accurate student network via massive knowledge distillation. Their experimental results show that the EEL is able to narrow the performance between the teacher and student networks on given tasks. Zhang et al. [245] developed an evolutionary knowledge distillation method to improve the effectiveness of knowledge transfer. In this method, an evolutionary teacher was learned online and consistently transfers intermediate knowledge to the student network to narrow the gap of the learning capability between them.",
            "score": 0.5254753731683708,
            "section_title": "Knowledge Distillation.",
            "char_start_offset": 57561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1305
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 710,
                    "end": 715,
                    "matchedPaperCorpusId": "210844903"
                },
                {
                    "start": 1001,
                    "end": 1006,
                    "matchedPaperCorpusId": "232352737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "235489777",
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "text": "Since then, researchers attempt to explore variants of knowledge distillation by using more supervised information from the teacher network. In [12], Romero et al. introduce a new metric of intermediate features between the teacher and student networks and add a regressor to match different size of teacher's and student's outputs. Zagoruyko et al. [13] propose to use the activation-based and gradient-based spatial attention maps from intermediate layers as the supervise information. Yim et al. [21] propose to use the flow of solution procedure (FSP) that is generated by computing the Gram matrix of features between layers to transfer knowledge. And the students imitate the process of solving problems by the teachers in FSP method. \n\nDifferent from the above methods, several researches adopts multiple teachers to supervise the student network's training. Shan et al. [22] conduct distillation by combining the knowledge of intermediate representations from multiple teacher networks. Shen et al. [23] extend this idea by learning a compact student model which is capable of handling the super task from multiple teachers. \n\nMoreover, knowledge distillation can be used by combining with the conventional DNNs compression and acceleration approaches. In [24], Mishra et al. propose a novel method to combine network quantization with knowledge distillation by jointly training a teacher network (full-precision) and a student network (low-precision) from scratch based on knowledge distillation. Besides, Zhou et al. [25] also propose a similar framework that the student network and the teacher network sharing lower layers and training simultaneously. Recently, researchers study knowledge distillation from another perspective rather than model compression. Born-again-network [26] optimizes the same network in generations by training the students parameterized identically to their teachers. Yang et al. [27] optimize deep networks in many generations and a few networks with the same architecture are optimized one by one. A similar idea has been proposed to extract useful information from earlier epochs in the same generation [28]. \n\nAs shown above, knowledge distillation typically transfers knowledge via feeding the sequence of random mini-batches sampled uniformly from the training data. In contrast, the student network in our proposed SLKD method is gradually guided using samples ordered in a meaningful sequence.",
            "score": 0.5251888923975367,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2440
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06451416015625
        },
        {
            "corpus_id": "232307742",
            "title": "Compacting Deep Neural Networks for Internet of Things: Methods and Applications",
            "text": "Heo et al. [127] proposed a method for knowledge transfer by extracting activation boundaries formed by cryptogenic neurons. He et al. [128] made some improvements to traditional distillation methods for the semantics split task. Liu et al. [129] transferred \"knowledge\" from multiple deep teacher networks to a deep network student network. Yang et al. [130] proposed a KD method based on transferring feature statistics from the teacher network to the student network. Song et al. [131] proposed a framework that transfers knowledge from a large pre-training model to a small model and performs both pre-training and fine-tuning at the same time.",
            "score": 0.5241779410033314,
            "section_title": "E. Other KD methods",
            "char_start_offset": 46459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 648
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 140,
                    "matchedPaperCorpusId": "75137175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0184783935546875
        },
        {
            "corpus_id": "235694419",
            "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation",
            "text": "The authors in [13] propose to use knowledge distillation for knowledge transfer from a teacher network to a student network. This process involves training the student network to match the output logits/soft predictions of the teacher network in addition to the training objective of the primary task. When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training. We use self-distillation in our approach to reduce the impact of biases in the model (see Sec. 4.1).",
            "score": 0.5240339901872932,
            "section_title": "Self-Distillation",
            "char_start_offset": 7755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 770
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1033935546875
        },
        {
            "corpus_id": "233407431",
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "text": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
            "score": 0.5239663277645721,
            "section_title": "Revisit of Knowledge Distillation",
            "char_start_offset": 8697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "244119160",
            "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
            "text": "Knowledge distillation describes a class of methods for the knowledge transfer from teacher network to student network. In our model, the student network S \u03b8 is trained over the same architecture but different parameters as teacher model T \u03b8 . The teacher has previously been trained, and we freeze its parameters when training the student network. \n\nWe transfer the knowledge from teacher to student by minimizing the similarity distance between the output of student network and the soft label generated by the teacher network. We use crossentropy loss to measure the two logits as (Romero et al., 2015). To further improve the sequence-tosequence student model, hard-assigned labels are also utilized. The final student network is trained to optimize the following compound objective: \n\nwhere H refers to the cross-entropy and V is a parameter to indicate the temperature of distillation. Later, we will use the method of the model level curriculum learning to process \u03bb in section 2.5. Note that the first term in Equation ( 3) corresponds to the traditional cross-entropy between the softmax layer's output of a (student) network and word distribution in response Y , whereas the second term is to learn from the softened output of the teacher network to strengthen its supervision for the student. \n\nIn the teacher model, we train it by using all the dataset with original order, while in the student model, the training starts from the step that consists of examples with the lowest difficulty. After that, data in the next step is aggregated to the current training dataset.",
            "score": 0.5227598202517589,
            "section_title": "Output Knowledge Distillation",
            "char_start_offset": 9984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 605,
                    "matchedPaperCorpusId": "2723173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05987548828125
        },
        {
            "corpus_id": "249805464",
            "title": "Edge Computing Technology Enablers: A Systematic Lecture Study",
            "text": "Knowledge distillation is a transfer learning technique that transmits knowledge from a complex model (teacher) to a simpler model (student) with fewer parameters. Knowledge distillation is an efficient solution for bringing high-performance neural networks to the Edge. Some examples of knowledge distillation applications include increasing image resolution [272], fast person reidentification in a camera surveillance environment [273] and visual dialog comprehension [274].",
            "score": 0.5224531656880671,
            "section_title": "5) Knowledge distillation",
            "char_start_offset": 80436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 477
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 365,
                    "matchedPaperCorpusId": "233401495"
                },
                {
                    "start": 433,
                    "end": 438,
                    "matchedPaperCorpusId": "54457546"
                },
                {
                    "start": 471,
                    "end": 476,
                    "matchedPaperCorpusId": "235298813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0352783203125
        },
        {
            "corpus_id": "247315450",
            "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation",
            "text": "Knowledge distillation (Hinton et al., 2015) is a class of methods that transfers knowledge from a pre-trained teacher network to a student network. Assume that we are training a classifier p(y|x; \u03b8) with |V| classes, and we can access the pre-trained teacher q(y|x). Instead of minimizing the crossentropy loss between the ground-truth label and the model output probability, knowledge distillation uses the teacher model prediction q(y|x) as a soft target and minimizes the loss:\n\n(1) In neural machine translation, the standard training objective is the cross-entropy loss, which minimizes the negative log-likelihood as follows:\n\nwhere X = {x 1 , ..., x N } and Y = {y 1 , ..., y T } are the source sentence and the target sentence, respectively. Kim and Rush (2016) proposed to train the student model to mimic the teacher's prediction at each decoding step, which is called Word-level Knowledge Distillation (Word-KD) and its loss is calculated as follows:\n\nq(y t = k|y <t , X)\u00d7 log p(y t = k|y <t , X, \u03b8).\n\n(3) Conventional offline knowledge distillation only allows the student to learn from static pre-trained teacher models. On the contrary, online knowledge distillation trains teachers from scratch and dynamically updates them, so the student learns from different teachers during the training process. Zhang et al. (2018) first overcame the offline limitation by training peer models simultaneously and conducted an online distillation in one-phase training between peer models. Since mutual learning requires training multiple networks, Zhu et al. (2018); Song and Chai (2018) proposed to use a single multi-branch network for online knowledge distillation, which treats each branch as a student and the ensemble of branches as a teacher. The multi-branch architecture subsequently became the mainstream for online knowledge distillation (Guo et al., 2020;Chen et al., 2020;Wu and Gong, 2020). Besides, Furlanello et al. (2018) performed iterative selfdistillation where the student network is identical to the teacher in terms",
            "score": 0.5212634622246787,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4743,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1316,
                    "end": 1335,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1552,
                    "end": 1569,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1571,
                    "end": 1591,
                    "matchedPaperCorpusId": "44119099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1380615234375
        },
        {
            "corpus_id": "270711372",
            "title": "Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction",
            "text": "The term \"knowledge distillation\" proposed by [Hinton et al., 2015] refers to a process in which a well-trained teacher model transfers its knowledge to a student model.One crucial role of knowledge distillation is performance enhancement.Given the prior knowledge from the teacher models, the student models may have better performance than the teacher models.Then few samples are illustrated.[Ahn et al., 2019] propose a creative framework that develops knowledge transfer by maximizing the information betwixt the teacher network and the student network.[Ahn et al., 2019] introduced this teacher-student mechanism to the transformer model to deal with image issues.",
            "score": 0.5207879120486507,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 24514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 557
                },
                {
                    "start": 557,
                    "end": 669
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 412,
                    "matchedPaperCorpusId": "17466161"
                },
                {
                    "start": 557,
                    "end": 575,
                    "matchedPaperCorpusId": "17466161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0296478271484375
        },
        {
            "corpus_id": "245334968",
            "title": "Anomaly Discovery in Semantic Segmentation via Distillation Comparison Networks",
            "text": "Knowledge distillation aims to transfer the knowledge of a large network (a.k.a teacher network) to a small network (a.k.a student network). The student network may have an approximate predictive ability as the teacher network. Knowledge distillation methods can be divided into three types: logits-based approaches (Cho and Hariharan 2019;Hinton, Vinyals, and Dean 2015;Phuong and Lampert 2019;Xie et al. 2020;Yang et al. 2019;Zhang et al. 2018), feature-based approaches (Komodakis and Zagoruyko 2017;Romero et al. 2014), and relation-based approaches (Tung and Mori 2019;Yim et al. 2017;Romero et al. 2014). In this paper, we follow the feature-based paradigm (Romero et al. 2014) by proposing a distribution distillation. However, DiCNet is flexible since there are no strict constraints for the model size of the student branch.",
            "score": 0.5204646481671442,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 833
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 340,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 371,
                    "end": 395,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 395,
                    "end": 411,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 411,
                    "end": 428,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 428,
                    "end": 446,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 473,
                    "end": 503,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 554,
                    "end": 574,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 574,
                    "end": 590,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0816650390625
        },
        {
            "corpus_id": "251493147",
            "title": "Self-Knowledge Distillation via Dropout",
            "text": "Knowledge distillation [20] is a learning method for transferring knowledge from a large and complex network (known as a teacher network) into a small and simple network (known as a student network). A number of variants have been proposed, inspired by the original method of knowledge distillation, such as the previously mentioned teacher-student framework [19,31,35]. \n\nOn the other hand, research that breaks away from the aforementioned teacher-student framework has been proposed. Deep Mutual Learning (DML) [49] is a distillation method in which a teacher network and a student network distill the knowledge from each other. Data-Distortion Guided Self-Distillation (DDGSD) [43] distills knowledge between different distorted data. Be Your Own Teacher (BYOT) [48] distills knowledge between its deeper and shallow layers. Class-wise self-knowledge distillation (CS-KD) [46] matches the posterior distributions of a model between intra-class instances. We visualize these prior methods in diagram forms in Figure 1. \n\nThese methods have attempted to distill the knowledge from a network within itself. Note that prior methods require subnetworks with additional parameters or request additional label information. Also, some research requires an additional procedure to distort the input data. We remark that these methods are computationally expensive by training additional networks from scratch or traversing two instances from the beginning to the end of a network, and these points are different from ours. \n\nSimilar to our work, several semi-supervised and selfsupervised learning methods have been investigated. Several studies attempt to solve the semi-supervised task using past selves as teacher networks [25,40]. Our method and the self-supervised literature [2,3,9,12,15,47] have a similar idea of comparing two representations, however, ours does not require augmentations of the input data and additional modules with parameters. The idea of distillation from the ensemble of model for uncertainty estimation is also investigated [8,26,29,30]. \n\nWe remark that there are several works on knowledge distillation related to dropout. Specifically, there are several works that distill the output from Monte Carlo dropout techniques [1,13].",
            "score": 0.5197694882229934,
            "section_title": "Related Work",
            "char_start_offset": 5318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1779,
                    "end": 1781,
                    "matchedPaperCorpusId": "227118869"
                },
                {
                    "start": 1781,
                    "end": 1783,
                    "matchedPaperCorpusId": "235651581"
                },
                {
                    "start": 1783,
                    "end": 1786,
                    "matchedPaperCorpusId": "219687798"
                },
                {
                    "start": 1786,
                    "end": 1789,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1789,
                    "end": 1792,
                    "matchedPaperCorpusId": "232110471"
                },
                {
                    "start": 2056,
                    "end": 2059,
                    "matchedPaperCorpusId": "173188734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128662109375
        },
        {
            "corpus_id": "264323617",
            "title": "Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation",
            "text": "The compression of a DNN is common practice for obtaining a lightweight network for low-cost and resource-scarce hardware devices. Knowledge distillation is a particular method that involves training to transfer knowledge from a larger network to a different one that has a significantly smaller size. Bucilua et al. [31] in their important paper successfully demonstrated for the first time that the knowledge acquired by a large ensemble of models can be transferred to a single small model. The aim of using this method is to train the student network so that it can reproduce the performance of the teacher network, but with fewer resources. \n\nThe process of applying knowledge distillation consists of the following main stages: (1) definition of teacher and student networks, (2) training of the teacher network, and (3) training of the student network with knowledge transfer from the teacher network. They are detailed below.",
            "score": 0.5194410857483636,
            "section_title": "III. METHODOLOGY A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 22290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0802001953125
        },
        {
            "corpus_id": "270077283",
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "score": 0.5194194945481558,
            "section_title": "Related Work",
            "char_start_offset": 5661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 152
                },
                {
                    "start": 154,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2140,
                    "end": 2143,
                    "matchedPaperCorpusId": "198179767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47998046875
        },
        {
            "corpus_id": "271488200",
            "title": "A Review of Recent Hardware and Software Advances in GPU-Accelerated Edge-Computing Single-Board Computers (SBCs) for Computer Vision",
            "text": "Knowledge distillation involves transferring knowledge from a large, cumbersome model (the teacher) to a smaller, more efficient model (the student). Techniques such as Hinton's Knowledge Distillation [84] and Born-Again Networks [85] aid in this knowledge transfer. Recent advancements incorporate attention mechanisms and multi-stage distillation to enhance the student model performance and accuracy. Knowledge distillation is pivotal for transferring knowledge from large, complex models to smaller, efficient ones, facilitating deployment on resource-constrained edge-computing devices. Various approaches have evolved over time, enabling the compression of complex models without significant performance loss. Early techniques focused on transferring knowledge from a large, well-trained teacher model to a smaller student model, typically involving mimicking the teacher's behaviour through soft labels or intermediate representations. Notable early approaches include the following: Recent advancements in knowledge distillation have focused on integrating selfdistillation techniques and exploring the synergy between different distillation approaches. These developments aim to enhance the scalability and adaptability of knowledge distillation methods, enabling the efficient deployment of compact and accurate CV models in diverse edge-computing applications.",
            "score": 0.5190670570868088,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 43009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "220364494"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "238203535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042572021484375
        },
        {
            "corpus_id": "227013462",
            "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation",
            "text": "Knowledge Distillation (KD) [3,7] is a contemporary technique for transferring learning across neural network models. Typically, knowledge from one or more complex and deep models (called Teachers) is distilled into a relatively lightweight model (called Student). The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via shar-ing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student. Thus, Knowledge Distillation has established itself as a very useful and practical tool because of its simplicity and potential. \n\nThe samples used for performing distillation constitute the \"Transfer set\", which is typically required to be constructed using the data sampled from the target distribution. Therefore, the most commonly used transfer set is the original training dataset on which the Teacher model was trained. However, this requirement has been identified as a limitation (e.g. [17,15]) since it is common now-a-days that many popular pre-trained models are released without providing access to the training data (e.g. Facebook's Deepface model trained on 4M confidential face images). This is due to one or more practical constraints such as (i) privacy (e.g. models trained on patients' data from hospitals), (ii) property (proprietary data of companies that invest on collection and annotation), and (iii) transience (observations from the training of a reinforcement learning environment do not exist). \n\nTo handle this \"data-free\" (or zero-shot) distillation scenario, most of the approaches broadly follow either of the two ways: (i) compose a synthetic transfer set by directly utilizing the trained Teacher model that acts as a proxy to the target data (e.g.",
            "score": 0.518925832646207,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 31,
                    "end": 33,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1355,
                    "end": 1358,
                    "matchedPaperCorpusId": "162183830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "236272577",
            "title": "Modeling microscopic and macroscopic information diffusion for rumor detection",
            "text": "To further improve the model performance on rumor detection task, inspired by knowledge distillation technique 43 -which involves capturing the \"dark knowledge\" from a teacher model to guide the learning of a student network, has emerged as an essential technique for model improving. We first train a teacher model via Algorithm 2, and then transfer the knowledge from the teacher model to a student model, here in our work, the student model has the same model architecture as the teacher model (self-distillation 44,45 ). Before introducing the concrete training procedure of MMRD with knowledge distillation, we first give the definition of the softmax with temperature: \n\nwhere \u03c4 is a temperature that is normally set to 1, using a higher value for temperature \u03c4 to produce a softer probability distribution over the class, which brings the advantage that the information carried by the negative label will be relatively amplified, and the model training will pay more attention to the negative label. \n\nThe concrete training procedure of the knowledge distillation is listed in Algorithm 4.6, and Figure 1C gives a visualization of Algorithm 4.6. The objective function of the knowledge distillation is a weighted average of two different objective functions. The first loss function is the cross-entropy with the soft targets and it is computed using the same high temperature \u03c4 t = in the softmax of the student model as was used for generating the soft targets from the teacher model. is soft output from student model. The second loss function is the cross-entropy with the ground truth. This is computed using exactly the same logits in softmax of the student model but at a temperature of 1. \n\nwhere y i is the ground truth and y \u03c4 H \u02c6= softmax(FC( ), = 1) \n\nis the hard output of student model. Finally, the objective function of knowledge distillation is: \n\nwhere \u03b2 is the balance weight, which always been a considerably lower value since the am- plitude of the gradients produced by the scale of the soft output as \u03c4 1 2 \u2215 . This ensures that the relative contributions of the hard and soft targets remain roughly unchanged. 43 gorithm 3. Training procedure of MMRD with knowledge distillation. \n\n\uf047 \uf050 , the max-order number K , temperature \u03c4 . \n\nInput: Student-optimized parameters \u0398. We now present the findings from our experimental evaluations.",
            "score": 0.5189229147097391,
            "section_title": "| Rumor detection with knowledge distilling",
            "char_start_offset": 20002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2210
                },
                {
                    "start": 2213,
                    "end": 2259
                },
                {
                    "start": 2262,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 113,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "199405591"
                },
                {
                    "start": 519,
                    "end": 521,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 2141,
                    "end": 2143,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "231201903",
            "title": "A Novel Multi-Knowledge Distillation Approach",
            "text": "In this paper, we propose a novel knowledge distillation approach, named multi-knowledge distillation (MKD) that employs autoencoders to learn and transfer multiple kinds of knowledge from the teacher network to the student network. Experimental results show that MKD achieves notable improvements than that is trained from scratch, and MKD is superior to other knowledge distillation approaches. \n\nIn the future work, we would like to utilize the reinforcement learning to choose the most useful knowledge to further narrow and even eliminate the performance gap between the teacher network and the student network.",
            "score": 0.5182619904728523,
            "section_title": "Conclusion",
            "char_start_offset": 12829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092529296875
        },
        {
            "corpus_id": "233210081",
            "title": "Dual discriminator adversarial distillation for data-free model compression",
            "text": "Caruana et al. [41] first confirm that one ensemble of networks could transfer the knowledge to the single network. Then Ba et al. [42] propose to teach the student network by penalizing the difference of logits between the teacher and student. Later, the concept of Knowledge Distillation (KD) is introduced by Hinton et al. [24] to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher. Note that, KD requires original training data to capture the valuable knowledge from the teacher network. However, the knowledge contained in the soft-labels is insufficient when the teacher network goes deeper. To tackle this issue, some improvements have been made, which extend KD by utilizing the intermediate representation as supervision. For example, Fitnets [27] forces the student to learn the similar intermediate features as teacher's which are defined as hints. Zagoruyko et al. [28] define the attention maps from the intermediate features as the knowledge and then obtain a better performance compared to the one using original feature itself. Moreover, FSP [29] designs the flow distillation loss to force the student to mimic flow matrices of teacher among the feature maps between two layers. RKD [30] transfers mutual relations of data examples by the distance-wise and angle-wise distillation losses. SP [31] preserves the pairwise similarities in student's representation space instead to mimic the representation space of the teacher. CTKD [43] combines the knowledge from different teacher models to improve the student's performance in KD. Due to excellent performance, knowledge distillation has been used to solve a variety of complex applications such as object detection [26,44], semantic segmentation [45], lane detection [46], face recognition [47][48][49] and action recognition [50]. Nevertheless, the above traditional data-driven knowledge distillation methods need full of original training data, which are difficult to be obtained in real world. Thus, several few-shot knowledge distillation approaches are proposed to di",
            "score": 0.5180468187640946,
            "section_title": "Data-driven knowledge distillation",
            "char_start_offset": 7759,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1358,
                    "end": 1362,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1500,
                    "end": 1504,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1747,
                    "end": 1751,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1988,
                    "end": 1991,
                    "matchedPaperCorpusId": "201666186"
                },
                {
                    "start": 2015,
                    "end": 2019,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 2036,
                    "end": 2040,
                    "matchedPaperCorpusId": "199405591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "259095575",
            "title": "Faithful Knowledge Distillation",
            "text": "Knowledge distillation (KD) is the process of transferring information and representations from a larger teacher network, f t : R n \u2192 [0, 1] C , to a usually significantly smaller student network, f s : R n \u2192 [0, 1] C , with the standard aim of improving the performance of the student over regular training. A popular form of KD which we refer to as standard distillation was introduced by Hinton, Vinyals, and Dean (2015, SD). SD encourages the clean outputs of a student network to match the clean outputs of its teacher whilst also balancing the accuracy of the student's predictions.",
            "score": 0.5177469602893308,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 588
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1719970703125
        },
        {
            "corpus_id": "273482581",
            "title": "Preview-based Category Contrastive Learning for Knowledge Distillation",
            "text": "The foundational concept of knowledge distillation was introduced by KD [19] where it fits the logits of the teacher and student model by reducing the KL divergence to transfer dark knowledge, which can significantly improve the performance of the student network without introducing extra parameters. Since then, numerous approaches have been proposed to explore various kinds of knowledge and narrow the gap between the teacher and the student. According to the type of knowledge used for distillation, existing methods can be mainly classified into three categories, i.e., response-based KD, feature-based KD, and relation-based KD [28]. \n\nResponse-based KD directly transfers the prediction of the penultimate output layer (i.e., logits) as knowledge and wants the student model to imitate the output of the teacher model directly. The most typical is the classic KD method [19]. DKD [29] revealed the classic KD method is a coupling formula and proposed decoupled knowledge distillation. LSKD [30] investigated the reason rendering knowledge distillation and label smoothing to exert distinct effects on model's potential ability in sequential knowledge transferring. MLD [31] explored stronger logits distillation through multi-level prediction alignment. Feature-based KD aims to transfer the feature of the intermediate layer as knowledge. FitNets [20] proposed to learn the middle layer of the network as knowledge. AT [21] introduced the attention mechanism and forced the student network to mimic the spatial maps of the teacher model. SemCKD [32] exploited intermediate knowledge by semantic calibration and feature-map transfer across multilayers. Different from the single instance-based knowledge, relation-based knowledge regards the relations and structures among various data samples or layers as knowledge. RKD [33] and CC [34] transferred the structured feature correlation among samples as knowledge. FSP [35] established the Gram matrix for different feature maps to transfer knowledge. LKD [36] distilled local correlation consistency. ICKD [37] mined structural information of inter-channel correlation. ReviewKD [38] studied the factor of connection path cross levels between teacher and student networks. EKD [39] improved the transfer effectiveness of teacher knowledge by evolutionary KD.",
            "score": 0.5175946155277442,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 6849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "264467753"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "matchedPaperCorpusId": "248598127"
                },
                {
                    "start": 1830,
                    "end": 1834,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1842,
                    "end": 1846,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1926,
                    "end": 1930,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 2013,
                    "end": 2017,
                    "matchedPaperCorpusId": "222178995"
                },
                {
                    "start": 2064,
                    "end": 2068,
                    "matchedPaperCorpusId": "244101388"
                },
                {
                    "start": 2137,
                    "end": 2141,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38671875
        },
        {
            "corpus_id": "268857025",
            "title": "Task Integration Distillation for Object Detectors",
            "text": "Knowledge distillation is an effective model compression technique that facilitates the transfer of knowledge from a large model to a smaller one, enabling the smaller model to achieve, or even approximate, the performance of the larger model.Initially introduced by Hinton et al. [12], knowledge distillation has been predominantly applied in the image classification domain.The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.\n\nKnowledge distillation was initially proposed in the domain of image classification and has since been widely applied across various fields.Currently, knowledge distillation can be categorized into three types based on the source of knowledge: response-based knowledge distillation [3,5,22,35,42], relation-based knowledge distillation [23,24], and feature-based knowledge distillation [10,11,13,14].Response-based knowledge distillation, the earliest proposed method, involves extracting the output of the teacher network's last layer and directly mimicking the teacher's final prediction.Relation-based knowledge distillation extracts relationships between different layers or data samples within the teacher network, transferring such relational knowledge to the student to achieve distillation.Feature-based knowledge distillation involves extracting and aligning feature maps from both the teacher and student models to the same size, then having the student model's feature maps fit those of the teacher model, facilitating the transfer of knowledge.Due to its ease of handling knowledge sources and almost uniform performance across most domains, feature-based knowledge distillation has garnered widespread attention from scholars for its strong versatility.\n\nKnowledge distillation methods are not only widely applied in the field of computer vision, such as image retrieval [34] and face recognition [17], but also in natural language processing [20,30], speech recognition [15], recommendation systems [28], information security [44], multimodal data [43], and finance and securities [8].Knowledge distillation, with its capability to transfer knowledge between different models, has garnered significant attention across multiple domains due to its versatility.",
            "score": 0.5173764328872882,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 588
                },
                {
                    "start": 590,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1856
                },
                {
                    "start": 1858,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 875,
                    "end": 877,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 877,
                    "end": 880,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 926,
                    "end": 930,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 980,
                    "end": 983,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "250581330"
                },
                {
                    "start": 2000,
                    "end": 2004,
                    "matchedPaperCorpusId": "260068567"
                },
                {
                    "start": 2074,
                    "end": 2078,
                    "matchedPaperCorpusId": "235417286"
                },
                {
                    "start": 2103,
                    "end": 2107,
                    "matchedPaperCorpusId": "2552056"
                },
                {
                    "start": 2130,
                    "end": 2134,
                    "matchedPaperCorpusId": "245693198"
                },
                {
                    "start": 2152,
                    "end": 2156,
                    "matchedPaperCorpusId": "237490408"
                },
                {
                    "start": 2185,
                    "end": 2188,
                    "matchedPaperCorpusId": "203605465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "221818663",
            "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters",
            "text": "Knowledge Distillation.\n\nKnowledge distillation (Hinton et al., 2015;Freitag et al., 2017;Lin et al., 2020) is a widely used model acceleration and compression technique (Jiao et al., 2019;Sanh et al., 2019;. It treats the network predictions as the knowledge learned by the teacher network, since these predicted distributions contain the ranking information on similarities among categories. It then transfers this knowledge to the student network by optimizing it toward these predictions. The followed work extends this idea by providing more knowledge from different sources to the student network. FitNets (Romero et al., 2015) uses not only the predictions but also the intermediate representations learned by the teacher network to supervise the student network. For the Seq2Seq model, Kim and Rush (2016) proposes to use the generated sequences as the sequence-level knowledge to guide the student network training. Moreover, self-knowledge distillation (Hahn and Choi, 2019) even shows that knowledge (representations) from the student network itself can improve the performance.\n\nOur weight distillation, on the other hand, explores a new source of knowledge and a new way to leverage this knowledge. It transfers the knowledge in parameters of the teacher network to the student network via a parameter generator. Therefore, it is orthogonal to other knowledge distillation variants.\n\nTransfer Learning. Transfer learning aims to transfer knowledge from the source domain with rich resources to the target domain with limited data. Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007;Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018;Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019;Devlin et al., 2019).\n\nPerhaps the most related work is Platanios et al. (2018). Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator",
            "score": 0.5173543994276021,
            "section_title": "Related Work",
            "char_start_offset": 19286,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 107,
                    "matchedPaperCorpusId": "220480965"
                },
                {
                    "start": 612,
                    "end": 633,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1782,
                    "end": 1804,
                    "matchedPaperCorpusId": "15036406"
                },
                {
                    "start": 1804,
                    "end": 1821,
                    "matchedPaperCorpusId": "8153773"
                },
                {
                    "start": 1925,
                    "end": 1946,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1946,
                    "end": 1963,
                    "matchedPaperCorpusId": "17429744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1610107421875
        },
        {
            "corpus_id": "270123290",
            "title": "Relation Modeling and Distillation for Learning with Noisy Labels",
            "text": "Knowledge distillation aims to train smaller, lightweight models (student networks) by leveraging supervised information from larger, high-performance models (teacher networks) to enhance their performance and accuracy.In this process, the supervised information obtained from the teacher network's outputs is referred to as \"knowledge,\" and the process of transferring this knowledge to the student network is termed \"distillation.\"For instance, Embedded Graph Alignment (EGA) [39] transfers knowledge from the teacher's network to the student's network by aligning the student's graph with the teacher's graph.A simple K-way projection method was used in [40] to transfer knowledge from various levels in the teacher's network to the student's network, distilling knowledge in this way.In DIST [41], it was pointed out that the prediction gap between teacher and student networks may be large, and the use of KL scatter will lead to a decrease in the training effect, and the method replaces KL scatter with Pearson correlation coefficient, which achieves some results.Knowledge transfer in CIRKD [42] using pixel-to-pixel and pixel-to-region distillation for matching with memory banks.",
            "score": 0.5170925161035932,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 1071
                },
                {
                    "start": 1071,
                    "end": 1189
                }
            ],
            "ref_mentions": [
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "257313180"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "248178091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.122314453125
        },
        {
            "corpus_id": "234884567",
            "title": "Compression of Person Re-identification Model Based on Depthwise Separable Convolutional Network",
            "text": "Weight pruning and matrix SVD decomposition are the methods of early model compression, but the compression rate is far from satisfactory. Knowledge distillation is an effective method for model compression, which transfers knowledge from a complex model to a simple model. The teacherstudent model has been widely used in semi-supervised learning, model compression and knowledge distillation. It can simplify the large-scale network model without losing too much precision, which makes it possible to distribute the network to the client. The core idea of the teacher-student model is to supervise the student network through the output of the teacher network so that the student net-work can learn the ability to recognize the key features from the teacher network. \n\nKnowledge distillation supervises and induces student network training through soft targets of teacher network. This is because the rough use of one-hot encoding will lose extra information about the similarity between classes and within classes. The teacher model outputs a continuous label prediction distribution for each sample, and the available monitoring information in this continuous prediction distribution is more than the traditional one-hot encoding. In addition, the method of adding loss function in the middle layer [5] can transfer the knowledge expressed in the middle layer by learning the middle layer feature map of the teacher network. The mean teacher model [6] averages model weight parameters by multiple iterative training to strengthen the supervision of unlabeled samples. Deep mutual learning [7] uses multiple student models instead of teacher models to supervise and train each other so that they can learn from each other and make progress together. Mutual mean teaching [8] design a symmetric network with hard labels and soft labels, and the two groups of networks average the model weight by learning from each other.",
            "score": 0.517076125140142,
            "section_title": "Related works",
            "char_start_offset": 2628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0482177734375
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "Previous knowledge distillation methods often transfer knowledge via minimizing the Kullback-Leibler (KL) divergence between the logits from the last layer of the teacher network and those from the last layer of the student network. Our method splits both the student and the teacher networks into several same level blocks, each of which is associated with an auxiliary classifier to get the softmax outputs. That is to say, each block's auxiliary classifier of the teacher network corresponds to the same block's auxiliary classifier of the student network. The logit outputs as the knowledge will then be distilled from the teacher network's auxiliary classifier to the student network's auxiliary classifier at the same stage, which can enable student network to learn more knowledge from teacher network at different levels. The distillation loss between student network and teacher network is formulated as: where K is the number of auxiliary classifiers. Note that the temperature used in p s i and p t i is denoted as T 1 according to Eq. ( 1). The superscripts s and t represent the student network and the teacher network, respectively. L K L (.) represents the Kullback-Leibler (KL) divergence, which is defined as: \n\nBy using the multi-stage offline teacher-student knowledge distillation, we allow each student's shallow/deep block to mimic the corresponding teacher's shallow/deep block via learning from teacher logit knowledge.",
            "score": 0.5167574470882311,
            "section_title": "Stage-Wise Response Distillation",
            "char_start_offset": 16603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1443
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14453125
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "Existing teacher-student knowledge distillation methods mainly focus on how to make full use of the teacher network to transfer more knowledge to the student network. In contrast, student self-distillation methods use a pre-trained student network as a teacher to teach a same randomly initialized student network, and transfer knowledge from one part of student network to guide the learning of other parts of the same student network. Generally, self-distillation methods always adopt the outputs of the last layer of the pre-trained student network as the regularization to guide the learning of the same student network (Yuan et al., 2020), and they also explore the knowledge from the subsequent layers of the student network to its preceding layers (Zhang et al., 2019). It is argued that the subsequent layers contain more compact semantic information but less redundant useless information, so that it is naturally assumed that distilling the knowledge from subsequent layers to the preceding ones can be better. However, we empirically found (shown in Sect. 5) that the self-distillation performance can be improved by distilling the knowledge from the preceding layers of the student network to its subsequent layers. The possible reason is that the preceding layers contain more dark knowledge (Hinton et al., 2015) that is constructive for self-distillation. From the student perspective, the knowledge from the preceding layers to its subsequent ones can be regarded as the previously learned knowledge, and the corresponding self-distillation is the self-reflection via reviewing the previously learned knowledge from the preceding layers. Thus, the student performance can be improved by consolidating the previously learned knowledge via self-distillation. \n\nHere, we propose a new student self-distillation method called knowledge review distillation based on the real-world learning practice, known as self-reflection. It is noteworthy that our knowledge review distills the logit information from the shallow layers to the deep layers, which is in the reverse direction of standard self-distillation. Particularly, it can also be shown in Fig. 1. In such an approach, the student network is able to continuously review the knowledge that has been gained so far at each stage, allowing itself to reflect on its learning. Our proposed method makes up for the deficiency of using student network's last block to improve self performance.",
            "score": 0.5162957359344171,
            "section_title": "Cross-Stage Review Distillation",
            "char_start_offset": 18082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2338
                },
                {
                    "start": 2339,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 643,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 755,
                    "end": 775,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1534423828125
        },
        {
            "corpus_id": "260447668",
            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
            "text": "In this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
            "score": 0.516232880397338,
            "section_title": "B. Key Idea and Training Framework",
            "char_start_offset": 18765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2175
                },
                {
                    "start": 2178,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "253236895",
            "title": "Teacher-Student Architecture for Knowledge Learning: A Survey",
            "text": "Inspired by recent efforts, multiple-teacher networks have been introduced in knowledge learning, where a student network simultaneously receives knowledge transferred from multiple teachers. Consequently, a student can robustly learn comprehensive and different knowledge under the guidance of multiple teacher networks. Averaging multiple teachers is a commonly-used approach to incorporate the potentially diverse knowledge from teachers (i.e., each teacher with an identical importance weight); concretely, a student network aims to learn the average softened logits of multiple teacher networks. Papernot et al. [63] introduce multiple teacher networks in Private Aggregation of Teacher Ensembles (PATE), where each teacher network is trained on a private and particular dataset; and then the student network aims to learn the average voting of soft logits of these teachers. In Mean Teacher [40], the average model weights of multiple teachers are regarded as the knowledge to guide the training of the student network. Note that multiple teacher networks could be heterogeneous since these teacher networks can be trained in various environments (e.g., different data distributions). This suggests that the transferred knowledge from various teachers can contribute differently to the student learning performance, so that the student network may learn more knowledge from similar teacher networks. As a result, averaging multiple teacher networks could be sub-optimal by assigning each teacher an identical importance weight. Thus, to learn more representative and critical knowledge from multiple teacher networks, some advanced teacher weighting approaches are introduced to assign a particular importance weight for each teacher network, so that the student network can learn more representative knowledge from important teachers. For example, Adaptive Multi-teacher Multilevel Knowledge Distillation (AMTML-KD) [41] includes multiple teacher networks, where each teacher network is learned an instance-level importance weight for adaptively integrating the intermediate feature representations from all teachers. Consequently, a student network can fully learn potentially diverse knowledge from multiple teachers. In addition to the logits from multiple teachers, You et al. [42] also additionally consider the relative similarity between intermediate representations of samples as one type of dark knowledge to guide the training of a student network. Concretely, the triplets are to encourage the consistency of relative similarity relationships between the student and the teachers.",
            "score": 0.5158112160993953,
            "section_title": "Multiple Teachers",
            "char_start_offset": 22145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2465
                },
                {
                    "start": 2466,
                    "end": 2598
                }
            ],
            "ref_mentions": [
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1923,
                    "end": 1927,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 2288,
                    "end": 2292,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2120361328125
        },
        {
            "corpus_id": "221703021",
            "title": "Noisy Self-Knowledge Distillation for Text Summarization",
            "text": "Knowledge Distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher.\n\nLet T and S denote teacher and student models, respectively. Let f T and f S be functions of the teacher and student. The models are typically neural networks and function f can be in principle defined using the output of any network layer (e.g., a hidden or softmax layer). Knowledge distillation methods are commonly expressed as minimizing 694 an objective function over training set X :\n\nwhere l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014;Romero et al., 2014;Zagoruyko and Komodakis, 2017;. Other work integrates an ensemble of teachers in order to improve the student (Urban et al., 2016), trains a succession of students (Furlanello et al., 2018), introduces a \"teacher assistant\" for better knowledge transfer (Mirzadeh et al., 2019), and regularizes multi-task agents (Parisotto et al., 2015;Teh et al., 2017) in reinforcement learning. Compared to direct training, knowledge distillation provides a more stable training process which leads to better performing student models (Hinton et al., 2015;Phuong and Lampert, 2019). Recent work (Furlanello et al., 2018;Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section).\n\nKnowledge distillation has been also shown to improve results for various NLP tasks.  use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016;",
            "score": 0.51576144778242,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6953,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1060,
                    "end": 1082,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1102,
                    "end": 1132,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1266,
                    "end": 1291,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1439,
                    "end": 1456,
                    "matchedPaperCorpusId": "31009408"
                },
                {
                    "start": 1645,
                    "end": 1670,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1684,
                    "end": 1709,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07904052734375
        },
        {
            "corpus_id": "266359238",
            "title": "Decoupled Knowledge with Ensemble Learning for Online Distillation",
            "text": "Knowledge distillation is usually applied to model compression, wich is divided into offline and online knowledge distillation [6]. \n\nOffline knowledge distillation demands a pretrained teacher network and a student network, and the student learns from the teacher and the ground truth simultaneously. Hinton et al. put forward the concept of knowledge distillation and proposed to transfer knowledge from a cumbersome teacher to a compact student by aligning the soft distribution of the teacher and the student [9]. In general, offline knowledge distillation tends to train a student network with surprisingly good performance and is very effective for compressing a large network, however, it usually follows a two-stage paradigm, which greatly increases the training time and computational overhead. \n\nOnline knowledge distillation is a onestage end-to-end training method, which transfers knowledge among multiple networks in a mutual manner with no need for extra time and computing resources to pretrain a cumbersome teacher network. Zhang et al. pioneer a deep mutual learning method to explore the feasibility of online knowledge distillation [32], which distills knowledge among multiple parallel models with the same input. For maintaining the diverisity of the multiple networks, Guo et al. randomly augment the same input for each individual network and aggregate all the output logits into an ensemble soft label for optimizing each network in the online distillation [7]. Wu et al. aggregate with stacking strategy and utilize temporal mean teacher to derive a robust prediction for training and inference [27].",
            "score": 0.5156693974189479,
            "section_title": "Related work",
            "char_start_offset": 4403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1626
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1152,
                    "end": 1156,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1121826171875
        },
        {
            "corpus_id": "198179767",
            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
            "text": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
            "score": 0.5155854766336335,
            "section_title": "B. Formulation",
            "char_start_offset": 17544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "3643430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55322265625
        },
        {
            "corpus_id": "202889259",
            "title": "Revisit Knowledge Distillation: a Teacher-free Framework",
            "text": "To examine the common belief on dark knowledge in KD, we conduct two exploratory experiments: \n\n1) The standard knowledge distillation is to adopt a teacher to teach a weaker student. What if we reverse the operation? Based on the common belief, the teacher should not be improved significantly because the student is too weak to transfer effective knowledge. 2) If we use a poorly-trained teacher which has much worse performance than the student to teach the student, it is assumed to bring no improvement to the latter. For example, if a poorly-trained teacher with only 10% accuracy is adopted in an image classification task, the student would learn from its soft targets with 90% error, thus the student should not be improved or even suffer worse performance. We name the \"student teach teacher\" as Reversed Knowledge Distillation (Re-KD), and the \"poorlytrained teacher teach student\" as Defective Knowledge Distillation (De-KD) (Fig. 1). We conduct Re-KD and De-KD experiments on CIFAR10, CIFAR100 and Tiny-ImageNet datasets with a variety of neural networks. For fair comparisons, all experiments are conducted with the same settings. Detailed implementation and experiment settings are given in Appendix A.1.",
            "score": 0.5143051762907599,
            "section_title": "EXPLORATORY EXPERIMENTS AND COUNTERINTUITIVE OBSERVATIONS",
            "char_start_offset": 5433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0296478271484375
        },
        {
            "corpus_id": "242069370",
            "title": "Boost Precision Agriculture with Unmanned Aerial Vehicle Remote Sensing and Edge Intelligence: A Survey",
            "text": "The main objective of knowledge distillation is to train a student network from the teacher network while maintaining its generalization capability [204]. The student network is lighter, i.e., having a smaller model size and less computation, but with the same or comparable performance as the larger network. \n\nGreat efforts have been done to improve the supervision of student network by different knowledge transferred. Romero et al. [205] proposed a FitNets model which teaches the student network to imitate the hints from both middle layers and output layer of the teach network. Instead of hard labels that are used, the work in [206] utilizes soft labels as the representation from teacher network. Kim et al. [207] proposed a paraphrasing based knowledge transfer method which uses convolution operations to paraphrase the teacher model knowledge and translate it to a student model. From the point of teacher networks, student networks can also learn knowledge from multiple teachers [208]. \n\nIn the field of UAV based deep model inference, knowledge distillation is a promising direction. In [209], YOLO + MobileNet model acts as the teacher network, while the pruned model functions as the student network, and knowledge distillation algorithm is used to improve the detection accuracy of the pruned model. Qiu et al. [210] propose to distill knowledge to a lighter distilled network through soft labels from trained teacher network MobileNet. Similar applications using knowledge distillation for model compression can be found in [211,212].",
            "score": 0.5142625398442713,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 49981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1554
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 153,
                    "matchedPaperCorpusId": "211062209"
                },
                {
                    "start": 636,
                    "end": 641,
                    "matchedPaperCorpusId": "84176918"
                },
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 994,
                    "end": 999,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1103,
                    "end": 1108,
                    "matchedPaperCorpusId": "226730806"
                },
                {
                    "start": 1544,
                    "end": 1549,
                    "matchedPaperCorpusId": "231977039"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "233196782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033966064453125
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Several recent knowledge distillation methods focus on the strategy of knowledge distillation. Born again network (BAN) [27] presents the effectiveness of sequential knowledge distillation via the networks with an identical architecture. A curriculum learning method [28] employs the optimization trajectory of a teacher model to train students. Collaborative learning approaches [4,5,6] attempt to learn multiple models with distillation jointly, but their concept is not well-suited for asymmetric teacher-student relationship, which may lead to suboptimal convergence of student models. \n\nThe model capacity gap between a teacher and a student is addressed in [2,29,3]. TAKD [3] employs an extra network to reduce model capacity gap between teacher and student models, where a teacher transfers knowledge to a student via a teaching assistant network with an intermediate size. \n\nAn early stopping technique for training teacher networks is proposed to obtain better transferable representations and a neural architecture search is employed to identify a student model with the optimal size [2]. Our work proposes a novel student-friendly learning technique of a teacher network to facilitate knowledge distillation.",
            "score": 0.5134216467141843,
            "section_title": "How to distill",
            "char_start_offset": 5931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1219
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 383,
                    "end": 385,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 669,
                    "end": 671,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "matchedPaperCorpusId": "208513309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0196380615234375
        },
        {
            "corpus_id": "249209742",
            "title": "What Knowledge Gets Distilled in Knowledge Distillation?",
            "text": "Knowledge distillation aims to transfer useful information from a teacher network to a student network, with the primary goal of improving the student's performance for the task at hand. Over the years, there has a been a deluge of novel techniques and use cases of knowledge distillation. Yet, despite the various improvements, there seems to be a glaring gap in the community's fundamental understanding of the process. Specifically, what is the knowledge that gets distilled in knowledge distillation? In other words, in what ways does the student become similar to the teacher? Does it start to localize objects in the same way? Does it get fooled by the same adversarial samples? Does its data invariance properties become similar? Our work presents a comprehensive study to try to answer these questions and more. Our results, using image classification as a case study and three state-of-the-art knowledge distillation techniques, show that knowledge distillation methods can indeed indirectly distill other kinds of properties beyond improving task performance. And while we believe that understanding the distillation process is important in itself, we also demonstrate that our results can pave the path for important practical applications as well.",
            "score": 0.512452274768723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07275390625
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher). \n\nKnowledge distillation aims to improve the performance of the student network by providing additional supervision from a teacher network. To the best of our knowledge, exploiting knowledge transfer to compress model was first proposed in C. Bucilu\u01ce et al. [4]. They trained a compressed/ensemble model of strong classifiers with pseudo-labeled data, and reproduced the output of the original larger network. However, the work is limited to shallow models. The idea has been adopted in [5] as knowledge distillation to compress deep and wide networks into shallower ones, where the compressed model mimicked the function learned by the complex model. Hinton et al. [6] popularized the concept of Knowledge Distillation to be extended to more practical uses. The work in [6] proposed knowledge distillation as a more general case of C. Bucilu\u01ce et al. [4] by adopting the concept of temperature parameter at the output of teacher. The student was trained to predict the output and the classification labels. \n\nThe main idea of knowledge distillation approach is to shift knowledge from a large teacher model into a small one by learning the class distributions output via softmax [6]. It has even been observed that the student learns much faster and more reliably if trained using outputs of teacher as soft labels, instead of one-hot-encoded labels. \n\nSince then, a number of knowledge distillation methods have been proposed, each trying to capture and transfer some characteristics of the teacher such as the representation space, decision boundary or intra-data relationship. Despite its simplicity, knowledge distillation demonstrates promising results in various image classification tasks. \n\nKnowledge distillation has proven empirically to be an effective technique for training a compact model [7], [11], [17].",
            "score": 0.5122620510904639,
            "section_title": "II. KNOWLEDGE DISTILLATION",
            "char_start_offset": 2403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 462,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1380,
                    "end": 1383,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11920166015625
        },
        {
            "corpus_id": "237571632",
            "title": "Learning Versatile Convolution Filters for Efficient Visual Recognition",
            "text": "Knowledge Distillation. Knowledge distillation, also known as teacher-student framework, aims to transfer knowledge from teacher models to student models, where student models usually enjoy a lighter architecture than that of teacher models. Hinton et al. [22] first proposed the concept of knowledge distillation by introducing the teacher's softened output. Romero et al. [52] further distilled the knowledge underlying the features of the teacher model to the student model. Chen et al. [6] proposed to distill the student network without provided data. However, the major aim of knowledge distillation is to improve the performance of student models rather than designing compact networks. \n\nIn summary, pruning methods remove the unimportant weights or filters, while the proposed versatile filters generate subfilters from the intrinsic filters. Quantization methods represent the weights or activations in low-bit values, while our method operates at the filter level. Compared with the related works on extracting filters during inference, e.g., FSNet [75] enforces weight sharing across nearby filters and Savarese et al. [55] shares weights across layers, the proposed versatile filters generate subfilters by handdesigned or learnable patterns.",
            "score": 0.5122279813884851,
            "section_title": "Model Compression",
            "char_start_offset": 10165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1255
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "53408116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10760498046875
        },
        {
            "corpus_id": "276250328",
            "title": "Multi-Level Decoupled Relational Distillation for Heterogeneous Architectures",
            "text": "In this paper, we further explore how to effectively transfer the dark knowledge during heterogeneous distillation for the first time. In traditional homogeneous distillation, relational knowledge distillation (RKD) [31] is generally considered as an effective method for transferring dark knowledge, as shown in Fig. 1. RKD aligns correlations or dependencies among multiple instances between the student and teacher networks. However, we find that the direct use of RKD in heterogeneous distillation causes a new problem: the over-amplification of the role of dark knowledge, which may reduce the confidence in the correct category of the teacher model. Since the latter is equally important in heterogeneous distillation due to the variability between architectures, this can directly contribute to the failure of the RKD method, as shown in Fig. 2. Facing such a dilemma, a question naturally arises: can we effectively transfer the abundant dark knowledge while keeping the confidence of the correct category during heterogeneous distillation? \n\nTo answer this question, we present an innovative framework called Multi-Level Decoupled Relational Knowledge Distillation (MLDR-KD) for heterogeneous distillation. Specifically, we first propose Decoupled Finegrained Relation Alignment (DFRA), in which model logits are first decoupled into multiple finegrained relationships between different categories under each image and different images un-der each category. Due to the multiple steps finegrained decoupling, the subsequent alignment is sensitive to whether the model classifies correctly, and it can magnify the gap when the classification results of student model and teacher model are not aligned. As a result, our method can well transfer dark knowledge while enhancing the confidence of the classification results during heterogeneous distillation. Further, we apply the DFRA to both logit and feature levels, and present the Multi-Scale Dynamic Fusion (MSDF) module in the feature level. In the MSDF module, the multiscale feature maps of different stages in student model are projected into multiple logits, and a gated network is used to dynamically fuse these logits.",
            "score": 0.5118540457950675,
            "section_title": "Decoupled Relational KD",
            "char_start_offset": 2390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "214623002",
            "title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives",
            "text": "Knowledge Transfer. Our method also has a connection with the research field of Knowledge Transfer (KT). Top-performing deep CNN models suffering from intensive computational demands are hindered from being embedded into resource-aware applications. To narrow the gap between theoretical performance and real-world feasibility, Dark Knowledge Distillation [9] takes the probabilistic distribution prediction from a powerful but resourcehungry teacher model or an ensemble of teacher models as the soft target, to jointly regularize the optimization objective when training a smaller student model with given image samples and the corresponding one-hot labels. Intermediate feature maps are demonstrated to be effective hints to further advance the knowledge distillation process [32,42,45]. Extending the concept of knowledge distillation and its variants, Deep Mutual Learning [50] shows that the teacher model would benefit from the knowledge of the student model in turn, in contrast to the prevailing one-way teaching-learning mode. The newly established idea was soon used in person re-identification tasks [50,49]. Different from the method above in focus and formulation, our motivation is to solve the inherent deficiency hidden in the deeply-supervised training procedure, utilizing representation mimicking as a tool. Our proposed Dynamic Hierarchical Mimicking can be viewed as an internal knowledge transfer process limited in one single neural network among hierarchical auxiliary classifiers, which has never attracted enough attention from the research community. We also include a more comprehensive analysis of the differences between our method and KT in the supplementary materials.",
            "score": 0.5117812534192396,
            "section_title": "Related Work",
            "char_start_offset": 8302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 359,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46240234375
        },
        {
            "corpus_id": "249282185",
            "title": "Dynamic Knowledge Distillation with Noise Elimination for RGB-D Salient Object Detection",
            "text": "As mentioned above, Knowledge Distillation (KD) benefits the student model but the weight of knowledge transfer is still hand-designed. Piao [20] proposes an adaptive weight for cross-modal distillation. However, in [20] they only distill the depth information by considering the performance of teacher model. In our method, we consider both performances of teacher and student networks and combine these two factors as a dynamic weight for KD. \n\nConcretely, the accuracy of teacher model represents the detection performance which also indicates the confidence of knowledge. Inspired by IOU [16] used in SOD, we design a dynamic factor \u03b1 t to modulate the correct knowledge which can be transferred from the teacher model as follows: \n\nwhere P t and G represent the prediction of teacher model and the ground truth, respectively. \u03b1 t indicates the confidence of knowledge which can be transferred to the student model. Then, we propose another dynamic factor \u03b2 s to show the degree of desired knowledge for the student model as follows: \n\nwhere P s represents the prediction of student model. This dynamic factor \u03b2 s is error rate of the current training sample. In other words, knowledge distillation should also consider the current performance of student model. \u03b2 s is in- versely related to the accuracy between the output of student model and the ground truth. This indicates that hard samples which have large error rates need to learn more from the teacher model. Therefore, we propose a simple and effective formulation to find a plausible distillation weight \u03b8 t,s : \n\nhere tanh is treated as a scale function: \n\nMore specifically, we define the \u03b8 t,s by the weighted geometric mean of the knowledge confidence \u03b1 t from teacher and the knowledge demand \u03b2 s from student. We define the hyper-parameter p \u2208 [0, 1] to balance the ratio between the teacher and student networks. It is worth noting that large variation of \u03b8 t,s leads to convergence issue in training phase. In this case, we further use a tanh function to scale the \u03b8 t,s . The overall loss function can be formulated as: \n\nwhere L KL is the Kullback-Leibler divergence loss and L CE represents the cross-entropy loss. In the final network, we set the distillation temperature to 5 in L KL and p = 0.7.",
            "score": 0.5116125953729195,
            "section_title": "Dynamic Knowledge Distillation",
            "char_start_offset": 8372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "219962129"
                },
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "219962129"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "32797169"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12548828125
        },
        {
            "corpus_id": "263789814",
            "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
            "text": "Knowledge Distillation. Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals. SRRL [41] performs knowledge distillation by leveraging the teacher's projection matrix to train the student's representation via L2 loss. However, these works rely on a supervised pretrained teacher (with logits), and they may be not suitable for self-supervised pretrained teachers. SSKD [40] is proposed to combine the self-supervised auxiliary task and KD to transfer richer dark knowledge, but it cannot be trained in an end-to-end training way. Similar to logits matching, intermediate representation [32,43,42,36,22] are widely used for KD. FitNet [32] proposes to match the whole feature maps, which is difficult and may affect the convergence of the student in some cases. Attention transfer [43] utilizes spatial attention maps as the supervisory signal. AB [22] proposes to learn the activation boundaries of the hidden neurons in the teacher. SP [36] focuses on transferring the similar (dissimilar) activations between the teacher and student. However, most of these works depend on certain architectures, such as convolutional networks. Since these distillation methods involve knowledge matching in an individual sample, they are related to knowledge alignment. Our work also includes the knowledge alignment objective, and it doesn't rely on pretraining strategies or network architectures. \n\nKnowledge distillation and self-supervised learning. Self-supervised learning [30,2,8,20,6] focuses on learning low-dimensional representations by the instance discrimination, which usually requires a large number of negative samples. Recently, BYOL [18] and DINO [7] utilize the momentum encoder to avoid collapse without negatives. The momentum encoder can be considered as the mean teacher [34], which is built dynamically during the student training. For KD, the teacher is pretrained and fixed during distillation. Although different views (augmented images) are passed through networks in self-supervised learning, they are from the same original sample, and have the same semantic meaning.",
            "score": 0.5114456651582487,
            "section_title": "Related Work",
            "char_start_offset": 4102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 747,
                    "end": 750,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "207930212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "256105748",
            "title": "ProKD: An Unsupervised Prototypical Knowledge Distillation Network for Zero-Resource Cross-Lingual Named Entity Recognition",
            "text": "Knowledge distillation enables knowledge transfer from the teacher network to the student network (Hinton, Vinyals, and Dean 2015), where the student network is optimized by fitting the soft labels generated by the trained teacher network. Since the soft targets have a high entropy value, they provide more information per training case than the hard targets (Hinton, Vinyals, and Dean 2015), the student network can learn from the teacher network and perform well on unlabeled data. Knowledge distillation achieves significant results in various tasks such as model compression , image classification (Hinton, Vinyals, and Dean 2015), dialogue generation (Peng et al. 2019), machine translation (Weng et al. 2020), etc. In this paper, we choose knowledge distillation as the basic framework of our proposed approach for zero-resource cross-lingual NER.",
            "score": 0.5111502070420773,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7079,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "208617611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277099609375
        },
        {
            "corpus_id": "272753507",
            "title": "FAST GDRNPP: Improving the Speed of State-of-the-Art 6D Object Pose Estimation",
            "text": "Knowledge distillation (KD) in deep learning describes the process of extracting knowledge from one model (called the teacher) and transferring it to another model (the so called student) [CWZZ17, GYMT21]. The knowledge transfer can occur from the last layer, the entire teacher model, or specific parts of it, depending on the method used. KD can be applied to arbitrary domains of deep learning, but has been especially interesting for computer vision because of the large networks usually found within the domain. [WY21] provide an in-detail survey on the student-teacher framework applied to computer vision. Also, a very useful property of knowledge distillation, is the fact that models tend to learn faster from a teacher, than from ground truth data [PL19].",
            "score": 0.5110886919767377,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 765
                }
            ],
            "ref_mentions": [
                {
                    "start": 758,
                    "end": 764,
                    "matchedPaperCorpusId": "174800711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03240966796875
        },
        {
            "corpus_id": "248478798",
            "title": "DCML: Deep contrastive mutual learning for COVID-19 recognition",
            "text": "Deep neural networks have achieved remarkable results in the fields of CV, speech recognition, and natural language processing. To complete more complex tasks, the corresponding network must use a deeper or wider structure. Although these neural networks have achieved satisfactory performance on specific tasks, lots of computing requirements make them difficult to be deployed on those resource-constrained environments, which may limit their practicalities. To address this issue, Hinton [52] proposed the famous model distillation method. It regards a pre-trained large network as a teacher that provides additional knowledge to a small network (student). The student network imitates the category probability estimated by the teacher network. And the student network can even obtain better performance. The basic principle behind model distillation [52] is using the additional supervision from the teacher model to train the student model, which surpasses the traditional supervised learning objective. Since then, most offline distillation methods have followed this principle. In [52][53][54][55], the classification probability distribution of the corresponding teacher model was used as the additional supervision. However, the above traditional model distillation methods need a pre-trained large network. And they only employ a one-way knowledge transfer, which cannot take full advantage of the teacher and student networks.\n\nTo address this problem, Zhang [56] proposed a deep mutual learning (DML) strategy in which a group of student networks learns and guides each other throughout the whole training process. Instead of the statically one-way knowledge conversion between the teacher and student networks, DML uses multiple networks to train at the same time. Each network not only accepts the supervision from the ground-truth but also refers to the learning experience from the peer network. All these can further improve the generalization ability of the whole framework. Finally, the two networks share learning experiences (dark knowledge) to achieve mutual learning and common great progress. More importantly, online knowledge distillation (KD) between heterogeneous networks can be realized easily. Anil et al. [57] and Gao et al. [58] further extended the DML idea to accelerate the training procedure of a large-scale distributed neural network. Although the above researches have promoted the progress of KD, none of them absorbed the contrastive learning idea to the distillation procedure. Moreover, to",
            "score": 0.510304626686311,
            "section_title": "Deep mutual learning",
            "char_start_offset": 9907,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "208139041"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "159041346",
            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
            "text": "Knowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
            "score": 0.5097244147033593,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 322,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "5808102"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6123046875
        },
        {
            "corpus_id": "232380330",
            "title": "Distilling a Powerful Student Model via Online Knowledge Distillation",
            "text": "D EEP neural networks (DNNs) have achieved unprece- dented success in various visual tasks. Nevertheless, their extensive memory and computational requirements hinder their deployment in resource-limited devices. Several methods have been developed to derive a light-weight model with negligible performance compromise. Examples include network pruning [1], [2], parameter quantization [3], [4], low-rank decomposition [5], [6] and knowledge distillation [7], [8]. \n\nAmong them, knowledge distillation has received particular attention, transfering knowledge from a high-capacity teacher [7], [8], [9], or an online ensemble [10], [11], [12], to a student model. As illustrated in Fig. 1(a), Traditional knowledge distillation methods use a two-stage optimization where a cumbersome teacher network has to be trained in advance in order to yield a high-capacity model, which then serves as supervision information to guide the training of a light-weight student network. Though progress has been made, these methods heavily rely on an appropriate teacher model. As stressed in [13], [14], it is difficult to choose a suitable teacher model for the student model. \n\nThis has motivated the community to simplify the training procedure by exploring online knowledge distillation [10], [12], where a collection of student models are trained simultaneously in a collaborative manner without the involvement of a teacher model. As shown in Fig. 1, existing online knowledge distillation can be implemented by either mutual learning [10], [15], [16] or ensemble learning [12], [11], [17], [18], [19]. The former aligns the soft outputs of all students so as to allow message passing among them. Then, the student model with the optimal performance is adopted as the final model. However, the message passing does not guarantee that one single student will carry all the information of the ensemble. This limits the distillation performance.",
            "score": 0.5096001357942272,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 356,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 419,
                    "end": 422,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "1437449"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 625,
                    "end": 629,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1538,
                    "end": 1542,
                    "matchedPaperCorpusId": "222180022"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1582,
                    "end": 1586,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "275007781",
            "title": "Knowledge Reasoning- and Progressive Distillation-Integrated Detection of Electrical Construction Violations",
            "text": "This approach distills knowledge at different levels, reducing potential information loss in the transfer process to the maximum extent. \n\nFor example, at lower levels, the model might focus on learning basic visual features and patterns, such as the shape and color of safety helmets and harnesses. At higher levels, it can capture more complex behavior patterns, such as unsafe work postures or potential risky behaviors. \n\nTo better demonstrate the effectiveness of multi-level knowledge distillation, we visualized the feature maps of the teacher model, the original student model, and the student model after knowledge distillation, as shown in Figure 8. It is evident that after applying knowledge distillation, the model becomes more focused on the target and its corresponding regions. This highlights how the distillation process enhances the model's ability to capture relevant features, improving its overall detection performance. \n\nTo verify the effectiveness of progressive distillation, we conducted experiments using different teacher models. The results are shown in Table 5, and further demonstrate the impact of selecting various teacher models on the performance of the student network. This comparison highlights how different teacher models contribute to guiding the student network in learning more effectively, improving overall detection accuracy. It can be observed that compared to directly using a single advanced-teacher or primary-teacher model for knowledge distillation, the progressive distillation strategy achieves better results. Through phased learning, progressive distillation first leverages the rich expressive capabilities of the advanced teacher model to establish a solid knowledge foundation for the primary-teacher model. Subsequently, the primary teacher gradually guides the student model, enabling effective knowledge transfer. \n\nThis multi-stage process ensures that the student model can effectively absorb knowledge from both teacher models, leading to improved performance in complex scenarios such as detecting safety behaviors in power construction environments. The gradual knowledge transfer helps mitigate the gap between the advanced and student models, optimizing both the learning efficiency and detection accuracy. It is evident that after applying knowledge distillation, the model becomes more focused on the target and its corresponding regions. This highlights how the distillation process enhances the model's ability to capture relevant features, improving its overall detection performance.",
            "score": 0.509057782333602,
            "section_title": "Ablation Study",
            "char_start_offset": 46144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2410
                },
                {
                    "start": 2411,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14208984375
        },
        {
            "corpus_id": "231839582",
            "title": "Show, Attend and Distill: Knowledge Distillation via Attention-based Feature Matching",
            "text": "Knowledge distillation is the technique for transferring knowledge from a source neural network to a target neural network (Hinton, Vinyals, and Dean 2015). The source network, referred to as a teacher, indicates a large network that is highly regularized via pre-training, and the target network, referred to as a student, is a smaller network for a specific task. The pre-trained teacher directly informs the student of the solution and intermediate process of a problem, and this informative supervision enables fast and effective learning of the student. Based on knowledge distillation, recent studies have shown significant improvements in model compression (Hinton, Vinyals, and Dean 2015;Romero et al. 2014;Yim et al. 2017;Tian, Krishnan, and Isola 2019), crossdomain transfer learning (Orbes-Arteainst et al. 2019;Asami et al. 2017), and continual learning (Li and Hoiem 2017;Hou et al. 2018).\n\nFor the success of knowledge distillation, various distillation methods were introduced. Starting from transferring output probability distributions of the teacher (Hinton, Vinyals, and Dean 2015), intermediate features representations (Romero et al. 2014) and their variants (Zagoruyko and Komodakis 2016a;Park et al. 2019;Tian, Krishnan, and Isola 2019) are investigated to identify what knowledge of the teacher helps to build a better student. However, most studies manually links the teacher and student features and perform distillation through the links individually. This manual link selection does not consider the similarity between the teacher and student features, so there is a risk of forcing an incorrect intermediate process to the student. Furthermore, the link selection has a limitation on fully utilizing the whole knowledge of the teacher by choosing a few of all possible links.\n\nTo compensate for the limitation, Jang et al. (Jang et al. 2019) apply a meta-networks, \"learning to transfer (L2T)\", automatically determining the links. In more details, the metanetwork consists of individual gates for all possible links, and each gate determines whether distillation through the link contributes to decreasing the classification loss of the student. Their results prove that knowledge distillation with the identified links provides better performance than those with manually selected",
            "score": 0.5090309400934337,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 731,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 823,
                    "end": 841,
                    "matchedPaperCorpusId": "206742843"
                },
                {
                    "start": 885,
                    "end": 901,
                    "matchedPaperCorpusId": "52959234"
                },
                {
                    "start": 1211,
                    "end": 1228,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1852,
                    "end": 1869,
                    "matchedPaperCorpusId": "155092628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140869140625
        },
        {
            "corpus_id": "258564799",
            "title": "DynamicKD: An Effective Knowledge Distillation via Dynamic Entropy Correction-Based Distillation for Gap Optimizing",
            "text": "Therefore, this paper focuses on knowledge distillation-based model compression. \n\nIn the knowledge distillation, how the teacher network better guides the student network training has become vital research. Several researchers have studied various knowledge used in the distillation process. \n\nRomero et al. applied the middle layer features of the teacher network as knowledge to guide the student network learning [26]. Komodakis et al. transferred attention from the large-scale teacher network to the student network [27]. Damiano et al. viewed the knowledge transfer between teacher and student networks as maximizing the mutual information between teacher and student networks [28]. Zagoruyko et al. used the attention mechanism as a learnable knowledge, and they used the teacher network's attention knowledge to guide the student network's training. In neural networks, the convolutional layers map one feature to another. Yim et al. treated the mapping processing of features between layers as knowledge and used the FSP matrix to describe this knowledge so that the student network could imitate it [29]. The rich and varied knowledge exchange between the teacher and student networks helps the student network training. \n\nBut, these methods do not focus on the performance gap between the teacher and student networks, which may affect student network learning. \n\nSeveral works have studied the performance gap and proposed corresponding improvements. Cho et al. found that the underperformance teacher with early-stop training benefits the student [30]. Mirzadeh et al. found that when the gap between the teacher network and the student network is large, the student trained by a lower-performance lightweight teacher network performs better than the one taught by a higher-performance large-scale teacher network [16]. For this reason, he utilized a medium-sized neural network (called Teacher Assistant) to help the student network cross the large performance gap. Both methods mentioned above show that reducing the performance gap can improve distillation performance. \n\nHowever, these static methods do not correct the performance gap further during the distillation. The performance gap keeps changing with the performance improvement of the student network, so these strategies may still hinder the student network from imitating the high-performance teacher network. \n\nVarious knowledge distillation algorithms are available to continuously update the knowledge applied for the student network training during the distillation.",
            "score": 0.5088954778556225,
            "section_title": "Introduction",
            "char_start_offset": 2212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2388
                },
                {
                    "start": 2391,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0765380859375
        },
        {
            "corpus_id": "219530513",
            "title": "ResKD: Residual-Guided Knowledge Distillation",
            "text": "We categorize knowledge distillation methods in terms of the number of stages. Traditionally, knowledge distillation is a two-stage method, in which a teacher network is trained first, arXiv:2006.04719v4 [cs.CV] 9 Mar 2021 and then a student network is trained under the guidance of the teacher network. Bucil\u0203 et al. [24] pioneered the idea of transferring the knowledge from a cumbersome model to a small model. Hinton et al. [18] popularized this idea by the concept of knowledge distillation (KD), in which a student neural network is trained with the benefit of the soft targets provided by teacher networks. Compared to traditional onehot labels, the output from a teacher network contains more information about the fine-grained distribution of data, which helps the student achieve better performance. Recently, many works have focused on improving the information propagation way or putting strictness to the distillation process via optimization [25], [26], [27], [19], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37] to teach the student better. For example, Peng et al. [19] proposed that a student network should not only focus on mimicking from a teacher at an instance level, but also imitating the embedding space of a teacher so that the student can possess intra-class compactness and inter-class separability. In addition, the effect of different teachers is also researched [38], [39], [40]. For example, Sau et al. [39] proposed an approach to simulate the effect of multiple teachers by injecting noise to the training data and perturbing the logit outputs of a teacher. In such a way, the perturbed outputs not only simulate the setting of multiple teachers but also result in noise in the softmax layer, thus regularizing the distillation loss. With the help of many teachers, the student is improved a lot. Kang et al. [40] used Neural Architecture Search (NAS) to acquire knowledge for both the architecture and the parameters of the student network from different teachers.",
            "score": 0.5083792848663587,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 4359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "201805763"
                },
                {
                    "start": 974,
                    "end": 978,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "208513309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1669921875
        },
        {
            "corpus_id": "267688311",
            "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
            "text": "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network. In practice, many variations and improved distillation techniques are used. For example, different objective functions, loss functions, and regularization techniques can be used. In addition, merging knowledge from multiple teacher networks into student networks can also be beneficial. To achieve better performance, these techniques can be optimized for specific tasks and data [18]. \n\nKnowledge can be transferred using the output of the last layer via response-based knowledge distillation, which is relatively simple and does not require additional complex computation or design. However, it overlooks the output information of the middle layer, which limits the ability of the student network to fully capture the multi-level features of the teacher network. In addition to using the output of the last layer, the output of the middle layer can also facilitate effective student learning. Consequently, the combination of response-based and feature-based knowledge distillation has become a popular research area [19]. Fitnets [20] propose to use feature maps as output to match the feature maps of teachers and students for better learning. Zagoruyko and Komodakis [21] were inspired by the attention mechanism and proposed Attention Transfer (AT), replacing the original feature maps with attention maps for effective knowledge transfer.",
            "score": 0.5083262993629561,
            "section_title": "II. RELATED WORK A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 7051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2558
                }
            ],
            "ref_mentions": [
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 2232,
                    "end": 2236,
                    "matchedPaperCorpusId": "232232777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "254246234",
            "title": "A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning",
            "text": "Knowledge distillation [44] provides a strategy to transfer the knowledge learned by the teacher network to the student network, which has been applied in high-level computer vision tasks like object detection and image classification [136]. \n\nRecent work [54] presents three challenges for applying knowledge distillation to the dehazing task. First, what kind of teacher task can help the dehazing task. Second, how the teacher network helps the dehazing network during training. \n\nThird, which similarity measure between teacher task and student task should be chosen. \n\nwhere   represents the -th layer of the teacher network, and the corresponding   represents the -th layer of the student network;  is obtained by the normalization operation. KDDN can be trained without real transmission map, replaced by the residual between hazy and haze-free images. \n\nKTDN [147] is jointly trained using a teacher network and a dehazing network with the same structure. Through feature level loss, the prior knowledge possessed by the teacher network can be transferred to the dehazing network. \n\nSRKTDN [16] uses ResNet18 pre-trained on ImageNet (with classification layers removed) as the teacher network, transferring many statistical experiences to the Res2Net101 encoder for dehazing. DALF [38] integrates dual adversarial training into the training process of knowledge distillation to improve the imitation ability of the student network to the teacher network. Applying knowledge distillation to dehazing networks provides a new and efficient way to introduce external prior knowledge.",
            "score": 0.5080942635071871,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 32624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1587
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 235,
                    "end": 240,
                    "matchedPaperCorpusId": "182952755"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "221097144"
                },
                {
                    "start": 867,
                    "end": 872,
                    "matchedPaperCorpusId": "220206807"
                },
                {
                    "start": 1098,
                    "end": 1102,
                    "matchedPaperCorpusId": "235719578"
                },
                {
                    "start": 1289,
                    "end": 1293,
                    "matchedPaperCorpusId": "236303660"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05511474609375
        },
        {
            "corpus_id": "208202205",
            "title": "MSD: Multi-Self-Distillation Learning via Multi-classifiers within Deep Neural Networks",
            "text": "2.1 KNOWLEDGE DISTILLATION KD (knowledge distillation) is a model compression technique proposed by Bucilu et al. (2006). \n\nAnd it was utilized for neural networks in Hinton et al. (2015). Traditional KD try to transfer a big pretrained teacher network's knowledge to a smaller student network. In details, it compute a KL loss between the teacher and student output distributions. And this loss provides additional regularisation and supervision for the student. In this case, the student accuracy may be higher than the teacher. Various KD techniques have been proposed.",
            "score": 0.5080923365123176,
            "section_title": "RELATED WORK",
            "char_start_offset": 6465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 572
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 120,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1298828125
        },
        {
            "corpus_id": "265445729",
            "title": "Robustness-Reinforced Knowledge Distillation With Correlation Distance and Network Pruning",
            "text": "Knowledge distillation (KD) offers a solution by transferring knowledge from a more complex and high-performing network to a smaller, more efficient network. Over the years, there has been a surge of research in KD and the development of better distillation techniques. Since the concept of KD was first introduced by Hinton [16], it has expanded into two major approaches: logits-based [16], [29], [48], [49] and featurebased distillation [1], [6], [15], [26], [30], [31], [34], [38]. \n\nWhile feature-based distillation allows students to learn a wider range of information compared to logit-based distillation, it has limited practical applicability due to challenges related to accessing the intermediate layer in real-world scenarios, primarily because of privacy and security concerns [21]. Therefore, our focus is on logit-based distillation, which is more suitable for practical use. \n\nThe majority of logit-based distillation methods employ the Kullback-Leibler (KL) divergence to align the probability distributions between teacher and student models, representing the simplest and most straightforward approach to knowledge transfer in KD. However, depending on the entropy of the teacher's distribution, students using KD are prone to receiving unintended information from the teacher's distribution. In this paper, we conceptually describe the potential student's distribution based on teacher entropy and utilize a correlationbased distance to overcome this issue.",
            "score": 0.5069704279771101,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 6735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1477
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 445,
                    "end": 448,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 480,
                    "end": 484,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "260933721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "252780731",
            "title": "A Survey on Heterogeneous Federated Learning",
            "text": "This section provides a comprehensive survey of knowledge distillation and investigates how knowledge distillation can be used to tackle the model heterogeneity issue in FL. We first introduce the definition of knowledge distillation. Then, we review existing works that leverage knowledge distillation to tackle the model heterogeneity problem in FL. Interested readers may refer to [164] for a more detailed introduction. \n\n1) Distillation Approaches: Knowledge distillation was initially proposed to compress an ensemble of neural networks [162]. Three forms of knowledge can be leveraged to transfer knowledge from the teacher model to the student model: response-based knowledge, feature-based knowledge, and relationbased knowledge. Both response-based and featurebased knowledge leverage the output of specific layers to transfer knowledge from teacher to student. Relation-based knowledge takes the instance-wise or layer-wise distance as knowledge to train the student model. We elaborate these three forms of knowledge in Appendix B. \n\nKnowledge distillation for heterogeneous FL falls into the group-based online distillation scheme, which is an end-to-end training scheme. The idea of group-based online distillation is to train multiple student local models simultaneously by learning from ground-truth labels and distilling from their aggregated soft targets, a specific form of aggregation of intermediate peer predictions. In federated online distillation, the teacher model is the aggregation of the local models, and each local model is a student model [103], [100], [142]. Therefore, the teacher model is updated simultaneously with the student models, and the whole knowledge distillation framework is trained in one phase. The online distillation approaches proposed for non-federated settings are promising to be adopted to tackle heterogeneous FL problems. \n\nData-free knowledge distillation is also explored in heterogeneous FL to relax the requirement of a public dataset for knowledge transfer. Nayak et al. [165] first proposed zero-shot knowledge distillation by synthesizing the data impressions from the teacher model and using them as surrogates for knowledge distillation.",
            "score": 0.5069508332672779,
            "section_title": "A. Knowledge distillation",
            "char_start_offset": 80591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 384,
                    "end": 389,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1571,
                    "end": 1576,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1578,
                    "end": 1583,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1585,
                    "end": 1590,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054412841796875
        },
        {
            "corpus_id": "215238308",
            "title": "Towards Efficient Unconstrained Palmprint Recognition via Deep Distillation Hashing",
            "text": "KD is an effective technique for transferring information from a relatively complex deep model to a light model [49]. It has been widely used for model compression and acceleration to improve the performance of fast and light networks [50]. Hinton et al. [51] first distilled knowledge from an ensemble of pre-trained models to improve a small target net via high-temperature softmax training. Then, FitNet developed KD using the pre-trained wide and shallow teachers hint layers to assist thin and deep students by guided layers [52]. Li et al. [53] proposed a unified distillation framework to use a small clean dataset and label relations in knowledge graph to guide the distillation process. Yim et al. [54] proposed a method of transferring the distilled knowledge as the flow between two layers by computing the inner product between features, and the student model outperformed the original model that was trained from scratch. Recently, KD is also successfully used for pedestrian detection [55] and face recognition [56]. Chen et al. [57] proposed a teacher bounded loss and introduced adaptation layers to help student network to better learn from teacher distributions for object detection. Shu et al. [58] adopted adversarial training stratagem to help learn the student network, which integrated merits from both process-oriented learning and result-oriented learning. Malinin et al. [59] combined KD with ensemble learning and proposed ensemble distribution distillation to improved classification performance. Park et al. [60] proposed relational knowledge distillation (RKD) to transfer relations of data examples based on distance-wise and angle-wise distillation losses. Wei et al. [61] quantized a large network and then mimicked a quantized small network for object detection. The model improved the performance of a student network by transferring knowledge from a teacher network. A research similar to our work is knowledge distillation-based metric learning, which is adopted to improve the image embedding. Yu et al. [13] proposed a novel distillation loss, which compared the distance between the features of two samples in the teacher network with the distance of the same two samples in the student network.",
            "score": 0.5066624932032949,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 15437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "2745955"
                },
                {
                    "start": 255,
                    "end": 259,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "368747"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "14431108"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "131773901"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "141465546"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "24139282"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "252481152",
            "title": "Multiple-Stage Knowledge Distillation",
            "text": "In the past few decades, deep neural network (DNN) models have achieved remarkable success in various computer vision tasks, such as image classification [1], object detection [2], and semantic segmentation [3]. Additionally, this has had an impact on various fields [4]. DNN models can effectively extract features by augmenting \"width\" and \"depth\" to different degrees; however, increasing the number of layers in the model will increase the training difficulty. Therefore, other methods must be explored to improve the performance of the model. \n\nKnowledge distillation (KD) is a method of model compression [5][6][7] that can be applied to improve the performance of a model [8,9]. In the conventional KD method, the teacher network has a deeper layer and more parameters than those of the student network. When the student network mimics the output of the teacher network, the accuracy of the student network is increased and model compression is achieved. KD utilizes the \"dark knowledge\" of the teacher network to direct and increase the performance of the student network. Existing research reveals that the student network can outperform the teacher network [10]. The current focus of KD research is on studying the categories and methods of knowledge transfer; however, the methods of activating the learning potential of student networks are yet to be investigated. \n\nThe most effective teaching method fully utilizes the learning ability of a student. Additionally, to maximize the learning ability of a student, an appropriate learning style must be adopted. Furthermore, the general process of human learning is usually divided into several stages, such as a preview before class, classroom learning, and review after class. Each stage contributes to a different level of understanding, which can be revised with the assistance of the teacher and books. Eventually, the imparted knowledge can be correctly understood. Figure 1 describes our idea. The teaching process includes the knowledge imparted by the teacher and learning by the student, both of which are essential. Students' efforts alone cannot be used to improve the effectiveness of teaching; the teacher's methods of teaching are equally crucial.",
            "score": 0.5064025047415047,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "206770307"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "14124313"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "249688145"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 682,
                    "end": 684,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1167,
                    "end": 1171,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "251719031",
            "title": "Tree-structured Auxiliary Online Knowledge Distillation",
            "text": "Deep neural networks have led to a series of successes in computer vision and natural language processing [17]. They show superiority in representing complex concepts due to the large size of parameters [15]. However, the cumbersome neural networks are computationally expensive, hindering their applications in real-world problems with limited resources. To *Corresponding author. 1 Code is available at https://github.com/Linwenye/Tree-Supervised.\n\nthis end, knowledge distillation [12] is proposed to transfer the knowledge from a large teacher model to a compact student model. Traditional knowledge distillation requires a two-stage training process, in which a high-capacity teacher model is pretrained in the first stage, and then transfers the knowledge of the teacher to a compact student model in the second stage [3], [12]. This two-stage process increases the pipeline complexity and training cost. To simplify the distillation procedure, online knowledge distillation [16], [32] is proposed, which simultaneously trains a set of student models and distills their knowledge from each other in a peer-teaching manner. This approach requires a one-stage learning procedure and leverages peer network to provide the teacher knowledge. Recent researches on online knowledge distillation aim at improving the quality of the knowledge learned from peers. ONE [16] and CL-ILR [23] introduces an ensemble teacher which gathers the students' knowledge with a gate mechanism. OKDDip [6] further boosts the performance by maintaining the peer variety through an attention mechanism. These approaches mainly focus on the design of the distillation objective.\n\nBy contrast, in this work, we demonstrate that the design of the overall architecture for online knowledge distillation is another key factor for the performance of the student model. We adopt the original distillation objective as that in DML [32] without bells and whistles, and show that the tree structure for online knowledge distillation is the key to our state-ofthe-art performance. We propose a unified framework, the tree-structured auxiliary online knowledge distillation (TSA). During training, TSA hierarchically adds more auxiliary peers in later layers, which naturally forms a tree structure (See Fig. 1). Different branches construct different views of the inputs, which",
            "score": 0.505947830708972,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1485,
                    "end": 1488,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1904,
                    "end": 1908,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043701171875
        },
        {
            "corpus_id": "251719675",
            "title": "Multi-Granularity Distillation Scheme Towards Lightweight Semi-Supervised Semantic Segmentation",
            "text": "Knowledge Distillation [13,16] is a knowledge transfer technique that optimizes a lightweight student model with effective information transfer and supervision of a larger teacher model or ensembles. Besides the knowledge transfer in the outputs, the feature maps [28,35] in the intermediate layers of networks are used to improve the performance of the student networks. Moreover, some methods attempt to transfer the attention concepts [47][48][49] of feature maps from each channel in intermediate layers. Some approaches try to exploit the knowledge distillation on the dense prediction tasks [27,38,46,50]. In addition, [54] indicates that multiple teacher networks can provide more effective information for training a lightweight and ascendant student network. In this paper, we design a deeper and a wider model to play as the complementary teachers and provide multi-granularity auxiliary supervision.",
            "score": 0.5057469631576256,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 910
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "201809759"
                },
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "226292143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "232352737",
            "title": "Student Network Learning via Evolutionary Knowledge Distillation",
            "text": "And a framework Snapshot Distillation was proposed for teacher-student optimization in one generation [16], which extracted such information from earlier epochs in the same generation, meanwhile made sure that the difference between teacher and student is sufficiently large so as to prevent underfitting. After these, a novel two-level framework OKDDip [18] was proposed to perform distillation during training with multiple auxiliary peers and one group leader for effective online distillation. In OKDDip framework, the first-level distillation works as diversity maintained group distillation with several auxiliary peers, while the second-level distillation transfers the diversity enhanced group knowledge to the ultimate student model called group leader. \n\nThese online, timely and efficient training methods are promising and some good progress has been made in narrowing the capability gap between teacher and student model [15], [16], [42]. However, for online distillation, two factors still hold them back. First, although the teachers of online distillation methods are dynamic and have narrowed the capability gap, the gap still exists due to the lack of representation in detail and the process of learning. Second, owing to the absence of a qualified teacher role for online knowledge distillation, the insufficient and relatively unreliable supervision information will restrict the learning of student to some extent. There is still great room for improvement in knowledge transfer and representation. Therefore, we propose the evolutionary knowledge distillation approach to enhance the performance of student network learning by using an evolutionary teacher and focusing on intermediate knowledge of the teaching process dynamically.",
            "score": 0.5054928508387573,
            "section_title": "C. Online Knowledge Distillation",
            "char_start_offset": 10556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0207672119140625
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "Different types of knowledge has been used for distillation, including logits (Zhao et al., 2022;Zhang et al., 2022;Phan et al., 2022), feature maps (Yim et al., 2017;Zagoruyko & Komodakis, 2017;Heo et al., 2019;Huang et al., 2022), and sample relationships (Tung & Mori, 2019;Yang et al., 2022). Recently, different types of channel features have also been explored as the knowledge for distillation (Shu et al., 2021;Liu et al., 2021;Li et al., 2021;Muhammad et al., 2021;Zhou et al., 2006;Qu et al., 2020;Fan et al., 2022;Ge et al., 2019). Specifically, Shu et al. (2021) proposed channel-wise knowledge distillation, which distills the knowledge from the channel-wise probability maps of the teacher network. Zhou et al. (2006) proposed channel distillation, which transfers the channel information from a teacher network to a student network. In (Li et al., 2021), the authors proposed a new channel correlation structure (CCS), which aims to guide the training of a student by applying fine-grained supervision, i.e., both inter-and intra-instance relationships. In (Liu et al., 2021), diversity-preserved knowledge was designed by discovering the teacher knowledge from inter-channel correlation. Qu et al. (2020) designed a hybrid attention transfer (H-AT), which calculates the channel-based attention knowledge through the output of the teacher's middle layers and then transfers it to the student network. Fan et al. (2022) proposed an online distillation method via channel self-supervision, which considers the sample, target, and network diversity knowledge on the dual-network multi-branch structure. Muhammad et al. (2021) proposed a new robust knowledge transfer via distilling activated channel maps to overcome the susceptibility of the natural samples.",
            "score": 0.5053511788850228,
            "section_title": "Teacher's Knowledge",
            "char_start_offset": 10209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 97,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 116,
                    "end": 134,
                    "matchedPaperCorpusId": "252576629"
                },
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 167,
                    "end": 195,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 195,
                    "end": 212,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 258,
                    "end": 277,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 401,
                    "end": 419,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 419,
                    "end": 436,
                    "matchedPaperCorpusId": "244101388"
                },
                {
                    "start": 492,
                    "end": 508,
                    "matchedPaperCorpusId": "222831725"
                },
                {
                    "start": 525,
                    "end": 541,
                    "matchedPaperCorpusId": "207954576"
                },
                {
                    "start": 557,
                    "end": 574,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 1072,
                    "end": 1090,
                    "matchedPaperCorpusId": "244101388"
                },
                {
                    "start": 1204,
                    "end": 1220,
                    "matchedPaperCorpusId": "222831725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05194091796875
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge Distillation: Knowledge distillation (KD) is an efficient method broadly used for transferring knowledge from a teacher network to a student network. In the knowledge distillation setting, a student model is trained to obtain the knowledge of a deeper or more complex teacher model and can therefore estimate the capacity of the powerful teacher model by incorporating the extra knowledge. KD was first introduced as an approach to compress large networks into smaller networks (Ba and Caruana, 2014;Bucilu\u01ce et al., 2006) for computational efficiency. The advances of KD, however, go beyond model compression. Zhang and Sabuncu (2020) empirically explained the reason behind the enhanced performance of self-distillation and proposed a framework that employs instance-specific regularization for teacher predictions. Phuong and Lampert (2019) examined the impact of distillation on student models by analyzing linear and deep linear classifiers. Unlike previous works, we are interested in analyzing the impact of knowledge distillation on the calibration of the models. Thus, we examine the calibration of large-scale pre-trained models through knowledge distillation. We further analyze the impact of dataset shift on calibration for all these settings. We evaluate the predictive uncertainty on both in-domain and out-of-domain test sets from known and unknown distributions on emotion-related datasets. \n\nMixup: Mixup (Zhang et al., 2018) was first proposed to improve the generalization of deep neural networks in computer vision. Since then, many studies have explored mixup in natural language processing tasks (Guo et al., 2019;Guo, 2020;Chen et al., 2020;Yin et al., 2021;Kong et al., 2020;Liang et al., 2021). Liang et al. (2021) proposed a data-agnostic distillation framework that leverages mixup to confer the student model with better generalization ability. Kong et al. (2020) examined BERT calibration using mixup by generating augmented samples based on a cosine distance of extracted features.",
            "score": 0.5053134475015317,
            "section_title": "Related Work",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 510,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 510,
                    "end": 531,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1432,
                    "end": 1452,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1646,
                    "end": 1656,
                    "matchedPaperCorpusId": "212928508"
                },
                {
                    "start": 1656,
                    "end": 1674,
                    "matchedPaperCorpusId": "216553182"
                },
                {
                    "start": 1674,
                    "end": 1691,
                    "matchedPaperCorpusId": "236477688"
                },
                {
                    "start": 1709,
                    "end": 1728,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 1730,
                    "end": 1749,
                    "matchedPaperCorpusId": "226226888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15087890625
        },
        {
            "corpus_id": "236782048",
            "title": "Research on the Theory and Application of Deep Interactive Learning",
            "text": "Knowledge distillation is defined as refining a larger neural network model into a smaller model [1]. The larger neural network is like a teacher network, while the smaller neural network can be regarded as a student network. In addition to probability distribution, other studies also try to extract various features to students. In training the model efficiency of a small student network, KD can achieve the same effect as some model compression methods, such as pruning [2] and quantification. Knowledge distillation is like a teacher instilling knowledge into students without getting feedback from students, which is obviously not conducive to students' learning. Therefore, some scholars have proposed a method of Deep Mutual Learning (DML) [11]. By interacting the characteristics of the output layer of the teacher network with the student network, kullback-leibler is used to measure the degree of interaction. In this framework, the boundary between each student network and the teacher network is no longer obvious, and students are more inclined to learn from each other. One advantage of this method is that it can flexibly apply any different network architecture. But the fly in the ointment is that this method can only exchange limited information, because it does not make full use of the rich information of teacher model.Another classical online distillation method is the on-the-fly native ensemble (ONE). It focus on improving the performance of student network using the gate of branch logic. But trying to tranfer the knowledge from teacher model to student model is focal point of this method.The shortcoming of the ONE is that it can only train a single architecture because of the branch gate logic. Our method is based on DML, and some improvements have been made.",
            "score": 0.5047180288442651,
            "section_title": "Knowledge Distillation (KD)",
            "char_start_offset": 5271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0265045166015625
        },
        {
            "corpus_id": "265444951",
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "text": "As the growing size of state-of-the-art deep learning models in computer vision (CV) [6,7,19,33] and natural language processing (NLP) [1,5], their deployment in resourcelimited settings becomes more challenging. Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form. *Corresponding author: camhero@gmail.com Previous works have introduced various forms of dark knowledge and refined the knowledge transfer process through structural modifications [2,9,17,22,26,35]. The effectiveness of these methods emphasizes the pivotal role dark knowledge plays within the KD framework. While these methods mostly employ a fixed training paradigm, adaptive distillation approaches have brought forth a more dynamic transfer process [3,13,18,20,31,37,38]. These approaches dynamically modulate the knowledge transfer, typically based on the teacher-student performance gap, ensuring a more tailored knowledge transfer. Despite their effectiveness, these methods often come with limitations such as being confined to specific frameworks [3,31,37,38], computationally intensives [18,20], or yielding marginal improvements [13]. Furthermore, we identify these methods may overlook the inherent student's bias toward specific knowledge in KD, leading to imbalanced learning. \n\nWhile KD achieves promising results by using Kullback-Leibler (KL) divergence to align the student's predictions with the teacher's, a notable performance gap between the models remains. We hypothesize that this gap may be attributed to the student's overconfident predictions. To investigate this, we employ entropy, a concept from information theory that quantifies the unpredictability or information of a random variable [28], to measure the confidence of predictions. We then utilize the kernel density estimation (KDE) to visualize and compare the entropy distributions of the teacher and student.",
            "score": 0.5046336387057245,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 88,
                    "end": 90,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 138,
                    "end": 140,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 743,
                    "end": 746,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 746,
                    "end": 748,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "258298441"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "matchedPaperCorpusId": "239024317"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 1025,
                    "end": 1028,
                    "matchedPaperCorpusId": "244119770"
                },
                {
                    "start": 1031,
                    "end": 1034,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "249626454"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "matchedPaperCorpusId": "239024317"
                },
                {
                    "start": 1325,
                    "end": 1328,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1328,
                    "end": 1331,
                    "matchedPaperCorpusId": "249626454"
                },
                {
                    "start": 1364,
                    "end": 1367,
                    "matchedPaperCorpusId": "244119770"
                },
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "125985701",
            "title": "Knowledge Distillation via Route Constrained Optimization",
            "text": "of models could be transferred to the other single model. Then Hinton et al. [10] further claimed that knowledge distillation (KD) could transfer distilled knowledge to student network efficiently. By increasing the temperature, the logits (the inputs to the final softmax) contain richer information than one-hot labels. Afterward, [14] proposed to learn the curriculum from data by a network called Mentor-Net. [18] adopted a method to learn from noisy labels.\n\nLearning Representation from Hint. Hint-based learning is often used for open-set classification such as face recognition and person Re-identification. FitNet [24] firstly introduced more supervision by exploiting intermediatelevel feature maps from the hidden layers of teacher to guide training process of student. Afterward, Zagoruyko et al. [30] proposed the method to transfer attention maps from teacher to student. Yim et al. [29] defined the distilled knowledge from teacher network as the flow of the solution process (FSP), which is calculated by the inner product between feature maps from two selected layers.\n\nPrevious knowledge transfer methods only supervise student with converged teacher, thus fail to capture the knowledge during teacher's training process. Our work differs from existing approaches in that we supervise student with the knowledge transferred from teacher's training trajectory.",
            "score": 0.5045882085583244,
            "section_title": "Related Work",
            "char_start_offset": 5767,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1209716796875
        },
        {
            "corpus_id": "227333801",
            "title": "Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression",
            "text": "Knowledge distillation compresses a DNN by training a small student model to mimic a larger teacher model (or ensemble of models). If the training is successful, the knowledge learned from the cumbersome teacher model will be distilled to the simplified student model without compromising accuracy. This method was first invented by Bucila et al. [25] and gained much attention after Hinton et al. generalized this idea in [15]. The knowledge is transferred from the larger model to the smaller model by minimizing a loss function where the target is the output of a softmax function (class probabilities) on the large model's logits. Knowledge distillation can be combined with other model compression techniques such as quantization [26], [27], [28] and pruning [3], [29].\n\nThe conventional knowledge distillation method [15] relies on the whole teacher models to generate student models, which can be viewed as global-wise distillation. This strategy needs a huge search space of the student network with a wide variety of network configurations that are intractable and unstable in real practice. To tackle this problem, Wang et al. proposed a progressive blockwise distillation method that can distill the knowledge of the entire teacher network by locally extracting the knowledge of each block and transfer to student sub-networks [19]. Although their method reduces the training time, the distillation process is still conducted in a serial manner. We take one step further in this paper to propose a novel parallel algorithm for blockwise knowledge distillation.",
            "score": 0.5041347134012744,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10740,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "51606880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2354736328125
        },
        {
            "corpus_id": "234336288",
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "score": 0.5037897715246848,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 8373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 57,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493408203125
        },
        {
            "corpus_id": "257756916",
            "title": "Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training",
            "text": "Knowledge distillation [52] is a promising solution, which aims to feed some extra knowledge learned from teacher networks (one or more complex networks) into a student network (much simpler network). With the help of a teacher, the student can achieve comparable accuracy but with much lower memory and computation costs compared with the teacher. Let q s and q t denote the softmax outputs of the student network and teacher network, respectively. The student network will be trained via aligning q s and q t . But in the case that q t is close to the one-hot code of true labels, the information contained in small values cannot be transferred to the student. Hence, a trick named temperature [52] is utilized to soften the distribution of both q s and q t . Networks compressed by low rank approximation is also a simpler network that can learn knowledge from the uncompressed version. In general, the decomposed networks are recovered by simply fine-tuning to minimize the cross-entropy function. However, the fine-tuning process always converges slowly and cannot recover the original accuracy well. Hence, this underlines the need for training the compressed network with information from the corresponding pre-training network. \n\nHowever, it was demonstrated in [39] that it is difficult to train a student network deeper than the teacher network with knowledge distillation due to the undesirable phenomenon of vanishing gradient. Hence, a novel knowledge transfer (KT) was proposed in [85], which aligns both outputs and intermediate responses from a teacher (original) network to its student (compressed) network. Experiments show that it surpasses the common fine-tuning and knowledge distillation, particularly with a high compression ratio. \n\nHowever, the KT method is still time-consuming and has a demand for a fully annotated large-scale training set, which may be infeasible in practice. Li et al. [80] proposed a revised knowledge distillation that only requires a few labelfree samples. It adds a 1\u00d71 Conv layer at the end of each block of the student network, and aligns block-level outputs of teacher and student by estimating the 1\u00d71 Conv layer's parameters using least-squared regression.",
            "score": 0.5031197747879805,
            "section_title": "4) Integration with Knowledge Distillation",
            "char_start_offset": 54069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "5575601"
                },
                {
                    "start": 1495,
                    "end": 1499,
                    "matchedPaperCorpusId": "52915624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.144775390625
        },
        {
            "corpus_id": "250264223",
            "title": "Informed Learning by Wide Neural Networks: Convergence, Generalization and Sampling Complexity",
            "text": "Knowledge distillation (Hinton et al., 2014;Furlanello et al., 2018;Phuong & Lampert, 2019;Allen-Zhu & Li, 2020) is an important technique to transfer prior knowledge from a pre-trained neural network (a.k.a. teacher network) to another network (a.k.a. student network), with the same or different architectures. Typically, given an (possibly unlabled) input, knowledge distillation is performed by matching the output of the student network with the output of the teacher network. In addition, labeled samples can also be included to introduce a label-based loss. Thus, by formulating g(X) as the output of the teacher network, knowledge distillation can be viewed as a particular instance of informed machine learning, where the knowledge comes from a teacher network and is usually assumed to be perfect.",
            "score": 0.502786507174729,
            "section_title": "F.4. Knowledge distillation and transfer",
            "char_start_offset": 71442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 807
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 44,
                    "end": 68,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259765625
        },
        {
            "corpus_id": "267068602",
            "title": "Knowledge distillation on spatial-temporal graph convolutional network for traffic prediction",
            "text": "In this paper, we use offline distillation, wherein the student distills information from the teacher network after the teacher network has been completely trainedAsif et al. ( 2020); Mirzadeh et al. \n\n(2020); Mobahi et al. (2020). We utilized two knowledge distillation techniques. The initial approach, known as response-based distillation, involves transmitting crucial information from the teacher network's outputs to the student network. This leads to the development of a simpler and faster model while simultaneously preserving performance. The second method, feature-based knowledge distillation, focuses on hidden layer's knowledge, encompassing spatial and temporal correlations among graph nodes (Figure 1).",
            "score": 0.502660203723285,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 719
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 230,
                    "matchedPaperCorpusId": "211096976"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0965576171875
        },
        {
            "corpus_id": "252780856",
            "title": "Stimulative Training of Residual Networks: A Social Psychology Perspective of Loafing",
            "text": "As a classical method, knowledge distillation [12; 13] transfers the knowledge from a teacher network to a student network via approximating the logits [12; 14; 15] or features [16; 17; 18; 19] output. To avoid the huge cost of training a high performance teacher, some works abandon the naive teacherstudent framework, like mutual distillation [20] making group of students learn from each other online, and self distillation [21] transferring knowledge from deep layers to shallow layers. Generally, all these distillation methods need to introduce additional networks or structures, and employ fixed teacher-student pairs. As a comparison, our method does not require any additional network or structure, and the student network is a randomly sampled sub-network of a network. Besides, our method is essentially designed to address the loafing problem of residual networks, which is different from knowledge distillation that aims to obtain a compact network with acceptable accuracy.",
            "score": 0.5025031675706106,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 987
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052825927734375
        },
        {
            "corpus_id": "268512785",
            "title": "Multiple Teachers-Meticulous Student: A Domain Adaptive Meta-Knowledge Distillation Model for Medical Image Classification",
            "text": "Moreover, the analysis shows that increasing the variety of data distributions in the form of growing the number of teachers results in improving the efficiency of the student network on the target dataset.However, this increase in diversity can mitigate the focus of the knowledge distillation process on a specific teacher, which reduces the efficiency of the student network on that dataset.As a result, the reduction of diversity in the process of knowledge distillation increases the efficiency of the student network on the dataset where the teacher trained on that dataset is available among the teacher networks.",
            "score": 0.5021584093528109,
            "section_title": "5-Discussion",
            "char_start_offset": 34483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014007568359375
        },
        {
            "corpus_id": "220935852",
            "title": "Teacher-Student Training and Triplet Loss for Facial Expression Recognition under Occlusion",
            "text": "Knowledge distillation [3], [4] is a recently studied approach [34], [35], [36], [37], [38], [39] that enables the transfer of knowledge between neural networks. Knowledge distillation is a framework that unifies model compression [3], [4], [34] and learning under privileged information [35], [40], the former one being more popular than the latter. In model compression, knowledge from a large neural network [3], [36] or an ensemble of large neural models [4], [38], [39] is distilled into a small neural network, that runs efficiently during inference. In learning under privileged information, knowledge from a neural model training on privileged information (additional data representation not available at test time) is transferred to another neural model that does not have access to the privileged information. In our paper, we are not interested in compressing neural models, but in learning under privileged information. In particular, we study teacher-student training strategies, in which the teacher neural network can learn from fully-visible faces and the student neural network can learn from occluded faces only. In this context, hidden (occluded) face regions represent the privileged information. \n\nTo our knowledge, we are the first to propose the distillation of knowledge using triplet loss. We note that there are previous works [34], [36], [38], [39] that distilled triplets or the metric space from a teacher network to a student network. Different from these methods, we do not aim to transfer the metric space learned by a teacher network, but to transfer knowledge from the teacher using metric learning, which is fundamentally different.",
            "score": 0.5016446928298555,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 9531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1667
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 75,
                    "end": 79,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "12342641"
                },
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 459,
                    "end": 462,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1371,
                    "end": 1375,
                    "matchedPaperCorpusId": "102351826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158447265625
        },
        {
            "corpus_id": "254246486",
            "title": "Single image calibration using knowledge distillation approaches",
            "text": "Knowledge distillation is first proposed in [22] to transfer knowledge from a large pre-trained teacher network (or network assembly) to a smaller student network for more efficient deployment. Thereafter, knowledge distillation was introduced in LwF (learning without forgetting) to avoid catastrophic forgetting by appointing a previous snapshot of the model as a teacher while new tasks are learned [12]. More precisely, LwF preserves the outputs of the old model by optimizing a loss function defined as: \n\nwhere: L new is the common cross-entropy, which encourages new task predictions \u0177n to be consistent with ground truths y n ; L old is a distillation loss used to prevent the actual outputs of the updated network \u0177o from deviating too much from stored outputs of its older version; y o and \u03bb 0 is a loss balance weight. The original distillation loss function is based on a modified cross-entropy loss that produces a softer probability distribution over classes [22]. In our case, we solve our regression problem using a smooth-L1 loss (Eq. 4), which is less sensitive to outliers than the mean square error loss. \n\nWhere n is the number of regression outputs, and L is the batch size.",
            "score": 0.5014761914403463,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1196
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "As implied by its name, logic-based KD [43] is a distillation paradigm that employs logic within teacher models for knowledge transfer.We can formulate the general knowledge distillation loss function as follows:\n\nwhere  s ,  t \u2208 R  denote the logits output of the student and teacher network, respectively. is a temperature parameter that adjusts the smoothness of the logits. represents the number of classes.The Kullback-Leibler divergence (KLD) [43] loss can also be replaced with other functions, such as Reverse Kullback-Leibler (RKL) [20,53,65,96] distillation, Jenson-Shannon (JS) [129] distillation, etc.",
            "score": 0.5014363124014629,
            "section_title": "Logits-based KD",
            "char_start_offset": 3560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 212
                },
                {
                    "start": 214,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 613
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "3826036"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "251493147"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "107645"
                },
                {
                    "start": 589,
                    "end": 594,
                    "matchedPaperCorpusId": "233168978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014556884765625
        },
        {
            "corpus_id": "269033278",
            "title": "CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction",
            "text": "Why knowledge distillation works?Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance.\n\nRecent decoupled knowledge distillation (DKD) [33] reveals that the target distillation is related to training difficulty, and the non-target distillation provides knowledge among classes.For the kth prediction network, given its logits z (k) , the cross-entropy (CE) of DKD can be written as:\n\nwhere \u03b2 is a hyper-parameter of non-target distillation, and the target probability distribution of a student model is\n\nwhile the non-target probability distribution of a student is\n\ni , ..., q\n\nand \" * \" is the index of the target (ground-truth).For simplicity, we use tilde (\u223c) as the symbol of corresponding probability distribution from its teacher.Basically, each sequence sample is involved with a target crime event (observed intent) while other unobserved intents are covered in numerous non-target crime events.It is appropriate to model them separately through DKD.However, based on our follow-up analysis in Sec.3.2 and Sec.3.3 about target and non-target parts in Eq. ( 2), we find it is intractable for existing KD to handle numerous candidate crime events.This further inspire our CrimeAlarm.",
            "score": 0.5009363760250231,
            "section_title": "Decoupled Knowledge Distillation",
            "char_start_offset": 4475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 33,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 376
                },
                {
                    "start": 378,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 671
                },
                {
                    "start": 673,
                    "end": 791
                },
                {
                    "start": 793,
                    "end": 854
                },
                {
                    "start": 856,
                    "end": 866
                },
                {
                    "start": 868,
                    "end": 920
                },
                {
                    "start": 920,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "277765347",
            "title": "Debiased Distillation for Consistency Regularization",
            "text": "Knowledge distillation transfers \"dark knowledge\" from a large teacher model to a smaller student model, yielding a highly efficient network. \nTo improve network's generalization ability, existing works use a larger temperature coefficient for knowledge distillation. Nevertheless, these methods may lower the target category's confidence and lead to ambiguous recognition of similar samples. To mitigate this issue, some studies introduce intra-batch distillation to reduce prediction discrepancy. However, these methods overlook the inconsistency between background information and the target category, which may increase prediction bias due to noise disturbance. Additionally, label imbalance from random sampling and batch size can undermine network generalization reliability. To tackle these challenges, we propose a simple yet effective Intra-class Knowledge Distillation (IKD) method that facilitates knowledge sharing within the same class to ensure consistent predictions. First, we initialize the matrix and the vector to store logits and class counts provided by the teacher, respectively. Then, in the first epoch, we calculate the sum of logits and sample counts per class and perform KD to prevent knowledge omission. Finally, in subsequent training, we update the matrix to obtain the average logits and compute the KL divergence between the student's output and the updated matrix according to the label index. This process ensures intra-class consistency and improves the student's performance. Furthermore, this method theoretically reduces prediction bias by ensuring intra-class consistency. Extensive experiments on the CIFAR-100, ImageNet-1K, and Tiny-ImageNet datasets validate the superiority of IKD.",
            "score": 0.5009163165982239,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "To improve the efficacy of knowledge transfer, the relationships between the model complexity and existing distillation schemes or other novel distillation schemes (Sun et al. 2021) should be further investigated. \n\nCurrently, most KD methods focus on new types of knowledge or distillation loss functions, leaving the design of the teacher-student architectures poorly investigated (Nowak and Corso 2018;Crowley et al. 2018;Kang et al. 2020;Liu et al. 2019i;Ashok et al. 2018;Liu et al. 2019a). In fact, apart from the knowledge and distillation algorithms, the relationship between the structures of the teacher and the student also significantly influences the performance of knowledge distillation. For example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capacity gap between the teacher model and the student model (Zhang et al. 2019b;Kang et al. 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana 2014). Therefore, the design of an effective student model or construction of a proper teacher model are still challenging problems in knowledge distillation. \n\nDespite a huge number of the knowledge distillation methods and applications, the understanding of knowledge distillation including theoretical explanations and empiri-cal evaluations remains insufficient (Lopez-Paz et al. 2016;Phuong and Lampert 2019a;Cho and Hariharan 2019). For example, distillation can be viewed as a form of learning with privileged information (Lopez-Paz et al. 2016). The assumption of linear teacher and student models enables the study of the theoretical explanations of characteristics of the student learning via distillation (Phuong and Lampert 2019a). Furthermore, some empirical evaluations and analysis on the efficacy of knowledge distillation were performed by Cho and Hariharan (2019). However, a deep understanding of generalizability of knowledge distillation, especially how to measure the quality of knowledge or the quality of the teacher-student architecture, is still very difficult to attain.",
            "score": 0.5008254356966302,
            "section_title": "Challenges",
            "char_start_offset": 82695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 425,
                    "end": 442,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 442,
                    "end": 459,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 459,
                    "end": 477,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 477,
                    "end": 493,
                    "matchedPaperCorpusId": "69629714"
                },
                {
                    "start": 895,
                    "end": 915,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 915,
                    "end": 932,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 1497,
                    "end": 1520,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1520,
                    "end": 1545,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1545,
                    "end": 1568,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1660,
                    "end": 1683,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1847,
                    "end": 1873,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1988,
                    "end": 2012,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017852783203125
        },
        {
            "corpus_id": "222124879",
            "title": "Online Knowledge Distillation via Multi-branch Diversity Enhancement",
            "text": "Traditional knowledge distillation methods have two stages that require a pretrained teacher model to provide soft output for distillation. Different from above complex training methods, several works adopts collaboratively training strategy. Simultaneously training a group of student models based on each other's predictions is an effective single-stage distillation method, which can be a good substitute for pretrained teacher models. Some methods [16,18] solve this problem. The online knowledge distillation was completed through mutual instruction between two peers [16]. However, the lack of a high-capacity teacher model will decrease the distillation efficiency. In [17,40], each student model learns from the average of the predictions generated by a group of students and obtains a better teacher model effect. ONE found that simply averaging the results would reduce the diversity among students, affecting the training of branch-based models. ONE generates the importance score corresponding to each student through the gate module. By assigning different importance score to each branch, a high-capacity teacher model is constructed, which can leverage knowledge from training data more effectively. OKDDip [19] proposed the concept of two-level distillation. The ensemble results of auxiliary peer networks were distilled into the group leader. The diversified peer network plays a key role in improving distillation performance.",
            "score": 0.5007585871017757,
            "section_title": "Online Knowledge Distillation",
            "char_start_offset": 7086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1445
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 680,
                    "end": 683,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "208526905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014556884765625
        },
        {
            "corpus_id": "219329081",
            "title": "Learning Target-Domain-Specific Classifier for Partial Domain Adaptation",
            "text": "With an alternative classifier learning strategy, our method is also similar to the Teacher-Student model [39] for knowledge distillation and model compression. Generally, the latter is used to transfer knowledge from a complex (Teacher) network to a simple (Student) network. Only the student network is trained in an iterative manner to learn useful information from the Teacher network as much as possible. Empirical results of Hinton et al. [40] have shown that the knowledge distillation scheme can offer student networks with comparable accuracy of the Teacher model. However, both motivation and mechanism of PEAL are different from the Teacher-Student model. The main difference is that the unequal status between teacher and student is eliminated from PEAL.",
            "score": 0.5006368842969706,
            "section_title": "D. Peers Assisted Learning",
            "char_start_offset": 20522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 766
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "263861232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06695556640625
        },
        {
            "corpus_id": "224802975",
            "title": "Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher",
            "text": "Knowledge distillation is a strategy of training a student network with guide of the soft output from a teacher network. It has been a successful method of model compression and knowledge transfer. However, currently knowledge distillation lacks a convincing theoretical understanding. On the other hand, recent finding on neural tangent kernel enables us to approximate a wide neural network with a linear model of the network's random features. In this paper, we theoretically analyze the knowledge distillation of a wide neural network. First we provide a transfer risk bound for the linearized model of the network. Then we propose a metric of the task's training difficulty, called data inefficiency. Based on this metric, we show that for a perfect teacher, a high ratio of teacher's soft labels can be beneficial. Finally, for the case of imperfect teacher, we find that hard labels can correct teacher's wrong prediction, which explains the practice of mixing hard and soft labels.",
            "score": 0.5003198612588593,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0640869140625
        },
        {
            "corpus_id": "269753667",
            "title": "A Unified Asymmetric Knowledge Distillation Framework for Image Classification",
            "text": "Knowledge distillation is a model compression technique that transfers knowledge learned by teacher networks to student networks. Existing knowledge distillation methods greatly expand the forms of knowledge, but also make the distillation models complex and symmetric. However, few studies have explored the commonalities among these methods. In this study, we propose a concise distillation framework to unify these methods and a method to construct asymmetric knowledge distillation under the framework. Asymmetric distillation aims to enable differentiated knowledge transfers for different distillation objects. We designed a multi-stage shallow-wide branch bifurcation method to distill different knowledge representations and a grouping ensemble strategy to supervise the network to teach and learn selectively. Consequently, we conducted experiments using image classification benchmarks to verify the proposed method. Experimental results show that our implementation can achieve considerable improvements over existing methods, demonstrating the effectiveness of the method and the potential of the framework.",
            "score": 0.5002586598087397,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1365966796875
        },
        {
            "corpus_id": "228098458",
            "title": "Two-Stage Model Compression and Acceleration: Optimal Student Network for Better Performance",
            "text": "In the training stage of STCA, we transfer knowledge from the supernet to the subnet effective based Knowledge distillation. Knowledge distillation utilizes a large network with more parameters and calculations to transfer knowledge to a small network with fewer parameters and calculations to improving the performance of the small network. Ba and Caruana [1] proposed that a shallow net with fewer parameters can achieve similar results with a deep net, and uses a large network to transfer knowledge to a small network. The concept of knowledge distillation was first proposed by Hinton et al. [11]. First, they input the outputs of the teacher network and student network to the softmax function to get the soft-target, which contains more information than the outputs. Then, the student network learns from the soft-target and realize knowledge transfer. FitNets [23] extracts the middle layer features of the teacher network to obtain more knowledge for knowledge transfer and successfully mimics a deeper student network. Zagoruyko and Komodakis [25] proposed that only use the feature map of the neural network to transfer knowledge from teacher network to student network is inefficient and uses the attention mechanism to extract the attention map from the feature map to implement knowledge transfer more efficient. Yang et al. [24] proposed to add the noise to the teacher network. It makes the teacher network contains more soften information and reduces the performance of the teacher network to a certain extent. But the soften knowledge from the teacher network improves the generalized ability of the student network. Knowledge distillation based on generative adversarial network(KDGAN) [32] introduce to train the student network to improve its performance by adversarial learning loss greatly. During the training, the conditional generative adversarial networks is used to discriminate the outputs of the teacher network and the student network. Furlanello et al. [33] proposed the Born Again Neural Networks(BornNet), which train a small network in generations under the supervision of a large network. After multiple generations of training, the small network finally obtains performance beyond the large network.",
            "score": 0.500070571887182,
            "section_title": "B. KNOWLEDGE DISTILLATION",
            "char_start_offset": 11552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053009033203125
        },
        {
            "corpus_id": "162184150",
            "title": "Adversarially Robust Distillation",
            "text": "In the previous experiments, we see that a student network may inherit a significant amount of robustness from a robust teacher network during knowledge distillation. However, some robust teachers are not conducive to this robustness transfer. We use robust WideResNet (34-10) models trained using adversarial training and TRADES to show that while these models do transfer robustness during knowledge distillation, they transfer less than the weaker ResNet18 teacher network from the previous section (See Table 4). Additionally, a robust WRN teacher model transfers almost no robustness under knowledge distillation against 20-step PGD untargeted attacks on CIFAR-100, a much harder dataset for robustness to untargeted attacks than CIFAR-10 (See Table  5). Further experiments show that robustness transfer diminishes rapidly as we decrease \u03b1 from our default value of 1. For these reasons, we develop ARD for distilling a variety of teachers in order to produce robust students. Using ARD, robustness is preserved on architectures and datasets that do not transfer robustness under vanilla knowledge distillation.",
            "score": 0.4995899833621119,
            "section_title": "Not all robust networks are good teachers, and robustness does not transfer on some datasets",
            "char_start_offset": 14848,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038543701171875
        },
        {
            "corpus_id": "258557811",
            "title": "Distilled Mid-Fusion Transformer Networks for Multi-Modal Human Activity Recognition",
            "text": "In this section, we conduct an ablation study to evaluate the knowledge distillation method's influence to improve the student network's performance. For each experimental setting, we train a raw student network without applying the knowledge distillation step. The results are shown in table 5. For the experiments on the UTD-MHAD dataset, we use Top-1 accuracy, while for the experiments on the MMAct dataset, we use the F1score. \n\nThe results show that there is an improvement in terms of performance when a teacher network is used to train the student network. The maximum improvement is 1.19% when the 50-50 setting is used on the UTD-MHAD dataset. For the MMAct dataset, there is an improvement of 0.31% when the cross-subject setting is applied. While there is a minor improvement (0.18%) under the cross-session setting, the evaluation protocol is subject-dependent so it cannot reflect the situation in the real-world condition. The results are in accordance with our motivation that by applying the Knowledge Distillation approach, we can transfer the knowledge from a complex teacher model to a smaller student network to improve its performance.",
            "score": 0.49926737697437645,
            "section_title": "Effeteness of Knowledge Distillation",
            "char_start_offset": 37925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Different teacher architectures can provide their own useful knowledge for a student network. The multiple teacher networks can be individually and integrally used for distillation during the period of training a student network. In a typical teacher-student framework, the teacher usually has a large model or an ensemble of large models. To transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al. 2015) Yuan et al. 2021). A generic framework for multi-teacher distillation is shown in Fig. 11. Multiple teacher networks have turned out to be effective for training student model usually using logits and feature representation as the knowledge. In addition to the averaged logits from all teachers, You et al. (2017) further incorporated features from the intermediate layers in order to encourage the dissimilarity among different training samples. To utilize both logits and intermediate features, Chen et al. (2019b) used two teacher networks, in which one teacher transfers response-based knowledge to the student and the other teacher transfers feature-based knowledge to the student. Fukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each iteration. To transfer featurebased knowledge from multiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak 2020;Asif et al. 2020). Born again networks address multiple teachers in a step-by-step manner, i.e., the student at the t step is used as the teacher of the student at the t + 1 step (Furlanello et al. 2018),  and similar ideas can be found in Yang et al. (2019a). To efficiently perform knowledge transfer and explore the power of multiple teachers, several alternative methods have been proposed to simulate multiple teachers by adding different types of noise to a given teacher (Sau and Balasubramanian 2016) or by using stochastic blocks and skip connections (Lee et al. 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al. 2019a;Luo et al. 2019;Shen et al. 2019b;Luo et al. 2020).",
            "score": 0.4992658424333506,
            "section_title": "Multi-teacher Distillation",
            "char_start_offset": 38164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 517,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 796,
                    "end": 813,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1459,
                    "end": 1479,
                    "matchedPaperCorpusId": "220378802"
                },
                {
                    "start": 1479,
                    "end": 1496,
                    "matchedPaperCorpusId": "202660953"
                },
                {
                    "start": 1719,
                    "end": 1738,
                    "matchedPaperCorpusId": "21668571"
                },
                {
                    "start": 2039,
                    "end": 2057,
                    "matchedPaperCorpusId": "210952293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041839599609375
        },
        {
            "corpus_id": "203593636",
            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
            "text": "By replacing the standard convolution with several cheap convolutions, a variety of CNN models can be compressed and accelerated substantially. It is simple and straightforward to train the compressed models with cheap convolutions from scratch to improve the accuracy. By doing this, it however leads to a limited improvement on accuracy, which is due to a limited knowledge used only by the ground-truth labels. \n\nAlternatively, knowledge distillation [30,32] is becoming a promising solution, which aims to transfer more knowledge from a teacher network to a student network to boost the accuracy of the student network. For this paper, we first review two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: dark knowledge (DK) [30] and attention transfer (AT) [32]. In that case, we can select a CNN model with the standard convolution as a teacher, while a model with cheap convolution by keeping the teacher's architecture is regarded as a student. Then, we propose our online distillation (OD) method to replace the pretrained teacher network by constructing online from the multiple student networks with the same architecture, and train both teacher and student networks in a one-shot manner. \n\nDark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels. \n\nAttention transfer.",
            "score": 0.49923301362947653,
            "section_title": "Online Distillation",
            "char_start_offset": 18581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "245329688",
            "title": "Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching",
            "text": "Knowledge distillation [20] aims to transfer knowledge from a well-trained teacher network to a compact student network. In the classical knowledge distillation approaches, the student networks are supervised by information extracted from the teacher networks, such as predicted probabilities [19], [20], intermediate features [35], etc. Differently, the self-distillation mechanism transfers knowledge within a model itself [36], [37], [38]. For example, [36] transfers knowledge from the deeper layers of a neural network into its shallow layers. [38] proposes to guide the learning of a current network layer by the output of network layer behind it. [37] utilizes the information of earlier training epochs to supervise the later training epochs. The selfdistillation mechanism has been applied in many fields like classification [36], weakly-supervised object detection [39], etc. In this paper, we introduce the self-distillation mechanism into the training process of the WSSS model and propose a novel self-dual teaching strategy to facilitate an effective knowledge distillation under the weak supervision.",
            "score": 0.49915888427213606,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1115
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "225062031"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09124755859375
        },
        {
            "corpus_id": "237250417",
            "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "text": "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022). Knowledge distillation is a prominent method for training compact networks to achieve comparable performance to a deep network. Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the \"dark knowledge\" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model. Since its introduction, several works (Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Park et al., 2019;Sun et al., 2019;Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer. In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) . However, the intermediate teachers are heuristically selected independently of the training process and the evolution of the teacher network is discrete. In contrast, MetaDistil employs meta learning to make the teacher model adapt to the current state of the student model and provide a continuously evolving meta-teacher that can better teach the student. Concurrently, Park et al. (2021) and Shi et al. (2021) propose to update the teacher model jointly with the student model with task specific objectives (e.g., cross-entropy loss) during the KD process and add constraints to keep student and teacher similar to each other. Their approaches makes the teacher model aware of the student model by constraining the teacher model's capacity. However, the teacher models in their methods are still not optimized for knowledge transfer. In addition, Zhang et al. (2018) introduced deep mutual learning where multiple models learn collaboratively and teach each other throughout the training process. While it is focused on a different setting where different models have approximately the same capacity and are learned from scratch, it also",
            "score": 0.4989153696390407,
            "section_title": "Related Work",
            "char_start_offset": 9555,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 600,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 600,
                    "end": 618,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 618,
                    "end": 635,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1119,
                    "end": 1142,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38818359375
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.\n\nSince then, this technology has rapidly attracted widespread attention from academia and industry, and has been expanded and deepened in several ways: Early stage (2015-2018) : Initial research has focused on simplifying network structure, reducing model volume and computational requirements, while maintaining the predictive performance of the model.In this period, the basic framework of knowledge distillation and the design of loss function were established.Technical deepening (2019-2021) : Researchers began to explore more refined distillation methods, including multi-teacher distillation, feature-stage distillation, relational distillation, etc.\n\nIn the field of deep learning, model knowledge lies in the configuration of parameters it learns through training, which guides the model on how best to extract features from input data and make predictions.Large networks, thanks to their large number of parameters and complex structure design, can capture deeper feature associations on large-scale data sets, showing superior learning ability and generalization performance.However, this advantage is often difficult to play directly in resource-constrained real-world application scenarios, because they have high requirements for computing resources and storage space.Knowledge distillation technology is born to solve this contradiction, it focuses not only on the final classification or regression results of the model output, but also on the teaching of the teacher model's confidence distribution (i.e., soft label) that each sample belongs to various categories.By designing a specific training mechanism, the student model tries to imitate the soft decision-making process while learning the real label, so that it can also \"inherit\" the decision logic and deep understanding of the data of the teacher model under limited parameters.This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
            "score": 0.498895774874191,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 12089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1993
                },
                {
                    "start": 1993,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047088623046875
        },
        {
            "corpus_id": "235680434",
            "title": "Dual bidirectional mutual distillation based on dense connections",
            "text": "Deep neural network (DNN) has achieved great success in computer vision processing. Although the performance is attractive, there are a lot of parameters in the popular deep neural network model. This leads to high computational costs and high requirements for device memory. For solving this problem, various network model compression techniques [1] been put forward one after another. Recently, knowledge distillation (KD) is a promising compact and accurate solution series model that has attracted increasing attention. Knowledge distillation is a [2] of model compression techniques for learning compact deep neural network models in which a smaller network (student) is trained to simulate representations of large networks (teachers) with higher accuracy. The popularity of knowledge distillation mainly lies in its simplicity and versatility. Teacher-based student model learning is very simple, and there is no limit to the network structure of both models. By making students' predictions consistent with teachers' predictions, students can improve their performance. Recently, some studies have shown that instead of using pre-trained teachers to train students, it is also effective to learn from each other by peer teaching, which is called online distillation. The traditional distillation approach is a two-stage process that requires pre-training of a strong network of teachers and individual transfers of knowledge to a relatively small and untrained network of students, which requires more training time and computational cost. Online knowledge distillation follows a one-stage end-to-end training strategy to optimize the performance of the target network without pre-training high-capacity teachers. On the other hand, in the online mutual distillation, there is no specific teacher-student role, from the beginning of training, all networks learn simultaneously through mutual teaching. The results obtained by the network trained in this online distillation mode are not only better than the separately trained crossentropy loss network, but also better than the network trained in the traditional offline distillation mode from the pre-trained teacher network. \n\nThe existing methods of knowledge distillation are all implemented by mining new information knowledge or designing new distillation loss functions. for this, we propose a new perspective: by designing the connecting path between teachers and students, we train together from scratch and realize the gradient propagation of the displayed layer to layer.",
            "score": 0.4982384151010861,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2185
                },
                {
                    "start": 2188,
                    "end": 2336
                },
                {
                    "start": 2337,
                    "end": 2541
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "10137788"
                },
                {
                    "start": 552,
                    "end": 555,
                    "matchedPaperCorpusId": "2887064"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0379638671875
        },
        {
            "corpus_id": "219636179",
            "title": "Knowledge Distillation Meets Self-Supervision",
            "text": "Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting \"richer dark knowledge\" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs.",
            "score": 0.4980829148799004,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "269137377",
            "title": "Federated Distillation: A Survey",
            "text": "These methods focus on transferring knowledge from the middle layers of the teacher model to guide the training of the student model, helping the student model learn crucial representations and features at an intermediate level.\n\n3) Parameter Knowledge -In this category, partially trained parameters or network modules of the teacher model are directly used as knowledge during distillation training.This method is often combined with other distillation approaches to enhance knowledge transfer [63].Specifically, PESF-KD proposes an efficient approach to transferring knowledge from the teacher network to the student network by updating specific parameters of the pre-trained teacher model [64].FSKD compresses the pre-trained teacher model to obtain a student model and adjusts the feature dimensions using 1 \u00d7 1 convolutions between layers, allowing the student model to achieve performance comparable to traditional fine-tuned distillation methods with a small number of samples [65].Similarly, IAKD [66] and NGFSKD [67] assist distillation by replacing modules of the teacher model and the student model.The distillation loss for individual module replacement is expressed as:\n\nwhere L CE represents the calculation of cross-entropy, and M T o and M S o are the outputs of the corresponding modules in the teacher and student networks.Recently, SAKD proposes merging teacher and student networks of the same style system into a multi-path network.\n\nDuring training, a different teacher network module is dynamically selected to replace the corresponding student network module for each sample, resulting in a student model with superior performance [68].These methods leverage parameter knowledge from the teacher model to guide the training of the student model, either by updating specific parameters or by replacing modules.This enables the student model to learn from the partially trained teacher model, improving its performance and reducing its complexity.",
            "score": 0.49781537951991084,
            "section_title": "Soft targets Teacher model Loss",
            "char_start_offset": 14697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1454
                },
                {
                    "start": 1456,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "216035835"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "234805083"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "228063796"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "248503415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0196075439453125
        },
        {
            "corpus_id": "236782270",
            "title": "Adaptive Teacher Finetune: Towards high-performance knowledge distillation through adaptive fine-tuning",
            "text": "Knowledge distillation is a widely used method to transfer knowledge from a large model to a small model. Traditional methods use pre-trained large models to supervise the training of small models, called Offline Knowledge Distillation, However, the structural gap between teachers and students limits its performance. After that, Oneline Knowledge Distillation retrained the teacher-student network from the beginning and the method of echo teaching greatly improved the performance. But there is very little work to explore the difference between the two. In this paper, we first point out that the essential difference between Offline and Oneline Knowledge Distillation is actually whether the weight of the teacher-student network has a process of mutual adaptation. If they adopt the teacher network and the student network jointly train to implement Offline Knowledge Distillation, there is no obvious difference in the final performance, no matter whether it is a joint distillation training. This shows that teacher-student network adaptation is important for Knowledge Distillation. Then, we propose an Adaptive Teacher Finetune (ATF) to adapt the teacher model to the student network. It will use student model information for Tinetune during the Offline Knowledge Distillation process. With normalized logical distribution and alpha-divergence, the performance improvement of ATF clearly exceeds the existing Offline and Oneline Knowledge Distillation method. Extensive experiments conducted on cifar and ImageNet support our aforementioned analysis and conclusions. With the newly introduced ATF, we obtained state-of-the-art performance on ResNet 18 on ImageNet.",
            "score": 0.49649373650210804,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "264052001",
            "title": "Decentralized Heterogeneous Federal Distillation Learning Based on Blockchain",
            "text": "Knowledge distillation is a technique that extracts valuable insights from complex models and condenses them into a singular, streamlined model, thereby enabling its deployment in real-world applications. Knowledge distillation [15] is a knowledge transfer and model compression algorithm proposed by Geoffrey Hinton et al. in 2015. For a specific character, through the use of a knowledge distillation algorithm, the information of an ideally trained teacher network containing more knowledge can be transferred to a smaller untrained student network. \n\nIn this paper, the loss function L student of the student network can be defined as: \n\nLCE is the cross entropy loss function, LKL is the Kullback Leibler (KL) divergence, p student and p teacher are the outputs of the network after the softmax activation function, z is the output logits of the neural network, and T is the temperature, which is generally set as 1. The primary purpose of temperature is to reduce the loss of knowledge contained in the small probability results caused by excessive probability differences. KL divergence can measure the difference between the two models. The larger the KL divergence, the more significant the distribution difference between the models, and the smaller the KL divergence, the smaller the distribution difference between the two models. The formula of KL divergence is: \n\nwhere P(x) and Q(x) respectively represent the output of different networks after the softmax activation function.",
            "score": 0.4964847307291316,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042236328125
        },
        {
            "corpus_id": "257504799",
            "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
            "text": "Surprisingly, student networks trained using online distillation methods have shown the potential to compete with, and in some cases, even outperform networks trained with a strong pre-trained teacher network [44]. However, while most existing collaborative learning methods focus on designing more complex training graphs and using auxiliary branches to improve the distillation of the high-level dark knowledge like probability distributions and feature embeddings from the feature extractors [44,7,4,19,18,37], the importance of knowledge from low-level features is greatly underestimated. To this end, we propose MetaMixer, a regularization strategy that can boost the transfer of multi-level knowledge, including both low-level knowledge that contains localizable features and high-level knowledge that are more abstract and focuses on the whole image. \n\nOur main contributions are listed as follows: \n\n(1) We proposed MetaMixer, a regularization method for online knowledge distillation. \n\n(2) We stressed the importance of low-level knowledge in online KD, and we proposed a distillation strategy that can take advantage of dark knowledge with different levels of abstraction. \n\n(3) We evaluated MetaMixer on multiple benchmark datasets and network structures and verified our higher performance over the state-of-the-art online KD methods.",
            "score": 0.4963042453733102,
            "section_title": "Introduction",
            "char_start_offset": 1799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1347
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 499,
                    "end": 501,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 501,
                    "end": 503,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "246872166"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "219531897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2744140625
        },
        {
            "corpus_id": "265351966",
            "title": "Leveraging different learning styles for improved knowledge distillation in biomedical imaging",
            "text": "The knowledge can be transferred in an online or offline manner from the teacher to the student networks.In offline knowledge distillation (KD (off)), training is done in two steps; first, the teacher network is pretrained on a dataset, and then the knowledge is distilled to train the student network, whereas in online knowledge distillation (KD (on)) [10], both teacher and student networks are trained simultaneously in a single training step.",
            "score": 0.4960238447232038,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 447
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039642333984375
        },
        {
            "corpus_id": "252262956",
            "title": "A Novel Knowledge Distillation Method for Self-Supervised Hyperspectral Image Classification",
            "text": "As the number of network layers deepens, current deep-learning models are becoming more and more complex, while the computational resources required to consume them become increasingly large. To alleviate this problem, Hinton et al. proposed the knowledge distillation method [53]. Traditional knowledge distillation methods train a teacher model on a known dataset and then supervise the training of a student model using the soft labels of the teacher model as well as the real labels. In general, the higher the training accuracy of the teacher model compared to the student model, the more effective the distillation effect is [36,54]. According to the present traditional method, a series of novel distillation models have been proposed [55][56][57][58]. Traditional knowledge distillation between models often suffers from inefficient knowledge transfer and requires a lot of experimentation to find the optimal teacher model. For this reason, a novel approach to knowledge distillation is proposed. This is called self-distillation, where the network itself acts as both a teacher model and a student model. Knowledge distillation usually takes place between different layers of the network. In [59], a self-distillation strategy is proposed that achieves improved computational efficiency by designing in a new network structure for knowledge distillation at each layer of the network. Additionally, in [60], the simultaneous use of soft labels and feature maps to achieve knowledge distillation is proposed. Since knowledge distillation enables the knowledge contained in the teacher model to be transferred to the student model, the trained student model can be utilized to achieve good classification results using only a small number of labeled samples. Given the advantages of knowledge distillation on a limited sample dataset, we added it to the training. This is different from traditional methods, which use teacher models to generate soft labels. In summary, we are implementing hierarchical prediction by adding a fully connected layer to each layer of the network and combining it with soft labels to achieve knowledge distillation. Since no teacher model is used, it can be said that this is a self-distilling approach.",
            "score": 0.49584198929782997,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "271244914",
            "title": "Relational Representation Distillation",
            "text": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
            "score": 0.49581009610294546,
            "section_title": "Related Work",
            "char_start_offset": 4321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 693,
                    "end": 696,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "251903477",
            "title": "Towards Federated Learning against Noisy Labels via Local Self-Regularization",
            "text": "Knowledge distillation [8,19] aims to transfer knowledge from a complex teacher model to a lightweight student model by minimizing the output or intermediate features of these two networks. Although knowledge distillation has successfully applied in many fields, such as model heterogeneity in FL [32] and domain adaption in transfer learning [46], obtaining a complex teacher model still requires a large amount of extra training overhead. To further improve efficiency for knowledge transferring, self knowledge distillation [44,57] (or called self distillation) aims to utilize knowledge from model itself without the involvement of an explicit teacher network. Existing methods usually exploit knowledge from model itself [14] or distill knowledge between different network layers [20,71]. Different from these above-mentioned methods, we utilize self distillation at the instance level to minimize the discrepancy of the model output between the original and augmented samples.",
            "score": 0.4957251213134413,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 982
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "220514674"
                },
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "211096976"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "199453123"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "199405591"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "258714078",
            "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
            "text": "The Kullback-Liebler loss provides additional gradient information to the distillation loss that transfers a teacher's knowledge to a student. Properly softening the hyper-parameter Temperature, \u03c4, is required to produce the softening effect that introduces dark knowledge for the",
            "score": 0.4954987916806899,
            "section_title": "Issues with knowledge distillation",
            "char_start_offset": 14882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05645751953125
        },
        {
            "corpus_id": "266977228",
            "title": "Mutual Distillation Learning for Person Re-Identification",
            "text": "Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network [45]. In terms of representation of the knowledge to be distilled, existing models typically use teacher's class probabilities [46] and/or feature representation [47]. And, some methods have proposed an alternative approach where an ensemble of students teaches each other through mutual distillation [48]. \n\nFor current distillation techniques, the emphasis is placed on utilizing multiple backbones to facilitate knowledge transfer. In contrast, our approach employs a single backbone network with two distinct branches. Significantly, these two branches extract features from different perspectives, ensuring the effectiveness of knowledge transfer. Framewok Overview: As shown in Figure . 2, the overall structure of our proposed network includes two branches and a knowledge distillation and fusion module. We adopt ResNet as the backbone network, with two branches sharing the lowlevel network while independently utilizing the high-level network. These branches are tailored to extract different features using varied techniques, while the knowledge distillation and fusion module conducts mutual distillation and integrates the outputs of both branches to extract meaningful information. The algorithm utilized in the training process is explicated in Algorithm. 1, adopting an end-to-end framework that offers a convenient approach to training and f Hard All , f Sof t All , and f f usion correspond to all features generated by the Hard Content Branch, all features produced by the Soft Content Branch, and the fused output from the Knowledge Distillation and Fusion Module. Each component in our model plays a crucial role in the overall architecture and we will describe how these components perform in the next subsections.",
            "score": 0.4952993258565807,
            "section_title": "E. Distillation learning",
            "char_start_offset": 13287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09173583984375
        },
        {
            "corpus_id": "219531897",
            "title": "Peer Collaborative Learning for Online Knowledge Distillation",
            "text": "Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a high-capacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-the-art methods.",
            "score": 0.49507686722089295,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050811767578125
        },
        {
            "corpus_id": "252668749",
            "title": "Using Knowledge Distillation to improve interpretable models in a retail banking context",
            "text": "Dark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation. While the available body of research for those is thinner, other approaches are thus included in this paper, and will be detailed alongside the seminal technique.",
            "score": 0.49506449754752213,
            "section_title": "Multiple flavors of knowledge distillation",
            "char_start_offset": 4539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 753
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "247476179",
            "title": "Decoupled Knowledge Distillation",
            "text": "The concept of knowledge distillation (KD) was firstly proposed by Hinton et al. in [12]. KD defines a learning manner where a bigger teacher network is employed to guide the training of a smaller student network for many tasks [12,17,18]. The \"dark knowledge\" is transferred to students via soft labels from teachers. For raising the attention on negative logits, the hyper-parameter temperature was introduced. The following works can be divided into two types, distillation from logits [3,6,22,40,44] and intermediate features [10,11,14,15,23,25,28,33,34,41,43].\n\nPrevious works of logit distillation mainly focus on proposing effective regularization and optimization methods rather than novel methods. DML [44] proposes a mutual learning manner to train students and teachers simultaneously. TAKD [22] introduces an intermediate-sized network named \"teacher assistant\" to bridge the gap between teachers and students. Besides, several works also focus on interpreting the classical KD method [2,26].\n\nState-of-the-art methods are mainly based on intermediate features, which can directly transfer representations from the teacher to the student [10,11,28] or transfer the correlation between samples captured in the teacher to the student [23,33,34]. Most of the feature-based methods could achieve preferable performances (significant higher than logits-based methods), yet involving considerably high computational and storage costs.\n\nThis paper focuses on analyzing what limits the potential of logits-based methods and revitalizing logit distillation.",
            "score": 0.49497418867901866,
            "section_title": "Related work",
            "char_start_offset": 6277,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 492,
                    "end": 494,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 552,
                    "end": 555,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1154,
                    "end": 1157,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4365234375
        },
        {
            "corpus_id": "237940419",
            "title": "Partial to Whole Knowledge Distillation: Progressive Distilling Decomposed Knowledge Boosts Student Better",
            "text": "Cheng et al. (2020) explains the working mechanism of knowledge distillation by quantifying the knowledge encoded in the intermediate layer, and they find out that knowledge distillation makes student learn more visual concepts. In this paper, we do not intend to further demystify dark knowledge, but this interesting observation motivates us to rethink knowledge distillation schemes from the perspective of knowledge quantity rather than knowledge quality. Orange arrows in Figure 1 represents the feature information corresponding to different knowledge quantity with one given knowledge type. Cho and Hariharan (2019) empirically finds that the teacher accuracy is a poor predictor of student performance, which implies that teacher consisting of more knowledge may not be a better teacher. TAKD (Mirzadeh et al. 2020) introduces a teacher assistant with intermediate less knowledge to make the students learn better. Thus, we suppose that knowledge quantity of teacher has a great influence on the efficacy of knowledge distillation. \n\nBased on the above observation, we survey most knowledge distillation literature whether it is offline or online paradigms. Then, we find out that teacher transfer the whole knowledge (logits, intermediate feature maps, etc obtained with the whole feed-forward computation graph) to student at once, which is counter-intuitive in the view of human beings. Imagining that a human teacher imparts all of what he has learned throughout his life to student, is it easy for student to accept it, especially in the early learning phases with poor cognitive ability? Hinton describes GLOM (Hinton 2021) to parse an image into a part-whole hierarchy, which motivates us to model the knowledge quantity of teacher into the partial-whole paradigm. We further make an intuitive hypothesis that progressively distilling knowledge from teacher with a partial-whole paradigm boosts student better (named partial-whole hypothesis for short). \n\nTo verify partial-whole hypothesis, we propose a partial to whole knowledge distillation (PWKD) paradigm as Figure 1 shows. We define a new setup of knowledge decomposition to parse knowledge of teacher into partial-whole knowledge. Different from GLOM (Hinton 2021) parsing image into independent partial representation, our goal is much simpler and aims to decompose knowledge representation into monotonically increasing fragments.",
            "score": 0.49478585600164104,
            "section_title": "Introduction",
            "char_start_offset": 1655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1968
                },
                {
                    "start": 1971,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 622,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 801,
                    "end": 822,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09771728515625
        },
        {
            "corpus_id": "257233096",
            "title": "Graph-based Knowledge Distillation: A survey and experimental evaluation",
            "text": "Knowledge distillation [12] was originally proposed for model compression. Unlike pruning and quantification in model compression, knowledge distillation (KD) uses the T-S framework to pre-train a large teacher model to distill to obtain a lightweight student model, enhancing the generalization ability of the student model and achieving better performance and higher precision. Through distillation, \"knowledge\" (soft label supervised information) in the teacher model is transferred to the student model. In this way, student models can reduce the complexity of time and space, which can also learn soft label information (containing inter-category information) that is not available in the one-hot label without losing the quality of the prediction. Generally, KD can be divided into two technical routes in accordance with the different ways of knowledge transfer. \n\nThe first is response-based distillation, which is closely related to label smoothing [111], using the output probability of the teacher model as smoothing labels to train students. [12] is the pioneering work of knowledge distillation, proposed by Hinton in 2015, which was first proposed to transfer the output probability by the softmax layer of the teacher model to the student model as a \"softtarget\" to improve the performance of the student model. To learn the feedback information in the student network, DML [112] proposes the strategy of deep mutual learning, allowing a group of students to train simultaneously on the network and realize mutual learning and progress through the supervision of real labels and the learning experience of peer network output results. BAN [113] uses an integrated approach to train the student model so that its network structure is the same as that of the teacher model, which significantly outperforms the teacher model in computer vision and language modeling downstream tasks. \n\nAnother type of knowledge distillation is the featurebased distillation method, in which the semantic information contained in the middle layer feature representation in the teacher network structure as knowledge transfer to the student model. FitNet [114] is the first classical work to adopt this method, leveraging the output of the teacher network and the feature embedding of the middle layer as supervision information to extend KD and realize the problem of deep model network compression.",
            "score": 0.4946018283685899,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 958,
                    "end": 963,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1389,
                    "end": 1394,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1654,
                    "end": 1659,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1995849609375
        },
        {
            "corpus_id": "201107180",
            "title": "Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation",
            "text": "Knowledge distillation [8] adopts a teacher-guidingstudent strategy where a small student network learns to imitate the output of a large teacher network. In this way, the large teacher network can transfer knowledge to the student network with smaller model size, which is widely applied to model compression. Following [8], some works are proposed to exploit the intermediate representation to optimize the learning of student network, such as Fit-Net [26], DK 2 PNet [32], AT [36] and NST [13]. In summary, these works pay more attention on knowledge transfer among the same classification task. Transfer learning is proposed to transfer knowledge from source domain to target domain to save data on target domain [24]. It contains two main research directions: cross-domain transfer learning [22,12,10,4] and cross-task one [9,3,5,35]. In the case of cross-domain transfer learning, the dataset adopted by source domain and the counterpart of target domain are different in domain but the same in category. Also, crosstask transfer learning adopts the datasets that have the same domain but different categories. Transfer learning mainly focuses on compensating for the deficit of data on target domain with enough data on source domain. By contrast, our approach amalgamates multiple pre-trained models to obtain a multitalented model using unlabelled data.\n\nTo exploit knowledge of massive trained deep-learningbased models, researchers have made some promising attempts. MTZ [7] merges multiple correlated trained models by sharing neurons among these models for cross-model compression. Knowledge flow [14] transfers knowledge from multiple teacher models to student one with strategy that student learns to predict with the help of teachers, but gradually reduce the dependency on teachers, finally predict independently. Despite very promising solutions, the above approaches still depend on labelled dataset, which is not suitable for our application scenario where no human labels are available.\n\nThe approach of [28] proposes to transfer knowledge from multiple trained models into a single one in a layerwise manner with unlabelled dataset. It adopts an auto-  encoder architecture to amalgamate features from multiple single-task teachers. Several knowledge amalgamation methods are also proposed to handle the above task [33,23,34",
            "score": 0.4945046393508803,
            "section_title": "Related Work",
            "char_start_offset": 5423,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "37879517"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "740063"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "13798326"
                },
                {
                    "start": 800,
                    "end": 803,
                    "matchedPaperCorpusId": "52956362"
                },
                {
                    "start": 803,
                    "end": 806,
                    "matchedPaperCorpusId": "1010081"
                },
                {
                    "start": 806,
                    "end": 808,
                    "matchedPaperCorpusId": "52453713"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "225718"
                },
                {
                    "start": 831,
                    "end": 833,
                    "matchedPaperCorpusId": "43993788"
                },
                {
                    "start": 833,
                    "end": 835,
                    "matchedPaperCorpusId": "26286062"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "43975260"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "69629714"
                },
                {
                    "start": 2025,
                    "end": 2029,
                    "matchedPaperCorpusId": "53231387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0261993408203125
        },
        {
            "corpus_id": "237513676",
            "title": "New Perspective on Progressive GANs Distillation for One-class Novelty Detection",
            "text": "To reduce the large computation and storage cost of deep convolutional neural networks, knowledge distillation can transfer the generalization ability of a large network (or an ensemble of networks) to a light-weight network. Figure 2 (a), Hinton et al. [15] used the outputs of the softmax layer of a teacher network as the target function to train the student network. Romero et al. [39], illustrated in Figure 2 (b), proposed that a student network with random initialization can imitate the intermediate representations of the teacher network to improve its own performance. In order to ensure the student network to learn the true data distribution from the teacher network, knowledge distillation with a discriminator was used for distinguishing features extracted from the teacher and student networks, Figure 2 (c) [51], [52], [25]. Li et al. [24] combined neural architecture search and knowledge distillation to compress the generator for controllable image synthesis. \n\nGANs [14] have been applied to many real world applications such as domain adaptation, image generation, and anomaly detection. However, to our knowledge, there is no related works that deploy the knowledge distillation on two standard GANs. Therefore, this paper designs a distillation loss to transfer knowledge from the teacher GAN to the student GAN, in which knowledge distillation is considered as a progressive learning process that can continuously improve the performance of student networks.",
            "score": 0.4943388520817623,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 8641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "213175568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1900634765625
        },
        {
            "corpus_id": "246285427",
            "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding",
            "text": "should be noted that though the proposed method is also based on knowledge distillation, our reverse distillation is the first to adopt an encoder and a decoder to construct the T-S model. The heterogeneity of the teacher and student networks and reverse data flow in knowledge distillation distinguishes our method from prior arts.",
            "score": 0.49410525913498493,
            "section_title": "Related Work",
            "char_start_offset": 9706,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0244293212890625
        },
        {
            "corpus_id": "271956980",
            "title": "Bring the Power of Diffusion Model to Defect Detection",
            "text": "Large-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
            "score": 0.4939122247535405,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 7189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "259697005"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "249145972"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "251196646"
                },
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06439208984375
        },
        {
            "corpus_id": "233407431",
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "text": "Knowledge distillation. Knowledge distillation [21] aims to transfer the \"dark\" knowledge learned from a highcapacity teacher network to a student network via soft labels. The soft labels can be the class probabilities [6,14,56] or the feature representations [34,48] output by the teacher, containing more complete structured information than the one-hot ground-truth labels. The distillation process can be formed by a \"teacher-student\" framework [21,40], a \"peer-teaching\" framework [14,28,59], or a self-distillation framework [6,25,52,56,58]. Our BAKE is mostly related to the self-distillation methods, i.e., teaching a single network using its own knowledge. However, most of them [6,25,58] only considered the knowledge of individual instances, resulting in sub-optimal learning targets. Recent works introduced to preserve the predictive consistency between intraimage (original v.s. perturbed) [52] or intra-class (images out of the same class) [56] samples. However, they only focused on pairwise images, carrying limited information compared to the ensembled batch knowledge of BAKE. More importantly, they simply defined the positive pairs using constant instance or class IDs, which may incur false supervisions as their visual features might be actually dissimilar, especially after the random crop augmentation [43]. \n\nKnowledge ensembling. It is well-known that an ensemble of multiple networks generally yields better predictions than a single network in the ensemble. The ensembling technologies aim to generate robust supervision signals via aggregating models [13,15,46] or predictions [14,27,28,40,41,47]. Several attempts leveraged the spirit of knowledge ensembling in distillation tasks, dubbed \"ensemble distillation\" methods. For example, CRD [47] and MEAL [40] proposed to enhance the soft targets by en-sembling the knowledge of multiple pre-trained teacher networks. KDCL [14] introduced to aggregate the information from multiple independent students, which are collaboratively training.",
            "score": 0.4932959202209346,
            "section_title": "Related Works",
            "char_start_offset": 4865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 453,
                    "end": 456,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 537,
                    "end": 540,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 694,
                    "end": 697,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "matchedPaperCorpusId": "219687798"
                },
                {
                    "start": 1585,
                    "end": 1588,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1588,
                    "end": 1591,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1614,
                    "end": 1617,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1617,
                    "end": 1620,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1623,
                    "end": 1626,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "54447578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46923828125
        },
        {
            "corpus_id": "231698725",
            "title": "Deep Epidemiological Modeling by Black-box Knowledge Distillation: An Accurate Deep Learning Model for COVID-19",
            "text": "Knowledge distillation (Hinton, Vinyals, and Dean 2015) is widely used to solve deep neural network compression problem. Conventional distillation process is carried out by training a smaller neural network called student model with class probability, which is referred to as \"dark knowledge\", to retain the performance of original cumbersome ensemble of models called teacher model. This approach can effectively reduce model size, which makes complex models feasible for real-world applications. Many complex applications in computer vision or natural language processing have justified its merits for model size reduction. For example, Dis-tilBERT (Sanh et al. 2019) successfully reduces the size of original BERT model by 40% with maintaining accuracy; TinyBERT (Jiao et al. 2019) leverages knowledge distillation to design a framework for the reduction of transformerbased language model, which leads to the models with lower time and space complexity, thus facilitating its application; relational knowledge distillation (Park et al. 2019) further optimizes distillation process and enables more productive student model, which can even outperform teacher model. However, this effective approach has not been applied to solve complex epidemiological modeling, especially the infeasibility of mixture epidemiological models.  Figure 1: Modeling with black-box knowledge distillation. Teacher model is an accurate but significantly complex comprehensive simulation system. Both observation and projection sequences are simulated results. Model query is optimized by sequence mixup.",
            "score": 0.4932131476274907,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5321,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1027,
                    "end": 1044,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "272969060",
            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
            "text": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
            "score": 0.49305636869784075,
            "section_title": "Preliminaries",
            "char_start_offset": 8573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "231662091",
            "title": "Collaborative Teacher-Student Learning via Multiple Knowledge Transfer",
            "text": "However, most existing KD methods only consider either knowledge from individual instance features to maintain instance consistency between teacher and student or knowledge from instance relations to preserve the instance correlation consistency. There are a few works that consider more than one kind of knowledge in knowledge distillation [37,14] at the same time and explore the efficacy of each kind of knowledge. \n\nTransferring different types of knowledge can be implemented with different distillation methods, e.g., offline distillation, online distillation and self-distillation [1]. Most of the KD methods employ offline distillation, which is one-way knowledge transfer from a pre-trained large teacher to a small student [20,22,13]. In offline distillation, the capacity gap caused by a fixed teacher-student architecture and the requirement of a large dataset for pre-training the teacher often result in a degraded performance [22]. Thus, finding a proper teacher-student architecture in offline distillation is challenging. In contrast, online distillation provides a one-phase end-to-end training scheme via teacher-student collaborative learning on a peer-network architecture instead of a fixed one [25,33,36,32,28,12]. \n\nSelf-distillation performs online distillation within the same network to reduce model over-fitting [23,24]. Online distillation and self-distillation are promising methods for knowledge distillation as they bridge the capacity gap via avoiding the need of a large teacher network, leading to an improved performance. However, both KD methods used individually are limited to knowledge distillation from a single source, i.e., individual instances, online distillation could further suffer from the poor instance consistency between peer networks caused by the discrepancy in their network outputs. \n\nConsequently, it is desirable to have a unified framework that can integrate the advantages of different KD methods and make efficient use of different types of knowledge.",
            "score": 0.49299402895624755,
            "section_title": "Introduction",
            "char_start_offset": 2036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "201107180"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1221,
                    "end": 1224,
                    "matchedPaperCorpusId": "27384186"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "125950115"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "226841849"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1344,
                    "end": 1347,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03277587890625
        },
        {
            "corpus_id": "252613769",
            "title": "Remote Sensing Image Scene Classification via Self-Supervised Learning and Knowledge Distillation",
            "text": "Knowledge distillation can be viewed as normalizing the training of the student network with soft targets that carry the \"dark knowledge\" of the teacher network. We combine two knowledge distillation methods so that the backbone can obtain knowledge from the branch and itself. The knowledge distillation methods include soft logit distillation and batch distillation. \n\nWe denote Z = [z 1 , z 2 , . . . , z N ] \u2208 R N\u00d7M as the logit vectors of the backbone in the same batch. For the j-th sample, the backbone logit vector z j = [z 1 j , z 2 j , . . . , z M j ] \u2208 R 1\u00d7M , where M is the number of categories, and N is the number of samples in a batch.",
            "score": 0.4929889787980792,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 25158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 651
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272705078125
        },
        {
            "corpus_id": "257513218",
            "title": "Model Compression for Deep Neural Networks: A Survey",
            "text": "As shown in Figure 5, knowledge distillation is a teacher-student architecture [106][107][108].The teacher network is a complex pre-trained network, and the student network is a simple small network.The teacher network provides the student network with prior knowledge so that the student network achieves similar performance to that of the teacher network.Deploying deep models in mobile devices is challenging due to the limited processing power and memory of these devices.To address these issues, Bucilu\u0203 et al. [109] first proposed model compression to transfer information from a large model to train a small model without significant accuracy degradation.Henceforth, the training of small models by large models was called knowledge distillation [108,110,111].Chen et al. [112] posited that feature embedding from deep neural networks could convey complementary information and, thus, proposed a novel knowledge-distilling strategy to improve its performance.The main idea of knowledge distillation was that the student model imitated the teacher model to achieve competitive, or even superior, performance.The key focus was how to transfer knowledge from a large teacher model to a small student model.\n\nIn the process of knowledge distillation, knowledge types, distillation strategies, and teacher-student architectures have played key roles in the student learning process.The activations, neurons, and features of the middle layer were available as knowledge to guide the learning of the student model [113][114][115][116][117]. The relationship between different activations, neurons, and features contained the rich knowledge learned by the teacher model [118][119][120][121][122]. As shown in Figure 6, three methods of knowledge distillation were introduced.These three distillation methods are described in detail in the following sections.",
            "score": 0.4928785316334541,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 26253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1210
                },
                {
                    "start": 1212,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 89,
                    "matchedPaperCorpusId": "236159551"
                },
                {
                    "start": 516,
                    "end": 521,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 779,
                    "end": 784,
                    "matchedPaperCorpusId": "52927917"
                },
                {
                    "start": 1524,
                    "end": 1529,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1534,
                    "end": 1539,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1669,
                    "end": 1674,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1674,
                    "end": 1679,
                    "matchedPaperCorpusId": "195847947"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "102351826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "260955936",
            "title": "Self-Distillation for Randomized Neural Networks",
            "text": "Knowledge distillation (KD) is a conventional method in the field of deep learning that enables the transfer of dark knowledge from a teacher model to a student model, consequently improving the performance of the student model. In randomized neural networks, due to the simple topology of network architecture and the insignificant relationship between model performance and model size, KD is not able to improve model performance. In this work, we propose a self-distillation pipeline for randomized neural networks: the predictions of the network itself are regarded as the additional target, which are mixed with the weighted original target as a distillation target containing dark knowledge to supervise the training of the model. All the predictions during multi-generation self-distillation process can be integrated by a multi-teacher method. By induction, we have additionally arrived at the methods for infinite self-distillation (ISD) of randomized neural networks. We then provide relevant theoretical analysis about the self-distillation method for randomized neural networks. Furthermore, we demonstrated the effectiveness of the proposed method in practical applications on several benchmark datasets.",
            "score": 0.4924390031367991,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "258686392",
            "title": "On enhancing the robustness of Vision Transformers: Defensive Diffusion",
            "text": "Knowledge distillation is a process of training a smaller, faster, and more efficient model, known as the student model, to mimic the behavior of a larger and more accurate model, known as the teacher model. This technique has gained significant attention in recent years due to its ability to compress complex models, reduce their computational requirements, and accelerate their inference time. One of the earliest works on knowledge distillation was proposed by [13], where they introduced the concept of transferring the knowledge of a deep neural network to a smaller network by minimizing the difference between the outputs of the teacher and student models. In their work, they demonstrated that knowledge distillation could significantly improve the performance of a smaller network on various image classification tasks. Following this work, several studies have explored different aspects of knowledge distillation, such as the choice of the loss function, the impact of the teacher and student model architectures, and the effect of data augmentation techniques. One such work is the study by [14], where they proposed a novel loss function, called the attention transfer loss, which leverages the attention maps of the teacher model to guide the training of the student model. Their results showed that attention transfer loss could significantly improve the performance of the student model on various image classification tasks. Moreover, recent research has extended knowledge distillation to other domains, such as natural language processing, speech recognition, and object detection [15].",
            "score": 0.49223932024058703,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "234292017",
            "title": "Knowledge from the original network: restore a better pruned network with knowledge distillation",
            "text": "s accuracy. However this process below is often difficult to recover the accuracy of excessively pruned networks.\n\nKnowledge distillation, as another compression strategy, aims to transfer dark knowledge in logits outputs [14], feature maps [13,18], and relationship diagrams [26] from a larger pre-trained teacher network to a smaller student network, allowing the student network to mimic the teacher network performance. The strategy of knowledge distillation can better improve some smaller networks' accuracy than directly training them with one-hot labels.\n\nIn this paper, we motivate to solve the accuracy loss problem in pruning, and merge the two pruning method. We propose a new strategy to recover the pruned neural network: we replace the fine-tuning procedure in pruning pipeline to knowledge distillation and transfer the knowledge from the original un-pruned network to the pruned network to increase the accuracy of the pruned network. We pruned an overparameterized network and then used the original network as the teacher, and the pruned network as a student for knowledge distillation. As the teacher network and the student network have the same structure (the student network can be seen as a sub-network of the teacher network), the student network can better fit the representation of the teacher network [5,31]. So the proposed method is more effective than simply mechanically combining network pruning and knowledge distillation.\n\nOur new method combines the advantages of several model compression methods. Compared to the latest knowledge distillation methods [2,33], our method focuses on generating a student model from the original model by pruning. Therefore, we get a generated student network better suits the teacher network than manually selected network in simple knowledge distillation methods. Then compared to the simple pruning methods [7,22,37], the idea of knowledge transfer is used to retraining the pruning network in our proposed method. With the help of effective knowledge distillation methods, we can significantly improve the pruned model performance. To maximize the use of both methods, We carefully designed the method framework and the training pipeline. An end-to-end high efficiency model compression method is proposed in the paper.\n\nContributions:\n\n-We proposed a new pruning pipeline",
            "score": 0.49125253057070084,
            "section_title": "Introduction",
            "char_start_offset": 2045,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1329,
                    "end": 1332,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1589,
                    "end": 1592,
                    "matchedPaperCorpusId": "56356408"
                },
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "210844903"
                },
                {
                    "start": 1878,
                    "end": 1881,
                    "matchedPaperCorpusId": "744803"
                },
                {
                    "start": 1881,
                    "end": 1884,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1884,
                    "end": 1887,
                    "matchedPaperCorpusId": "27494814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466064453125
        },
        {
            "corpus_id": "236984375",
            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
            "text": "Knowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a wi used knowledge transfer technology in the deep learning field. First, a well-trained bust, high-precision teacher network is needed. Its output is softened with temperatu to provide more information entropy, which extracts hidden knowledge behind its ou layer. Then, a relatively small student network is trained to imitate the teacher netwo probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the prob ity is expressed as {0, 1, 0}. After the inference of teacher network and student network, the alg rithm outputs classification results q and q', so that the image is declared as a cat. However, th image also shows some dog traits, which is not shown obviously in q and q'. After softening th teacher network, the dark knowledge appears. The classification result is q'', which provides m dark knowledge. Training the student network with the teacher network makes the student ne work more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL d gence to replace cross-entropy loss (CE) to make the final loss function become: \n\nThe workflow of L1-norm-based model pruning, in which the light-colored structure should be pruned. If one filter in conv i is pruned, its corresponding feature map in layer i will be removed. Then, the filters in conv i + 1 will be adjusted to fit structural changes. \n\nIn terms of the pruning process, the L1-norm based pruning method [5] provides two ideas: \n\n1. \n\nOne-shot pruning followed by retraining: this method is fast but cannot ensure that the accuracy of the pruned model is as stable as the original one.",
            "score": 0.49113319889114115,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.456298828125
        },
        {
            "corpus_id": "211146720",
            "title": "Multilinear Compressive Learning with Prior Knowledge",
            "text": "The term Knowledge Distillation (KD) was coined in [36], in which the authors proposed a neural network training technique that utilizes the prediction from a pre-trained high capacity network (the teacher network) to provide supervisory signals to a smaller network (the student network) along with labeled data. The intuition behind this training technique is that with higher capacity, it is easier for the teacher network to discover better data representation, and the form of knowledge provided via the teacher network's prediction helps guide the student network to better solutions. \n\nThere have been several works investigating variants of this technique. For example, KD that involves multiple student and teacher networks [39] has been shown to be more robust than a single teacher-student pair. In other works [40], [41], [42], in addition to the predicted probabilities, knowledge coming from intermediate layers of the teacher network has also been proven to be useful for the student. While KD has often been considered in the context of model compression, i.e., to train low capacity models with better performances, this paradigm has also been successfully applied to distributed training with student and teacher networks having the same architecture [43]. \n\nIn our work, we propose to use KD as a method to progressively incorporate prior information into CL models. While in a general setting it is unclear how to select the immediate source and target layers to transfer knowledge from the teacher to the student, we will show next that in the context of CL, and especially in the MCL framework, pairing the intermediate teachers and students is easy since the learning system is modularized into different components with different functionalities.",
            "score": 0.49105526516652814,
            "section_title": "D. Knowledge Distillation",
            "char_start_offset": 13360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1770
                }
            ],
            "ref_mentions": [
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "28101867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11920166015625
        },
        {
            "corpus_id": "236772780",
            "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR",
            "text": "While transfer learning focuses on transferring the weights of a big network to another big network, knowledge distillation aims to transfer the representational learning from a large and deep neural network (the teacher model) to a smaller network (the student model). If knowledge distillation is widely successful, neural networks could potentially be applied in many industry applications. \n\nThe key idea of knowledge distillation is to use soft probabilities of the teacher network ( teacher's logits ) to supervise the training of the student network in addition to available class labels [3]. Intuitively, we may think that these soft probabilities reveal more information that the teacher has discovered than just the class labels alone. Similarly, a student produces a softened class probability distribution. The Loss function then is the linear combination of the knowledge distillation loss (compared to teacher's logits), and the usual cross entropy loss (compared to ground truth label) [10]. \n\nIn SimCLR as well as in our project, however, the student network is trained on unlabelled data. So it is relying entirely on teacher's logits. The loss function is just the knowledge distillation loss.",
            "score": 0.4909270825224322,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1211
                }
            ],
            "ref_mentions": [
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1611328125
        },
        {
            "corpus_id": "211259505",
            "title": "Multi-representation knowledge distillation for audio classification",
            "text": "We transfer the aggregated knowledge of branches in the distillation phase by reducing the difference of information between teachers and students. Kullback-Leibler (KL) divergence could measure the difference between the two distributions. In the knowledge distillation process, We use the averaged soft labels, which had aggregated knowledge from all branches as the teacher to guide the training of branches. For each branch, the similarity between its soft labels P i j and the fixed averaged soft labels P j is calculated as follows:\n\nwhere P j i denote the soft labels for j-th sample of a branch network f i , and j donate the j-th category.\n\nIn previous work for knowledge distillation [23,50], it has been found that combining supervision from onehot labels with the supervision of teacher information leads to smoother optimization and a better-performed network. We set the distillation loss in the distillation training phase as follows:\n\nThe training process of the framework is summarized in Algorithm 1. Different from traditional two-stage distillation, we adopted the cyclic distillation strategy, a large cycle including three phases: the single branch training, knowledge aggregation, and knowledge distillation. Three phases correspond to information generation, information aggregation, and information feedback, respectively. With the cyclic training phase, branch networks better capture and utilize complementary information of multiple representations. After the branch networks converge, any branch classifier in the framework could be applied independently for inferring according to the data representation and the resources limits. We can also use the ensemble network of multiple branch networks for better classification performance when the model complexity is not a limitation.",
            "score": 0.49081862598634707,
            "section_title": "Knowledge distillation",
            "char_start_offset": 19809,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 698,
                    "end": 701,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02001953125
        },
        {
            "corpus_id": "231201903",
            "title": "A Novel Multi-Knowledge Distillation Approach",
            "text": "There are a lot of algorithms [1]- [3] for compressing and accelerating deep neural networks. Among them, the knowledge distillation approaches are promising as they can provide extra supervision from the teacher network to the student network. There has been a long line of research on the knowledge distillation approaches. Bucila et al. [4] first proposed to approximate an ensemble of teacher networks with a compact student network. After that, Hinton et al. [5] revisited this idea, and put forward to KD, KD managed to match the softened outputs of the softmax layers between the teacher network and the student network because those of the teacher network can provide more information than raw labels. Further, Romero et al. [6] proposed FitNets, Fit-Nets distilled the FM from the teacher network to improve the performance of the student network. Instead of imitating the FM from the teacher network directly, AT [7] enforced the student network to mimic the teacher network by a manually-designed EFM based on an attention mechanism. Recently, RKD [8] utilized the similarity relationships among several samples' FM as a new knowledge to enhance the generalization ability of the student network. \n\nAlthough the existing knowledge distillation approaches have gained success to some extent, there are still several shortcomings as follows: (1) By transferring the knowledge from FM directly, the teacher network may distill some useless even harmful information to the student network, let alone the high dimensionality of FM may be a burden to achieve an efficient transfer. (2) The existing approaches for transferring EFM are based on manuallydesigned mechanisms, which may be not reasonable, and (3) few approach could extract and distill rich information from the teacher network to the student network by several aspects. \n\nTo address the above issues, this paper proposes a novel knowledge distillation approach, dubbed multi-knowledge distillation (MKD). MKD consists of two stages. In the first stage, it trains the teacher network with an autoencoder as well as the student network with its corresponding autoencoder. The autoencoders are connected with the intermediate layers of the teacher network and the student network to learn EFM automatically.",
            "score": 0.49049914174087506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 33,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "19207026"
                },
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1059,
                    "end": 1062,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "273501602",
            "title": "CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision Large Language Models",
            "text": "Knowledge distillation (Gou et al., 2021) aims to enhance the learning capabilities of models by leveraging the strengths of large pre-trained models without relying on extensive labeled datasets. In knowledge distillation, a teacher model, usually a large and well-trained network, transfers its knowledge to a smaller student model, guiding the student to mimic the teacher's predictions. Knowledge distillation eliminates the need for labeled data by using the intrinsic information within the data itself to guide the learning process. The teacher model generates pseudo-labels or other forms of supervision signals from the data, which the student model then uses to learn. These signals can come from various forms of supervision, such as predicting parts of the data from other parts (Lee et al., 2021), identifying patterns within the data (Ericsson et al., 2022), or utilizing domain-specific transformations and augmentations (Bucci et al., 2021). \n\n3 Methodology",
            "score": 0.4903155101818767,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 973
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 41,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 791,
                    "end": 809,
                    "matchedPaperCorpusId": "220935764"
                },
                {
                    "start": 848,
                    "end": 871,
                    "matchedPaperCorpusId": "239017006"
                },
                {
                    "start": 936,
                    "end": 956,
                    "matchedPaperCorpusId": "220768714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05889892578125
        },
        {
            "corpus_id": "277754927",
            "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
            "text": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.",
            "score": 0.4902311551688844,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "118649278",
            "title": "Variational Information Distillation for Knowledge Transfer",
            "text": "Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding hand-crafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.",
            "score": 0.4898190565549847,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.175537109375
        },
        {
            "corpus_id": "270088865",
            "title": "DE-MKD: Decoupled Multi-Teacher Knowledge Distillation Based on Entropy",
            "text": "The complexity of deep neural network models (DNNs) severely limits their application on devices with limited computing and storage resources. Knowledge distillation (KD) is an attractive model compression technology that can effectively alleviate this problem. Multi-teacher knowledge distillation (MKD) aims to leverage the valuable and diverse knowledge distilled by multiple teacher networks to improve the performance of the student network. Existing approaches typically rely on simple methods such as averaging the prediction logits or using sub-optimal weighting strategies to fuse distilled knowledge from multiple teachers. However, employing these techniques cannot fully reflect the importance of teachers and may even mislead student\u2019s learning. To address this issue, we propose a novel Decoupled Multi-Teacher Knowledge Distillation based on Entropy (DE-MKD). DE-MKD decouples the vanilla knowledge distillation loss and assigns adaptive weights to each teacher to reflect its importance based on the entropy of their predictions. Furthermore, we extend the proposed approach to distill the intermediate features from multiple powerful but cumbersome teachers to improve the performance of the lightweight student network. Extensive experiments on the publicly available CIFAR-100 image classification benchmark dataset with various teacher-student network pairs demonstrated the effectiveness and flexibility of our approach. For instance, the VGG8|ShuffleNetV2 model trained by DE-MKD reached 75.25%|78.86% top-one accuracy when choosing VGG13|WRN40-2 as the teacher, setting new performance records. In addition, surprisingly, the distilled student model outperformed the teacher in both teacher-student network pairs.",
            "score": 0.4895325145047258,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08270263671875
        },
        {
            "corpus_id": "53488567",
            "title": "Structured Pruning for Efficient ConvNets via Incremental Regularization",
            "text": "(4) Knowledge distillation transfers the learned knowledge from a large teacher model (or ensemble of models) to a small student model, which is pioneered by [37], [38] and refined by Hinton et al. [39]. Ever since, various definitions of knowledge such as attention [40] and metric structure [41] have been proposed to transfer the network expressiveness.",
            "score": 0.48948952420207037,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 356
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "19207026"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09912109375
        },
        {
            "corpus_id": "249209818",
            "title": "Parameter-Efficient and Student-Friendly Knowledge Distillation",
            "text": "With the continuous growth of data scale and computing resources, the scale of deep neural networks has rapidly increased, from millions of parameters to billions of parameters [1]. Although these huge models, such as BERT [9], GPT-3 [4], and CLIP [33], have powerful multitasking capabilities, they cannot be deployed on some edge devices with limited computing resources, which further limits the environmentally friendly and efficient deployment of deep learning applications. \n\nKnowledge distillation (KD) [17], as an important method for model compression, has been widely used in various fields [51; 20; 41; 49] of deep learning. This paradigm utilizes a pre-trained teacher network to obtain a student network that is close to the teacher network but with fewer parameters. In the traditional distillation framework [41; 31; 30], the prediction output (soft label) is produced by the fixed teacher, as shown in Figure 1(a) . However, limited by the capacity of the student model, the teacher's knowledge cannot be well transferred to such small models, which makes traditional distillation methods suboptimal [20; 27]. Based on this phenomenon, we propose the first research question: RQ1: How to transfer teachers' knowledge to students more effectively? \n\nIn recent works [29; 20; 45; 35], the teacher network and the student network are jointly trained to make the teacher's knowledge more friendly to the students. The online distillation methods, such as  [50] and other online KD methods, which get better knowledge transfer and need training teachers and students together. (b) PESF-KD updates the parameters of adapter modules of the teacher with ground-truth labels and the feedback from student outputs, while the rest of the parameters of the teacher are all fixed. This method makes knowledge transfer more friendly and effective. \n\nDML [50] and KDCL [13], usually update most or even all the parameters of the teacher by using the real labels and the feedback information (soft labels) of the students, as shown in Figure 1(b).",
            "score": 0.4891967566251681,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "231914655"
                },
                {
                    "start": 223,
                    "end": 226,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04046630859375
        },
        {
            "corpus_id": "258179371",
            "title": "MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning",
            "text": "Knowledge distillation aims to transfer knowledge from a strong teacher to a weaker student network to facilitate supervised learning. Generally, the distillation method can be divided into three types: response-based distillation that matches the softened logits of teachers and students [19], the representation-based distillation that matches the feature maps [24,28,40], and the relation-based distillation that matches the sample relations. [38,47].\n\nWhile originating from the resource-efficient deep learning, knowledge distillation has found wider applications in such areas as incomplete multimodal learning. Here, it is used to transfer the privileged modality information that can only be accessed during the training stage from the teacher to the student [3,29]. Since the input of the teacher and student network is different in incomplete multimodal learning, transferring knowledge by representation-based methods may lead to overfitting [15]. Recent methods focus on transferring the privileged modality information by the relation-based methods [7,22,48]. However, these prior arts usually consider different instances equally and ignore their specificity, which would lead to sub-optimal performance.",
            "score": 0.4889839529799957,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7965,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 450,
                    "end": 453,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "56517364"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "234233268"
                },
                {
                    "start": 953,
                    "end": 957,
                    "matchedPaperCorpusId": "53044539"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "238636331"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "221543802"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "matchedPaperCorpusId": "235658355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "235455170",
            "title": "CILEA-NET: Curriculum-Based Incremental Learning Framework for Remote Sensing Image Classification",
            "text": "After generating the curriculum C s for the new data stream s, we train the student network on the new set of data in the order specified by the curriculum. Simultaneously, we retain a small fraction of samples from the previous streams as D s mem for repetitive replay. We also use the knowledge distillation technique as proposed by [24] to transfer the knowledge gained until the (s \u2212 1) th stream from the teacher network to the student. Algorithm 1 describes the complete training process for streams s > 1. \n\nUnlike the traditional teacher-student approach, which uses a fixed teacher network to transfer information, we propose a unique pseudo-teacher-student approach. For every stream, s > 1, the student network from the previous stream serves as the new teacher, distilling the prior knowledge to the current student network while the network learns novel tasks. \n\nWe achieve knowledge transfer from the teacher to student network employing the method proposed in [24] using \n\nwhere p i is the soft probability obtained by performing distillation over the logit z i by comparing it with the other logits. \n\nT is called the temperature parameter. When T = 1, it acts like a normal softmax where the class with the highest score significantly influences the loss. When T > 1, the classes with comparatively lower scores also influence the loss and result in a more fine-grained representation. The value of T is kept equal to 2 as empirically obtained in [24] for optimal performance. We utilize the incoming data arranged based on the curriculum and the retained data samples from memory to train the network. We train the network incrementally with the cost function as a combination of classification and knowledge distillation loss. We use the classical multiclass cross-entropy loss as the classification loss and the Kullback-Liebler divergence loss as the knowledge distillation loss function acting as the regularizer over distilled information from the teacher network to the student to ensure minimum forgetting. The cross-distilled loss function L T is defined as \n\nThe distillation loss is applied to the old classes' classification layers, while the multiclass cross entropy is employed upon all classification layers. L C is the multiclass cross-entropy loss applied to the samples from both the new stream of data and also the previous data.",
            "score": 0.4889804796442917,
            "section_title": "D. Curriculum-Based Incremental Learning",
            "char_start_offset": 20264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2365
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "237250417",
            "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "text": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
            "score": 0.4881949549822755,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09637451171875
        },
        {
            "corpus_id": "232352554",
            "title": "GridDehazeNet+: An Enhanced Multi-Scale Network With Intra-Task Knowledge Transfer for Single Image Dehazing",
            "text": "One popular application of knowledge distillation [37] is for network compression, where the learned logits from a large network (i.e., teacher network) is transferred to a small network (i.e., student network). Compared to the teacher network, the student network is much easier to deploy, possibly at the cost of a potential performance drop. [38] suggested that the intermediate representations from the teacher network can be leveraged to further improve the training process of the student network. In recent years, knowledge distillation has been proved useful not only for network compression, but also for various computer vision tasks, including object detection [39], semantic segmentation [40], image synthesis [41], style transfer [42], etc. Knowledge distillation found its first application to single image dehazing in [43], where the teacher and student networks share the same architecture but are responsible for image reconstruction and image dehazing tasks, respectively. In contrast, for the Knowledge Distilling Dehazing Network (KDDN) proposed in [44], the architectures of teacher and student networks are tailored to the designated tasks; besides, multiple features, rather than only one intermediate feature, are distilled to improve the effectiveness of knowledge transfer. \n\nDifferent from [43], [44], where knowledge transfer is carried out among heterogeneous tasks, we perform ITKT with teacher and student networks working on the same task (i.e, dehazing) but taking different data as inputs. Intuitively, the synthetic domain knowledge yields useful insights into translated data, where the haze effect does not admit a simple mathematical characterization. Therefore, the characteristics of intermediate features distilled from the teacher network can greatly benefit the learning process of the student network, enabling it to deliver satisfactory dehazing results on realworld hazy images.",
            "score": 0.4881638988896637,
            "section_title": "C. Knowledge Distillation and Transfer",
            "char_start_offset": 11962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "182952755"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "226292143"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "209405263"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "220487081"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "220206807"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "221097144"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "220206807"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "221097144"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10107421875
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "Knowledge distillation. Knowledge distillation (KD) (Hinton et al., 2015) transfers knowledge from a teacher to a smaller student model. Methods improve this by focusing on logits or intermediate features (Sun et al., 2024;Jin et al., 2023;Zhao et al., 2022a;Li et al., 2023;Passalis et al., 2021;Tian et al., 2019;Zagoruyko & Komodakis, 2017a;Heo et al., 2019a;Chen et al., 2021b;Heo et al., 2019b;Kim et al., 2018). Standard methods prioritize fully converged teachers with high performance, yet the performance gap can hinder knowledge transfer (Wang et al., 2022;Gao et al., 2020;Cho & Hariharan, 2019;Yuan et al., 2019). Strategies to address this include using intermediatestage teachers (Cho & Hariharan, 2019;Zhao et al., 2022b), pre-training student-friendly teacher model (Yang et al., 2019a;Park et al., 2021;Dong et al., 2024), introducing intermediate-sized assistant teachers (Mirzadeh et al., 2020;Son et al., 2021) or introducing auxiliary networks (Gao et al., 2021). These methods often rely on specially designed and pre-trained intermediate models. Feature-based methods like DTSKD (Li et al., 2024) and DiffKD (Huang et al., 2023) focus on bridging semantic gaps or denoising features. SCKD (Zhu & Wang, 2021) optimizes transfer using gradient similarity. Recent works refine soft labels (Yuan et al., 2024;Rao et al., 2023) or student's output entropy (Zhu et al., 2024a) to enhance knowledge transfer. In contrast, our GPD constructs a trainable dynamic teacher based on the student model, maintaining an appropriate accuracy gap throughout distillation for effective knowledge transfer. Reparameterization.",
            "score": 0.48811874179789677,
            "section_title": "RELATED WORK",
            "char_start_offset": 6141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 315,
                    "end": 344,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 548,
                    "end": 567,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 694,
                    "end": 717,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 782,
                    "end": 802,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 820,
                    "end": 838,
                    "matchedPaperCorpusId": "249642077"
                },
                {
                    "start": 890,
                    "end": 913,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 913,
                    "end": 930,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "229400079"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "268439162"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1374,
                    "end": 1393,
                    "matchedPaperCorpusId": "258564799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054107666015625
        },
        {
            "corpus_id": "247641700",
            "title": "Group channel pruning and spatial attention distilling for object detection",
            "text": "The purpose of knowledge distillation is to transfer the knowledge learned from the teacher network to the student network to improve the performance of the student network. The research of knowledge distillation focuses on two aspects. One is which object in the network is selected as knowledge. The other is how to measure whether the student network learns the knowledge, which is reflected in how to design the loss function of distillation. Concerning what to be selected as knowledge, current distillation methods can be divided into three categories: 1. Using the final class information output of the teacher network as knowledge as in [43][44][45]. 2. Using the middle feature layer of teachers' network as knowledge as in [46,47]. 3. Using the structural relationship between the layers of teacher network as knowledge as in [24]. In the classification network, [47] proposed to extract the attention from the feature layer and express the attention information in a heat map. Then, the loss function is constructed using the attention of the teacher network and the student network.\n\nSearch to Distill: [48] introduced the knowledge distillation method into NAS, and obtained the following conclu-sions through experiments, the structure of student network determines the upper bound that the distillation effect can reach, and the distillation effect is better when the network structure of students and teachers is similar. Inspired by the above research work, we combine pruning with knowledge distillation. In this paper, the idea of our distillation is inspired by [47], and we improved the method to make it suitable for object detection. Especially, we extract spatial attention from the feature layers of each group and give each group spatial attention with different weights for distilling.",
            "score": 0.48811034477330456,
            "section_title": "Knowledge distillation",
            "char_start_offset": 13631,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "221114780"
                },
                {
                    "start": 649,
                    "end": 653,
                    "matchedPaperCorpusId": "221916163"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "51883340"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "221562334"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015960693359375
        },
        {
            "corpus_id": "275920765",
            "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
            "text": "Knowledge distillation (KD) Hinton et al. [2015] is a technique in machine learning that transfers the learned information from a complex model (often referred to as the teacher) to a simpler model (the student). This method attracted attention for achieving model compression with minimal performance loss, and has been applied across various domains, including image classification Liu et al. [2018], Xu et al. [2020], object detection Chen et al. [2017], and natural language processing Calderon et al. [2023], Gu et al. [2024]. \n\nAmong the various forms of KD, self-distillation (SD), originally termed born again neural network Furlanello et al. [2018] is particularly intriguing. In SD, the teacher and student models share Preprint. Under review. arXiv:2501.16226v3 [stat.ML] 17 May 2025 identical architectures. This means that SD does not attempt the model compression; rather, it retrains the student model using the teacher's output. SD presents a intriguing paradox: despite training an identical model on the same dataset, the student model can outperform the teacher Furlanello et al. [2018], Hahn and Choi [2019], Clark et al. [2019]. \n\nTwo main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data. The second hypothesis attributes the improvement to a denoising effect Das and Sanghavi [2023], Das et al. [2024] where the teacher model reduces the influence of the incorrect noisy labels in the training data, enabling the student model to learn a more reliable representation of the underlying patterns Pareek et al. [2024]. \n\nAlthough these hypotheses offer plausible explanations, the optimal behavior of SD, achieved through hyperparameter optimization and repeated iterations Pareek et al. [2024], remains poorly understood.",
            "score": 0.48773099076943,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 384,
                    "end": 401,
                    "matchedPaperCorpusId": "52290108"
                },
                {
                    "start": 403,
                    "end": 419,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 438,
                    "end": 456,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 633,
                    "end": 657,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1081,
                    "end": 1105,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1705,
                    "end": 1728,
                    "matchedPaperCorpusId": "256416199"
                },
                {
                    "start": 1940,
                    "end": 1960,
                    "matchedPaperCorpusId": "271039191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "272945756",
            "title": "Fast reconstruction of milling temperature field based on CNN-GRU machine learning models",
            "text": "Knowledge distillation is an instructor-student training structure that typically utilizes a student model with a simpler network structure to learn the knowledge provided by an instructor model that has been trained with a more complex network structure; this approach trades a slight performance loss for faster computation and smaller model parameters. Knowledge distillation works by training the student model with both the predictions of the teacher model (soft labeling) and the real data (hard labeling), and calculating the weighted total loss of the student model on both the soft and hard labels, essentially \"migrating\" the knowledge learned by the teacher model to the student model. The structure of the knowledge distillation strategy used in this paper is shown in Figure 6. \n\nThe specific knowledge distillation strategy process is as follows: \n\n(1) The raw data that has been preprocessed is input to both the teacher model and the student model, the teacher model is the CNN-GRU model constructed in the previous section, and the student model is a small model with a single CNN layer and a single GRU layer. ( 2  (5) The distillation loss and the student loss are weighted to obtain the total loss, and the gradient of each parameter is updated in the backpropagation process. \n\nThe following are the calculation formulas involved in the knowledge distillation operation process: \n\nKnowledge distillation soft labeling calculation formula as Equation ( 5): \n\nwhere T is the distillation temperature coefficient, used to control the \"hardness\" of the soft label. When T is larger, the soft label distribution area is uniform, more softened, when T is smaller, the soft label distribution closer to the hard label. \n\nDistillation loss of the loss function LOSSsoft formula is as Equation ( 6): \n\nwhere k is the total number of samples, p i (u i ,T) is the ith output of the teacher model at temperature coefficient T, and p i (z i ,T) is the ith output of the student model at temperature coefficient T. \n\nThe loss function LOSShard for student loss is formulated as Equation ( 7): \n\nwhere y i 7is a vector of hard labels representing the class i output of the unsoftened student model. \n\nThe total loss of knowledge distillation can be expressed as Equation ( 8): \n\n( )",
            "score": 0.4877022381919759,
            "section_title": "Temperature boundary condition estimation model based on knowledge distillation with gated convolutional recurrent networks",
            "char_start_offset": 29942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2282
                },
                {
                    "start": 2285,
                    "end": 2288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.082275390625
        },
        {
            "corpus_id": "235358983",
            "title": "Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model",
            "text": "Knowledge distillation. Knowledge distillation is first introduced in (Bucilu\u01ce et al., 2006) and generalized in (Ba & Caruana, 2014;Hinton et al., 2015), which is a popular network compression scheme to train a compact student network by mimicking the softmax output predicted by a high-capacity teacher or ensemble of models. Besides transferring the knowledge of class probabilities, many variants have been proposed to add extra regulations or alignments between the teacher and the student to improve the performance (Romero et al., 2014;Yim et al., 2017;Kim et al., 2018;Heo et al., 2019). For example, FitNet (Romero et al., 2014) introduces an extra loss term that matches the values of the intermediate hidden layers of the teacher and the student, which allows fast training of deeper student models. (Zagoruyko & Komodakis, 2016) defines the attention of DNNs and uses it as the additional transferred knowledge. \n\nKnowledge distillation with limited data. To mitigate the storage and transmission costs of large training datasets, several studies propose the concept of few-shot KD, which generates pseudo samples with the help of a small number of the original training samples (Kimura et al., 2018;Wang et al., 2020;Li et al., 2020). Another study suggests that instead of the raw data, some surrogates with much smaller sizes (also known as metadata) can be used to distill the knowledge from the teacher. (Lopes et al., 2017) leverages the statistical features of the activations of the teacher to train a compact student without access to the original data. However, releasing this kind of metadata along with the pre-trained teacher is usually not a common scenario. \n\nZero-shot knowledge distillation. To deal with the scenario when training data is not accessible, (Nayak et al., 2019) proposes zero-shot knowledge distillation (ZSKD). The authors model the softmax output space of the teacher with a Dirichlet distribution and samples soft labels as the targets. Randomly generated noise inputs are optimized towards these targets via backpropagation and are used as the transfer set.",
            "score": 0.48759661216062755,
            "section_title": "Related Work",
            "char_start_offset": 6300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 92,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 542,
                    "end": 559,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 576,
                    "end": 593,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1211,
                    "end": 1229,
                    "matchedPaperCorpusId": "214727875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07708740234375
        },
        {
            "corpus_id": "249282309",
            "title": "ORC: Network Group-based Knowledge Distillation using Online Role Change",
            "text": "Deep learning using convolutional neural networks (CNN) is making significant progress in computer vision tasks (e.g., object detection, classification, segmentation). For making the efficient network, many trials have been studied from quantization [36,3,30,19] to pruning [13,23,9,10]. After Hinton's proposal [16] on Knowledge Distillation (KD), KD methods have been proposed in various forms [31,39,34,35,38,4,6] but most of them leverage a pair of a single large teacher and a small student for knowledge transfer. However, there is a clear limitation to improving the performance of the student successfully be-cause the teacher network is egocentric and complacent in spite of its good performance and note that the teacher is independently trained in advance without the consideration of the student's characteristics. Eventually, the teacher's knowledge is transferred from the viewpoint the teacher has learned in advance, not the direction in which the student can learn successfully, and the student network easily overfits in an undesired direction. Another limitation of KD is that when the difference of network sizes between the teacher and student networks is large, a single teacher-based KD does not properly transfer the teacher's knowledge to the student as described in [25,33]. \n\nRecently, the multiple teacher-based KD methods [25,33,43,8] have been proposed to solve the issues mentioned above. Specifically, Teacher Assistant-based KD (TAKD) [25,33] was proposed as a method of teaching students using multiple networks with different network sizes and online distillation methods [43,22,8] simultaneously learned the knowledge from the multiple networks of the similar capacity from the beginning. Most of all, the fundamental problem of the multiple network-based KD is that, as shown in Fig. 1 (a), the networks are trained by using even knowledge from immature networks. As a result, the collaborative knowledge that should be used for learning can be contaminated by the false knowledge.",
            "score": 0.4875849485016974,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 254,
                    "end": 256,
                    "matchedPaperCorpusId": "13253967"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "52048008"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 406,
                    "end": 409,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 409,
                    "end": 412,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 412,
                    "end": 414,
                    "matchedPaperCorpusId": "249209917"
                },
                {
                    "start": 414,
                    "end": 416,
                    "matchedPaperCorpusId": "128358783"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1296,
                    "end": 1299,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1355,
                    "end": 1358,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1358,
                    "end": 1361,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1361,
                    "end": 1363,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1472,
                    "end": 1475,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1611,
                    "end": 1614,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1614,
                    "end": 1616,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0169830322265625
        },
        {
            "corpus_id": "246652277",
            "title": "KENN: Enhancing Deep Neural Networks by Leveraging Knowledge for Time Series Forecasting",
            "text": "In this section, we review some of the relevant work in the literature specifically pertaining to knowledge sharing including multi-modal learning approaches. \n\nKnowledge Distillation based techniques aim to transfer knowledge from the teacher network to a student network. This knowledge is typically transferred in a process where teacher produces soft predictions that the student network tries to emulate. (Hu et al., 2016) utilized iterative knowledge distillation to transfer knowledge. They used a teacher network that was made from first-order logic rules and used it to train the student DNN model. They updated both the teacher as well as the student at each iteration during the learning process. The main attempt was to find a teacher network that can match the rule set in terms of predictions while not diverging significantly from the labels in the data. Similarly, (Xie et al., 2019) used a teacher network that was trained on JFT-300m dataset that contained 300 million images. After which they used a teacher student setting on ImageNet dataset where the teacher network produced soft predictions that student network followed. Knowledge distillation can also be in the form of feature map transfer that provides student network an attention map for intermediary layers. As a result the student network learns which part of the intermediate feature map does the teacher network pay attention too. Such approach is used in the work presented by (Chen et al., 2021). However, mostly distillation method rely on minimizing KL-divergence between the distributions of the prediction made by the teacher and the student to make the two distributions similar. KL-divergence can not be directly applied for forecasting task since the output of the network is not a distribution but instead is an estimate of future time series values. Moreover, Since the task of the student network in this framework is to emulate the predictions made by the teacher, this leads to the strength of the student network being ignored by the system and scenarios where teacher is flawed or unreliable is not catered in distillation setting. \n\nAttention based models have gained considerable popularity recently. (Li et al., 2019) proposes a convolutional self-attention mechanism for transformer networks that used causal convolutions of different kernel sizes.",
            "score": 0.48744413347058896,
            "section_title": "Related work",
            "char_start_offset": 7345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2352
                }
            ],
            "ref_mentions": [
                {
                    "start": 1462,
                    "end": 1481,
                    "matchedPaperCorpusId": "227335337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046478271484375
        },
        {
            "paperId": "dfa0e785ae04e0b5851ded616f0021fc033b5337",
            "corpusId": 264131526,
            "title": "TSOSVNet: Teacher-student collaborative knowledge distillation for Online Signature Verification",
            "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
            "year": 2023,
            "referenceCount": 46,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCVW60793.2023.00082?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCVW60793.2023.00082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276468147",
                    "name": "Chandra Sekhar V"
                },
                {
                    "authorId": "2156593228",
                    "name": "Avinash Gautam"
                },
                {
                    "authorId": "2276464658",
                    "name": "Viswanath P"
                },
                {
                    "authorId": "2276464951",
                    "name": "Sreeja Sr"
                },
                {
                    "authorId": "2276447367",
                    "name": "Rama Krishna Sai G"
                }
            ],
            "abstract": "Online signature verification (OSV) is a standardized personal authentication scheme with wide social acceptance in critical real-time applications include access control, m-commerce, etc. Even though the current advances in Deep learning (DL) technologies catalysed state-of-the-art frameworks for challenging domains like computer vision, speech recognition, etc., the DL-based frameworks are voluminous with huge trainable parameters and are hard to deploy in real-time systems demanding faster inference. To adopt DL into OSV for improved performance, we propose an OSV framework made up of teacher-student collaborative knowledge distillation (TSKD) technique. A heavy Transformer based teacher is trained first and the teacher knowledge is distilled into a very lightweight Convolutional Neural Network (CNN) based student. A well trained teacher network results in an efficient deep representative feature learning by the student and results in a performance improvement. In a thorough set of experiments with three popular and standard datasets, i.e., the MCYT-100, SUSIG, and SVC, TSOSVNet framework, with a CNN based student model requiring only 3266 trainable parameters results in an EER of 12.42% compared to the recent SOTA 13.38% by a model with 206277 parameters in skilled_01 category of MCYT-100 dataset. In comparison to cutting-edge CNN-based OSV models, the proposed TSOSVNet produced a state-of-the-art EER in the most of the test categories with an average of 90% lesser trainable parameters.",
            "corpus_id": "264131526",
            "text": "Online signature verification (OSV) is a standardized personal authentication scheme with wide social acceptance in critical real-time applications include access control, m-commerce, etc. Even though the current advances in Deep learning (DL) technologies catalysed state-of-the-art frameworks for challenging domains like computer vision, speech recognition, etc., the DL-based frameworks are voluminous with huge trainable parameters and are hard to deploy in real-time systems demanding faster inference. To adopt DL into OSV for improved performance, we propose an OSV framework made up of teacher-student collaborative knowledge distillation (TSKD) technique. A heavy Transformer based teacher is trained first and the teacher knowledge is distilled into a very lightweight Convolutional Neural Network (CNN) based student. A well trained teacher network results in an efficient deep representative feature learning by the student and results in a performance improvement. In a thorough set of experiments with three popular and standard datasets, i.e., the MCYT-100, SUSIG, and SVC, TSOSVNet framework, with a CNN based student model requiring only 3266 trainable parameters results in an EER of 12.42% compared to the recent SOTA 13.38% by a model with 206277 parameters in skilled_01 category of MCYT-100 dataset. In comparison to cutting-edge CNN-based OSV models, the proposed TSOSVNet produced a state-of-the-art EER in the most of the test categories with an average of 90% lesser trainable parameters.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01177978515625
        },
        {
            "paperId": "0c6f133b9759da4337c49fce557d33651abf3cbf",
            "corpusId": 273228189,
            "title": "Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325005464",
                    "name": "Wenqi Niu"
                },
                {
                    "authorId": "2239571985",
                    "name": "Yingchao Wang"
                },
                {
                    "authorId": "2325010212",
                    "name": "Guohui Cai"
                },
                {
                    "authorId": "2239436936",
                    "name": "Hanpo Hou"
                }
            ],
            "abstract": "Knowledge Distillation (KD) has emerged as a pivotal technique for neural network compression and performance enhancement. Most KD methods aim to transfer dark knowledge from a cumbersome teacher model to a lightweight student model based on Kullback-Leibler (KL) divergence loss. However, the student performance improvements achieved through KD exhibit diminishing marginal returns, where a stronger teacher model does not necessarily lead to a proportionally stronger student model. To address this issue, we empirically find that the KL-based KD method may implicitly change the inter-class relationships learned by the student model, resulting in a more complex and ambiguous decision boundary, which in turn reduces the model's accuracy and generalization ability. Therefore, this study argues that the student model should learn not only the probability values from the teacher's output but also the relative ranking of classes, and proposes a novel Correlation Matching Knowledge Distillation (CMKD) method that combines the Pearson and Spearman correlation coefficients-based KD loss to achieve more efficient and robust distillation from a stronger teacher model. Moreover, considering that samples vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based loss and Spearman-based loss. CMKD is simple yet practical, and extensive experiments demonstrate that it can consistently achieve state-of-the-art performance on CIRAR-100 and ImageNet, and adapts well to various teacher architectures, sizes, and other KD methods.",
            "corpus_id": "273228189",
            "text": "Knowledge Distillation (KD) has emerged as a pivotal technique for neural network compression and performance enhancement. Most KD methods aim to transfer dark knowledge from a cumbersome teacher model to a lightweight student model based on Kullback-Leibler (KL) divergence loss. However, the student performance improvements achieved through KD exhibit diminishing marginal returns, where a stronger teacher model does not necessarily lead to a proportionally stronger student model. To address this issue, we empirically find that the KL-based KD method may implicitly change the inter-class relationships learned by the student model, resulting in a more complex and ambiguous decision boundary, which in turn reduces the model's accuracy and generalization ability. Therefore, this study argues that the student model should learn not only the probability values from the teacher's output but also the relative ranking of classes, and proposes a novel Correlation Matching Knowledge Distillation (CMKD) method that combines the Pearson and Spearman correlation coefficients-based KD loss to achieve more efficient and robust distillation from a stronger teacher model. Moreover, considering that samples vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based loss and Spearman-based loss. CMKD is simple yet practical, and extensive experiments demonstrate that it can consistently achieve state-of-the-art performance on CIRAR-100 and ImageNet, and adapts well to various teacher architectures, sizes, and other KD methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.484619140625
        },
        {
            "paperId": "904761418683af1097576254505506c67dce2d46",
            "corpusId": 259588980,
            "title": "Empowering In-Network Classification in Programmable Switches by Binary Decision Tree and Knowledge Distillation",
            "venue": "IEEE/ACM Transactions on Networking",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 11,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNET.2023.3287091?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNET.2023.3287091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1823901871",
                    "name": "Guorui Xie"
                },
                {
                    "authorId": "1930238",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2219436067",
                    "name": "Guanglin Duan"
                },
                {
                    "authorId": "2222546554",
                    "name": "Jiaye Lin"
                },
                {
                    "authorId": "2115459638",
                    "name": "Yutao Dong"
                },
                {
                    "authorId": "50262192",
                    "name": "Yong Jiang"
                },
                {
                    "authorId": "2192855736",
                    "name": "Dan Zhao"
                },
                {
                    "authorId": "20277304",
                    "name": "Yuan Yang"
                }
            ],
            "abstract": "Given the high packet processing efficiency of programmable switches (e.g., P4 switches of Tbps), several works are proposed to offload the decision tree (DT) to P4 switches for in-network classification. Although the DT is suitable for the match-action paradigm in P4 switches, the range match rules used in the DT may not be supported across devices of different P4 standards. Additionally, emerging models including neural networks (NNs) and ensemble models, have shown their superior performance in networking tasks. But their sophisticated operations pose new challenges to the deployment of these models in switches. In this paper, we propose Mousikav2 to address these drawbacks successfully. First, we design a new tree model, i.e., the binary decision tree (BDT). Unlike the DT, our BDT consists of classification rules in the form of bits, which is a good fit for the standard ternary match supported by different hardware/software switches. Second, we introduce a teacher-student knowledge distillation architecture in Mousikav2, which enables the general transfer from other sophisticated models to the BDT. Through this transfer, sophisticated models are indirectly deployed in switches to avoid switch constraints. Finally, a lightweight P4 program is developed to perform classification tasks in switches with the BDT after knowledge distillation. Experiments on three networking tasks and three commodity switches show that Mousikav2 not only improves the classification accuracy by 3.27%, but also reduces the switch stage and memory usage by $2.00\\times $ and 28.67%, respectively. Code is available at https://github.com/xgr19/Mousika.",
            "corpus_id": "259588980",
            "text": "Given the high packet processing efficiency of programmable switches (e.g., P4 switches of Tbps), several works are proposed to offload the decision tree (DT) to P4 switches for in-network classification. Although the DT is suitable for the match-action paradigm in P4 switches, the range match rules used in the DT may not be supported across devices of different P4 standards. Additionally, emerging models including neural networks (NNs) and ensemble models, have shown their superior performance in networking tasks. But their sophisticated operations pose new challenges to the deployment of these models in switches. In this paper, we propose Mousikav2 to address these drawbacks successfully. First, we design a new tree model, i.e., the binary decision tree (BDT). Unlike the DT, our BDT consists of classification rules in the form of bits, which is a good fit for the standard ternary match supported by different hardware/software switches. Second, we introduce a teacher-student knowledge distillation architecture in Mousikav2, which enables the general transfer from other sophisticated models to the BDT. Through this transfer, sophisticated models are indirectly deployed in switches to avoid switch constraints. Finally, a lightweight P4 program is developed to perform classification tasks in switches with the BDT after knowledge distillation. Experiments on three networking tasks and three commodity switches show that Mousikav2 not only improves the classification accuracy by 3.27%, but also reduces the switch stage and memory usage by $2.00\\times $ and 28.67%, respectively. Code is available at https://github.com/xgr19/Mousika.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0115966796875
        },
        {
            "paperId": "47a0ea7e549fac9ba2f3e34c046fccc31faa06e7",
            "corpusId": 271587537,
            "title": "Research on Real-time Identification of Traffic States at the Network Level Based on Knowledge Distillation",
            "venue": "Proceedings of the 2024 3rd International Conference on Cryptography, Network Security and Communication Technology",
            "year": 2024,
            "referenceCount": 12,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3673277.3673361?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3673277.3673361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313457144",
                    "name": "Zhenxing Zhao"
                },
                {
                    "authorId": "2260015492",
                    "name": "Wei Zeng"
                },
                {
                    "authorId": "2313616143",
                    "name": "Chenjia Tang"
                }
            ],
            "abstract": "To address the low generalization performance and limited applicability of traffic flow identification models, this paper proposes a deep neural network model to achieve network-level traffic flow state identification. Kernel-based fuzzy C-means algorithm is used to cluster unlabeled data, and the obtained data labels are appropriately softened, allowing the labels to include more \"dark knowledge\" for neural network learning. By introducing the knowledge distillation idea and using the teacher-to-student knowledge transfer mode, the model is subjected to supervised reinforcement training. Compared with the traditional training method, the knowledge distillation training method enables the model to learn the intrinsic features of data. To test the classification performance of the model, experiments were conducted on authoritative public datasets. The experimental results show that the model with knowledge distillation improves accuracy by 10.43% and Macro F1 by 11.47% compared to the normal model, which verifies that the knowledge distillation idea can effectively improve the generalization performance and applicability of the model.",
            "corpus_id": "271587537",
            "text": "To address the low generalization performance and limited applicability of traffic flow identification models, this paper proposes a deep neural network model to achieve network-level traffic flow state identification. Kernel-based fuzzy C-means algorithm is used to cluster unlabeled data, and the obtained data labels are appropriately softened, allowing the labels to include more \"dark knowledge\" for neural network learning. By introducing the knowledge distillation idea and using the teacher-to-student knowledge transfer mode, the model is subjected to supervised reinforcement training. Compared with the traditional training method, the knowledge distillation training method enables the model to learn the intrinsic features of data. To test the classification performance of the model, experiments were conducted on authoritative public datasets. The experimental results show that the model with knowledge distillation improves accuracy by 10.43% and Macro F1 by 11.47% compared to the normal model, which verifies that the knowledge distillation idea can effectively improve the generalization performance and applicability of the model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10357666015625
        },
        {
            "paperId": "d70c6cfb2b1faa8f51dd1ff133027ae8c4ef32d4",
            "corpusId": 271359328,
            "title": "DAFD-net: a domain adaptive feature distillation network for dark object detection based on semi-supervised learning",
            "venue": "International Conference on Image Processing and Artificial Intelligence",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3035117?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3035117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312463345",
                    "name": "Guanzhi Ding"
                },
                {
                    "authorId": "2313232844",
                    "name": "Zhenhao Yang"
                },
                {
                    "authorId": "2312892040",
                    "name": "Xiaobin Guo"
                }
            ],
            "abstract": "Object detection represents a fundamental task within the realm of computer vision. However, achieving object detection in the dark is still a substantial challenge due to the low contrast of images and the lack of large-scale labeled dark image datasets. One possible solution is to transfer the knowledge of the trained model from a normal illumination source domain to a dark target domain by developing a domain adaptation method. Therefore, we propose a domain adaptive feature distillation network (DAFD-Net) to improve the accuracy of dark object detection. Specifically, we guide the feature extraction of source and target domains by adopting the knowledge distillation training strategy of the teacher-student model and introduce domain adversarial loss and feature distillation loss to jointly conduct semi-supervised optimal learning of the network. Experimental results demonstrate that our DAFD-Net achieves superior performance on the semi- supervised dark detection task.",
            "corpus_id": "271359328",
            "text": "Object detection represents a fundamental task within the realm of computer vision. However, achieving object detection in the dark is still a substantial challenge due to the low contrast of images and the lack of large-scale labeled dark image datasets. One possible solution is to transfer the knowledge of the trained model from a normal illumination source domain to a dark target domain by developing a domain adaptation method. Therefore, we propose a domain adaptive feature distillation network (DAFD-Net) to improve the accuracy of dark object detection. Specifically, we guide the feature extraction of source and target domains by adopting the knowledge distillation training strategy of the teacher-student model and introduce domain adversarial loss and feature distillation loss to jointly conduct semi-supervised optimal learning of the network. Experimental results demonstrate that our DAFD-Net achieves superior performance on the semi- supervised dark detection task.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.3505859375
        },
        {
            "paperId": "8a740411aafb8bf88038bd6b44c52f76954f8140",
            "corpusId": 261030812,
            "title": "Unlimited Knowledge Distillation for Action Recognition in the Dark",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 46,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.09327",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.09327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140636945",
                    "name": "Ruibing Jin"
                },
                {
                    "authorId": "2604251",
                    "name": "Guosheng Lin"
                },
                {
                    "authorId": "1390606776",
                    "name": "Min Wu"
                },
                {
                    "authorId": "2143453415",
                    "name": "Jie Lin"
                },
                {
                    "authorId": "2207333293",
                    "name": "Zhengguo Li"
                },
                {
                    "authorId": "2108674591",
                    "name": "Xiaoli Li"
                },
                {
                    "authorId": "48354147",
                    "name": "Zhenghua Chen"
                }
            ],
            "abstract": "Dark videos often lose essential information, which causes the knowledge learned by networks is not enough to accurately recognize actions. Existing knowledge assembling methods require massive GPU memory to distill the knowledge from multiple teacher models into a student model. In action recognition, this drawback becomes serious due to much computation required by video process. Constrained by limited computation source, these approaches are infeasible. To address this issue, we propose an unlimited knowledge distillation (UKD) in this paper. Compared with existing knowledge assembling methods, our UKD can effectively assemble different knowledge without introducing high GPU memory consumption. Thus, the number of teaching models for distillation is unlimited. With our UKD, the network's learned knowledge can be remarkably enriched. Our experiments show that the single stream network distilled with our UKD even surpasses a two-stream network. Extensive experiments are conducted on the ARID dataset.",
            "corpus_id": "261030812",
            "text": "Dark videos often lose essential information, which causes the knowledge learned by networks is not enough to accurately recognize actions. Existing knowledge assembling methods require massive GPU memory to distill the knowledge from multiple teacher models into a student model. In action recognition, this drawback becomes serious due to much computation required by video process. Constrained by limited computation source, these approaches are infeasible. To address this issue, we propose an unlimited knowledge distillation (UKD) in this paper. Compared with existing knowledge assembling methods, our UKD can effectively assemble different knowledge without introducing high GPU memory consumption. Thus, the number of teaching models for distillation is unlimited. With our UKD, the network's learned knowledge can be remarkably enriched. Our experiments show that the single stream network distilled with our UKD even surpasses a two-stream network. Extensive experiments are conducted on the ARID dataset.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.054901123046875
        },
        {
            "paperId": "fa53646903f3fcc0acb9709edabe43eb129488ac",
            "corpusId": 264372297,
            "title": "Improving Knowledge Distillation via Head and Tail Categories",
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "year": 2024,
            "referenceCount": 71,
            "citationCount": 9,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325814?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215812572",
                    "name": "Liuchi Xu"
                },
                {
                    "authorId": "2157467240",
                    "name": "Jin Ren"
                },
                {
                    "authorId": "2151325820",
                    "name": "Zhenhua Huang"
                },
                {
                    "authorId": "2265252477",
                    "name": "Weishi Zheng"
                },
                {
                    "authorId": "2261262093",
                    "name": "Yunwen Chen"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a technique that transfers \u201cdark knowledge\u201d from a deep teacher network (teacher) to a shallow student network (student). Despite significant advances in KD, existing work has not adequately mined two crucial types of knowledge: 1) the knowledge of head categories, which represents the relationship between the target category and its similar categories. Our findings reveal that this highly similar (complex) knowledge is essential for improving student\u2019s performance; and 2) the effectively utilized knowledge of tail categories. Existing studies often treat the non-target categories collectively without sufficiently considering the effectiveness of knowledge from tail categories. To tackle these challenges, we reformulate classical KD (ReKD) into two components: Top- $K$ Inter-class Similar Distillation (TISD) and Non-Top- $K$ Inter-class Discriminability (NTID). Firstly, TISD captures and imparts the knowledge of head categories to the student. Our experimental results have verified that TISD is particularly effective in transferring the knowledge of head categories, even in fine-grained dataset classification. Secondly, we theoretically show that the weighting coefficient of NTID increases with the probability of Top- $K$ , leading to stronger suppression of knowledge transfer for tail categories. This observation explains why difficult samples are more informative than simple ones. To better utilize both types of knowledge, we optimize both TISD and NTID using different weighting coefficients, thereby enhancing the student\u2019s ability to learn this valuable knowledge from both head and tail categories. Furthermore, our extensive experimental results demonstrate that ReKD achieves state-of-the-art performance on various image classification datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K, as well as object detection and instance segmentation using the MS-COCO dataset.",
            "corpus_id": "264372297",
            "text": "Knowledge distillation (KD) is a technique that transfers \u201cdark knowledge\u201d from a deep teacher network (teacher) to a shallow student network (student). Despite significant advances in KD, existing work has not adequately mined two crucial types of knowledge: 1) the knowledge of head categories, which represents the relationship between the target category and its similar categories. Our findings reveal that this highly similar (complex) knowledge is essential for improving student\u2019s performance; and 2) the effectively utilized knowledge of tail categories. Existing studies often treat the non-target categories collectively without sufficiently considering the effectiveness of knowledge from tail categories. To tackle these challenges, we reformulate classical KD (ReKD) into two components: Top- $K$ Inter-class Similar Distillation (TISD) and Non-Top- $K$ Inter-class Discriminability (NTID). Firstly, TISD captures and imparts the knowledge of head categories to the student. Our experimental results have verified that TISD is particularly effective in transferring the knowledge of head categories, even in fine-grained dataset classification. Secondly, we theoretically show that the weighting coefficient of NTID increases with the probability of Top- $K$ , leading to stronger suppression of knowledge transfer for tail categories. This observation explains why difficult samples are more informative than simple ones. To better utilize both types of knowledge, we optimize both TISD and NTID using different weighting coefficients, thereby enhancing the student\u2019s ability to learn this valuable knowledge from both head and tail categories. Furthermore, our extensive experimental results demonstrate that ReKD achieves state-of-the-art performance on various image classification datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K, as well as object detection and instance segmentation using the MS-COCO dataset.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.61083984375
        },
        {
            "paperId": "ebfabf78f1b9efa18ef5019e7d120e52e46d8ce3",
            "corpusId": 274655700,
            "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 3,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.09388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331698741",
                    "name": "Junjie Zhou"
                },
                {
                    "authorId": "2273931950",
                    "name": "Ke Zhu"
                },
                {
                    "authorId": "2274078411",
                    "name": "Jianxin Wu"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, and allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.",
            "corpus_id": "274655700",
            "text": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, and allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.65869140625
        },
        {
            "paperId": "1e2ff4ed630d1821035243928471b37aca01b178",
            "corpusId": 266998295,
            "title": "Learning From Human Educational Wisdom: A Student-Centered Knowledge Distillation Method",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2024,
            "referenceCount": 74,
            "citationCount": 20,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/10400954.pdf",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2024.3354928?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2024.3354928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264231953",
                    "name": "Shunzhi Yang"
                },
                {
                    "authorId": "2265009921",
                    "name": "Jinfeng Yang"
                },
                {
                    "authorId": "2265725768",
                    "name": "MengChu Zhou"
                },
                {
                    "authorId": "2151325820",
                    "name": "Zhenhua Huang"
                },
                {
                    "authorId": "2265252477",
                    "name": "Weishi Zheng"
                },
                {
                    "authorId": "2215798762",
                    "name": "Xiong Yang"
                },
                {
                    "authorId": "2264910280",
                    "name": "Jin Ren"
                }
            ],
            "abstract": "Existing studies on knowledge distillation typically focus on teacher-centered methods, in which the teacher network is trained according to its own standards before transferring the learned knowledge to a student one. However, due to differences in network structure between the teacher and the student, the knowledge learned by the former may not be desired by the latter. Inspired by human educational wisdom, this paper proposes a Student-Centered Distillation (SCD) method that enables the teacher network to adjust its knowledge transfer according to the student network's needs. We implemented SCD based on various human educational wisdom, e.g., the teacher network identified and learned the knowledge desired by the student network on the validation set, and then transferred it to the latter through the training set. To address the problems of current deficiency knowledge, hard sample learning and knowledge forgetting faced by a student network in the learning process, we introduce and improve Proportional-Integral-Derivative (PID) algorithms from automation fields to make them effective in identifying the current knowledge required by the student network. Furthermore, we propose a curriculum learning-based fuzzy strategy and apply it to the proposed PID control algorithm, such that the student network in SCD can actively pay attention to the learning of challenging samples after with certain knowledge. The overall performance of SCD is verified in multiple tasks by comparing it with state-of-the-art ones. Experimental results show that our student-centered distillation method outperforms existing teacher-centered ones.",
            "corpus_id": "266998295",
            "text": "Existing studies on knowledge distillation typically focus on teacher-centered methods, in which the teacher network is trained according to its own standards before transferring the learned knowledge to a student one. However, due to differences in network structure between the teacher and the student, the knowledge learned by the former may not be desired by the latter. Inspired by human educational wisdom, this paper proposes a Student-Centered Distillation (SCD) method that enables the teacher network to adjust its knowledge transfer according to the student network's needs. We implemented SCD based on various human educational wisdom, e.g., the teacher network identified and learned the knowledge desired by the student network on the validation set, and then transferred it to the latter through the training set. To address the problems of current deficiency knowledge, hard sample learning and knowledge forgetting faced by a student network in the learning process, we introduce and improve Proportional-Integral-Derivative (PID) algorithms from automation fields to make them effective in identifying the current knowledge required by the student network. Furthermore, we propose a curriculum learning-based fuzzy strategy and apply it to the proposed PID control algorithm, such that the student network in SCD can actively pay attention to the learning of challenging samples after with certain knowledge. The overall performance of SCD is verified in multiple tasks by comparing it with state-of-the-art ones. Experimental results show that our student-centered distillation method outperforms existing teacher-centered ones.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0615234375
        },
        {
            "paperId": "e204f2e0e75c44c3032a441525255433df82f34c",
            "corpusId": 273227005,
            "title": "Two-Stage Approach for Targeted Knowledge Transfer in Self-Knowledge Distillation",
            "venue": "IEEE/CAA Journal of Automatica Sinica",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2024.124629?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2024.124629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290035516",
                    "name": "Zimo Yin"
                },
                {
                    "authorId": "2142824615",
                    "name": "Jian Pu"
                },
                {
                    "authorId": "2261082542",
                    "name": "Yijie Zhou"
                },
                {
                    "authorId": "2251995827",
                    "name": "Xiangyang Xue"
                }
            ],
            "abstract": "Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network. To optimize computational expenditure and memory utilization, self-knowledge distillation (SKD) extracts dark knowledge from the model itself rather than an external teacher network. However, previous SKD methods performed distillation indiscriminately on full datasets, overlooking the analysis of representative samples. In this work, we present a novel two-stage approach to providing targeted knowledge on specific samples, named two-stage approach self-knowledge distillation (TOAST). We first soften the hard targets using class medoids generated based on logit vectors per class. Then, we iteratively distill the under-trained data with past predictions of half the batch size. The two-stage knowledge is linearly combined, efficiently enhancing model performance. Extensive experiments conducted on five backbone architectures show our method is model-agnostic and achieves the best generalization performance. Besides, TOAST is strongly compatible with existing augmentation-based regularization methods. Our method also obtains a speedup of up to 2.95x compared with a recent state-of-the-art method.",
            "corpus_id": "273227005",
            "text": "Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network. To optimize computational expenditure and memory utilization, self-knowledge distillation (SKD) extracts dark knowledge from the model itself rather than an external teacher network. However, previous SKD methods performed distillation indiscriminately on full datasets, overlooking the analysis of representative samples. In this work, we present a novel two-stage approach to providing targeted knowledge on specific samples, named two-stage approach self-knowledge distillation (TOAST). We first soften the hard targets using class medoids generated based on logit vectors per class. Then, we iteratively distill the under-trained data with past predictions of half the batch size. The two-stage knowledge is linearly combined, efficiently enhancing model performance. Extensive experiments conducted on five backbone architectures show our method is model-agnostic and achieves the best generalization performance. Besides, TOAST is strongly compatible with existing augmentation-based regularization methods. Our method also obtains a speedup of up to 2.95x compared with a recent state-of-the-art method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.650390625
        },
        {
            "paperId": "c02b92e4ffb3edd22280154b535017de8a1938c9",
            "corpusId": 259697005,
            "title": "Unsupervised Deep Learning for Phase Retrieval via Teacher-Student Distillation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "referenceCount": 59,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25306/25078",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v37i2.25306?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v37i2.25306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217653",
                    "name": "Yuhui Quan"
                },
                {
                    "authorId": "2136453688",
                    "name": "Zhile Chen"
                },
                {
                    "authorId": "153063366",
                    "name": "T. Pang"
                },
                {
                    "authorId": "90137165",
                    "name": "Hui Ji"
                }
            ],
            "abstract": "Phase retrieval (PR) is a challenging nonlinear inverse problem in scientific imaging that involves reconstructing the phase of a signal from its intensity measurements. Recently, there has been an increasing interest in deep learning-based PR. Motivated by the challenge of collecting ground-truth (GT) images in many domains, this paper proposes a fully-unsupervised learning approach for PR, which trains an end-to-end deep model via a GT-free teacher-student online distillation framework. Specifically, a teacher model is trained using a self-expressive loss with noise resistance, while a student model is trained with a consistency loss on augmented data to exploit the teacher's dark knowledge. Additionally, we develop an enhanced unfolding network for both the teacher and student models. Extensive experiments show that our proposed approach outperforms existing unsupervised PR methods with higher computational efficiency and performs competitively against supervised methods.",
            "corpus_id": "259697005",
            "text": "Phase retrieval (PR) is a challenging nonlinear inverse problem in scientific imaging that involves reconstructing the phase of a signal from its intensity measurements. Recently, there has been an increasing interest in deep learning-based PR. Motivated by the challenge of collecting ground-truth (GT) images in many domains, this paper proposes a fully-unsupervised learning approach for PR, which trains an end-to-end deep model via a GT-free teacher-student online distillation framework. Specifically, a teacher model is trained using a self-expressive loss with noise resistance, while a student model is trained with a consistency loss on augmented data to exploit the teacher's dark knowledge. Additionally, we develop an enhanced unfolding network for both the teacher and student models. Extensive experiments show that our proposed approach outperforms existing unsupervised PR methods with higher computational efficiency and performs competitively against supervised methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.163330078125
        },
        {
            "paperId": "ea38e0918c8457e646f6ac74ea5e00032b767800",
            "corpusId": 267660554,
            "title": "Rolling bearing fault diagnosis method based on enhanced knowledge distillation and lightweight convolution network",
            "venue": "2023 4th International Conference on Computer Engineering and Intelligent Control (ICCEIC)",
            "year": 2023,
            "referenceCount": 13,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCEIC60201.2023.10426666?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCEIC60201.2023.10426666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269506462",
                    "name": "Jinlong Yang"
                },
                {
                    "authorId": "2284148685",
                    "name": "Li Feng"
                },
                {
                    "authorId": "2284448272",
                    "name": "Yuanming Lu"
                }
            ],
            "abstract": "In order to solve the problems of high time ductility, low calculation accuracy and unfriendly hardware deployment in practical engineering in rolling bearing fault diagnosis methods under strong noise background, a new kind of Improved Knowledge Distillation was proposed. (IKD) lightweight convolutional Neural Network fault diagnosis method. In order to reduce the increase of computing time caused by channel addition in the process of convolution, standard pointwise convolution and channel shuffle are used to design the basic unit of the network. On this basis, the IKD algorithm is used to train the student network (IKDStu) based on the pre-trained teacher network (TeacherNet). Simulation analysis shows that under the background of the given data with strong noise, the student network trained by the proposed method can ensure the accuracy of diagnosis, reduce the size of the model and the number of parameters, and improve the efficiency of diagnosis.",
            "corpus_id": "267660554",
            "text": "In order to solve the problems of high time ductility, low calculation accuracy and unfriendly hardware deployment in practical engineering in rolling bearing fault diagnosis methods under strong noise background, a new kind of Improved Knowledge Distillation was proposed. (IKD) lightweight convolutional Neural Network fault diagnosis method. In order to reduce the increase of computing time caused by channel addition in the process of convolution, standard pointwise convolution and channel shuffle are used to design the basic unit of the network. On this basis, the IKD algorithm is used to train the student network (IKDStu) based on the pre-trained teacher network (TeacherNet). Simulation analysis shows that under the background of the given data with strong noise, the student network trained by the proposed method can ensure the accuracy of diagnosis, reduce the size of the model and the number of parameters, and improve the efficiency of diagnosis.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.03912353515625
        },
        {
            "paperId": "d96da629082933f55eda67c543e93ae143c1c7bf",
            "corpusId": 238253065,
            "title": "Student Helping Teacher: Teacher Evolution via Self-Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2021,
            "referenceCount": 52,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.00329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146248526",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2144439048",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "38609959",
                    "name": "Lingfeng Yang"
                },
                {
                    "authorId": "2146236917",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "2069544640",
                    "name": "Zhigeng Pan"
                }
            ],
            "abstract": "Knowledge distillation usually transfers the knowledge from a pre-trained cumbersome teacher network to a compact student network, which follows the classical teacher-teaching-student paradigm. Based on this paradigm, previous methods mostly focus on how to efficiently train a better student network for deployment. Different from the existing practices, in this paper, we propose a novel student-helping-teacher formula, Teacher Evolution via Self-Knowledge Distillation (TESKD), where the target teacher (for deployment) is learned with the help of multiple hierarchical students by sharing the structural backbone. The diverse feedback from multiple students allows the teacher to improve itself through the shared feature representations. The effectiveness of our proposed framework is demonstrated by extensive experiments with various network settings on two standard benchmarks including CIFAR-100 and ImageNet. Notably, when trained together with our proposed method, ResNet-18 achieves 79.15% and 71.14% accuracy on CIFAR-100 and ImageNet, outperforming the baseline results by 4.74% and 1.43%, respectively. The code is available at: https://github.com/zhengli427/TESKD.",
            "corpus_id": "238253065",
            "text": "Knowledge distillation usually transfers the knowledge from a pre-trained cumbersome teacher network to a compact student network, which follows the classical teacher-teaching-student paradigm. Based on this paradigm, previous methods mostly focus on how to efficiently train a better student network for deployment. Different from the existing practices, in this paper, we propose a novel student-helping-teacher formula, Teacher Evolution via Self-Knowledge Distillation (TESKD), where the target teacher (for deployment) is learned with the help of multiple hierarchical students by sharing the structural backbone. The diverse feedback from multiple students allows the teacher to improve itself through the shared feature representations. The effectiveness of our proposed framework is demonstrated by extensive experiments with various network settings on two standard benchmarks including CIFAR-100 and ImageNet. Notably, when trained together with our proposed method, ResNet-18 achieves 79.15% and 71.14% accuracy on CIFAR-100 and ImageNet, outperforming the baseline results by 4.74% and 1.43%, respectively. The code is available at: https://github.com/zhengli427/TESKD.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.08526611328125
        },
        {
            "paperId": "bb9002506248171cba69a1922ebe653b0a4f23ba",
            "corpusId": 268922589,
            "title": "Decoupled Knowledge Distillation via Spatial Feature Blurring for Hyperspectral Image Classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/4609443/4609444/10487889.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSTARS.2024.3383854?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSTARS.2024.3383854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260768363",
                    "name": "Wen Xie"
                },
                {
                    "authorId": "2295081394",
                    "name": "Zhezhe Zhang"
                },
                {
                    "authorId": "2143819911",
                    "name": "Licheng Jiao"
                },
                {
                    "authorId": "2238112199",
                    "name": "Jin Wang"
                }
            ],
            "abstract": "It is well known that distillation learning has the ability to enhance the performance of a light (student) model by transferring knowledge from a heavy (teacher) model, without incurring additional computational and storage costs. This article proposes an improved decoupled knowledge distillation (DKD) strategy for hyperspectral image (HSI) classification. A spatial feature blurring (SFB) module is designed to improve the classification performance of the student network when using DKD strategy. The SFB module utilizes randomly initialized 2-D standard normal distribution tensors to blur the spatial features of HSI, which increases the complexity of the data. This aligns with the characteristics of DKD, which transfers more useful knowledge under the condition of sample complexity. To effectively transfer knowledge, this article proposes a robust teacher network named the dual-branch spatial transformer-spectral transformer (DBSTST) network. This network describes the spatial and spectral long-range dependencies of HSI, addressing the limitations of convolutional neural networks in capturing only local features due to their fixed receptive fields. More specifically, the DBSTST network adopts spatial transformer-spectral transformer, which is composed of a parallel spatial-spectral multihead self-attention (PS2MHSA) module, aiming to describe pixel-level spatial long-range dependencies and spectral correlations in HSI. Simultaneously, the introduction of spatial-spectral positional embedding into PS2MHSA enhances positional awareness. We demonstrated the effectiveness of our proposed method on four publicly available HSI datasets. The student network achieves classification performance improvement and surpasses some other networks. Moreover, when compared with state-of-the-art classification methods, the DBSTST network also exhibits significant improvements in classification performance.",
            "corpus_id": "268922589",
            "text": "It is well known that distillation learning has the ability to enhance the performance of a light (student) model by transferring knowledge from a heavy (teacher) model, without incurring additional computational and storage costs. This article proposes an improved decoupled knowledge distillation (DKD) strategy for hyperspectral image (HSI) classification. A spatial feature blurring (SFB) module is designed to improve the classification performance of the student network when using DKD strategy. The SFB module utilizes randomly initialized 2-D standard normal distribution tensors to blur the spatial features of HSI, which increases the complexity of the data. This aligns with the characteristics of DKD, which transfers more useful knowledge under the condition of sample complexity. To effectively transfer knowledge, this article proposes a robust teacher network named the dual-branch spatial transformer-spectral transformer (DBSTST) network. This network describes the spatial and spectral long-range dependencies of HSI, addressing the limitations of convolutional neural networks in capturing only local features due to their fixed receptive fields. More specifically, the DBSTST network adopts spatial transformer-spectral transformer, which is composed of a parallel spatial-spectral multihead self-attention (PS2MHSA) module, aiming to describe pixel-level spatial long-range dependencies and spectral correlations in HSI. Simultaneously, the introduction of spatial-spectral positional embedding into PS2MHSA enhances positional awareness. We demonstrated the effectiveness of our proposed method on four publicly available HSI datasets. The student network achieves classification performance improvement and surpasses some other networks. Moreover, when compared with state-of-the-art classification methods, the DBSTST network also exhibits significant improvements in classification performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.082275390625
        },
        {
            "paperId": "09f558e5148cc011e8d538634008ab3311eed4dc",
            "corpusId": 260202287,
            "title": "LHAR: Lightweight Human Activity Recognition on Knowledge Distillation",
            "venue": "IEEE journal of biomedical and health informatics",
            "year": 2023,
            "referenceCount": 34,
            "citationCount": 10,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JBHI.2023.3298932?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JBHI.2023.3298932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "32913412",
                    "name": "Shizhuo Deng"
                },
                {
                    "authorId": "49251984",
                    "name": "Jiaqi Chen"
                },
                {
                    "authorId": "2064884434",
                    "name": "Da Teng"
                },
                {
                    "authorId": "2154928625",
                    "name": "Chuangui Yang"
                },
                {
                    "authorId": null,
                    "name": "Dongyue Chen"
                },
                {
                    "authorId": "2056066947",
                    "name": "Tongtong Jia"
                },
                {
                    "authorId": "71421876",
                    "name": "Hao Wang"
                }
            ],
            "abstract": "Sensor-based Human Activity Recognition (HAR) is widely used in daily life and is the basic-level bridge to virtual healthcare in the metaverse. The current challenge is the low recognition accuracy for personalized users on smart wearable devices. The limited resource cannot support large deep learning models updated locally. Besides, integrating and transmitting sensor data to the cloud would reduce the efficiency. Considering the tradeoff between performance and complexity, we propose a Lightweight Human Activity Recognition (LHAR) framework. In LHAR, we combine the cross-people HAR task with the lightweight model task. LHAR framework is designed on the teacher-student architecture and the student network consists of multiple depthwise separable convolution layers to achieve fewer parameters. The dark knowledge distilled from the complex teacher model enhances the generalization ability of LHAR. To achieve effective knowledge distillation, we propose two optimization methods. Firstly, we train the teacher model by ensemble learning to promote teacher performance. Secondly, a multi-channel data augmentation method is proposed for the diversity of the dataset, which is a plug-in operation for the ensemble teacher model. In the experiments, we compare LHAR with state-of-art models in comparison evaluation, ablation study and the hyperparameter analysis, which proves the better performance of LHAR in efficiency and effectiveness.",
            "corpus_id": "260202287",
            "text": "Sensor-based Human Activity Recognition (HAR) is widely used in daily life and is the basic-level bridge to virtual healthcare in the metaverse. The current challenge is the low recognition accuracy for personalized users on smart wearable devices. The limited resource cannot support large deep learning models updated locally. Besides, integrating and transmitting sensor data to the cloud would reduce the efficiency. Considering the tradeoff between performance and complexity, we propose a Lightweight Human Activity Recognition (LHAR) framework. In LHAR, we combine the cross-people HAR task with the lightweight model task. LHAR framework is designed on the teacher-student architecture and the student network consists of multiple depthwise separable convolution layers to achieve fewer parameters. The dark knowledge distilled from the complex teacher model enhances the generalization ability of LHAR. To achieve effective knowledge distillation, we propose two optimization methods. Firstly, we train the teacher model by ensemble learning to promote teacher performance. Secondly, a multi-channel data augmentation method is proposed for the diversity of the dataset, which is a plug-in operation for the ensemble teacher model. In the experiments, we compare LHAR with state-of-art models in comparison evaluation, ablation study and the hyperparameter analysis, which proves the better performance of LHAR in efficiency and effectiveness.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09588623046875
        },
        {
            "paperId": "6bd21397caeb78502f4563a0ac3210b41e2dc1ba",
            "corpusId": 275515396,
            "title": "Balance Divergence for Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.07804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330592318",
                    "name": "Yafei Qi"
                },
                {
                    "authorId": "2327944220",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2330272227",
                    "name": "Zhaoning Zhang"
                },
                {
                    "authorId": "2340159125",
                    "name": "Yaping Liu"
                },
                {
                    "authorId": "2340163684",
                    "name": "Yongmin Zhang"
                }
            ],
            "abstract": "Knowledge distillation has been widely adopted in computer vision task processing, since it can effectively enhance the performance of lightweight student networks by leveraging the knowledge transferred from cumbersome teacher networks. Most existing knowledge distillation methods utilize Kullback-Leibler divergence to mimic the logit output probabilities between the teacher network and the student network. Nonetheless, these methods may neglect the negative parts of the teacher's ''dark knowledge'' because the divergence calculations may ignore the effect of the minute probabilities from the teacher's logit output. This deficiency may lead to suboptimal performance in logit mimicry during the distillation process and result in an imbalance of information acquired by the student network. In this paper, we investigate the impact of this imbalance and propose a novel method, named Balance Divergence Distillation. By introducing a compensatory operation using reverse Kullback-Leibler divergence, our method can improve the modeling of the extremely small values in the negative from the teacher and preserve the learning capacity for the positive. Furthermore, we test the impact of different temperature coefficients adjustments, which may conducted to further balance for knowledge transferring. We evaluate the proposed method on several computer vision tasks, including image classification and semantic segmentation. The evaluation results show that our method achieves an accuracy improvement of 1%~3% for lightweight students on both CIFAR-100 and ImageNet dataset, and a 4.55% improvement in mIoU for PSP-ResNet18 on the Cityscapes dataset. The experiments show that our method is a simple yet highly effective solution that can be smoothly applied to different knowledge distillation methods.",
            "corpus_id": "275515396",
            "text": "Knowledge distillation has been widely adopted in computer vision task processing, since it can effectively enhance the performance of lightweight student networks by leveraging the knowledge transferred from cumbersome teacher networks. Most existing knowledge distillation methods utilize Kullback-Leibler divergence to mimic the logit output probabilities between the teacher network and the student network. Nonetheless, these methods may neglect the negative parts of the teacher's ''dark knowledge'' because the divergence calculations may ignore the effect of the minute probabilities from the teacher's logit output. This deficiency may lead to suboptimal performance in logit mimicry during the distillation process and result in an imbalance of information acquired by the student network. In this paper, we investigate the impact of this imbalance and propose a novel method, named Balance Divergence Distillation. By introducing a compensatory operation using reverse Kullback-Leibler divergence, our method can improve the modeling of the extremely small values in the negative from the teacher and preserve the learning capacity for the positive. Furthermore, we test the impact of different temperature coefficients adjustments, which may conducted to further balance for knowledge transferring. We evaluate the proposed method on several computer vision tasks, including image classification and semantic segmentation. The evaluation results show that our method achieves an accuracy improvement of 1%~3% for lightweight students on both CIFAR-100 and ImageNet dataset, and a 4.55% improvement in mIoU for PSP-ResNet18 on the Cityscapes dataset. The experiments show that our method is a simple yet highly effective solution that can be smoothly applied to different knowledge distillation methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.45849609375
        },
        {
            "paperId": "634f6a6a2d196edfd5f939f73cf30283b9dcb5e2",
            "corpusId": 266232183,
            "title": "Partial Feedback Control for Knowledge Distillation",
            "venue": "ICCVIT",
            "year": 2023,
            "referenceCount": 22,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3627341.3630386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3627341.3630386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352991408",
                    "name": "Shiqi Chen"
                },
                {
                    "authorId": "2189925632",
                    "name": "Wenxiao Tang"
                },
                {
                    "authorId": "2274286578",
                    "name": "Qing Kang"
                },
                {
                    "authorId": "2274310442",
                    "name": "Zenghong Chen"
                },
                {
                    "authorId": "2275000419",
                    "name": "Zheng Liu"
                }
            ],
            "abstract": "Knowledge distillation transfers dark knowledge from cumbersome teachers to lightweight students. Recently, some mainstream knowledge distillation methods impose strict constraints on students to expect similar output to the teacher network. Nevertheless, these approaches severely increase the burden of the student networks learning and constraint them to fully receive the teacher\u2019s knowledge without self-thinking, which significantly restricts their ability to learn independently. In this paper, we propose a simple yet effective Partial Feedback Control for Knowledge Distillation (PFC-KD), which performs partial supervision on the student network at the penultimate layer. Specifically, we utilize the deviation of feature maps of the student network from that of the teacher network as a feedback signal, and select features with large variances as beneficial knowledge that students need to learn in that iteration. Extensive experiments conducted on image classification task and object detection task with a significant results improvement, which demonstrates the effectiveness of our proposed PFC-KD. Code is available at https://github.com/SCUT-BIP-Lab/PFC-KD.",
            "corpus_id": "266232183",
            "text": "Knowledge distillation transfers dark knowledge from cumbersome teachers to lightweight students. Recently, some mainstream knowledge distillation methods impose strict constraints on students to expect similar output to the teacher network. Nevertheless, these approaches severely increase the burden of the student networks learning and constraint them to fully receive the teacher\u2019s knowledge without self-thinking, which significantly restricts their ability to learn independently. In this paper, we propose a simple yet effective Partial Feedback Control for Knowledge Distillation (PFC-KD), which performs partial supervision on the student network at the penultimate layer. Specifically, we utilize the deviation of feature maps of the student network from that of the teacher network as a feedback signal, and select features with large variances as beneficial knowledge that students need to learn in that iteration. Extensive experiments conducted on image classification task and object detection task with a significant results improvement, which demonstrates the effectiveness of our proposed PFC-KD. Code is available at https://github.com/SCUT-BIP-Lab/PFC-KD.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.39794921875
        }
    ],
    "quotes": {
        "cost": 0.224334,
        "quotes": [
            {
                "idx": 0,
                "key": "[159041346 | Nayak et al. | 2019 | Citations: 245]",
                "snippets": "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 626,
                        "end": 1128,
                        "sentence_offsets": [
                            {
                                "start": 583,
                                "end": 819
                            },
                            {
                                "start": 820,
                                "end": 1021
                            },
                            {
                                "start": 1022,
                                "end": 1128
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[198179767 | Zhao et al. | 2019 | Citations: 59]",
                "snippets": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process...Hinton et al. (Hinton et al., 2015) extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "B. Formulation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 636,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 59
                            },
                            {
                                "start": 60,
                                "end": 214
                            },
                            {
                                "start": 215,
                                "end": 308
                            },
                            {
                                "start": 309,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 516
                            },
                            {
                                "start": 517,
                                "end": 637
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process"
                    },
                    {
                        "section_title": "B. Formulation",
                        "pdf_hash": "",
                        "start": 1311,
                        "end": 1637,
                        "sentence_offsets": [
                            {
                                "start": 1263,
                                "end": 1454
                            },
                            {
                                "start": 1455,
                                "end": 1529
                            },
                            {
                                "start": 1530,
                                "end": 1636
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Hinton et al. (Hinton et al., 2015) extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[203593636 | Xie et al. | 2019 | Citations: 12]",
                "snippets": "Dark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Online Distillation",
                        "pdf_hash": "",
                        "start": 1266,
                        "end": 2100,
                        "sentence_offsets": [
                            {
                                "start": 1266,
                                "end": 1281
                            },
                            {
                                "start": 1282,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1506
                            },
                            {
                                "start": 1509,
                                "end": 1679
                            },
                            {
                                "start": 1682,
                                "end": 1759
                            },
                            {
                                "start": 1762,
                                "end": 1858
                            },
                            {
                                "start": 1861,
                                "end": 1938
                            },
                            {
                                "start": 1939,
                                "end": 2100
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Dark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[203642142 | Dong et al. | 2019 | Citations: 41]",
                "snippets": "To explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1152,
                        "end": 1726,
                        "sentence_offsets": [
                            {
                                "start": 1152,
                                "end": 1393
                            },
                            {
                                "start": 1394,
                                "end": 1495
                            },
                            {
                                "start": 1496,
                                "end": 1726
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[212855595 | Zhang et al. | 2020 | Citations: 35]",
                "snippets": "In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model...The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017)).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 339,
                        "end": 815,
                        "sentence_offsets": [
                            {
                                "start": 339,
                                "end": 816
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model"
                    },
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1361,
                        "end": 1783,
                        "sentence_offsets": [
                            {
                                "start": 1361,
                                "end": 1642
                            },
                            {
                                "start": 1643,
                                "end": 1782
                            }
                        ],
                        "ref_mentions": [
                            "29308926"
                        ],
                        "quote": "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[219636179 | Xu et al. | 2020 | Citations: 285]",
                "snippets": "Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 220,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[224801420 | Deng et al. | 2020 | Citations: 0]",
                "snippets": "The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Logit-based Distillation Approaches",
                        "pdf_hash": "",
                        "start": 212,
                        "end": 354,
                        "sentence_offsets": [
                            {
                                "start": 212,
                                "end": 354
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[231648215 | Feng et al. | 2021 | Citations: 10]",
                "snippets": "For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1585,
                        "end": 2053,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[231925118 | Park et al. | 2021 | Citations: 101]",
                "snippets": "Since Hinton et al. (Hinton et al., 2015) introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "What to distill",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 317,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 317
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Since Hinton et al. (Hinton et al., 2015) introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[232269823 | Zhao et al. | 2021 | Citations: 3]",
                "snippets": "Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation.",
                        "pdf_hash": "",
                        "start": 641,
                        "end": 992,
                        "sentence_offsets": [
                            {
                                "start": 641,
                                "end": 817
                            },
                            {
                                "start": 818,
                                "end": 992
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[233210081 | Zhao et al. | 2021 | Citations: 20]",
                "snippets": "It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data-driven knowledge distillation",
                        "pdf_hash": "",
                        "start": 368,
                        "end": 685,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[233407431 | Ge et al. | 2021 | Citations: 26]",
                "snippets": "Knowledge distillation (Hinton et al., 2015) can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Revisit of Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 172,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 172
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (Hinton et al., 2015) can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[235489777 | Zhao et al. | 2021 | Citations: 24]",
                "snippets": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[236984375 | Wang et al. | 2021 | Citations: 3]",
                "snippets": "The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 510,
                        "end": 1159,
                        "sentence_offsets": [
                            {
                                "start": 510,
                                "end": 550
                            },
                            {
                                "start": 551,
                                "end": 630
                            },
                            {
                                "start": 631,
                                "end": 785
                            },
                            {
                                "start": 786,
                                "end": 875
                            },
                            {
                                "start": 876,
                                "end": 940
                            },
                            {
                                "start": 941,
                                "end": 1010
                            },
                            {
                                "start": 1011,
                                "end": 1159
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[237091534 | Ren et al. | 2021 | Citations: 39]",
                "snippets": "In this way, the student network attempts to learn dark knowledge (Hinton et al., 2015) that contains the similarities between different classes, which can not be provided by the ground truth labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 370,
                        "end": 552,
                        "sentence_offsets": [
                            {
                                "start": 370,
                                "end": 552
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "In this way, the student network attempts to learn dark knowledge (Hinton et al., 2015) that contains the similarities between different classes, which can not be provided by the ground truth labels."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[238223320 | Ghosh | 2021 | Citations: 3]",
                "snippets": "KD uses the \"dark knowledge\" (softened logit output of the bottom output layer of teacher network) that is transferred to student network. This dark knowledge is more than interlabel correlations and one-hot encoding of labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Distilling Neural Networks",
                        "pdf_hash": "",
                        "start": 637,
                        "end": 864,
                        "sentence_offsets": [
                            {
                                "start": 637,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 864
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "KD uses the \"dark knowledge\" (softened logit output of the bottom output layer of teacher network) that is transferred to student network. This dark knowledge is more than interlabel correlations and one-hot encoding of labels."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[247521335 | Ye et al. | 2022 | Citations: 21]",
                "snippets": "In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task.\n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task.\n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[248683566 | Xu et al. | 2022 | Citations: 44]",
                "snippets": "Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020). This concept was first proposed in (Bucila et al., 2006), but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[167217261 | Tan et al. | 2019 | Citations: 18189]": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                    "[222310537 | Bashir et al. | 2020 | Citations: 43]": "We present an information-theoretic framework for understanding overfitting and underfitting in machine learning and prove the formal undecidability of determining whether an arbitrary classification algorithm will overfit a dataset. Measuring algorithm capacity via the information transferred from datasets to models, we consider mismatches between algorithm capacities and datasets to provide a signature for when a model can overfit or underfit a dataset. We present results upper-bounding algorithm capacity, establish its relationship to quantities in the algorithmic search framework for machine learning, and relate our work to recent information-theoretic approaches to generalization.",
                    "[8451212 | Kim et al. | 2016 | Citations: 1123]": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 519,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 270
                            },
                            {
                                "start": 271,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 519
                            }
                        ],
                        "ref_mentions": [
                            "167217261",
                            "32588614",
                            "222310537",
                            "206596723",
                            "8451212",
                            "219559263",
                            "11253972"
                        ],
                        "quote": "Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020). This concept was first proposed in (Bucila et al., 2006), but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[249017724 | Bai et al. | 2022 | Citations: 0]",
                "snippets": "Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102351826 | Yu et al. | 2019 | Citations: 109]": "Metric learning networks are used to compute image embeddings, which are widely used in many applications such as image retrieval and face recognition. In this paper, we propose to use network distillation to efficiently compute image embeddings with small networks. Network distillation has been successfully applied to improve image classification, but has hardly been explored for metric learning. To do so, we propose two new loss functions that model the communication of a deep teacher network to a small student network. We evaluate our system in several datasets, including CUB-200-2011, Cars-196, Stanford Online Products and show that embeddings computed using small student networks perform significantly better than those computed using standard networks of similar size. Results on a very compact network (MobileNet-0.25), which can be used on mobile devices, show that the proposed method can greatly improve Recall@1 results from 27.5\\% to 44.6\\%. Furthermore, we investigate various aspects of distillation for embeddings, including hint and attention layers, semi-supervised learning and cross quality distillation. (Code is available at https://github.com/yulu0724/EmbeddingDistillation).",
                    "[102483463 | Peng et al. | 2019 | Citations: 513]": "Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 577,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 157
                            },
                            {
                                "start": 158,
                                "end": 312
                            },
                            {
                                "start": 313,
                                "end": 456
                            },
                            {
                                "start": 457,
                                "end": 577
                            }
                        ],
                        "ref_mentions": [
                            "102351826",
                            "102351826",
                            "102483463"
                        ],
                        "quote": "Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[251066725 | Liang et al. | 2022 | Citations: 19]",
                "snippets": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231925118 | Park et al. | 2021 | Citations: 101]": "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.",
                    "[203642130 | Cho et al. | 2019 | Citations: 619]": "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models.",
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.",
                    "[214727822 | Yun et al. | 2020 | Citations: 281]": "Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks.",
                    "[233714221 | Kim et al. | 2020 | Citations: 182]": "The generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model\u2019s own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.",
                    "[235390933 | Stanton et al. | 2021 | Citations: 222]": "Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher -- and that more closely matching the teacher paradoxically does not always lead to better student generalization.",
                    "[244680427 | Zhu et al. | 2021 | Citations: 83]": "Knowledge distillation (KD) transfers the dark knowledge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacher\u2019s knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the capacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Customized Knowledge Distillation (SCKD), examines the capacity mismatch between teacher and student from the perspective of gradient similarity. We formulate the knowledge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We validate our methods on multiple datasets with various teacher-student configurations on image classification, object detection, and semantic segmentation.",
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
                    "[70335318 | Xu et al. | 2019 | Citations: 175]": "Knowledge distillation is an effective technique that has been widely used for transferring knowledge from a network to another network. Despite its effective improvement of network performance, the dependence of accompanying assistive models complicates the training process of single network in the need of large memory and time cost. In this paper, we design a more elegant self-distillation mechanism to transfer knowledge between different distorted versions of same training data without the reliance on accompanying models. Specifically, the potential capacity of single network is excavated by learning consistent global feature distributions and posterior distributions (class probabilities) across these distorted versions of data. Extensive experiments on multiple datasets (i.e., CIFAR-10/100 and ImageNet) demonstrate that the proposed method can effectively improve the generalization performance of various network architectures (such as AlexNet, ResNet, Wide ResNet, and DenseNet), outperform existing distillation methods with little extra training efforts."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 678,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 156
                            },
                            {
                                "start": 157,
                                "end": 323
                            },
                            {
                                "start": 324,
                                "end": 678
                            }
                        ],
                        "ref_mentions": [
                            "203642130",
                            "212908749",
                            "244680427",
                            "231925118",
                            "4110009",
                            "233714221",
                            "70335318",
                            "214727822",
                            "235390933"
                        ],
                        "quote": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[252668749 | Biehler et al. | 2022 | Citations: 2]",
                "snippets": "Dark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multiple flavors of knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 590,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 243
                            },
                            {
                                "start": 244,
                                "end": 590
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Dark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[254044469 | Zhang et al. | 2022 | Citations: 2]",
                "snippets": "The concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 405,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 59
                            },
                            {
                                "start": 60,
                                "end": 202
                            },
                            {
                                "start": 203,
                                "end": 405
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[256900863 | Zhang et al. | 2023 | Citations: 1]",
                "snippets": "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 617,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 209
                            },
                            {
                                "start": 210,
                                "end": 617
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[257504799 | Wang et al. | 2023 | Citations: 1]",
                "snippets": "The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in (Bucila et al., 2006). However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conventional Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 449,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 131
                            },
                            {
                                "start": 132,
                                "end": 449
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in (Bucila et al., 2006). However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[260447668 | Cui et al. | 2022 | Citations: 3]",
                "snippets": "Compared to direct learning with labels, the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Key Idea and Training Framework",
                        "pdf_hash": "",
                        "start": 153,
                        "end": 1040,
                        "sentence_offsets": [
                            {
                                "start": 111,
                                "end": 187
                            },
                            {
                                "start": 190,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 462
                            },
                            {
                                "start": 463,
                                "end": 524
                            },
                            {
                                "start": 525,
                                "end": 643
                            },
                            {
                                "start": 644,
                                "end": 897
                            },
                            {
                                "start": 898,
                                "end": 1079
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Compared to direct learning with labels, the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[260704230 | Hu et al. | 2023 | Citations: 19]",
                "snippets": "Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Theoretical Understandings of Regression-Based Knowledge Learning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 216,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 216
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[263789814 | Ding et al. | 2020 | Citations: 4]",
                "snippets": "Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 24,
                        "end": 226,
                        "sentence_offsets": [
                            {
                                "start": 24,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 226
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[264372297 | Xu et al. | 2024 | Citations: 9]",
                "snippets": "Knowledge distillation (KD) is a technique that transfers \"dark knowledge\" from a deep teacher network (teacher) to a shallow student network (student).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge distillation (KD) is a technique that transfers \"dark knowledge\" from a deep teacher network (teacher) to a shallow student network (student).",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[264516404 | Yang et al. | 2023 | Citations: 4]",
                "snippets": "In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 199,
                        "end": 317,
                        "sentence_offsets": [
                            {
                                "start": 199,
                                "end": 317
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[264555654 | Jung et al. | 2023 | Citations: 5]",
                "snippets": "In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 171,
                        "end": 456,
                        "sentence_offsets": [
                            {
                                "start": 171,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 456
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[264590688 | Wang et al. | 2023 | Citations: 1]",
                "snippets": "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT (Kim et al., 2018) uses convolutional operations to transfer dark knowledge; 2) DeiT (Touvron et al., 2020) proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP (Li et al., 2021) exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD (Cardace et al., 2022) exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020). This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219962714 | Yuan et al. | 2020 | Citations: 527]": "Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization.",
                    "[229363322 | Touvron et al. | 2020 | Citations: 6805]": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
                    "[245006036 | Li et al. | 2021 | Citations: 81]": "Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods.",
                    "[252918735 | Cardace et al. | 2022 | Citations: 16]": "Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation.",
                    "[3608236 | Kim et al. | 2018 | Citations: 551]": "Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods."
                },
                "metadata": [
                    {
                        "section_title": "B. Contrastive Learning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1538,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 56
                            },
                            {
                                "start": 57,
                                "end": 706
                            },
                            {
                                "start": 709,
                                "end": 818
                            },
                            {
                                "start": 819,
                                "end": 937
                            },
                            {
                                "start": 938,
                                "end": 1056
                            },
                            {
                                "start": 1057,
                                "end": 1224
                            },
                            {
                                "start": 1225,
                                "end": 1423
                            },
                            {
                                "start": 1424,
                                "end": 1538
                            }
                        ],
                        "ref_mentions": [
                            "3608236",
                            "229363322",
                            "245006036",
                            "252918735",
                            "219962714",
                            "219962714"
                        ],
                        "quote": "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT (Kim et al., 2018) uses convolutional operations to transfer dark knowledge; 2) DeiT (Touvron et al., 2020) proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP (Li et al., 2021) exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD (Cardace et al., 2022) exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020). This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[265384964 | Xie et al. | 2023 | Citations: 6]",
                "snippets": "With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1893,
                        "end": 2255,
                        "sentence_offsets": [
                            {
                                "start": 1893,
                                "end": 2131
                            },
                            {
                                "start": 2132,
                                "end": 2255
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[265444951 | Su et al. | 2023 | Citations: 2]",
                "snippets": "Knowledge Distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features...Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge Distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 213,
                        "end": 563,
                        "sentence_offsets": [
                            {
                                "start": 213,
                                "end": 562
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[267413204 | Jin et al. | 2024 | Citations: 47]",
                "snippets": "The dark knowledge method (Hinton et al., 2015) further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "B.2.1 Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 732,
                        "end": 1025,
                        "sentence_offsets": [
                            {
                                "start": 732,
                                "end": 865
                            },
                            {
                                "start": 866,
                                "end": 1025
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "The dark knowledge method (Hinton et al., 2015) further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[267657497 | Kim et al. | 2023 | Citations: 4]",
                "snippets": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 175,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[268857025 | Su et al. | 2024 | Citations: 1]",
                "snippets": "The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 376,
                        "end": 588,
                        "sentence_offsets": [
                            {
                                "start": 376,
                                "end": 493
                            },
                            {
                                "start": 493,
                                "end": 588
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[269033278 | Hu et al. | 2024 | Citations: 1]",
                "snippets": "Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Decoupled Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 33,
                        "end": 376,
                        "sentence_offsets": [
                            {
                                "start": 33,
                                "end": 189
                            },
                            {
                                "start": 189,
                                "end": 209
                            },
                            {
                                "start": 209,
                                "end": 376
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[269317596 | Guo et al. | 2024 | Citations: 3]",
                "snippets": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 440,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 88
                            },
                            {
                                "start": 89,
                                "end": 180
                            },
                            {
                                "start": 181,
                                "end": 329
                            },
                            {
                                "start": 330,
                                "end": 440
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2)."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[269362788 | Li et al. | 2024 | Citations: 12]",
                "snippets": "The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 417,
                        "end": 682,
                        "sentence_offsets": [
                            {
                                "start": 417,
                                "end": 586
                            },
                            {
                                "start": 586,
                                "end": 682
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[269921267 | Mei et al. | 2024 | Citations: 20]",
                "snippets": "Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge distillation",
                        "pdf_hash": "",
                        "start": 419,
                        "end": 692,
                        "sentence_offsets": [
                            {
                                "start": 419,
                                "end": 894
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model"
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[270389751 | Li et al. | 2024 | Citations: 1]",
                "snippets": "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 356,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 194
                            },
                            {
                                "start": 194,
                                "end": 356
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[271244914 | Giakoumoglou et al. | 2024 | Citations: 0]",
                "snippets": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 400,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 86
                            },
                            {
                                "start": 87,
                                "end": 204
                            },
                            {
                                "start": 205,
                                "end": 400
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs."
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[273227005 | Yin et al. | 2024 | Citations: 0]",
                "snippets": "Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[273811396 | Yu et al. | 2024 | Citations: 2]",
                "snippets": "The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" (Wang et al., 2020), whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215745611 | Wang et al. | 2020 | Citations: 700]": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 209,
                        "end": 439,
                        "sentence_offsets": [
                            {
                                "start": 209,
                                "end": 439
                            }
                        ],
                        "ref_mentions": [
                            "215745611"
                        ],
                        "quote": "The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" (Wang et al., 2020), whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student."
                    }
                ]
            },
            {
                "idx": 44,
                "key": "[273821996 | Li et al. | 2024 | Citations: 1]",
                "snippets": "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features (Heo et al., 2019)(Heo et al., 2018)(Kim et al., 2018)(Park et al., 2019)(Peng et al., 2019)39,(Tung et al., 2019)43,(Yim et al., 2017)[73] and distillation from logits (Cho et al., 2019)(Furlanello et al., 2018)(Mirzadeh et al., 2019)(Yang et al., 2018)(Zhao et al., 2016). Many studies (Cho et al., 2021)(Hu et al., 2020)(Rahimpour et al., 2021)21,(Wang et al., 2023)[51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102483181 | Heo et al. | 2019 | Citations: 584]": "We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page.",
                    "[102483463 | Peng et al. | 2019 | Citations: 513]": "Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods.",
                    "[131765296 | Park et al. | 2019 | Citations: 1424]": "Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets.",
                    "[198179476 | Tung et al. | 2019 | Citations: 981]": "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach.",
                    "[203642130 | Cho et al. | 2019 | Citations: 619]": "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models.",
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.",
                    "[221543802 | Hu et al. | 2020 | Citations: 129]": "The joint use of multiple imaging modalities for medical image segmentation has been widely studied in recent years. The fusion of information from different modalities has demonstrated to improve the segmentation accuracy, with respect to mono-modal segmentations, in several applications. However, acquiring multiple modalities is usually not possible in a clinical setting due to a limited number of physicians and scanners, and to limit costs and scan time. Most of the time, only one modality is acquired. In this paper, we propose KD-Net, a framework to transfer knowledge from a trained multi-modal network (teacher) to a mono-modal one (student). The proposed method is an adaptation of the generalized distillation framework where the student network is trained on a subset (1 modality) of the teacher\u2019s inputs (n modalities). We illustrate the effectiveness of the proposed framework in brain tumor segmentation with the BraTS 2018 dataset. Using different architectures, we show that the student network effectively learns from the teacher and always outperforms the baseline mono-modal network in terms of segmentation accuracy.",
                    "[233219684 | Cho et al. | 2021 | Citations: 17]": "In this work, we address the issues of the missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality\u2013the ground truth answers\u2013that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the \"Big\" Teacher, that takes the image/question/answer triplet as its input and out-performs the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performance of our method and a diverse possibility for future research.",
                    "[245445463 | Rahimpour et al. | 2021 | Citations: 30]": "Convolutional neural networks (CNNs) for brain tumor segmentation are generally developed using complete sets of magnetic resonance imaging (MRI) sequences for both training and inference. As such, these algorithms are not trained for realistic, clinical scenarios where parts of the MRI sequences which were used for training, are missing during inference. To increase clinical applicability, we proposed a cross-modal distillation approach to leverage the availability of multi-sequence MRI data for training and generate an enriched CNN model which uses only single-sequence MRI data for inference but outperforms a single-sequence CNN model. We assessed the performance of the proposed method for whole tumor and tumor core segmentation with multi-sequence MRI data available for training but only <inline-formula><tex-math notation=\"LaTeX\">$T_{1}$</tex-math></inline-formula>-weighted (<inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula>) sequence data available for inference, using BraTS 2018, and in-house datasets. Results showed that cross-modal distillation significantly improved the Dice score for both whole tumor and tumor core segmentation when only <inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula> sequence data were available for inference. For the evaluation using the in-house dataset, cross-modal distillation achieved an average Dice score of 79.04% and 69.39% for whole tumor and tumor core segmentation, respectively, while a single-sequence U-Net model using <inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula> sequence data for both training and inference achieved an average Dice score of 73.60% and 62.62%, respectively. These findings confirmed cross-modal distillation as an effective method to increase the potential of single-sequence CNN models such that segmentation performance is less compromised by missing MRI sequences or having only one MRI sequence available for segmentation.",
                    "[263605398 | Wang et al. | 2023 | Citations: 33]": "The problem of missing modalities is both critical and non-trivial to be handled in multi-modal models. It is common for multi-modal tasks that certain modalities contribute more compared to other modalities, and if those important modalities are missing, the model performance drops significantly. Such fact remains unexplored by current multi-modal approaches that recover the representation from missing modalities by feature reconstruction or blind feature aggregation from other modalities, instead of extracting useful information from the best performing modalities. In this paper, we propose a Learnable Cross-modal Knowledge Distillation (LCKD) model to adaptively identify important modalities and distil knowledge from them to help other modalities from the cross-modal perspective for solving the missing modality issue. Our approach introduces a teacher election procedure to select the most ``qualified'' teachers based on their single modality performance on certain tasks. Then, cross-modal knowledge distillation is performed between teacher and student modalities for each task to push the model parameters to a point that is beneficial for all tasks. Hence, even if the teacher modalities for certain tasks are missing during testing, the available student modalities can accomplish the task well enough based on the learned knowledge from their automatically elected teacher modalities. Experiments on the Brain Tumour Segmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods by a considerable margin, improving the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score.",
                    "[3608236 | Kim et al. | 2018 | Citations: 551]": "Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.",
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
                    "[5299559 | Zhao et al. | 2016 | Citations: 12033]": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
                    "[53213211 | Heo et al. | 2018 | Citations: 527]": "An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classificationfriendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art.",
                    "[54436113 | Yang et al. | 2018 | Citations: 193]": "Optimizing a deep neural network is a fundamental task in computer vision, yet direct training methods often suffer from over-fitting. Teacher-student optimization aims at providing complementary cues from a model trained previously, but these approaches are often considerably slow due to the pipeline of training a few generations in sequence, i.e., time complexity is increased by several times. This paper presents snapshot distillation (SD), the first framework which enables teacher-student optimization in one generation. The idea of SD is very simple: instead of borrowing supervision signals from previous generations, we extract such information from earlier epochs in the same generation, meanwhile make sure that the difference between teacher and student is sufficiently large so as to prevent under-fitting. To achieve this goal, we implement SD in a cyclic learning rate policy, in which the last snapshot of each cycle is used as the teacher for all iterations in the next cycle, and the teacher signal is smoothed to provide richer information. In standard image classification benchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracy gain without heavy computational overheads. We also verify that models pre-trained with SD transfers well to object detection and semantic segmentation in the PascalVOC dataset."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 583,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 329
                            },
                            {
                                "start": 330,
                                "end": 431
                            },
                            {
                                "start": 432,
                                "end": 583
                            }
                        ],
                        "ref_mentions": [
                            "102483181",
                            "53213211",
                            "3608236",
                            "131765296",
                            "102483463",
                            "198179476",
                            "206596723",
                            "203642130",
                            "4110009",
                            "212908749",
                            "54436113",
                            "5299559",
                            "233219684",
                            "221543802",
                            "245445463",
                            "263605398"
                        ],
                        "quote": "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features (Heo et al., 2019)(Heo et al., 2018)(Kim et al., 2018)(Park et al., 2019)(Peng et al., 2019)39,(Tung et al., 2019)43,(Yim et al., 2017)[73] and distillation from logits (Cho et al., 2019)(Furlanello et al., 2018)(Mirzadeh et al., 2019)(Yang et al., 2018)(Zhao et al., 2016). Many studies (Cho et al., 2021)(Hu et al., 2020)(Rahimpour et al., 2021)21,(Wang et al., 2023)[51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities."
                    }
                ]
            },
            {
                "idx": 45,
                "key": "[274655700 | Zhou et al. | 2024 | Citations: 3]",
                "snippets": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 46,
                "key": "[275920765 | Takanami et al. | 2025 | Citations: 1]",
                "snippets": "Two main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1152,
                        "end": 1633,
                        "sentence_offsets": [
                            {
                                "start": 1152,
                                "end": 1244
                            },
                            {
                                "start": 1245,
                                "end": 1354
                            },
                            {
                                "start": 1357,
                                "end": 1529
                            },
                            {
                                "start": 1530,
                                "end": 1633
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Two main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data."
                    }
                ]
            },
            {
                "idx": 47,
                "key": "[276249293 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 166,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 166
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network."
                    }
                ]
            },
            {
                "idx": 48,
                "key": "[277754927 | Zhou et al. | 2025 | Citations: 0]",
                "snippets": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 216,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.026964,
        "cot": "To effectively organize the information from the quotes about dark knowledge in knowledge distillation, I'll create dimensions that address different aspects of the topic for a scientific audience.\n\nFirst, I need an \"Introduction\" section to provide the basic definition and origin of dark knowledge. This should be a synthesis to clearly explain what dark knowledge is and its importance in knowledge distillation.\n\nThen, I'll create a \"Nature of Dark Knowledge\" dimension to explain what exactly constitutes dark knowledge in the context of knowledge distillation. This should be a synthesis to cohesively explain the concept.\n\nNext, a \"Technical Implementation\" dimension should explain how dark knowledge is extracted and transferred in practice. This will cover temperature parameters, softmax functions, and loss functions. This should be a synthesis since it requires explaining interconnected technical concepts.\n\nA \"Benefits of Dark Knowledge Transfer\" dimension will explain why dark knowledge is valuable and what advantages it provides. This should be a synthesis to coherently explain the advantages.\n\nFinally, an \"Applications and Variations\" dimension will cover the various contexts in which dark knowledge distillation has been applied. This should be a list format since it's enumerating different applications.\n\nThese dimensions will provide a comprehensive understanding of dark knowledge in knowledge distillation from definition to implementation to benefits to applications.",
        "plan": {
            "Introduction to Dark Knowledge and Knowledge Distillation (synthesis)": [
                5,
                17,
                27,
                32,
                34,
                42,
                45,
                47,
                48
            ],
            "Nature of Dark Knowledge (synthesis)": [
                0,
                1,
                3,
                4,
                6,
                9,
                13,
                14,
                15,
                16,
                18,
                20,
                24,
                29,
                31,
                33,
                36,
                39,
                46
            ],
            "Technical Implementation of Dark Knowledge Transfer (synthesis)": [
                2,
                4,
                7,
                8,
                10,
                11,
                12,
                21,
                22,
                23,
                26,
                28,
                35,
                37,
                40,
                41,
                43
            ],
            "Benefits of Dark Knowledge Transfer (synthesis)": [
                0,
                3,
                4,
                11,
                14,
                18,
                19,
                24,
                30,
                32
            ],
            "Applications and Variations of Dark Knowledge (list)": [
                25,
                30,
                38,
                44
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Dark Knowledge and Knowledge Distillation",
                "tldr": "Knowledge distillation is a technique that transfers \"dark knowledge\" from a larger teacher network to a smaller student network, enabling efficient model compression. This approach allows smaller models to achieve performance comparable to their larger counterparts while requiring fewer computational resources. (9 sources)",
                "text": "\nKnowledge distillation (KD) has emerged as a pivotal technique in the field of machine learning, particularly for model compression and transfer learning. At its core, knowledge distillation involves extracting \"dark knowledge\" from a complex teacher network and transferring it to guide the learning of a more lightweight student network <Paper corpusId=\"219636179\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. This approach enables smaller models to approximate the performance of their larger counterparts while requiring significantly fewer computational resources <Paper corpusId=\"274655700\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277754927\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nThe concept of knowledge distillation was initially proposed in 2006, though it wasn't explicitly explained at that time. In 2014, the method was formally defined as a process where a student network learns from the soft targets output by a teacher network <Paper corpusId=\"248683566\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper>. Since then, knowledge distillation has gained substantial attention as an effective strategy for enhancing the generalization capabilities of more compact neural networks <Paper corpusId=\"273227005\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nThe fundamental principle behind knowledge distillation is that a smaller student model can benefit not only from ground-truth labels but also from the \"dark knowledge\" embedded in a larger teacher model <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>. This dark knowledge represents implicit insights not present in the ground-truth labels alone, allowing the student model to achieve performance levels that would be difficult to reach through conventional training methods <Paper corpusId=\"276249293\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nIn practical terms, knowledge distillation creates a pathway for transferring the sophisticated learning patterns of complex networks to simpler, more deployable architectures <Paper corpusId=\"264372297\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This has significant implications for applications where computational efficiency is crucial, as it enables the development of models that maintain high accuracy while being much more efficient than their teacher counterparts <Paper corpusId=\"267657497\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Xu et al., 2020)",
                        "snippets": [
                            "Knowledge distillation, which involves extracting the \"dark knowledge\" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning."
                        ],
                        "paper": {
                            "corpus_id": 219636179,
                            "title": "Knowledge Distillation Meets Self-Supervision",
                            "authors": [
                                {
                                    "authorId": "46538811",
                                    "name": "Guodong Xu"
                                },
                                {
                                    "authorId": "2117940996",
                                    "name": "Ziwei Liu"
                                },
                                {
                                    "authorId": "2108536754",
                                    "name": "Xiaoxiao Li"
                                },
                                {
                                    "authorId": "1717179",
                                    "name": "Chen Change Loy"
                                }
                            ],
                            "year": 2020,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 285
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy."
                        ],
                        "paper": {
                            "corpus_id": 274655700,
                            "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
                            "authors": [
                                {
                                    "authorId": "2331698741",
                                    "name": "Junjie Zhou"
                                },
                                {
                                    "authorId": "2273931950",
                                    "name": "Ke Zhu"
                                },
                                {
                                    "authorId": "2274078411",
                                    "name": "Jianxin Wu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.65869140625
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy."
                        ],
                        "paper": {
                            "corpus_id": 277754927,
                            "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
                            "authors": [
                                {
                                    "authorId": "2331698741",
                                    "name": "Junjie Zhou"
                                },
                                {
                                    "authorId": "2273931950",
                                    "name": "Ke Zhu"
                                },
                                {
                                    "authorId": "2274078411",
                                    "name": "Jianxin Wu"
                                }
                            ],
                            "year": 2025,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.66015625
                    },
                    {
                        "id": "(Xu et al., 2022)",
                        "snippets": [
                            "Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020). This concept was first proposed in (Bucila et al., 2006), but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation."
                        ],
                        "paper": {
                            "corpus_id": 248683566,
                            "title": "Teacher-student collaborative knowledge distillation for image classification",
                            "authors": [
                                {
                                    "authorId": "2817613",
                                    "name": "Chuanyun Xu"
                                },
                                {
                                    "authorId": "2164051968",
                                    "name": "Wenjian Gao"
                                },
                                {
                                    "authorId": "2164318078",
                                    "name": "Tian Li"
                                },
                                {
                                    "authorId": "2164821600",
                                    "name": "Nanlan Bai"
                                },
                                {
                                    "authorId": "2155121570",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2145954082",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Applied intelligence (Boston)",
                            "n_citations": 44
                        },
                        "score": 0.6728515625
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "Knowledge distillation (KD) enhances student network generalization by transferring dark knowledge from a complex teacher network."
                        ],
                        "paper": {
                            "corpus_id": 273227005,
                            "title": "Two-Stage Approach for Targeted Knowledge Transfer in Self-Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2290035516",
                                    "name": "Zimo Yin"
                                },
                                {
                                    "authorId": "2142824615",
                                    "name": "Jian Pu"
                                },
                                {
                                    "authorId": "2261082542",
                                    "name": "Yijie Zhou"
                                },
                                {
                                    "authorId": "2251995827",
                                    "name": "Xiangyang Xue"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE/CAA Journal of Automatica Sinica",
                            "n_citations": 0
                        },
                        "score": 0.650390625
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "Knowledge Distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features",
                            "Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form."
                        ],
                        "paper": {
                            "corpus_id": 265444951,
                            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268314644",
                                    "name": "Chi-Ping Su"
                                },
                                {
                                    "authorId": "1569686364",
                                    "name": "Ching-Hsun Tseng"
                                },
                                {
                                    "authorId": "2338265886",
                                    "name": "Bin Pu"
                                },
                                {
                                    "authorId": "2338506240",
                                    "name": "Lei Zhao"
                                },
                                {
                                    "authorId": "2328588941",
                                    "name": "Zhuangzhuang Chen"
                                },
                                {
                                    "authorId": "2116351339",
                                    "name": "Shin-Jye Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.56494140625
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network."
                        ],
                        "paper": {
                            "corpus_id": 276249293,
                            "title": "Contrastive Representation Distillation via Multi-Scale Feature Decoupling",
                            "authors": [
                                {
                                    "authorId": "2344789398",
                                    "name": "Cuipeng Wang"
                                },
                                {
                                    "authorId": "2344967066",
                                    "name": "Tieyuan Chen"
                                },
                                {
                                    "authorId": "2344788684",
                                    "name": "Haipeng Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6796875
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Knowledge distillation (KD) is a technique that transfers \"dark knowledge\" from a deep teacher network (teacher) to a shallow student network (student)."
                        ],
                        "paper": {
                            "corpus_id": 264372297,
                            "title": "Improving Knowledge Distillation via Head and Tail Categories",
                            "authors": [
                                {
                                    "authorId": "2215812572",
                                    "name": "Liuchi Xu"
                                },
                                {
                                    "authorId": "2157467240",
                                    "name": "Jin Ren"
                                },
                                {
                                    "authorId": "2151325820",
                                    "name": "Zhenhua Huang"
                                },
                                {
                                    "authorId": "2265252477",
                                    "name": "Weishi Zheng"
                                },
                                {
                                    "authorId": "2261262093",
                                    "name": "Yunwen Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
                            "n_citations": 9
                        },
                        "score": 0.61083984375
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks."
                        ],
                        "paper": {
                            "corpus_id": 267657497,
                            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
                            "authors": [
                                {
                                    "authorId": "2268350036",
                                    "name": "Seonghak Kim"
                                },
                                {
                                    "authorId": "2156910329",
                                    "name": "Gyeongdo Ham"
                                },
                                {
                                    "authorId": "2268370058",
                                    "name": "Suin Lee"
                                },
                                {
                                    "authorId": "2268310103",
                                    "name": "Donggon Jang"
                                },
                                {
                                    "authorId": "2145154407",
                                    "name": "Daeshik Kim"
                                }
                            ],
                            "year": 2023,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 4
                        },
                        "score": 0.68896484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Nature of Dark Knowledge",
                "tldr": "Dark knowledge refers to the rich information embedded in a teacher model's output probability distributions that goes beyond one-hot labels. This includes class similarity relationships, uncertainty estimates, and implicit insights that help student models generalize better. (19 sources)",
                "text": "\nDark knowledge, a term popularized by Hinton et al., represents the subtle information contained within a neural network's output probability distribution that is not captured by simple one-hot encoded labels <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This knowledge primarily manifests in the relative probabilities assigned to incorrect classes, revealing how the teacher model perceives similarities between different categories <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper> <Paper corpusId=\"237091534\" paperTitle=\"(Ren et al., 2021)\" isShortName></Paper>.\n\nAt its core, dark knowledge encompasses the instance-to-class similarities that provide richer supervision signals for student networks <Paper corpusId=\"224801420\" paperTitle=\"(Deng et al., 2020)\" isShortName></Paper>. When a teacher network processes an image, it might assign a high probability to the correct class (e.g., \"cat\") but also non-zero probabilities to visually similar classes (e.g., \"dog\"), capturing nuanced visual relationships that are absent in binary ground-truth labels <Paper corpusId=\"236984375\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This creates a more informative training signal that helps student models develop better generalization capabilities <Paper corpusId=\"267413204\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>.\n\nThe nature of dark knowledge extends beyond just inter-class relationships. It encompasses the decision-making processes and uncertainty estimates of the teacher model <Paper corpusId=\"269921267\" paperTitle=\"(Mei et al., 2024)\" isShortName></Paper>. For instance, in complex discrimination tasks, dark knowledge can help refine noisy labels and provide insights into the underlying data structure that might not be explicitly present in the original training dataset <Paper corpusId=\"203642142\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper> <Paper corpusId=\"252668749\" paperTitle=\"(Biehler et al., 2022)\" isShortName></Paper>.\n\nTemperature scaling plays a critical role in revealing dark knowledge. By increasing the temperature parameter in the softmax function, the probability distribution becomes smoother, allowing more nuanced information about category interrelationships to be transferred to the student model <Paper corpusId=\"265384964\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>. This smoother distribution yields smaller variance in gradients, often leading to faster convergence during student network training <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nDark knowledge can manifest at various network levels. While high-layer features typically relate to specific task objectives, lower-layer features represent more generic attributes like edges and corners. All these features contain valuable dark knowledge that can be transferred to guide the student network's training process <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper>. This multi-level knowledge transfer creates a comprehensive learning framework where students benefit from both task-specific and general feature representations developed by the teacher.\n\nIn practical applications, dark knowledge serves as an additional source of supervision alongside ground-truth labels <Paper corpusId=\"232269823\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>. This dual supervision approach enables student networks to learn both the primary task objectives and the subtle class relationships captured by the teacher <Paper corpusId=\"247521335\" paperTitle=\"(Ye et al., 2022)\" isShortName></Paper>. The integration of dark knowledge has proven particularly effective in various contexts, including image classification, metric learning, and even lossy compression tasks where perfect reconstruction is rarely achievable <Paper corpusId=\"249017724\" paperTitle=\"(Bai et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.\n\nThe effectiveness of dark knowledge transfer explains why student models often outperform models trained directly on hard labels alone. By learning from the teacher's probability distributions, student networks acquire information about unobserved intents and subtle data relationships that facilitate better decision boundaries and improved generalization <Paper corpusId=\"269033278\" paperTitle=\"(Hu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275920765\" paperTitle=\"(Takanami et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264555654\" paperTitle=\"(Jung et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nayak et al., 2019)",
                        "snippets": [
                            "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability."
                        ],
                        "paper": {
                            "corpus_id": 159041346,
                            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                            "authors": [
                                {
                                    "authorId": "143747407",
                                    "name": "Gaurav Kumar Nayak"
                                },
                                {
                                    "authorId": "2217000",
                                    "name": "Konda Reddy Mopuri"
                                },
                                {
                                    "authorId": "7155274",
                                    "name": "Vaisakh Shaj"
                                },
                                {
                                    "authorId": "144682140",
                                    "name": "R. Venkatesh Babu"
                                },
                                {
                                    "authorId": "1429640900",
                                    "name": "Anirban Chakraborty"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 245
                        },
                        "score": 0.6123046875
                    },
                    {
                        "id": "(Ren et al., 2021)",
                        "snippets": [
                            "In this way, the student network attempts to learn dark knowledge (Hinton et al., 2015) that contains the similarities between different classes, which can not be provided by the ground truth labels."
                        ],
                        "paper": {
                            "corpus_id": 237091534,
                            "title": "Online Multi-Granularity Distillation for GAN Compression",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yuxi Ren"
                                },
                                {
                                    "authorId": "2118432533",
                                    "name": "Jie Wu"
                                },
                                {
                                    "authorId": "2118724465",
                                    "name": "Xuefeng Xiao"
                                },
                                {
                                    "authorId": "1706007",
                                    "name": "Jianchao Yang"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 39
                        },
                        "score": 0.53564453125
                    },
                    {
                        "id": "(Deng et al., 2020)",
                        "snippets": [
                            "The soft targets contain the information about instance-to-class similarities (i..e, dark knowledge) that can improve the student performance."
                        ],
                        "paper": {
                            "corpus_id": 224801420,
                            "title": "Locally Linear Region Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2150478789",
                                    "name": "Xiang Deng"
                                },
                                {
                                    "authorId": "2118748124",
                                    "name": "Zhongfei Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.67919921875
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics."
                        ],
                        "paper": {
                            "corpus_id": 236984375,
                            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
                            "authors": [
                                {
                                    "authorId": "2109643902",
                                    "name": "Jinghan Wang"
                                },
                                {
                                    "authorId": "2151302904",
                                    "name": "Guangyue Li"
                                },
                                {
                                    "authorId": "2107940856",
                                    "name": "Wenzhao Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Inf.",
                            "n_citations": 3
                        },
                        "score": 0.60888671875
                    },
                    {
                        "id": "(Jin et al., 2024)",
                        "snippets": [
                            "The dark knowledge method (Hinton et al., 2015) further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label."
                        ],
                        "paper": {
                            "corpus_id": 267413204,
                            "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
                            "authors": [
                                {
                                    "authorId": "2308012469",
                                    "name": "Can Jin"
                                },
                                {
                                    "authorId": "2282541442",
                                    "name": "Tong Che"
                                },
                                {
                                    "authorId": "2282596442",
                                    "name": "Hongwu Peng"
                                },
                                {
                                    "authorId": "2282543805",
                                    "name": "Yiyuan Li"
                                },
                                {
                                    "authorId": "2237790577",
                                    "name": "Marco Pavone"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 47
                        },
                        "score": 0.53662109375
                    },
                    {
                        "id": "(Mei et al., 2024)",
                        "snippets": [
                            "Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model"
                        ],
                        "paper": {
                            "corpus_id": 269921267,
                            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
                            "authors": [
                                {
                                    "authorId": "2298916636",
                                    "name": "Taiyuan Mei"
                                },
                                {
                                    "authorId": "2298918720",
                                    "name": "Yun Zi"
                                },
                                {
                                    "authorId": "2222987403",
                                    "name": "X. Cheng"
                                },
                                {
                                    "authorId": "2297725659",
                                    "name": "Zijun Gao"
                                },
                                {
                                    "authorId": "2297735971",
                                    "name": "Qi Wang"
                                },
                                {
                                    "authorId": "2302372513",
                                    "name": "Haowei Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
                            "n_citations": 20
                        },
                        "score": 0.56396484375
                    },
                    {
                        "id": "(Dong et al., 2019)",
                        "snippets": [
                            "To explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels."
                        ],
                        "paper": {
                            "corpus_id": 203642142,
                            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
                            "authors": [
                                {
                                    "authorId": "145496882",
                                    "name": "Bin Dong"
                                },
                                {
                                    "authorId": "104162954",
                                    "name": "Jikai Hou"
                                },
                                {
                                    "authorId": "48518029",
                                    "name": "Yiping Lu"
                                },
                                {
                                    "authorId": "47294286",
                                    "name": "Zhihua Zhang"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 41
                        },
                        "score": 0.64013671875
                    },
                    {
                        "id": "(Biehler et al., 2022)",
                        "snippets": [
                            "Dark knowledge refers to information not directly encoded in the original training dataset, which nevertheless is relevant to the prediction task at hand. It is made explicit by a teacher model, then passed down through knowledge distillation. Although the term has been coined in the frame of soft targets (Hinton, Vinyals, and Dean, 2015), we argue that the teacher-student framework, along with the general idea of transmitting information from the one to the other through model outputs, can in practice be used in other settings while still being referred to as knowledge distillation."
                        ],
                        "paper": {
                            "corpus_id": 252668749,
                            "title": "Using Knowledge Distillation to improve interpretable models in a retail banking context",
                            "authors": [
                                {
                                    "authorId": "2186740719",
                                    "name": "Maxime Biehler"
                                },
                                {
                                    "authorId": "2186740508",
                                    "name": "Mohamed Guermazi"
                                },
                                {
                                    "authorId": "2186740498",
                                    "name": "C'elim Starck"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.67138671875
                    },
                    {
                        "id": "(Xie et al., 2023)",
                        "snippets": [
                            "With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation."
                        ],
                        "paper": {
                            "corpus_id": 265384964,
                            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268047558",
                                    "name": "Jinzhou Xie"
                                },
                                {
                                    "authorId": "2268031861",
                                    "name": "Hongmin Zhao"
                                }
                            ],
                            "year": 2023,
                            "venue": "Fire",
                            "n_citations": 6
                        },
                        "score": 0.630859375
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model",
                            "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."
                        ],
                        "paper": {
                            "corpus_id": 212855595,
                            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "47295143",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2269366",
                                    "name": "Xiaobo Shu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "2079682",
                                    "name": "Tingwen Liu"
                                },
                                {
                                    "authorId": "48019474",
                                    "name": "Jiapeng Zhao"
                                },
                                {
                                    "authorId": "2108645146",
                                    "name": "Quangang Li"
                                },
                                {
                                    "authorId": "48358041",
                                    "name": "Li Guo"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 35
                        },
                        "score": 0.818359375
                    },
                    {
                        "id": "(Zhao et al., 2019)",
                        "snippets": [
                            "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process",
                            "Hinton et al. (Hinton et al., 2015) extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks."
                        ],
                        "paper": {
                            "corpus_id": 198179767,
                            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "10944885",
                                    "name": "Changrui Chen"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE Transactions on Cybernetics",
                            "n_citations": 59
                        },
                        "score": 0.55322265625
                    },
                    {
                        "id": "(Zhao et al., 2021)",
                        "snippets": [
                            "Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network."
                        ],
                        "paper": {
                            "corpus_id": 232269823,
                            "title": "Similarity Transfer for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "2058266174",
                                    "name": "Kun Gong"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "145429878",
                                    "name": "Hui Yu"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.81787109375
                    },
                    {
                        "id": "(Ye et al., 2022)",
                        "snippets": [
                            "In detail, a strong classifier, e.g., a neural network trained with deeper architectures [5], high-quality images [6], or precise optimization strategies [7], [8], acts as a \"teacher\" and guides the training of a \"student\" model by richer supervision, so that the learning experience from a related task is reused in the current task.\n\nThe teacher's class posterior probability over an instance is the most common dark knowledge, as it indicates the teacher's estimation of how similar an instance is to candidate categories. Besides the extreme \"black or white\" supervision, the student is asked to align its posterior with the teacher during its training progress."
                        ],
                        "paper": {
                            "corpus_id": 247521335,
                            "title": "Generalized Knowledge Distillation via Relationship Matching",
                            "authors": [
                                {
                                    "authorId": "2151459740",
                                    "name": "Han-Jia Ye"
                                },
                                {
                                    "authorId": "2115435395",
                                    "name": "Su Lu"
                                },
                                {
                                    "authorId": "1721819",
                                    "name": "De-chuan Zhan"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 21
                        },
                        "score": 0.771484375
                    },
                    {
                        "id": "(Bai et al., 2022)",
                        "snippets": [
                            "Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted."
                        ],
                        "paper": {
                            "corpus_id": 249017724,
                            "title": "Improving the Latent Space of Image Style Transfer",
                            "authors": [
                                {
                                    "authorId": "48442720",
                                    "name": "Yun-Hao Bai"
                                },
                                {
                                    "authorId": "47073960",
                                    "name": "Cairong Wang"
                                },
                                {
                                    "authorId": "2117729099",
                                    "name": "C. Yuan"
                                },
                                {
                                    "authorId": "2140245719",
                                    "name": "Yanbo Fan"
                                },
                                {
                                    "authorId": "2144537318",
                                    "name": "Jue Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.71826171875
                    },
                    {
                        "id": "(Cui et al., 2022)",
                        "snippets": [
                            "Compared to direct learning with labels, the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio."
                        ],
                        "paper": {
                            "corpus_id": 260447668,
                            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                            "authors": [
                                {
                                    "authorId": "2172485573",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "47093519",
                                    "name": "Jiajia Guo"
                                },
                                {
                                    "authorId": "2113999930",
                                    "name": "Zheng Cao"
                                },
                                {
                                    "authorId": "120710335",
                                    "name": "Huaze Tang"
                                },
                                {
                                    "authorId": "2257212132",
                                    "name": "Chao-Kai Wen"
                                },
                                {
                                    "authorId": "2227268421",
                                    "name": "Shi Jin"
                                },
                                {
                                    "authorId": "2288090155",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2240356164",
                                    "name": "Xiaolin Hou"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Communications",
                            "n_citations": 3
                        },
                        "score": 0.66015625
                    },
                    {
                        "id": "(Hu et al., 2024)",
                        "snippets": [
                            "Different from one-hot labels (hard target), the probability distributions of event classes (soft target) provide knowledge among unobserved intents (a.k.a.dark knowledge [6]).Knowledge distillation (KD) is an effective paradigm to transfer such knowledge from teacher networks to student networks and obtain better generalization performance."
                        ],
                        "paper": {
                            "corpus_id": 269033278,
                            "title": "CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction",
                            "authors": [
                                {
                                    "authorId": "2000918408",
                                    "name": "Kaixi Hu"
                                },
                                {
                                    "authorId": "2155688849",
                                    "name": "Lin Li"
                                },
                                {
                                    "authorId": "2027162851",
                                    "name": "Qing Xie"
                                },
                                {
                                    "authorId": "2070898550",
                                    "name": "Xiaohui Tao"
                                },
                                {
                                    "authorId": "2256933615",
                                    "name": "Guandong Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Database Systems for Advanced Applications",
                            "n_citations": 1
                        },
                        "score": 0.794921875
                    },
                    {
                        "id": "(Takanami et al., 2025)",
                        "snippets": [
                            "Two main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data."
                        ],
                        "paper": {
                            "corpus_id": 275920765,
                            "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
                            "authors": [
                                {
                                    "authorId": "2281746039",
                                    "name": "Kaito Takanami"
                                },
                                {
                                    "authorId": "2342462960",
                                    "name": "Takashi Takahashi"
                                },
                                {
                                    "authorId": "2342406436",
                                    "name": "Ayaka Sakata"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.64404296875
                    },
                    {
                        "id": "(Jung et al., 2023)",
                        "snippets": [
                            "In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge."
                        ],
                        "paper": {
                            "corpus_id": 264555654,
                            "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2262406221",
                                    "name": "Yeongseo Jung"
                                },
                                {
                                    "authorId": "2262217139",
                                    "name": "Eunseo Jung"
                                },
                                {
                                    "authorId": "2262372006",
                                    "name": "Lei Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 5
                        },
                        "score": 0.720703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Implementation of Dark Knowledge Transfer",
                "tldr": "Dark knowledge transfer is implemented using temperature-scaled softmax outputs from a teacher model to guide student learning. The process involves a combined loss function that balances distillation loss (comparing teacher and student soft probabilities) with conventional loss from ground-truth labels. (18 sources)",
                "text": "\nThe technical implementation of dark knowledge transfer centers around extracting and transferring the probability distributions from a teacher network to a student network through a carefully designed training procedure. This process begins with both networks processing the same input data, producing their respective logits (pre-softmax outputs) <Paper corpusId=\"203593636\" paperTitle=\"(Xie et al., 2019)\" isShortName></Paper> <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nA crucial element in this transfer is the temperature parameter (\u03c4), which controls the \"softness\" of the probability distributions <Paper corpusId=\"264516404\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. When applied to the softmax function, this parameter produces softer probability distributions by dividing the logits before applying softmax: P = softmax(Z/\u03c4) <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"268857025\" paperTitle=\"(Su et al., 2024)\" isShortName></Paper>. Higher temperature values lead to smoother distributions that better reveal the subtle class relationships captured by the teacher network <Paper corpusId=\"269317596\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nThe knowledge distillation loss function typically combines two components: a distillation loss and a conventional task loss <Paper corpusId=\"235489777\" paperTitle=\"(Zhao et al._1, 2021)\" isShortName></Paper>. The distillation loss measures the difference between the teacher's and student's soft probability distributions, commonly using Kullback-Leibler (KL) divergence <Paper corpusId=\"271244914\" paperTitle=\"(Giakoumoglou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257504799\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This can be formulated as:\n\n```\nL_KD = KL(softmax(Z_T/\u03c4) || softmax(Z_S/\u03c4))\n```\n\nWhere Z_T and Z_S represent the logits from the teacher and student networks, respectively <Paper corpusId=\"231648215\" paperTitle=\"(Feng et al., 2021)\" isShortName></Paper>. For regression problems, this loss might be reformulated as the mean squared error between the teacher's and student's logits <Paper corpusId=\"231648215\" paperTitle=\"(Feng et al., 2021)\" isShortName></Paper>.\n\nThe conventional task loss typically uses cross-entropy between the student's predictions and ground-truth labels. The final training objective combines these losses with a weighting parameter (\u03b1) to balance their contributions <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>:\n\n```\nL_total = \u03b1 * L_KD + (1-\u03b1) * L_CE\n```\n\nThis dual supervision approach enables the student model to simultaneously learn from both ground-truth labels and the teacher's dark knowledge <Paper corpusId=\"254044469\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"263789814\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. The dark knowledge transfer occurs primarily through the soft labels, which provide additional supervisory signals beyond what's available in one-hot encoded ground-truth labels <Paper corpusId=\"233210081\" paperTitle=\"(Zhao et al._2, 2021)\" isShortName></Paper> <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nDuring training, the student network is optimized to match both the ground-truth labels and the probability outputs of the teacher model <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This regularizes the student's learning process <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper> and helps it converge faster by leveraging the smaller variance in gradients produced by the teacher's soft distributions <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nThe entire knowledge distillation process can be viewed as a teaching mechanism where information flows from the teacher to the student through soft labels (logits) <Paper corpusId=\"273811396\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215745611\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper>. This transfer of dark knowledge allows the student model to learn more efficiently and achieve performance closer to that of the teacher model, despite having fewer parameters <Paper corpusId=\"270389751\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Xie et al., 2019)",
                        "snippets": [
                            "Dark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels."
                        ],
                        "paper": {
                            "corpus_id": 203593636,
                            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
                            "authors": [
                                {
                                    "authorId": "2212036699",
                                    "name": "Jiao Xie"
                                },
                                {
                                    "authorId": "3431378",
                                    "name": "Shaohui Lin"
                                },
                                {
                                    "authorId": "2121310989",
                                    "name": "Yichen Zhang"
                                },
                                {
                                    "authorId": "39378434",
                                    "name": "Linkai Luo"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.7490234375
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model",
                            "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."
                        ],
                        "paper": {
                            "corpus_id": 212855595,
                            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "47295143",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2269366",
                                    "name": "Xiaobo Shu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "2079682",
                                    "name": "Tingwen Liu"
                                },
                                {
                                    "authorId": "48019474",
                                    "name": "Jiapeng Zhao"
                                },
                                {
                                    "authorId": "2108645146",
                                    "name": "Quangang Li"
                                },
                                {
                                    "authorId": "48358041",
                                    "name": "Li Guo"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 35
                        },
                        "score": 0.818359375
                    },
                    {
                        "id": "(Yang et al., 2023)",
                        "snippets": [
                            "In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced."
                        ],
                        "paper": {
                            "corpus_id": 264516404,
                            "title": "Attention and feature transfer based knowledge distillation",
                            "authors": [
                                {
                                    "authorId": "2243407272",
                                    "name": "Guoliang Yang"
                                },
                                {
                                    "authorId": "2220900849",
                                    "name": "Shuaiying Yu"
                                },
                                {
                                    "authorId": "2261922204",
                                    "name": "Yangyang Sheng"
                                },
                                {
                                    "authorId": "2257352809",
                                    "name": "Hao Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Scientific Reports",
                            "n_citations": 4
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Su et al., 2024)",
                        "snippets": [
                            "The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced."
                        ],
                        "paper": {
                            "corpus_id": 268857025,
                            "title": "Task Integration Distillation for Object Detectors",
                            "authors": [
                                {
                                    "authorId": "2152173244",
                                    "name": "Hai Su"
                                },
                                {
                                    "authorId": "2294574600",
                                    "name": "Zhenwen Jian"
                                },
                                {
                                    "authorId": "2112454661",
                                    "name": "Songsen Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.71826171875
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2)."
                        ],
                        "paper": {
                            "corpus_id": 269317596,
                            "title": "Shared Knowledge Distillation Network for Object Detection",
                            "authors": [
                                {
                                    "authorId": "2158003499",
                                    "name": "Zhen Guo"
                                },
                                {
                                    "authorId": "2297246012",
                                    "name": "Pengzhou Zhang"
                                },
                                {
                                    "authorId": "2297096848",
                                    "name": "Peng Liang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Electronics",
                            "n_citations": 3
                        },
                        "score": 0.63720703125
                    },
                    {
                        "id": "(Zhao et al._1, 2021)",
                        "snippets": [
                            "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:"
                        ],
                        "paper": {
                            "corpus_id": 235489777,
                            "title": "Knowledge Distillation via Instance-level Sequence Learning",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                },
                                {
                                    "authorId": "2108144626",
                                    "name": "Qiong Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 24
                        },
                        "score": 0.8056640625
                    },
                    {
                        "id": "(Giakoumoglou et al., 2024)",
                        "snippets": [
                            "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs."
                        ],
                        "paper": {
                            "corpus_id": 271244914,
                            "title": "Relational Representation Distillation",
                            "authors": [
                                {
                                    "authorId": "2196360101",
                                    "name": "Nikolaos Giakoumoglou"
                                },
                                {
                                    "authorId": "2292259667",
                                    "name": "Tania Stathaki"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.66064453125
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in (Bucila et al., 2006). However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network."
                        ],
                        "paper": {
                            "corpus_id": 257504799,
                            "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2041323382",
                                    "name": "Maorong Wang"
                                },
                                {
                                    "authorId": "49948838",
                                    "name": "L. Xiao"
                                },
                                {
                                    "authorId": "145572097",
                                    "name": "T. Yamasaki"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.55712890625
                    },
                    {
                        "id": "(Feng et al., 2021)",
                        "snippets": [
                            "For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits."
                        ],
                        "paper": {
                            "corpus_id": 231648215,
                            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2152751276",
                                    "name": "Lingyun Feng"
                                },
                                {
                                    "authorId": "2642333",
                                    "name": "Minghui Qiu"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "16215052",
                                    "name": "Haitao Zheng"
                                },
                                {
                                    "authorId": "2115382645",
                                    "name": "Ying Shen"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 10
                        },
                        "score": 0.71337890625
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1."
                        ],
                        "paper": {
                            "corpus_id": 256900863,
                            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                            "authors": [
                                {
                                    "authorId": "2135847074",
                                    "name": "Xiongtao Zhang"
                                },
                                {
                                    "authorId": "2206403890",
                                    "name": "Zezong Yin"
                                },
                                {
                                    "authorId": "3247526",
                                    "name": "Yunliang Jiang"
                                },
                                {
                                    "authorId": "1390650781",
                                    "name": "Yizhang Jiang"
                                },
                                {
                                    "authorId": "2340356",
                                    "name": "Da-Song Sun"
                                },
                                {
                                    "authorId": "2189281",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.80419921875
                    },
                    {
                        "id": "(Zhang et al., 2022)",
                        "snippets": [
                            "The concept of KD was first proposed by Hinton et al. [10]. KD directs the student training by leveraging the dark knowledge of teacher model, and enhances the performance of student model successfully. Dark knowledge, which can provide additional information to supervise the training process compared to simply utilizing ground-truth labels, is obtained from teacher networks in features or soft logits."
                        ],
                        "paper": {
                            "corpus_id": 254044469,
                            "title": "Class-aware Information for Logit-based Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Shuoxi Zhang"
                                },
                                {
                                    "authorId": "29901869",
                                    "name": "Hanpeng Liu"
                                },
                                {
                                    "authorId": "1706504",
                                    "name": "J. Hopcroft"
                                },
                                {
                                    "authorId": "1702188",
                                    "name": "Kun He"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.72900390625
                    },
                    {
                        "id": "(Ding et al., 2020)",
                        "snippets": [
                            "Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals."
                        ],
                        "paper": {
                            "corpus_id": 263789814,
                            "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
                            "authors": [
                                {
                                    "authorId": "2064424445",
                                    "name": "Fei Ding"
                                },
                                {
                                    "authorId": "2257087597",
                                    "name": "Yin Yang"
                                },
                                {
                                    "authorId": "2256778126",
                                    "name": "Hongxin Hu"
                                },
                                {
                                    "authorId": "2095713717",
                                    "name": "V. Krovi"
                                },
                                {
                                    "authorId": "2140495064",
                                    "name": "Feng Luo"
                                }
                            ],
                            "year": 2020,
                            "venue": "",
                            "n_citations": 4
                        },
                        "score": 0.58935546875
                    },
                    {
                        "id": "(Zhao et al._2, 2021)",
                        "snippets": [
                            "It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher."
                        ],
                        "paper": {
                            "corpus_id": 233210081,
                            "title": "Dual discriminator adversarial distillation for data-free model compression",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "2185595070",
                                    "name": "Milos Manic"
                                },
                                {
                                    "authorId": "46544755",
                                    "name": "Huiyu Zhou"
                                },
                                {
                                    "authorId": "145429878",
                                    "name": "Hui Yu"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Machine Learning and Cybernetics",
                            "n_citations": 20
                        },
                        "score": 0.57421875
                    },
                    {
                        "id": "(Ge et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 233407431,
                            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
                            "authors": [
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2087535968",
                                    "name": "Ching Lam Choi"
                                },
                                {
                                    "authorId": "2115477215",
                                    "name": "Xiao Zhang"
                                },
                                {
                                    "authorId": "46737362",
                                    "name": "Peipei Zhao"
                                },
                                {
                                    "authorId": "2075369514",
                                    "name": "Feng Zhu"
                                },
                                {
                                    "authorId": "145638781",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "47893312",
                                    "name": "Hongsheng Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.56103515625
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" (Wang et al., 2020), whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student."
                        ],
                        "paper": {
                            "corpus_id": 273811396,
                            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
                            "authors": [
                                {
                                    "authorId": "2163284473",
                                    "name": "Chengting Yu"
                                },
                                {
                                    "authorId": "2274194299",
                                    "name": "Fengzhao Zhang"
                                },
                                {
                                    "authorId": "2255346632",
                                    "name": "Ruizhe Chen"
                                },
                                {
                                    "authorId": "2311458018",
                                    "name": "Zuozhu Liu"
                                },
                                {
                                    "authorId": "2110408145",
                                    "name": "Shurun Tan"
                                },
                                {
                                    "authorId": "2326182494",
                                    "name": "Erping Li"
                                },
                                {
                                    "authorId": "2115787618",
                                    "name": "Aili Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Transactions on Artificial Intelligence",
                            "n_citations": 2
                        },
                        "score": 0.6953125
                    },
                    {
                        "id": "(Wang et al., 2020)",
                        "snippets": [
                            "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning."
                        ],
                        "paper": {
                            "corpus_id": 215745611,
                            "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks",
                            "authors": [
                                {
                                    "authorId": "2144734901",
                                    "name": "Lin Wang"
                                },
                                {
                                    "authorId": "51182421",
                                    "name": "Kuk-Jin Yoon"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 700
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."
                        ],
                        "paper": {
                            "corpus_id": 270389751,
                            "title": "Multistage feature fusion knowledge distillation",
                            "authors": [
                                {
                                    "authorId": "2307186661",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2307436738",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2305765306",
                                    "name": "Pengfei Lv"
                                },
                                {
                                    "authorId": "2305752302",
                                    "name": "Pan He"
                                },
                                {
                                    "authorId": "2287501699",
                                    "name": "Zheng Zhou"
                                },
                                {
                                    "authorId": "2817613",
                                    "name": "Chuanyun Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.53515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Benefits of Dark Knowledge Transfer",
                "tldr": "Dark knowledge transfer provides numerous advantages including enhanced generalization, faster convergence, improved handling of noisy labels, and the ability to transfer semantic class relationships that aren't captured in one-hot encodings. (15 sources)",
                "text": "\nThe transfer of dark knowledge from teacher to student networks offers significant benefits beyond simply creating smaller models. One of the primary advantages is enhanced generalization capability. By learning from soft labels rather than one-hot vectors, student networks inherit the teacher's ability to generalize better across diverse data <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This improved generalization stems from the implicit class relationship information embedded in the teacher's probability distributions.\n\nDark knowledge transfer also leads to faster training convergence for student networks. The soft distributions provided by the teacher yield smaller variance in gradients, which often results in more stable and expedited training <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This efficiency benefit is particularly valuable when training large networks or working with limited computational resources.\n\nAnother important advantage is the ability to refine noisy labels in the training data. The dark knowledge from teacher models can help correct inconsistencies or errors in ground-truth labels, providing a more robust training signal <Paper corpusId=\"203642142\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>. This label refinement capability makes knowledge distillation particularly valuable in scenarios where obtaining perfect ground-truth labels is challenging or prohibitively expensive.\n\nDark knowledge transfer also provides a powerful regularization effect on student networks. By requiring students to match both ground-truth labels and teacher distributions, the training process becomes more constrained and less prone to overfitting <Paper corpusId=\"233407431\" paperTitle=\"(Ge et al., 2021)\" isShortName></Paper>. This regularization helps students develop more robust decision boundaries.\n\nIn domains involving lossy compression, dark knowledge offers particularly valuable benefits. Since perfect reconstruction is rarely achievable in such tasks, learning from a teacher's outputs provides a more feasible sub-optimal solution compared to learning directly from ground-truth data <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>. The teacher effectively demonstrates achievable performance levels given specific constraints.\n\nThe benefits of dark knowledge extend beyond traditional applications, with recent research showing its effectiveness across various tasks including image classification, object detection, and even part segmentation <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"229363322\" paperTitle=\"(Touvron et al., 2020)\" isShortName></Paper> <Paper corpusId=\"245006036\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper> <Paper corpusId=\"252918735\" paperTitle=\"(Cardace et al., 2022)\" isShortName></Paper>. Remarkably, even weak teachers with lower accuracy than their students can still provide beneficial dark knowledge through regularization effects <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"219962714\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.\n\nPerhaps most importantly, dark knowledge provides semantic similarity information between different classes that cannot be captured by traditional one-hot encoded labels <Paper corpusId=\"249017724\" paperTitle=\"(Bai et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237091534\" paperTitle=\"(Ren et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. By transferring this relational understanding, student networks develop more nuanced decision boundaries that better reflect the underlying structure of the data <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>. This enables students to make more informed predictions, particularly in cases where class distinctions are subtle or overlapping.\n\nThrough these combined benefits, dark knowledge transfer creates a pathway for developing more efficient, accurate, and robust neural networks that maintain high performance despite their reduced size and computational requirements <Paper corpusId=\"251066725\" paperTitle=\"(Liang et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Nayak et al., 2019)",
                        "snippets": [
                            "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability."
                        ],
                        "paper": {
                            "corpus_id": 159041346,
                            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                            "authors": [
                                {
                                    "authorId": "143747407",
                                    "name": "Gaurav Kumar Nayak"
                                },
                                {
                                    "authorId": "2217000",
                                    "name": "Konda Reddy Mopuri"
                                },
                                {
                                    "authorId": "7155274",
                                    "name": "Vaisakh Shaj"
                                },
                                {
                                    "authorId": "144682140",
                                    "name": "R. Venkatesh Babu"
                                },
                                {
                                    "authorId": "1429640900",
                                    "name": "Anirban Chakraborty"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 245
                        },
                        "score": 0.6123046875
                    },
                    {
                        "id": "(Ge et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 233407431,
                            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
                            "authors": [
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2087535968",
                                    "name": "Ching Lam Choi"
                                },
                                {
                                    "authorId": "2115477215",
                                    "name": "Xiao Zhang"
                                },
                                {
                                    "authorId": "46737362",
                                    "name": "Peipei Zhao"
                                },
                                {
                                    "authorId": "2075369514",
                                    "name": "Feng Zhu"
                                },
                                {
                                    "authorId": "145638781",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "47893312",
                                    "name": "Hongsheng Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.56103515625
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model",
                            "The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al., 2017))."
                        ],
                        "paper": {
                            "corpus_id": 212855595,
                            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "47295143",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2269366",
                                    "name": "Xiaobo Shu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "2079682",
                                    "name": "Tingwen Liu"
                                },
                                {
                                    "authorId": "48019474",
                                    "name": "Jiapeng Zhao"
                                },
                                {
                                    "authorId": "2108645146",
                                    "name": "Quangang Li"
                                },
                                {
                                    "authorId": "48358041",
                                    "name": "Li Guo"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 35
                        },
                        "score": 0.818359375
                    },
                    {
                        "id": "(Dong et al., 2019)",
                        "snippets": [
                            "To explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels."
                        ],
                        "paper": {
                            "corpus_id": 203642142,
                            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
                            "authors": [
                                {
                                    "authorId": "145496882",
                                    "name": "Bin Dong"
                                },
                                {
                                    "authorId": "104162954",
                                    "name": "Jikai Hou"
                                },
                                {
                                    "authorId": "48518029",
                                    "name": "Yiping Lu"
                                },
                                {
                                    "authorId": "47294286",
                                    "name": "Zhihua Zhang"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 41
                        },
                        "score": 0.64013671875
                    },
                    {
                        "id": "(Cui et al., 2022)",
                        "snippets": [
                            "Compared to direct learning with labels, the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio."
                        ],
                        "paper": {
                            "corpus_id": 260447668,
                            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                            "authors": [
                                {
                                    "authorId": "2172485573",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "47093519",
                                    "name": "Jiajia Guo"
                                },
                                {
                                    "authorId": "2113999930",
                                    "name": "Zheng Cao"
                                },
                                {
                                    "authorId": "120710335",
                                    "name": "Huaze Tang"
                                },
                                {
                                    "authorId": "2257212132",
                                    "name": "Chao-Kai Wen"
                                },
                                {
                                    "authorId": "2227268421",
                                    "name": "Shi Jin"
                                },
                                {
                                    "authorId": "2288090155",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2240356164",
                                    "name": "Xiaolin Hou"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Communications",
                            "n_citations": 3
                        },
                        "score": 0.66015625
                    },
                    {
                        "id": "(Wang et al._1, 2023)",
                        "snippets": [
                            "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT (Kim et al., 2018) uses convolutional operations to transfer dark knowledge; 2) DeiT (Touvron et al., 2020) proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP (Li et al., 2021) exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD (Cardace et al., 2022) exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020). This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks."
                        ],
                        "paper": {
                            "corpus_id": 264590688,
                            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
                            "authors": [
                                {
                                    "authorId": "2186275686",
                                    "name": "Jiatai Wang"
                                },
                                {
                                    "authorId": "2244022079",
                                    "name": "Zhiwei Xu"
                                },
                                {
                                    "authorId": "2262805047",
                                    "name": "Xin Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7421875
                    },
                    {
                        "id": "(Touvron et al., 2020)",
                        "snippets": [
                            "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models."
                        ],
                        "paper": {
                            "corpus_id": 229363322,
                            "title": "Training data-efficient image transformers & distillation through attention",
                            "authors": [
                                {
                                    "authorId": "2113243762",
                                    "name": "Hugo Touvron"
                                },
                                {
                                    "authorId": "51021910",
                                    "name": "M. Cord"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                },
                                {
                                    "authorId": "1403239967",
                                    "name": "Francisco Massa"
                                },
                                {
                                    "authorId": "3469062",
                                    "name": "Alexandre Sablayrolles"
                                },
                                {
                                    "authorId": "2065248680",
                                    "name": "Herv'e J'egou"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 6805
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods."
                        ],
                        "paper": {
                            "corpus_id": 245006036,
                            "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation",
                            "authors": [
                                {
                                    "authorId": "2155120199",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2144440085",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2115657967",
                                    "name": "Yujie Wang"
                                },
                                {
                                    "authorId": "2145441956",
                                    "name": "Shanshan Zhang"
                                },
                                {
                                    "authorId": "47095791",
                                    "name": "Yichao Wu"
                                },
                                {
                                    "authorId": "152335674",
                                    "name": "Ding Liang"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 81
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cardace et al., 2022)",
                        "snippets": [
                            "Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation."
                        ],
                        "paper": {
                            "corpus_id": 252918735,
                            "title": "Self-Distillation for Unsupervised 3D Domain Adaptation",
                            "authors": [
                                {
                                    "authorId": "2131012006",
                                    "name": "Adriano Cardace"
                                },
                                {
                                    "authorId": "46297738",
                                    "name": "Riccardo Spezialetti"
                                },
                                {
                                    "authorId": "80804241",
                                    "name": "Pierluigi Zama Ramirez"
                                },
                                {
                                    "authorId": "2607607",
                                    "name": "Samuele Salti"
                                },
                                {
                                    "authorId": "9395079",
                                    "name": "L. D. Stefano"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 16
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yuan et al., 2020)",
                        "snippets": [
                            "Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization."
                        ],
                        "paper": {
                            "corpus_id": 219962714,
                            "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization",
                            "authors": [
                                {
                                    "authorId": "2087091296",
                                    "name": "Li Yuan"
                                },
                                {
                                    "authorId": "40983412",
                                    "name": "Francis E. H. Tay"
                                },
                                {
                                    "authorId": "2108696460",
                                    "name": "Guilin Li"
                                },
                                {
                                    "authorId": "2155456820",
                                    "name": "Tao Wang"
                                },
                                {
                                    "authorId": "33221685",
                                    "name": "Jiashi Feng"
                                }
                            ],
                            "year": 2020,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 527
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bai et al., 2022)",
                        "snippets": [
                            "Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted."
                        ],
                        "paper": {
                            "corpus_id": 249017724,
                            "title": "Improving the Latent Space of Image Style Transfer",
                            "authors": [
                                {
                                    "authorId": "48442720",
                                    "name": "Yun-Hao Bai"
                                },
                                {
                                    "authorId": "47073960",
                                    "name": "Cairong Wang"
                                },
                                {
                                    "authorId": "2117729099",
                                    "name": "C. Yuan"
                                },
                                {
                                    "authorId": "2140245719",
                                    "name": "Yanbo Fan"
                                },
                                {
                                    "authorId": "2144537318",
                                    "name": "Jue Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.71826171875
                    },
                    {
                        "id": "(Ren et al., 2021)",
                        "snippets": [
                            "In this way, the student network attempts to learn dark knowledge (Hinton et al., 2015) that contains the similarities between different classes, which can not be provided by the ground truth labels."
                        ],
                        "paper": {
                            "corpus_id": 237091534,
                            "title": "Online Multi-Granularity Distillation for GAN Compression",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yuxi Ren"
                                },
                                {
                                    "authorId": "2118432533",
                                    "name": "Jie Wu"
                                },
                                {
                                    "authorId": "2118724465",
                                    "name": "Xuefeng Xiao"
                                },
                                {
                                    "authorId": "1706007",
                                    "name": "Jianchao Yang"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 39
                        },
                        "score": 0.53564453125
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "Knowledge Distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features",
                            "Knowledge Distillation (KD) [8] offers a solution by enabling a compact \"student\" model to mimic a larger \"teacher\" model, allowing the student to learn from both ground-truth labels and the teacher's \"dark knowledge\" -the implicit insights not present in the ground-truth labels -enabling it to approach the teacher's performance in a compact form."
                        ],
                        "paper": {
                            "corpus_id": 265444951,
                            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268314644",
                                    "name": "Chi-Ping Su"
                                },
                                {
                                    "authorId": "1569686364",
                                    "name": "Ching-Hsun Tseng"
                                },
                                {
                                    "authorId": "2338265886",
                                    "name": "Bin Pu"
                                },
                                {
                                    "authorId": "2338506240",
                                    "name": "Lei Zhao"
                                },
                                {
                                    "authorId": "2328588941",
                                    "name": "Zhuangzhuang Chen"
                                },
                                {
                                    "authorId": "2116351339",
                                    "name": "Shin-Jye Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.56494140625
                    },
                    {
                        "id": "(Liang et al., 2022)",
                        "snippets": [
                            "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 251066725,
                            "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
                            "authors": [
                                {
                                    "authorId": "1387852255",
                                    "name": "Jiajun Liang"
                                },
                                {
                                    "authorId": "2304362074",
                                    "name": "Linze Li"
                                },
                                {
                                    "authorId": "2056410266",
                                    "name": "Z. Bing"
                                },
                                {
                                    "authorId": "2112526021",
                                    "name": "Borui Zhao"
                                },
                                {
                                    "authorId": "2179286812",
                                    "name": "Yao Tang"
                                },
                                {
                                    "authorId": "2179192518",
                                    "name": "Bo Lin"
                                },
                                {
                                    "authorId": "1934546",
                                    "name": "Haoqiang Fan"
                                }
                            ],
                            "year": 2022,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 19
                        },
                        "score": 0.7392578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Variations of Dark Knowledge",
                "tldr": "Dark knowledge transfer has been applied across diverse tasks beyond classification, including object detection, image segmentation, and multimodal learning. Various implementations have emerged, from feature-based distillation approaches to specialized techniques for handling missing modalities. (19 sources)",
                "text": "\nDark knowledge transfer has evolved beyond its initial applications, finding utility in numerous domains and specialized implementations:\n\n* **Classification Tasks**: The most common application of dark knowledge remains in image classification, where both intermediate feature embeddings and soft logits serve as the primary knowledge transferred from teacher to student networks <Paper corpusId=\"260704230\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>.\n\n* **Feature-based Distillation**: Multiple approaches focus on transferring knowledge through intermediate features rather than logits. Examples include:\n - Convolutional paraphrasing (FT) which uses convolutional operations to transform teacher's knowledge for student consumption <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"3608236\" paperTitle=\"(Kim et al., 2018)\" isShortName></Paper>.\n - Feature transform with margin ReLU, which redesigns distillation feature positions and uses partial L2 distance functions <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"102483181\" paperTitle=\"(Heo et al., 2019)\" isShortName></Paper>.\n - Activation boundary distillation, which transfers knowledge by focusing on the separating hyperplanes that determine neuron activation states <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"53213211\" paperTitle=\"(Heo et al., 2018)\" isShortName></Paper>.\n\n* **Logit-based Distillation**: These approaches focus on transferring knowledge through the model's output layers, with implementations including:\n - Self-distillation techniques where models learn from themselves <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n - Teacher-student matching with distillation tokens, as in DeiT which introduces a specific token to compute distillation loss <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"229363322\" paperTitle=\"(Touvron et al., 2020)\" isShortName></Paper>.\n\n* **Relation-based Distillation**: Beyond instance-level knowledge, some approaches transfer relationships between examples:\n - Correlation congruence methods that capture and transfer correlations between instances <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"102483463\" paperTitle=\"(Peng et al., 2019)\" isShortName></Paper>.\n - Relational knowledge distillation approaches that transfer mutual relations between data examples rather than individual activations <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"131765296\" paperTitle=\"(Park et al., 2019)\" isShortName></Paper>.\n - Similarity-preserving distillation that guides students to preserve pairwise similarities from teacher networks <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"198179476\" paperTitle=\"(Tung et al., 2019)\" isShortName></Paper>.\n\n* **Object Detection**: Dark knowledge transfer has been adapted for object detection tasks, with specialized techniques like Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) that address the unique challenges of detection compared to classification <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"245006036\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n* **Medical Imaging**: Knowledge distillation has proven particularly valuable in medical applications, especially for brain tumor segmentation where transferring knowledge from multi-modal networks to mono-modal ones helps address the clinical reality of limited modality availability <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"221543802\" paperTitle=\"(Hu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"245445463\" paperTitle=\"(Rahimpour et al., 2021)\" isShortName></Paper>.\n\n* **Multimodal Learning with Missing Modalities**: Dark knowledge transfer provides solutions for scenarios where some modalities are unavailable during inference:\n - Cross-modal distillation approaches that leverage multi-sequence MRI data for training while using only single-sequence data for inference <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"245445463\" paperTitle=\"(Rahimpour et al., 2021)\" isShortName></Paper>.\n - Learnable Cross-modal Knowledge Distillation (LCKD) that adaptively identifies important modalities and distills knowledge between them <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"263605398\" paperTitle=\"(Wang et al._2, 2023)\" isShortName></Paper>.\n - Teacher-student frameworks for Visual Question Answering with missing ground truth answers <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"233219684\" paperTitle=\"(Cho et al., 2021)\" isShortName></Paper>.\n - Transfer of \"dark knowledge\" from teacher networks trained on complete modalities to student networks trained on missing modalities <Paper corpusId=\"269362788\" paperTitle=\"(Li et al._2, 2024)\" isShortName></Paper>.\n\n* **Point Cloud Classification**: Self-distillation techniques have been applied to point cloud classification for unsupervised domain adaptation, helping to obtain discriminative feature spaces for target domains <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"252918735\" paperTitle=\"(Cardace et al., 2022)\" isShortName></Paper>.\n\n* **Sequential Knowledge Distillation**: Approaches like teacher assistants that bridge the gap between large teachers and small students when direct distillation is ineffective <Paper corpusId=\"273821996\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Hu et al., 2023)",
                        "snippets": [
                            "Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks."
                        ],
                        "paper": {
                            "corpus_id": 260704230,
                            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "11577774",
                                    "name": "Chengming Hu"
                                },
                                {
                                    "authorId": "1870442533",
                                    "name": "Xuan Li"
                                },
                                {
                                    "authorId": "2822590",
                                    "name": "Danyang Liu"
                                },
                                {
                                    "authorId": "107747459",
                                    "name": "Haolun Wu"
                                },
                                {
                                    "authorId": "2145308240",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2125037263",
                                    "name": "Ju Wang"
                                },
                                {
                                    "authorId": "2151061048",
                                    "name": "Xue Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 19
                        },
                        "score": 0.6376953125
                    },
                    {
                        "id": "(Wang et al._1, 2023)",
                        "snippets": [
                            "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT (Kim et al., 2018) uses convolutional operations to transfer dark knowledge; 2) DeiT (Touvron et al., 2020) proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP (Li et al., 2021) exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD (Cardace et al., 2022) exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020). This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks."
                        ],
                        "paper": {
                            "corpus_id": 264590688,
                            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
                            "authors": [
                                {
                                    "authorId": "2186275686",
                                    "name": "Jiatai Wang"
                                },
                                {
                                    "authorId": "2244022079",
                                    "name": "Zhiwei Xu"
                                },
                                {
                                    "authorId": "2262805047",
                                    "name": "Xin Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7421875
                    },
                    {
                        "id": "(Kim et al., 2018)",
                        "snippets": [
                            "Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods."
                        ],
                        "paper": {
                            "corpus_id": 3608236,
                            "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer",
                            "authors": [
                                {
                                    "authorId": "49476045",
                                    "name": "Jangho Kim"
                                },
                                {
                                    "authorId": "35869519",
                                    "name": "Seonguk Park"
                                },
                                {
                                    "authorId": "3160425",
                                    "name": "Nojun Kwak"
                                }
                            ],
                            "year": 2018,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 551
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features (Heo et al., 2019)(Heo et al., 2018)(Kim et al., 2018)(Park et al., 2019)(Peng et al., 2019)39,(Tung et al., 2019)43,(Yim et al., 2017)[73] and distillation from logits (Cho et al., 2019)(Furlanello et al., 2018)(Mirzadeh et al., 2019)(Yang et al., 2018)(Zhao et al., 2016). Many studies (Cho et al., 2021)(Hu et al., 2020)(Rahimpour et al., 2021)21,(Wang et al., 2023)[51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities."
                        ],
                        "paper": {
                            "corpus_id": 273821996,
                            "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
                            "authors": [
                                {
                                    "authorId": "2188978724",
                                    "name": "Mingcheng Li"
                                },
                                {
                                    "authorId": "2143920085",
                                    "name": "Dingkang Yang"
                                },
                                {
                                    "authorId": "2290474772",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2108655879",
                                    "name": "Shunli Wang"
                                },
                                {
                                    "authorId": "2279095231",
                                    "name": "Jiawei Chen"
                                },
                                {
                                    "authorId": "2186872968",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "2290653228",
                                    "name": "Jinjie Wei"
                                },
                                {
                                    "authorId": "2278886236",
                                    "name": "Yue Jiang"
                                },
                                {
                                    "authorId": "2303841599",
                                    "name": "Qingyao Xu"
                                },
                                {
                                    "authorId": "2298268194",
                                    "name": "Xiaolu Hou"
                                },
                                {
                                    "authorId": "2216487730",
                                    "name": "Mingyang Sun"
                                },
                                {
                                    "authorId": "2202592845",
                                    "name": "Ziyun Qian"
                                },
                                {
                                    "authorId": "2218981837",
                                    "name": "Dongliang Kou"
                                },
                                {
                                    "authorId": "2278978362",
                                    "name": "Lihua Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1
                        },
                        "score": 0.6357421875
                    },
                    {
                        "id": "(Heo et al., 2019)",
                        "snippets": [
                            "We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page."
                        ],
                        "paper": {
                            "corpus_id": 102483181,
                            "title": "A Comprehensive Overhaul of Feature Distillation",
                            "authors": [
                                {
                                    "authorId": "3086596",
                                    "name": "Byeongho Heo"
                                },
                                {
                                    "authorId": "47965071",
                                    "name": "Jeesoo Kim"
                                },
                                {
                                    "authorId": "2151587",
                                    "name": "Sangdoo Yun"
                                },
                                {
                                    "authorId": "46904404",
                                    "name": "Hyojin Park"
                                },
                                {
                                    "authorId": "3160425",
                                    "name": "Nojun Kwak"
                                },
                                {
                                    "authorId": "46174575",
                                    "name": "J. Choi"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 584
                        },
                        "score": 0
                    },
                    {
                        "id": "(Heo et al., 2018)",
                        "snippets": [
                            "An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classificationfriendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art."
                        ],
                        "paper": {
                            "corpus_id": 53213211,
                            "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons",
                            "authors": [
                                {
                                    "authorId": "3086596",
                                    "name": "Byeongho Heo"
                                },
                                {
                                    "authorId": "2646766",
                                    "name": "Minsik Lee"
                                },
                                {
                                    "authorId": "2151587",
                                    "name": "Sangdoo Yun"
                                },
                                {
                                    "authorId": "46174575",
                                    "name": "J. Choi"
                                }
                            ],
                            "year": 2018,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 527
                        },
                        "score": 0
                    },
                    {
                        "id": "(Furlanello et al., 2018)",
                        "snippets": [
                            "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                        ],
                        "paper": {
                            "corpus_id": 4110009,
                            "title": "Born Again Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2067208583",
                                    "name": "Tommaso Furlanello"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "143902495",
                                    "name": "Michael Tschannen"
                                },
                                {
                                    "authorId": "7326223",
                                    "name": "L. Itti"
                                },
                                {
                                    "authorId": "2047844",
                                    "name": "Anima Anandkumar"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1034
                        },
                        "score": 0
                    },
                    {
                        "id": "(Touvron et al., 2020)",
                        "snippets": [
                            "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models."
                        ],
                        "paper": {
                            "corpus_id": 229363322,
                            "title": "Training data-efficient image transformers & distillation through attention",
                            "authors": [
                                {
                                    "authorId": "2113243762",
                                    "name": "Hugo Touvron"
                                },
                                {
                                    "authorId": "51021910",
                                    "name": "M. Cord"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                },
                                {
                                    "authorId": "1403239967",
                                    "name": "Francisco Massa"
                                },
                                {
                                    "authorId": "3469062",
                                    "name": "Alexandre Sablayrolles"
                                },
                                {
                                    "authorId": "2065248680",
                                    "name": "Herv'e J'egou"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 6805
                        },
                        "score": 0
                    },
                    {
                        "id": "(Peng et al., 2019)",
                        "snippets": [
                            "Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods."
                        ],
                        "paper": {
                            "corpus_id": 102483463,
                            "title": "Correlation Congruence for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "153916114",
                                    "name": "Baoyun Peng"
                                },
                                {
                                    "authorId": "153104092",
                                    "name": "Xiao Jin"
                                },
                                {
                                    "authorId": "2108421213",
                                    "name": "Jiaheng Liu"
                                },
                                {
                                    "authorId": "5454120",
                                    "name": "Shunfeng Zhou"
                                },
                                {
                                    "authorId": "47095791",
                                    "name": "Yichao Wu"
                                },
                                {
                                    "authorId": "2146400394",
                                    "name": "Yu Liu"
                                },
                                {
                                    "authorId": "144032853",
                                    "name": "Dongsheng Li"
                                },
                                {
                                    "authorId": "2362362",
                                    "name": "Zhaoning Zhang"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 513
                        },
                        "score": 0
                    },
                    {
                        "id": "(Park et al., 2019)",
                        "snippets": [
                            "Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets."
                        ],
                        "paper": {
                            "corpus_id": 131765296,
                            "title": "Relational Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "107950764",
                                    "name": "Wonpyo Park"
                                },
                                {
                                    "authorId": "2111840349",
                                    "name": "Dongju Kim"
                                },
                                {
                                    "authorId": "144574822",
                                    "name": "Yan Lu"
                                },
                                {
                                    "authorId": "72643925",
                                    "name": "Minsu Cho"
                                }
                            ],
                            "year": 2019,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1424
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tung et al., 2019)",
                        "snippets": [
                            "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."
                        ],
                        "paper": {
                            "corpus_id": 198179476,
                            "title": "Similarity-Preserving Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2065573607",
                                    "name": "Frederick Tung"
                                },
                                {
                                    "authorId": "10771328",
                                    "name": "Greg Mori"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 981
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods."
                        ],
                        "paper": {
                            "corpus_id": 245006036,
                            "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation",
                            "authors": [
                                {
                                    "authorId": "2155120199",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2144440085",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2115657967",
                                    "name": "Yujie Wang"
                                },
                                {
                                    "authorId": "2145441956",
                                    "name": "Shanshan Zhang"
                                },
                                {
                                    "authorId": "47095791",
                                    "name": "Yichao Wu"
                                },
                                {
                                    "authorId": "152335674",
                                    "name": "Ding Liang"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 81
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hu et al., 2020)",
                        "snippets": [
                            "The joint use of multiple imaging modalities for medical image segmentation has been widely studied in recent years. The fusion of information from different modalities has demonstrated to improve the segmentation accuracy, with respect to mono-modal segmentations, in several applications. However, acquiring multiple modalities is usually not possible in a clinical setting due to a limited number of physicians and scanners, and to limit costs and scan time. Most of the time, only one modality is acquired. In this paper, we propose KD-Net, a framework to transfer knowledge from a trained multi-modal network (teacher) to a mono-modal one (student). The proposed method is an adaptation of the generalized distillation framework where the student network is trained on a subset (1 modality) of the teacher\u2019s inputs (n modalities). We illustrate the effectiveness of the proposed framework in brain tumor segmentation with the BraTS 2018 dataset. Using different architectures, we show that the student network effectively learns from the teacher and always outperforms the baseline mono-modal network in terms of segmentation accuracy."
                        ],
                        "paper": {
                            "corpus_id": 221543802,
                            "title": "Knowledge Distillation from Multi-modal to Mono-modal Segmentation Networks",
                            "authors": [
                                {
                                    "authorId": "2145917899",
                                    "name": "Minhao Hu"
                                },
                                {
                                    "authorId": "1987237543",
                                    "name": "Matthis Maillard"
                                },
                                {
                                    "authorId": "2129507184",
                                    "name": "Ya Zhang"
                                },
                                {
                                    "authorId": "1987210827",
                                    "name": "Tommaso Ciceri"
                                },
                                {
                                    "authorId": "146771548",
                                    "name": "Giammarco La Barbera"
                                },
                                {
                                    "authorId": "1695917",
                                    "name": "I. Bloch"
                                },
                                {
                                    "authorId": "1742163277",
                                    "name": "P. Gori"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                            "n_citations": 129
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rahimpour et al., 2021)",
                        "snippets": [
                            "Convolutional neural networks (CNNs) for brain tumor segmentation are generally developed using complete sets of magnetic resonance imaging (MRI) sequences for both training and inference. As such, these algorithms are not trained for realistic, clinical scenarios where parts of the MRI sequences which were used for training, are missing during inference. To increase clinical applicability, we proposed a cross-modal distillation approach to leverage the availability of multi-sequence MRI data for training and generate an enriched CNN model which uses only single-sequence MRI data for inference but outperforms a single-sequence CNN model. We assessed the performance of the proposed method for whole tumor and tumor core segmentation with multi-sequence MRI data available for training but only <inline-formula><tex-math notation=\"LaTeX\">$T_{1}$</tex-math></inline-formula>-weighted (<inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula>) sequence data available for inference, using BraTS 2018, and in-house datasets. Results showed that cross-modal distillation significantly improved the Dice score for both whole tumor and tumor core segmentation when only <inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula> sequence data were available for inference. For the evaluation using the in-house dataset, cross-modal distillation achieved an average Dice score of 79.04% and 69.39% for whole tumor and tumor core segmentation, respectively, while a single-sequence U-Net model using <inline-formula><tex-math notation=\"LaTeX\">$T_{\\text{1}w}$</tex-math></inline-formula> sequence data for both training and inference achieved an average Dice score of 73.60% and 62.62%, respectively. These findings confirmed cross-modal distillation as an effective method to increase the potential of single-sequence CNN models such that segmentation performance is less compromised by missing MRI sequences or having only one MRI sequence available for segmentation."
                        ],
                        "paper": {
                            "corpus_id": 245445463,
                            "title": "Cross-Modal Distillation to Improve MRI-Based Brain Tumor Segmentation With Missing MRI Sequences",
                            "authors": [
                                {
                                    "authorId": "32163043",
                                    "name": "Masoomeh Rahimpour"
                                },
                                {
                                    "authorId": "27011321",
                                    "name": "J. Bertels"
                                },
                                {
                                    "authorId": "152985178",
                                    "name": "A. Radwan"
                                },
                                {
                                    "authorId": "2135672010",
                                    "name": "Henri Vandermeulen"
                                },
                                {
                                    "authorId": "2680672",
                                    "name": "S. Sunaert"
                                },
                                {
                                    "authorId": "143908467",
                                    "name": "D. Vandermeulen"
                                },
                                {
                                    "authorId": "1709659",
                                    "name": "F. Maes"
                                },
                                {
                                    "authorId": "46523475",
                                    "name": "K. Goffin"
                                },
                                {
                                    "authorId": "2502283",
                                    "name": "M. Koole"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Biomedical Engineering",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al._2, 2023)",
                        "snippets": [
                            "The problem of missing modalities is both critical and non-trivial to be handled in multi-modal models. It is common for multi-modal tasks that certain modalities contribute more compared to other modalities, and if those important modalities are missing, the model performance drops significantly. Such fact remains unexplored by current multi-modal approaches that recover the representation from missing modalities by feature reconstruction or blind feature aggregation from other modalities, instead of extracting useful information from the best performing modalities. In this paper, we propose a Learnable Cross-modal Knowledge Distillation (LCKD) model to adaptively identify important modalities and distil knowledge from them to help other modalities from the cross-modal perspective for solving the missing modality issue. Our approach introduces a teacher election procedure to select the most ``qualified'' teachers based on their single modality performance on certain tasks. Then, cross-modal knowledge distillation is performed between teacher and student modalities for each task to push the model parameters to a point that is beneficial for all tasks. Hence, even if the teacher modalities for certain tasks are missing during testing, the available student modalities can accomplish the task well enough based on the learned knowledge from their automatically elected teacher modalities. Experiments on the Brain Tumour Segmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods by a considerable margin, improving the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score."
                        ],
                        "paper": {
                            "corpus_id": 263605398,
                            "title": "Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality",
                            "authors": [
                                {
                                    "authorId": "2253875173",
                                    "name": "Hu Wang"
                                },
                                {
                                    "authorId": "50581035",
                                    "name": "Yuanhong Chen"
                                },
                                {
                                    "authorId": "2132565465",
                                    "name": "Congbo Ma"
                                },
                                {
                                    "authorId": "2179001232",
                                    "name": "Jodie Avery"
                                },
                                {
                                    "authorId": "2178996607",
                                    "name": "Louise Hull"
                                },
                                {
                                    "authorId": "2253394660",
                                    "name": "Gustavo Carneiro"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                            "n_citations": 33
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cho et al., 2021)",
                        "snippets": [
                            "In this work, we address the issues of the missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality\u2013the ground truth answers\u2013that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the \"Big\" Teacher, that takes the image/question/answer triplet as its input and out-performs the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performance of our method and a diverse possibility for future research."
                        ],
                        "paper": {
                            "corpus_id": 233219684,
                            "title": "Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2191802",
                                    "name": "Jae-Won Cho"
                                },
                                {
                                    "authorId": "40622539",
                                    "name": "Dong-Jin Kim"
                                },
                                {
                                    "authorId": "2118888090",
                                    "name": "Jinsoo Choi"
                                },
                                {
                                    "authorId": "2116160249",
                                    "name": "Yunjae Jung"
                                },
                                {
                                    "authorId": "98758720",
                                    "name": "I. Kweon"
                                }
                            ],
                            "year": 2021,
                            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                            "n_citations": 17
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._2, 2024)",
                        "snippets": [
                            "The core concept of these efforts is to transfer \"dark knowledge\" from teacher networks trained by complete modalities to student networks trained by missing modalities.The teacher model typically produces more valuable feature presentations than the student model."
                        ],
                        "paper": {
                            "corpus_id": 269362788,
                            "title": "Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities",
                            "authors": [
                                {
                                    "authorId": "2188978724",
                                    "name": "Mingcheng Li"
                                },
                                {
                                    "authorId": "2143920085",
                                    "name": "Dingkang Yang"
                                },
                                {
                                    "authorId": "2212046663",
                                    "name": "Xiao Zhao"
                                },
                                {
                                    "authorId": "2186872968",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "2298314652",
                                    "name": "Yan Wang"
                                },
                                {
                                    "authorId": "2288061973",
                                    "name": "Kun Yang"
                                },
                                {
                                    "authorId": "2216487730",
                                    "name": "Mingyang Sun"
                                },
                                {
                                    "authorId": "2218981837",
                                    "name": "Dongliang Kou"
                                },
                                {
                                    "authorId": "2202592845",
                                    "name": "Ziyun Qian"
                                },
                                {
                                    "authorId": "2278978362",
                                    "name": "Lihua Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 12
                        },
                        "score": 0.62451171875
                    },
                    {
                        "id": "(Cardace et al., 2022)",
                        "snippets": [
                            "Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation."
                        ],
                        "paper": {
                            "corpus_id": 252918735,
                            "title": "Self-Distillation for Unsupervised 3D Domain Adaptation",
                            "authors": [
                                {
                                    "authorId": "2131012006",
                                    "name": "Adriano Cardace"
                                },
                                {
                                    "authorId": "46297738",
                                    "name": "Riccardo Spezialetti"
                                },
                                {
                                    "authorId": "80804241",
                                    "name": "Pierluigi Zama Ramirez"
                                },
                                {
                                    "authorId": "2607607",
                                    "name": "Samuele Salti"
                                },
                                {
                                    "authorId": "9395079",
                                    "name": "L. D. Stefano"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 16
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.175371
    }
}