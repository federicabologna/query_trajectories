{
    "query": "Analyze the sequential stages by which language models acquire linguistic capabilities during training, especially the progression from syntactic to semantic knowledge, and describe how these transitions are manifested and measured.",
    "user_id": "lib_user",
    "task_id": "24cafe6e-0f21-4e3a-90f6-9c057a5313db",
    "timestamp": "2025-06-23T23:53:54.816950",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 39,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.38015699999999997,
    "decomposed_query": {
        "rewritten_query": "Sequential stages by which language models acquire linguistic capabilities during training, especially the progression from syntactic to semantic knowledge, and how these transitions are manifested and measured.",
        "keyword_query": "language models linguistic capabilities training progression syntactic semantic knowledge transitions manifested measured",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.009984,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342412237",
                    "name": "Tatsuro Inaba"
                },
                {
                    "authorId": "2261744024",
                    "name": "Kentaro Inui"
                },
                {
                    "authorId": "2302320083",
                    "name": "Yusuke Miyao"
                },
                {
                    "authorId": "50856622",
                    "name": "Yohei Oseki"
                },
                {
                    "authorId": "2266692",
                    "name": "Benjamin Heinzerling"
                },
                {
                    "authorId": "2310230516",
                    "name": "Yu Takagi"
                }
            ],
            "abstract": "Large Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
            "corpus_id": 276902722,
            "sentences": [
                {
                    "corpus_id": "276902722",
                    "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
                    "text": "Large Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
                    "score": 0.5515748139319179,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                },
                {
                    "corpus_id": "276902722",
                    "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
                    "text": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, from multilingual translation to advanced semantic understanding (Bubeck et al., 2023). As these models become increasingly complex and widespread, the need to understand their internal mechanisms has grown significantly. This has fueled a surge of research aimed at interpreting their mechanisms and decision-making processes, leading to intriguing insights into their behavior (Casper et al., 2023;Bereska and Gavves, 2024). \n\nHowever, fundamental questions regarding how LLMs acquire and develop these capabilities remain poorly understood. For instance, do LLMs learn language-specific concepts independently, or do they simultaneously acquire cross-lingual concepts that generalize across languages? Similarly, is there a prioritization in learning low-level, tokenspecific features versus high-level, abstract concepts? \n\nIn this work, we address this gap by analyzing how the information encoded in the internal representations of LLMs evolves over time. Specifically, we employ sparse autoencoders (SAEs) (Bricken et al., 2023;Huben et al., 2024) to analyze the hidden representations from multiple checkpoints of a large language model. By examining the distribution of SAE features at each checkpoint, we identify the types of information the model encodes at different training stages of its development (see Figure 1). \n\nOur experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities.",
                    "score": 0.4267210424970997,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 945
                        },
                        {
                            "start": 948,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1865
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 520,
                            "end": 545,
                            "matchedPaperCorpusId": "269293418"
                        },
                        {
                            "start": 1155,
                            "end": 1174,
                            "matchedPaperCorpusId": "261934663"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders\n# Venue: arXiv.org\n# Authors: Tatsuro Inaba, Kentaro Inui, Yusuke Miyao, Yohei Oseki, Benjamin Heinzerling, Yu Takagi\n## Abstract\nLarge Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.\n## Introduction\nLarge Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, from multilingual translation to advanced semantic understanding (Bubeck et al., 2023). As these models become increasingly complex and widespread, the need to understand their internal mechanisms has grown significantly. This has fueled a surge of research aimed at interpreting their mechanisms and decision-making processes, leading to intriguing insights into their behavior (Casper et al., 2023;Bereska and Gavves, 2024). \n\nHowever, fundamental questions regarding how LLMs acquire and develop these capabilities remain poorly understood. For instance, do LLMs learn language-specific concepts independently, or do they simultaneously acquire cross-lingual concepts that generalize across languages? Similarly, is there a prioritization in learning low-level, tokenspecific features versus high-level, abstract concepts? \n\nIn this work, we address this gap by analyzing how the information encoded in the internal representations of LLMs evolves over time. Specifically, we employ sparse autoencoders (SAEs) (Bricken et al., 2023;Huben et al., 2024) to analyze the hidden representations from multiple checkpoints of a large language model. By examining the distribution of SAE features at each checkpoint, we identify the types of information the model encodes at different training stages of its development (see Figure 1). \n\nOur experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities.",
            "reference_string": "[276902722 | Inaba et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 67,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.568.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.06096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41019330",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "94064232",
                    "name": "Guy Hacohen"
                },
                {
                    "authorId": "1789171",
                    "name": "D. Weinshall"
                },
                {
                    "authorId": "2769805",
                    "name": "Omri Abend"
                }
            ],
            "abstract": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
            "corpus_id": 237491997,
            "sentences": [
                {
                    "corpus_id": "237491997",
                    "title": "The Grammar-Learning Trajectories of Neural Language Models",
                    "text": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
                    "score": 0.4268795056642635,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: The Grammar-Learning Trajectories of Neural Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Leshem Choshen, Guy Hacohen, D. Weinshall, Omri Abend\n## Abstract\nThe learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.\n",
            "reference_string": "[237491997 | Choshen et al. | 2021 | Citations: 29]"
        },
        {
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1416353805",
                    "name": "Max M\u00fcller-Eberstein"
                },
                {
                    "authorId": "3449407",
                    "name": "Rob van der Goot"
                },
                {
                    "authorId": "2256991041",
                    "name": "Barbara Plank"
                },
                {
                    "authorId": "2261492323",
                    "name": "Ivan Titov"
                }
            ],
            "abstract": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
            "corpus_id": 264451507,
            "sentences": [
                {
                    "corpus_id": "264451507",
                    "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
                    "text": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
                    "score": 0.5082029162209087,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                },
                {
                    "corpus_id": "264451507",
                    "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
                    "text": "Contemporary advances in NLP are built on the representational power of latent embedding spaces learned by self-supervised language models (LMs). At their core, these approaches are built on the distributional hypothesis (Harris, 1954;Firth, 1957), for which the effects of scale have been implicitly and explicitly studied via the community's use of increasingly large models and datasets (Teehan et al., 2022;Wei et al., 2022). The learning dynamics by which these capabilities emerge during LM pre-training have, however, remained largely understudied. Understanding how and when the  LM training objective begins to encode information that is relevant to downstream tasks is crucial, as this informs the limits of what can be learned using current objectives.",
                    "score": 0.4032821451586132,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 763
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 235,
                            "end": 247,
                            "matchedPaperCorpusId": "208093066"
                        },
                        {
                            "start": 390,
                            "end": 411,
                            "matchedPaperCorpusId": "247656607"
                        },
                        {
                            "start": 411,
                            "end": 428,
                            "matchedPaperCorpusId": "249674500"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88525390625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Max M\u00fcller-Eberstein, Rob van der Goot, Barbara Plank, Ivan Titov\n## Abstract\nRepresentational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.\n## Introduction\nContemporary advances in NLP are built on the representational power of latent embedding spaces learned by self-supervised language models (LMs). At their core, these approaches are built on the distributional hypothesis (Harris, 1954;Firth, 1957), for which the effects of scale have been implicitly and explicitly studied via the community's use of increasingly large models and datasets (Teehan et al., 2022;Wei et al., 2022). The learning dynamics by which these capabilities emerge during LM pre-training have, however, remained largely understudied. Understanding how and when the  LM training objective begins to encode information that is relevant to downstream tasks is crucial, as this informs the limits of what can be learned using current objectives.",
            "reference_string": "[264451507 | Muller-Eberstein et al. | 2023 | Citations: 10]"
        },
        {
            "title": "Language Models Learn POS First",
            "venue": "BlackboxNLP@EMNLP",
            "year": 2018,
            "reference_count": 7,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W18-5438.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W18-5438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                },
                {
                    "authorId": "144871732",
                    "name": "Adam Lopez"
                }
            ],
            "abstract": "A glut of recent research shows that language models capture linguistic structure. Such work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
            "corpus_id": 53633715,
            "sentences": [
                {
                    "corpus_id": "53633715",
                    "title": "Language Models Learn POS First",
                    "text": "A glut of recent research shows that language models capture linguistic structure. Such work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
                    "score": 0.48946290874603027,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                },
                {
                    "corpus_id": "53633715",
                    "title": "Language Models Learn POS First",
                    "text": "A glut of recent research shows that language models capture linguistic structure. Linzen et al. (2016) found that LSTM-based language models may encode syntactic information sufficient to favor verbs which match the number of their subject nouns. Liu et al. (2018) suggested that the high performance of LSTMs may depend on the linguistic structure of the input data, as performance on several artificial tasks was higher with natural language data than with artificial sequential data.\n\nSuch work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
                    "score": 0.47057161468340075,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.96142578125,
            "relevance_judgment_input_expanded": "# Title: Language Models Learn POS First\n# Venue: BlackboxNLP@EMNLP\n# Authors: Naomi Saphra, Adam Lopez\n## Abstract\nA glut of recent research shows that language models capture linguistic structure. Such work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.\n## Introduction\nA glut of recent research shows that language models capture linguistic structure. Linzen et al. (2016) found that LSTM-based language models may encode syntactic information sufficient to favor verbs which match the number of their subject nouns. Liu et al. (2018) suggested that the high performance of LSTMs may depend on the linguistic structure of the input data, as performance on several artificial tasks was higher with natural language data than with artificial sequential data.\n\nSuch work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
            "reference_string": "[53633715 | Saphra et al. | 2018 | Citations: 12]"
        },
        {
            "title": "Data Proportion Detection for Optimized Data Management for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303856806",
                    "name": "Hao Liang"
                },
                {
                    "authorId": "2320126993",
                    "name": "Keshi Zhao"
                },
                {
                    "authorId": "2323000103",
                    "name": "Yajie Yang"
                },
                {
                    "authorId": "2260817839",
                    "name": "Bin Cui"
                },
                {
                    "authorId": "2242124536",
                    "name": "Guosheng Dong"
                },
                {
                    "authorId": "2183768304",
                    "name": "Zenan Zhou"
                },
                {
                    "authorId": "2309265357",
                    "name": "Wentao Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks and domains, with data preparation playing a critical role in achieving these results. Pre-training data typically combines information from multiple domains. To maximize performance when integrating data from various domains, determining the optimal data proportion is essential. However, state-of-the-art (SOTA) LLMs rarely disclose details about their pre-training data, making it difficult for researchers to identify ideal data proportions. In this paper, we introduce a new topic, \\textit{data proportion detection}, which enables the automatic estimation of pre-training data proportions by analyzing the generated outputs of LLMs. We provide rigorous theoretical proofs, practical algorithms, and preliminary experimental results for data proportion detection. Based on these findings, we offer valuable insights into the challenges and future directions for effective data proportion detection and data management.",
            "corpus_id": 272910877,
            "sentences": [
                {
                    "corpus_id": "272910877",
                    "title": "Data Proportion Detection for Optimized Data Management for Large Language Models",
                    "text": "The pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts. \n\nIn the pre-training of large language models (LLMs), the goal is to predict the next token in a sequence by minimizing the negative log-likelihood of the predicted token given its context. The loss function for a sequence  0 ,  1 , . . . ,   is: \n\nThis function measures how well the model predicts each token given the preceding tokens, and by minimizing this loss, the model learns to generate contextually appropriate sequences. Attention mechanisms, particularly Transformer architectures, are employed to capture long-range dependencies in the data. \n\nScaling Laws and Data Quality: Scaling Laws describe the relationship between the size of the model, the amount of data, and performance [11,12]. As the number of parameters in the model increases, or as more data is introduced, the model's performance improves, but with diminishing returns at a certain point. This creates a challenge where simply increasing model size or dataset size does not guarantee proportionate gains in performance. However, high-quality data can break Scaling Laws by boosting model efficiency beyond what is expected from quantity alone [21]. \n\nOne key factor is data quality-carefully curated, diverse, and representative datasets lead to more efficient learning. High-quality data allows models to generalize better, even with fewer parameters or less training data. Optimal data proportioning across domains also plays a critical role in this process. For example, selecting the right mix of domain-specific and general data ensures that the model is not only learning from the most relevant examples but also retains a broad understanding of language use cases.",
                    "score": 0.43690237587476766,
                    "section_title": "PRELIMINARY 2.1 Pre-training Stage in LLMs",
                    "char_start_offset": 3427,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 324
                        },
                        {
                            "start": 325,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1298
                        },
                        {
                            "start": 1301,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1612
                        },
                        {
                            "start": 1613,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1872
                        },
                        {
                            "start": 1875,
                            "end": 1994
                        },
                        {
                            "start": 1995,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2184
                        },
                        {
                            "start": 2185,
                            "end": 2395
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1867,
                            "end": 1871,
                            "matchedPaperCorpusId": "250113273"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95556640625
                }
            ],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: Data Proportion Detection for Optimized Data Management for Large Language Models\n# Venue: arXiv.org\n# Authors: Hao Liang, Keshi Zhao, Yajie Yang, Bin Cui, Guosheng Dong, Zenan Zhou, Wentao Zhang\n## Abstract\nLarge language models (LLMs) have demonstrated exceptional performance across a wide range of tasks and domains, with data preparation playing a critical role in achieving these results. Pre-training data typically combines information from multiple domains. To maximize performance when integrating data from various domains, determining the optimal data proportion is essential. However, state-of-the-art (SOTA) LLMs rarely disclose details about their pre-training data, making it difficult for researchers to identify ideal data proportions. In this paper, we introduce a new topic, \\textit{data proportion detection}, which enables the automatic estimation of pre-training data proportions by analyzing the generated outputs of LLMs. We provide rigorous theoretical proofs, practical algorithms, and preliminary experimental results for data proportion detection. Based on these findings, we offer valuable insights into the challenges and future directions for effective data proportion detection and data management.\n## PRELIMINARY 2.1 Pre-training Stage in LLMs\nThe pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts. \n\nIn the pre-training of large language models (LLMs), the goal is to predict the next token in a sequence by minimizing the negative log-likelihood of the predicted token given its context. The loss function for a sequence  0 ,  1 , . . . ,   is: \n\nThis function measures how well the model predicts each token given the preceding tokens, and by minimizing this loss, the model learns to generate contextually appropriate sequences. Attention mechanisms, particularly Transformer architectures, are employed to capture long-range dependencies in the data. \n\nScaling Laws and Data Quality: Scaling Laws describe the relationship between the size of the model, the amount of data, and performance [11,12]. As the number of parameters in the model increases, or as more data is introduced, the model's performance improves, but with diminishing returns at a certain point. This creates a challenge where simply increasing model size or dataset size does not guarantee proportionate gains in performance. However, high-quality data can break Scaling Laws by boosting model efficiency beyond what is expected from quantity alone [21]. \n\nOne key factor is data quality-carefully curated, diverse, and representative datasets lead to more efficient learning. High-quality data allows models to generalize better, even with fewer parameters or less training data. Optimal data proportioning across domains also plays a critical role in this process. For example, selecting the right mix of domain-specific and general data ensures that the model is not only learning from the most relevant examples but also retains a broad understanding of language use cases.",
            "reference_string": "[272910877 | Liang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Language acquisition: do children and language models follow similar learning stages?",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.03586",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.03586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219624467",
                    "name": "Linnea Evanson"
                },
                {
                    "authorId": "3051598",
                    "name": "Yair Lakretz"
                },
                {
                    "authorId": "28972230",
                    "name": "J. King"
                }
            ],
            "abstract": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.",
            "corpus_id": 259089351,
            "sentences": [
                {
                    "corpus_id": "259089351",
                    "title": "Language acquisition: do children and language models follow similar learning stages?",
                    "text": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.",
                    "score": 0.5970661685834558,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94970703125
                },
                {
                    "corpus_id": "259089351",
                    "title": "Language acquisition: do children and language models follow similar learning stages?",
                    "text": "For both of these approaches, however, the measures from children at such early ages can be noisy and fragmented. \n\nInterestingly, these issues do not apply to modern language models. Deep learning architectures trained to predict words from their proximal contexts have proved immensely effective at learning to process natural language (Radford et al., 2019;Devlin et al., 2019). Unlike humans, these algorithms can be easily probed during training, at any time point and rate, and with unlimited number of test stimuli, without interfering with their language acquisition (Jawahar et al., 2019;Manning et al., 2020;Bowman and Dahl, 2021). Furthermore, high-performing deep nets have been shown to implicitly (Lakretz et al., 2019;Gulordava et al., 2018) or explicitly learn to represent and use syntactic structures (Manning et al., 2020), as well as to use features such as concreteness and lexical class to learn language (Chang and Bergen, 2022). Finally, and importantly, these deep neural networks have recently been shown to represent lexical, syntactic and compositional representations similarly to the adult brain (Jain and Huth, 2018;Toneva and Wehbe, 2019;Caucheteux and King, 2022;Pasquiou et al., 2022Pasquiou et al., , 2023;;Caucheteux et al., 2023). Evidencing similar learning trajectories in children and language models could thus provide an invaluable framework to better understand the computational principles underlying language acquisition. \n\nHere, we compare the trajectory of language acquisition between human children and modern language models. We focus on three main questions. First, do these models learn linguistic skills in a systematic order? Second, is this trajectory sequential or parallel? Third, is this trajectory similar to that of children? These hypotheses are illustrated in Figure 1. \n\nSpecifically, we train 48 GPT-2 architectures (Radford et al., 2019) from scratch, using a standard next-word prediction objective.",
                    "score": 0.4278303444784689,
                    "section_title": "Introduction",
                    "char_start_offset": 1909,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 116,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1466
                        },
                        {
                            "start": 1469,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1785
                        },
                        {
                            "start": 1786,
                            "end": 1831
                        },
                        {
                            "start": 1834,
                            "end": 1965
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 360,
                            "end": 380,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 575,
                            "end": 597,
                            "matchedPaperCorpusId": "195477534"
                        },
                        {
                            "start": 597,
                            "end": 618,
                            "matchedPaperCorpusId": "219315567"
                        },
                        {
                            "start": 618,
                            "end": 640,
                            "matchedPaperCorpusId": "233033916"
                        },
                        {
                            "start": 733,
                            "end": 756,
                            "matchedPaperCorpusId": "4460159"
                        },
                        {
                            "start": 819,
                            "end": 841,
                            "matchedPaperCorpusId": "219315567"
                        },
                        {
                            "start": 927,
                            "end": 951,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1126,
                            "end": 1147,
                            "matchedPaperCorpusId": "53983498"
                        },
                        {
                            "start": 1147,
                            "end": 1170,
                            "matchedPaperCorpusId": "167217728"
                        },
                        {
                            "start": 1170,
                            "end": 1196,
                            "matchedPaperCorpusId": "246902471"
                        },
                        {
                            "start": 1196,
                            "end": 1217,
                            "matchedPaperCorpusId": "250334501"
                        },
                        {
                            "start": 1217,
                            "end": 1242,
                            "matchedPaperCorpusId": "257232607"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85498046875
                }
            ],
            "relevance_judgement": 0.94970703125,
            "relevance_judgment_input_expanded": "# Title: Language acquisition: do children and language models follow similar learning stages?\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Linnea Evanson, Yair Lakretz, J. King\n## Abstract\nDuring language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.\n## Introduction\nFor both of these approaches, however, the measures from children at such early ages can be noisy and fragmented. \n\nInterestingly, these issues do not apply to modern language models. Deep learning architectures trained to predict words from their proximal contexts have proved immensely effective at learning to process natural language (Radford et al., 2019;Devlin et al., 2019). Unlike humans, these algorithms can be easily probed during training, at any time point and rate, and with unlimited number of test stimuli, without interfering with their language acquisition (Jawahar et al., 2019;Manning et al., 2020;Bowman and Dahl, 2021). Furthermore, high-performing deep nets have been shown to implicitly (Lakretz et al., 2019;Gulordava et al., 2018) or explicitly learn to represent and use syntactic structures (Manning et al., 2020), as well as to use features such as concreteness and lexical class to learn language (Chang and Bergen, 2022). Finally, and importantly, these deep neural networks have recently been shown to represent lexical, syntactic and compositional representations similarly to the adult brain (Jain and Huth, 2018;Toneva and Wehbe, 2019;Caucheteux and King, 2022;Pasquiou et al., 2022Pasquiou et al., , 2023;;Caucheteux et al., 2023). Evidencing similar learning trajectories in children and language models could thus provide an invaluable framework to better understand the computational principles underlying language acquisition. \n\nHere, we compare the trajectory of language acquisition between human children and modern language models. We focus on three main questions. First, do these models learn linguistic skills in a systematic order? Second, is this trajectory sequential or parallel? Third, is this trajectory similar to that of children? These hypotheses are illustrated in Figure 1. \n\nSpecifically, we train 48 GPT-2 architectures (Radford et al., 2019) from scratch, using a standard next-word prediction objective.",
            "reference_string": "[259089351 | Evanson et al. | 2023 | Citations: 29]"
        },
        {
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 94,
            "citation_count": 74,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.07311",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.07311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "2240524527",
                    "name": "Ravid Schwartz-Ziv"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "2240527814",
                    "name": "Matthew L. Leavitt"
                },
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                }
            ],
            "abstract": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
            "corpus_id": 261822542,
            "sentences": [
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
                    "score": 0.4865843250472737,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93505859375
                },
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset). Although the functional complexity of the model declines for the rest of training, it increases between these breaks. \u2022 We introduce a regularizer to examine the causal role of SAS (defined in Section 2.2) and use it to show that SAS is necessary for handling complex linguistic phenomena (Section 4.2) and that SAS competes with an alternative strategy that exhibits its own break in the loss curve, which we call the alternative strategy onset. \u2022 Section 4.3 shows that briefly suppressing SAS improves model quality and accelerates convergence. \n\nSuppressing past the alternative strategy onset damages performance and blocks SAS long-term, suggesting this phase transition terminates a critical learning period. BERT first learns to focus on syntactic neighbors with specialized attention heads, and then exhibits grammatical capabilities in its MLM objective. The former (internal) and the latter (external) model behaviors both emerge abruptly, at moments we respectively call the structure onset (\u25b2) and capabilities onset ( ) (quantified as described in Section 2.3). We separately visualize three runs with different seeds, noting that these seeds differ in the stability of Unlabeled Attachment Score (UAS; see Section 2.1) after the structure onset, but uniformly show that SAS emerges almost entirely in a brief window of time. We show (a) MLM loss, with 95% confidence intervals across samples bynonparametric bootstrapping; (b) internal grammar structure, measured by UAS on the parse induced by the attention distributions; and (c) external grammar capabilities, measured by average BLiMP accuracy with 95% confidence intervals across tasks by nonparametric bootstrapping.",
                    "score": 0.4199385203993197,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2202,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 784
                        },
                        {
                            "start": 787,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1924
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                },
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
                    "score": 0.6403740176307853,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 888
                        },
                        {
                            "start": 891,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 1942
                        },
                        {
                            "start": 1945,
                            "end": 2186
                        },
                        {
                            "start": 2187,
                            "end": 2262
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 588,
                            "end": 612,
                            "matchedPaperCorpusId": "253117181"
                        },
                        {
                            "start": 1567,
                            "end": 1588,
                            "matchedPaperCorpusId": "219315567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                },
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "Most work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions. \n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset. We show similar trajectories on the MultiBERTs (Sellam et al., 2022) reproductions (Appendix F). \n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other. We might compare this to the \"checkmate in one\" BIG-Bench task, a known breakthrough behavior in autoregressive language models (Srivastava et al., 2022). Only at a large scale can models accurately identify checkmate moves, but further exploration revealed that the model was progressing in a linear fashion at offering consistently valid chess moves before that point.",
                    "score": 0.45576542478575116,
                    "section_title": "THE SYNTAX ACQUISITION PHASE",
                    "char_start_offset": 11881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 494
                        },
                        {
                            "start": 497,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1356
                        },
                        {
                            "start": 1359,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1307,
                            "end": 1328,
                            "matchedPaperCorpusId": "235683534"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.93505859375,
            "relevance_judgment_input_expanded": "# Title: Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs\n# Venue: International Conference on Learning Representations\n# Authors: Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra\n## Abstract\nMost interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.\n## INTRODUCTION\nWhile language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.\n...\nAt the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset). Although the functional complexity of the model declines for the rest of training, it increases between these breaks. \u2022 We introduce a regularizer to examine the causal role of SAS (defined in Section 2.2) and use it to show that SAS is necessary for handling complex linguistic phenomena (Section 4.2) and that SAS competes with an alternative strategy that exhibits its own break in the loss curve, which we call the alternative strategy onset. \u2022 Section 4.3 shows that briefly suppressing SAS improves model quality and accelerates convergence. \n\nSuppressing past the alternative strategy onset damages performance and blocks SAS long-term, suggesting this phase transition terminates a critical learning period. BERT first learns to focus on syntactic neighbors with specialized attention heads, and then exhibits grammatical capabilities in its MLM objective. The former (internal) and the latter (external) model behaviors both emerge abruptly, at moments we respectively call the structure onset (\u25b2) and capabilities onset ( ) (quantified as described in Section 2.3). We separately visualize three runs with different seeds, noting that these seeds differ in the stability of Unlabeled Attachment Score (UAS; see Section 2.1) after the structure onset, but uniformly show that SAS emerges almost entirely in a brief window of time. We show (a) MLM loss, with 95% confidence intervals across samples bynonparametric bootstrapping; (b) internal grammar structure, measured by UAS on the parse induced by the attention distributions; and (c) external grammar capabilities, measured by average BLiMP accuracy with 95% confidence intervals across tasks by nonparametric bootstrapping.\n\n## THE SYNTAX ACQUISITION PHASE\nMost work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions. \n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset. We show similar trajectories on the MultiBERTs (Sellam et al., 2022) reproductions (Appendix F). \n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other. We might compare this to the \"checkmate in one\" BIG-Bench task, a known breakthrough behavior in autoregressive language models (Srivastava et al., 2022). Only at a large scale can models accurately identify checkmate moves, but further exploration revealed that the model was progressing in a linear fashion at offering consistently valid chess moves before that point.",
            "reference_string": "[261822542 | Chen et al. | 2023 | Citations: 74]"
        },
        {
            "title": "Interpreting Language Models Through Knowledge Graph Extraction",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 43,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.08546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50822485",
                    "name": "Vinitra Swamy"
                },
                {
                    "authorId": "1910588458",
                    "name": "Angelika Romanou"
                },
                {
                    "authorId": "2456863",
                    "name": "Martin Jaggi"
                }
            ],
            "abstract": "Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze\"fill-in-the-blank\"statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.",
            "corpus_id": 244129987,
            "sentences": [
                {
                    "corpus_id": "244129987",
                    "title": "Interpreting Language Models Through Knowledge Graph Extraction",
                    "text": "Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze\"fill-in-the-blank\"statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.",
                    "score": 0.4229616022693595,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                }
            ],
            "relevance_judgement": 0.927734375,
            "relevance_judgment_input_expanded": "# Title: Interpreting Language Models Through Knowledge Graph Extraction\n# Venue: arXiv.org\n# Authors: Vinitra Swamy, Angelika Romanou, Martin Jaggi\n## Abstract\nTransformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze\"fill-in-the-blank\"statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.\n",
            "reference_string": "[244129987 | Swamy et al. | 2021 | Citations: 20]"
        },
        {
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 87,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.71.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.07885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Leo Z. Liu"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                }
            ],
            "abstract": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
            "corpus_id": 233289478,
            "sentences": [
                {
                    "corpus_id": "233289478",
                    "title": "Probing Across Time: What Does RoBERTa Know and When?",
                    "text": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
                    "score": 0.39857759446045876,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92724609375
                }
            ],
            "relevance_judgement": 0.92724609375,
            "relevance_judgment_input_expanded": "# Title: Probing Across Time: What Does RoBERTa Know and When?\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Leo Z. Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith\n## Abstract\nModels of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.\n",
            "reference_string": "[233289478 | Liu et al. | 2021 | Citations: 87]"
        },
        {
            "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 47,
            "citation_count": 31,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.11758",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.11758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ],
            "abstract": "The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.",
            "corpus_id": 249017999,
            "sentences": [
                {
                    "corpus_id": "249017999",
                    "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
                    "text": "Large-scale language models pretrained jointly on text from many different languages (Delvin, 2019;Lample and Conneau, 2019;Lin et al., 2021) perform very well on various languages and on crosslingual transfer between them (e.g., Kondratyuk and Straka, 2019;Pasini et al., 2021). Due to this success, there has been a great deal of interest in uncovering what these models learn from the multilingual pretraining signal ( \u00a76). However, these works analyze a single model artifact: the final training checkpoint at which the model is considered to be converged. Recent work has also studied monolingual models by expanding the analysis to multiple pretraining checkpoints to see how model knowledge changes across time (Liu et al., 2021). \n\nWe analyze multilingual training checkpoints throughout the pretraining process in order to identify when multilingual models obtain their inlanguage and cross-lingual abilities. The case of multilingual language models is particularly interesting, as the model learns both to capture individual languages and to transfer between them just from unbalanced multitask language modeling for each language. \n\nSpecifically, we retrain a popular multilingual model, XLM-R (Conneau et al., 2020a), and run a suite of linguistic tasks covering 59 languages on checkpoints from across the pretraining process. 1 his suite evaluates different syntactic and semantic skills in both monolingual and cross-lingual transfer settings. While our analysis primarily focuses on the knowledge captured in model output representations over time, we also consider how the performance of internal layers changes during pretraining for a subset of tasks. \n\nOur analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics. In comparison, the order in which the model learns to transfer linguistic information between specific languages can vary wildly.",
                    "score": 0.49487521243134003,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1142
                        },
                        {
                            "start": 1145,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1671
                        },
                        {
                            "start": 1674,
                            "end": 1753
                        },
                        {
                            "start": 1754,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2123
                        },
                        {
                            "start": 2124,
                            "end": 2253
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 230,
                            "end": 258,
                            "matchedPaperCorpusId": "102351547"
                        },
                        {
                            "start": 258,
                            "end": 278,
                            "matchedPaperCorpusId": "231878544"
                        },
                        {
                            "start": 718,
                            "end": 736,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 1206,
                            "end": 1229,
                            "matchedPaperCorpusId": "207880568"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92333984375
                }
            ],
            "relevance_judgement": 0.92333984375,
            "relevance_judgment_input_expanded": "# Title: Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Terra Blevins, Hila Gonen, Luke Zettlemoyer\n## Abstract\nThe emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.\n## Introduction\nLarge-scale language models pretrained jointly on text from many different languages (Delvin, 2019;Lample and Conneau, 2019;Lin et al., 2021) perform very well on various languages and on crosslingual transfer between them (e.g., Kondratyuk and Straka, 2019;Pasini et al., 2021). Due to this success, there has been a great deal of interest in uncovering what these models learn from the multilingual pretraining signal ( \u00a76). However, these works analyze a single model artifact: the final training checkpoint at which the model is considered to be converged. Recent work has also studied monolingual models by expanding the analysis to multiple pretraining checkpoints to see how model knowledge changes across time (Liu et al., 2021). \n\nWe analyze multilingual training checkpoints throughout the pretraining process in order to identify when multilingual models obtain their inlanguage and cross-lingual abilities. The case of multilingual language models is particularly interesting, as the model learns both to capture individual languages and to transfer between them just from unbalanced multitask language modeling for each language. \n\nSpecifically, we retrain a popular multilingual model, XLM-R (Conneau et al., 2020a), and run a suite of linguistic tasks covering 59 languages on checkpoints from across the pretraining process. 1 his suite evaluates different syntactic and semantic skills in both monolingual and cross-lingual transfer settings. While our analysis primarily focuses on the knowledge captured in model output representations over time, we also consider how the performance of internal layers changes during pretraining for a subset of tasks. \n\nOur analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics. In comparison, the order in which the model learns to transfer linguistic information between specific languages can vary wildly.",
            "reference_string": "[249017999 | Blevins et al. | 2022 | Citations: 31]"
        },
        {
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.15419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2087001989",
                    "name": "Tyler A. Chang"
                },
                {
                    "authorId": "144035504",
                    "name": "Z. Tu"
                },
                {
                    "authorId": "24316216",
                    "name": "B. Bergen"
                }
            ],
            "abstract": "Abstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.",
            "corpus_id": 261277016,
            "sentences": [
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Our work provides evidence that there exist other capabilities or types of generalizations (e.g. non-syntactic abilities or even more fine-grained syntactic sub-abilities) that can only be learned after others, or at least only once the model reaches some particular state. Isolating these sequential dependencies is an exciting direction for future work. \n\nN-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; Chang and Bergen, 2022b), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; Liu et al., 2021). \n\nRobust model deployment. Our work also has implications for robust model deployment. High token frequencies and n-gram probabilities are by far the most influential predictors of early and stable learning in language models ( \u00a76.2, with marginal additional effects of context lengths and likelihoods). As language models are deployed in do-mains with highly-specific vocabulary terms (e.g. healthcare, law, and finance; Yang et al., 2024), the accurate prediction of infrequent domain-specific terms during text generation is likely to require extensive pre-training (late acquisition, likely mitigated by large pre-training datasets). Such domainspecific text generation is also likely to be unstable across models and pre-training steps (high variability, potentially more difficult to mitigate).",
                    "score": 0.4590501652219641,
                    "section_title": "Discussion",
                    "char_start_offset": 29451,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 355
                        },
                        {
                            "start": 358,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1388
                        },
                        {
                            "start": 1391,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2189
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1112,
                            "end": 1136,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1370,
                            "end": 1387,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 1811,
                            "end": 1829,
                            "matchedPaperCorpusId": "258331833"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "In the previous sections, we report general patterns during language model pre-training ( \u00a74), define ways to characterize learning curves ( \u00a75), and isolate specific features that predict the speed and stability of learning for individual tokens in context ( \u00a76). Our results contribute to ongoing work studying language model pre-training dynamics, with implications for robust model deployment. \n\nSequential learning. Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and Xia et al., 2023) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step. 10 Rather, they reflect some change in model processing that consistently occurs partially into pre-training (roughly step t \u0338 = 0). Because such a sudden change cannot be attributed to the specific examples observed (robust to random shuffling) or any change in the pre-training distribution at time t (the data is always i.i.d.), the primary remaining explanation is that the models' sudden \"learning\" at step t \u0338 = 0 is made possible by some systematic difference between models (and their optimizers) just before step t vs. at step 0. \n\nFramed from a potentially more interesting perspective, some types of language model \"learning\" appear to be dependent on previous learning and the linguistic abilities already present in the model. This aligns with previous work showing that language models acquire linguistic abilities in a systematic order during pre-training (Liu et al., 2021;Choshen et al., 2022), although not necessarily due to sequential dependencies. For example, Evanson et al. (2023) show that despite similar acquisition orders across models, different syntactic abilities are learned in parallel; performance for most individual abilities increases from the onset of pre-training. Our work provides evidence that there exist other capabilities or types of generalizations (e.g.",
                    "score": 0.5796959075790518,
                    "section_title": "Discussion",
                    "char_start_offset": 27189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 397
                        },
                        {
                            "start": 400,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1597
                        },
                        {
                            "start": 1600,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 2027
                        },
                        {
                            "start": 2028,
                            "end": 2261
                        },
                        {
                            "start": 2262,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 658,
                            "end": 674,
                            "matchedPaperCorpusId": "254877112"
                        },
                        {
                            "start": 1930,
                            "end": 1948,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 2041,
                            "end": 2062,
                            "matchedPaperCorpusId": "259089351"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.916015625
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Previous work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.",
                    "score": 0.5199418460785281,
                    "section_title": "Related Work",
                    "char_start_offset": 3961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 783
                        },
                        {
                            "start": 786,
                            "end": 874
                        },
                        {
                            "start": 877,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1171
                        },
                        {
                            "start": 1172,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1714
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 71,
                            "end": 95,
                            "matchedPaperCorpusId": "267903883"
                        },
                        {
                            "start": 123,
                            "end": 144,
                            "matchedPaperCorpusId": "259089351"
                        },
                        {
                            "start": 422,
                            "end": 443,
                            "matchedPaperCorpusId": "222140842"
                        },
                        {
                            "start": 443,
                            "end": 460,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 942,
                            "end": 967,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1046,
                            "end": 1069,
                            "matchedPaperCorpusId": "248986465"
                        },
                        {
                            "start": 1147,
                            "end": 1170,
                            "matchedPaperCorpusId": "257921893"
                        },
                        {
                            "start": 1198,
                            "end": 1215,
                            "matchedPaperCorpusId": "254877112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Language models have received unprecedented attention in recent years due to impressive performance on natural language tasks (e.g. OpenAI, 2022;Google, 2023;Anthropic, 2023). However, these models are initialized as random word (token) generators, and it remains unclear how the models achieve complex linguistic abilities during pre-training. Previous work has investigated when syntactic, semantic, and reasoning abilities emerge (Liu et al., 2021;Evanson et al., 2023), quantified ages of acquisition for tokens averaged over contexts (Chang and Bergen, 2022b), and extracted learning curves for individual examples (Xia et al., 2023). However, features that influence individual learning curves have yet to be identified (e.g. n-gram probabilities and context lengths). Given any token in context, it is largely unknown when or how stably that token would be learned. \n\nFrom a scientific perspective, understanding when examples are learned by language models can provide insights into possible mechanisms for language acquisition. Regardless of their similarity to human language processing, language models are exemplars of how learning from language statistics alone (i.e. \"distributional\" learning) can lead to complex linguistic abilities (Chang and Bergen, 2022b;Warstadt and Bowman, 2023;Mahowald et al., 2023). Notably, despite smoothly decreasing corpus-level loss and independent and identically distributed (i.i.d.) data throughout pre-training, individual text examples exhibit learning curves with sudden decreases and increases in loss ( \u00a75 and Xia et al., 2023). This highlights the importance of examining individual example learning curves for pre-training dynamics research; aggregate curves often do not capture the fluctuations exhibited by individual examples. Our work seeks to characterize these fine-grained convergence patterns in terms of simpler distributional statistics. \n\nFrom a practical perspective, understanding language model learning curves can inform the pre-training and deployment of language models. Learning curve results might allow NLP practitioners to determine how much pre-training is necessary for different capabilities and what behaviors will remain stable after additional pre-training (e.g. \"continual learning\" on more recent data; Jin et al., 2022).",
                    "score": 0.5100056535518896,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 872
                        },
                        {
                            "start": 875,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1786
                        },
                        {
                            "start": 1787,
                            "end": 1904
                        },
                        {
                            "start": 1907,
                            "end": 2044
                        },
                        {
                            "start": 2045,
                            "end": 2246
                        },
                        {
                            "start": 2247,
                            "end": 2307
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 433,
                            "end": 451,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 451,
                            "end": 472,
                            "matchedPaperCorpusId": "259089351"
                        },
                        {
                            "start": 539,
                            "end": 564,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 620,
                            "end": 638,
                            "matchedPaperCorpusId": "254877112"
                        },
                        {
                            "start": 1249,
                            "end": 1274,
                            "matchedPaperCorpusId": "238408423"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.880859375
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Tyler A. Chang, Z. Tu, B. Bergen\n## Abstract\nAbstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.\n## Introduction\nLanguage models have received unprecedented attention in recent years due to impressive performance on natural language tasks (e.g. OpenAI, 2022;Google, 2023;Anthropic, 2023). However, these models are initialized as random word (token) generators, and it remains unclear how the models achieve complex linguistic abilities during pre-training. Previous work has investigated when syntactic, semantic, and reasoning abilities emerge (Liu et al., 2021;Evanson et al., 2023), quantified ages of acquisition for tokens averaged over contexts (Chang and Bergen, 2022b), and extracted learning curves for individual examples (Xia et al., 2023). However, features that influence individual learning curves have yet to be identified (e.g. n-gram probabilities and context lengths). Given any token in context, it is largely unknown when or how stably that token would be learned. \n\nFrom a scientific perspective, understanding when examples are learned by language models can provide insights into possible mechanisms for language acquisition. Regardless of their similarity to human language processing, language models are exemplars of how learning from language statistics alone (i.e. \"distributional\" learning) can lead to complex linguistic abilities (Chang and Bergen, 2022b;Warstadt and Bowman, 2023;Mahowald et al., 2023). Notably, despite smoothly decreasing corpus-level loss and independent and identically distributed (i.i.d.) data throughout pre-training, individual text examples exhibit learning curves with sudden decreases and increases in loss ( \u00a75 and Xia et al., 2023). This highlights the importance of examining individual example learning curves for pre-training dynamics research; aggregate curves often do not capture the fluctuations exhibited by individual examples. Our work seeks to characterize these fine-grained convergence patterns in terms of simpler distributional statistics. \n\nFrom a practical perspective, understanding language model learning curves can inform the pre-training and deployment of language models. Learning curve results might allow NLP practitioners to determine how much pre-training is necessary for different capabilities and what behaviors will remain stable after additional pre-training (e.g. \"continual learning\" on more recent data; Jin et al., 2022).\n\n## Related Work\nPrevious work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.\n\n## Discussion\nIn the previous sections, we report general patterns during language model pre-training ( \u00a74), define ways to characterize learning curves ( \u00a75), and isolate specific features that predict the speed and stability of learning for individual tokens in context ( \u00a76). Our results contribute to ongoing work studying language model pre-training dynamics, with implications for robust model deployment. \n\nSequential learning. Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and Xia et al., 2023) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step. 10 Rather, they reflect some change in model processing that consistently occurs partially into pre-training (roughly step t \u0338 = 0). Because such a sudden change cannot be attributed to the specific examples observed (robust to random shuffling) or any change in the pre-training distribution at time t (the data is always i.i.d.), the primary remaining explanation is that the models' sudden \"learning\" at step t \u0338 = 0 is made possible by some systematic difference between models (and their optimizers) just before step t vs. at step 0. \n\nFramed from a potentially more interesting perspective, some types of language model \"learning\" appear to be dependent on previous learning and the linguistic abilities already present in the model. This aligns with previous work showing that language models acquire linguistic abilities in a systematic order during pre-training (Liu et al., 2021;Choshen et al., 2022), although not necessarily due to sequential dependencies. For example, Evanson et al. (2023) show that despite similar acquisition orders across models, different syntactic abilities are learned in parallel; performance for most individual abilities increases from the onset of pre-training. Our work provides evidence that there exist other capabilities or types of generalizations (e.g.\n...\nOur work provides evidence that there exist other capabilities or types of generalizations (e.g. non-syntactic abilities or even more fine-grained syntactic sub-abilities) that can only be learned after others, or at least only once the model reaches some particular state. Isolating these sequential dependencies is an exciting direction for future work. \n\nN-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; Chang and Bergen, 2022b), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; Liu et al., 2021). \n\nRobust model deployment. Our work also has implications for robust model deployment. High token frequencies and n-gram probabilities are by far the most influential predictors of early and stable learning in language models ( \u00a76.2, with marginal additional effects of context lengths and likelihoods). As language models are deployed in do-mains with highly-specific vocabulary terms (e.g. healthcare, law, and finance; Yang et al., 2024), the accurate prediction of infrequent domain-specific terms during text generation is likely to require extensive pre-training (late acquisition, likely mitigated by large pre-training datasets). Such domainspecific text generation is also likely to be unstable across models and pre-training steps (high variability, potentially more difficult to mitigate).",
            "reference_string": "[261277016 | Chang et al. | 2023 | Citations: 13]"
        },
        {
            "title": "When Do You Need Billions of Words of Pretraining Data?",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 68,
            "citation_count": 141,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.acl-long.90.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.04946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9227100",
                    "name": "Yian Zhang"
                },
                {
                    "authorId": "46236380",
                    "name": "Alex Warstadt"
                },
                {
                    "authorId": "1993902967",
                    "name": "Haau-Sing Li"
                },
                {
                    "authorId": "3644767",
                    "name": "Samuel R. Bowman"
                }
            ],
            "abstract": "NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.",
            "corpus_id": 226289695,
            "sentences": [
                {
                    "corpus_id": "226289695",
                    "title": "When Do You Need Billions of Words of Pretraining Data?",
                    "text": "NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.",
                    "score": 0.45215723126860907,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                },
                {
                    "corpus_id": "226289695",
                    "title": "When Do You Need Billions of Words of Pretraining Data?",
                    "text": "the edge probing suite of NLP tasks (Tenney et al., 2019b) to measure the quality of the syntactic and semantic features that can be extracted by a downstream classifier with each level of pretraining. Second, we apply minimum description length (MDL) probing (Voita and Titov, 2020) to the edge probing suite, with the goal of quantifying the accessibility of these features. Third, we test the models' knowledge of various syntactic phenomena using unsupervised acceptability judgments on the BLiMP suite (Warstadt et al., 2020a). Fourth, we probe the models' world knowledge and commonsense knowledge using unsupervised language model knowledge probing with the LAMA suite (Petroni et al., 2019). Finally, we fine-tune the models on five tasks from SuperGLUE  to measure their ability to solve conventional NLU tasks.\n\nFor each evaluation method, we fit an exponential learning curve to the results as a function of the amount of pretraining data, shown in Figure 1. We have two main findings: First, the results of classifier probing, MDL probing, and unsupervised relative acceptability judgement (BLiMP) show that the linguistic knowledge of models pretrained on 100M words and 30B words is similar, as is the description length of linguistic features. Second, RoBERTa requires billions of words of pretraining data to effectively acquire factual knowledge and to make substantial improvements in performance on dowstream NLU tasks. From these results, we conclude that there are skills critical to solving downstream NLU tasks that LMs can only acquire with billions of words of pretraining data. Future work will likely need to look beyond core linguistic knowledge if we are to better understand and advance the abilities of large language models.",
                    "score": 0.401519469357862,
                    "section_title": "Introduction",
                    "char_start_offset": 1862,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 36,
                            "end": 58,
                            "matchedPaperCorpusId": "108300988"
                        },
                        {
                            "start": 260,
                            "end": 283,
                            "matchedPaperCorpusId": "214693050"
                        },
                        {
                            "start": 507,
                            "end": 531,
                            "matchedPaperCorpusId": "208527435"
                        },
                        {
                            "start": 676,
                            "end": 698,
                            "matchedPaperCorpusId": "15641339"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85498046875
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: When Do You Need Billions of Words of Pretraining Data?\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yian Zhang, Alex Warstadt, Haau-Sing Li, Samuel R. Bowman\n## Abstract\nNLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.\n## Introduction\nthe edge probing suite of NLP tasks (Tenney et al., 2019b) to measure the quality of the syntactic and semantic features that can be extracted by a downstream classifier with each level of pretraining. Second, we apply minimum description length (MDL) probing (Voita and Titov, 2020) to the edge probing suite, with the goal of quantifying the accessibility of these features. Third, we test the models' knowledge of various syntactic phenomena using unsupervised acceptability judgments on the BLiMP suite (Warstadt et al., 2020a). Fourth, we probe the models' world knowledge and commonsense knowledge using unsupervised language model knowledge probing with the LAMA suite (Petroni et al., 2019). Finally, we fine-tune the models on five tasks from SuperGLUE  to measure their ability to solve conventional NLU tasks.\n\nFor each evaluation method, we fit an exponential learning curve to the results as a function of the amount of pretraining data, shown in Figure 1. We have two main findings: First, the results of classifier probing, MDL probing, and unsupervised relative acceptability judgement (BLiMP) show that the linguistic knowledge of models pretrained on 100M words and 30B words is similar, as is the description length of linguistic features. Second, RoBERTa requires billions of words of pretraining data to effectively acquire factual knowledge and to make substantial improvements in performance on dowstream NLU tasks. From these results, we conclude that there are skills critical to solving downstream NLU tasks that LMs can only acquire with billions of words of pretraining data. Future work will likely need to look beyond core linguistic knowledge if we are to better understand and advance the abilities of large language models.",
            "reference_string": "[226289695 | Zhang et al. | 2020 | Citations: 141]"
        },
        {
            "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 92,
            "citation_count": 221,
            "influential_citation_count": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/130402/2/2020.acl-main.158.pdf",
                "status": "GREEN",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.03692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112327566",
                    "name": "Jennifer Hu"
                },
                {
                    "authorId": "24339276",
                    "name": "Jon Gauthier"
                },
                {
                    "authorId": "1483502658",
                    "name": "Peng Qian"
                },
                {
                    "authorId": "51445267",
                    "name": "Ethan Gotlieb Wilcox"
                },
                {
                    "authorId": "50007746",
                    "name": "R. Levy"
                }
            ],
            "abstract": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
            "corpus_id": 218571018,
            "sentences": [
                {
                    "corpus_id": "218571018",
                    "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
                    "text": "This work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments. Our results dissociate model perplexity and performance in syntactic generalization tests, suggesting that the two metrics capture complementary features of language model knowledge. In a controlled evaluation of different model classes and datasets, we find model architecture plays a more important role than training data scale in yielding correct syntactic generalizations. Our circuit-level analysis reveals consistent failure on Licensing but inconsistent behavior on other circuits, suggesting that different syntactic circuits make use of different underlying processing capacities. In addition to the insight these results provide about neural NLP systems, they also bear on questions central to cognitive science and linguistics, putting lower bounds on what syntactic knowledge can be acquired from string input alone. \n\nTargeted syntactic evaluation is just one in a series of complementary methods being developed to assess the learning outcomes of neural language processing models. Other methods include classifying sentences as grammatical or ungrammatical (Warstadt et al., 2019b), decoding syntactic features from a model's internal state (Belinkov et al., 2017;Giulianelli et al., 2018), or transfer learning to a strictly syntactic task such as parsing or POS tagging (Hewitt and Manning, 2019). As each task brings an explicit set of assumptions, complementary assessment methods can collectively provide greater insight into models' learning outcomes. \n\nAlthough this paper, together with Warstadt et al. (2020), report what is to our knowledge the largestscale targeted syntactic evaluations to date, we emphasize that they are only first steps toward a comprehensive understanding of the syntactic capabilities of contemporary language models. This understanding will be further advanced by new targeted-evaluation test suites covering a still wider variety of syntactic phenomena, additional trained models with more varied hyperparameters and randomization seeds, and new architectural innovations. Humans develop extraordinary grammatical capabilities through exposure to natural linguistic input. It remains to be seen to just what extent contemporary artificial systems do the same.",
                    "score": 0.48971318895126115,
                    "section_title": "Discussion",
                    "char_start_offset": 25249,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 961
                        },
                        {
                            "start": 964,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1605
                        },
                        {
                            "start": 1608,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 2156
                        },
                        {
                            "start": 2157,
                            "end": 2256
                        },
                        {
                            "start": 2257,
                            "end": 2343
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1205,
                            "end": 1229,
                            "matchedPaperCorpusId": "44072099"
                        },
                        {
                            "start": 1289,
                            "end": 1312,
                            "matchedPaperCorpusId": "7100502"
                        },
                        {
                            "start": 1312,
                            "end": 1337,
                            "matchedPaperCorpusId": "52090220"
                        },
                        {
                            "start": 1420,
                            "end": 1446,
                            "matchedPaperCorpusId": "106402715"
                        },
                        {
                            "start": 1643,
                            "end": 1665,
                            "matchedPaperCorpusId": "208527435"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: A Systematic Assessment of Syntactic Generalization in Neural Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Gotlieb Wilcox, R. Levy\n## Abstract\nWhile state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.\n## Discussion\nThis work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments. Our results dissociate model perplexity and performance in syntactic generalization tests, suggesting that the two metrics capture complementary features of language model knowledge. In a controlled evaluation of different model classes and datasets, we find model architecture plays a more important role than training data scale in yielding correct syntactic generalizations. Our circuit-level analysis reveals consistent failure on Licensing but inconsistent behavior on other circuits, suggesting that different syntactic circuits make use of different underlying processing capacities. In addition to the insight these results provide about neural NLP systems, they also bear on questions central to cognitive science and linguistics, putting lower bounds on what syntactic knowledge can be acquired from string input alone. \n\nTargeted syntactic evaluation is just one in a series of complementary methods being developed to assess the learning outcomes of neural language processing models. Other methods include classifying sentences as grammatical or ungrammatical (Warstadt et al., 2019b), decoding syntactic features from a model's internal state (Belinkov et al., 2017;Giulianelli et al., 2018), or transfer learning to a strictly syntactic task such as parsing or POS tagging (Hewitt and Manning, 2019). As each task brings an explicit set of assumptions, complementary assessment methods can collectively provide greater insight into models' learning outcomes. \n\nAlthough this paper, together with Warstadt et al. (2020), report what is to our knowledge the largestscale targeted syntactic evaluations to date, we emphasize that they are only first steps toward a comprehensive understanding of the syntactic capabilities of contemporary language models. This understanding will be further advanced by new targeted-evaluation test suites covering a still wider variety of syntactic phenomena, additional trained models with more varied hyperparameters and randomization seeds, and new architectural innovations. Humans develop extraordinary grammatical capabilities through exposure to natural linguistic input. It remains to be seen to just what extent contemporary artificial systems do the same.",
            "reference_string": "[218571018 | Hu et al. | 2020 | Citations: 221]"
        },
        {
            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326447675",
                    "name": "Qiyuan Yang"
                },
                {
                    "authorId": "2294722814",
                    "name": "Pengda Wang"
                },
                {
                    "authorId": "2326296255",
                    "name": "Luke D. Plonsky"
                },
                {
                    "authorId": "2250843586",
                    "name": "Frederick L. Oswald"
                },
                {
                    "authorId": "7315244",
                    "name": "Hanjie Chen"
                }
            ],
            "abstract": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition. Notably, in generation tasks, LMs are more similar to human performance in areas where information is easier to extract from the corpus, such as average word length, clauses, and auxiliary verbs. Newer LMs did not exhibit significant progress in terms of specific dimensions, such as clauses and auxiliary verbs, where the variation across corpora is relatively limited. Register theory offers a plausible explanation for these observations, suggesting that the linguistic features of the training data have a substantial impact on the models' abilities.",
            "corpus_id": 273403825,
            "sentences": [
                {
                    "corpus_id": "273403825",
                    "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
                    "text": "As one of the unique abilities of humans, the acquisition of language has long been a key area of research in psycholinguistics. During the process of language acquisition, humans go through multiple stages, from imitation and rule learning to complex contextual understanding (Goldberg, 2005). These stages bear some resemblance to the way current LMs are trained. For instance, LMs learn the statistical patterns and grammatical rules of language through training on large-scale data, similar to how infants develop language abilities by receiving a vast amount of input through listening and speaking. If we design theory-driven tests based on the human language acquisition process to evaluate the capabilities of LMs, it could help us better understand the nature, potential, and limitations of LMs in their development. \n\nOur work draws on classical theories of human language development to assess LMs in terms of a three-stage human language development framework (Chomsky, 2014;Loban, 1976;Pinker, 2003), as shown in Figure 1. The first stage involves developing basic language understanding, similar to early language acquisition in infants. At this stage, we evaluate the model's ability to recognize vocabulary, grasp syntax, and perform simple reasoning. In the second stage, the focus shifts to mastering complex grammar and semantics, where the model demonstrates a deeper understanding of language rules and logical relationships between sentences. The third stage assesses advanced language abilities, evaluating the model's capacity for complex reasoning and logical analysis. \n\nWe further investigate another theory: register theory in linguistics, which posits that different language use scenarios influence the form and structure of language (Halliday, 1977;Matthiessen, 1993). This theory offers insights into the extent to which models' abilities depend on the linguistic features encountered in specific situations, referred to as registers. In LMs, the training corpus will reflect some registers but not others, which can raise general questions or concerns about the generaliz-ability and biases contained in any given corpus. \n\nWe evaluated 15 LMs from 2019 to 2024, excluding instruction fine-tuned or chat versions, with varying parameter sizes (see \u00a74.1).",
                    "score": 0.4227106823263234,
                    "section_title": "Introduction",
                    "char_start_offset": 2100,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 825
                        },
                        {
                            "start": 828,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1594
                        },
                        {
                            "start": 1597,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1966
                        },
                        {
                            "start": 1967,
                            "end": 2154
                        },
                        {
                            "start": 2157,
                            "end": 2287
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.91357421875,
            "relevance_judgment_input_expanded": "# Title: From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition\n# Venue: arXiv.org\n# Authors: Qiyuan Yang, Pengda Wang, Luke D. Plonsky, Frederick L. Oswald, Hanjie Chen\n## Abstract\nWe examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition. Notably, in generation tasks, LMs are more similar to human performance in areas where information is easier to extract from the corpus, such as average word length, clauses, and auxiliary verbs. Newer LMs did not exhibit significant progress in terms of specific dimensions, such as clauses and auxiliary verbs, where the variation across corpora is relatively limited. Register theory offers a plausible explanation for these observations, suggesting that the linguistic features of the training data have a substantial impact on the models' abilities.\n## Introduction\nAs one of the unique abilities of humans, the acquisition of language has long been a key area of research in psycholinguistics. During the process of language acquisition, humans go through multiple stages, from imitation and rule learning to complex contextual understanding (Goldberg, 2005). These stages bear some resemblance to the way current LMs are trained. For instance, LMs learn the statistical patterns and grammatical rules of language through training on large-scale data, similar to how infants develop language abilities by receiving a vast amount of input through listening and speaking. If we design theory-driven tests based on the human language acquisition process to evaluate the capabilities of LMs, it could help us better understand the nature, potential, and limitations of LMs in their development. \n\nOur work draws on classical theories of human language development to assess LMs in terms of a three-stage human language development framework (Chomsky, 2014;Loban, 1976;Pinker, 2003), as shown in Figure 1. The first stage involves developing basic language understanding, similar to early language acquisition in infants. At this stage, we evaluate the model's ability to recognize vocabulary, grasp syntax, and perform simple reasoning. In the second stage, the focus shifts to mastering complex grammar and semantics, where the model demonstrates a deeper understanding of language rules and logical relationships between sentences. The third stage assesses advanced language abilities, evaluating the model's capacity for complex reasoning and logical analysis. \n\nWe further investigate another theory: register theory in linguistics, which posits that different language use scenarios influence the form and structure of language (Halliday, 1977;Matthiessen, 1993). This theory offers insights into the extent to which models' abilities depend on the linguistic features encountered in specific situations, referred to as registers. In LMs, the training corpus will reflect some registers but not others, which can raise general questions or concerns about the generaliz-ability and biases contained in any given corpus. \n\nWe evaluated 15 LMs from 2019 to 2024, excluding instruction fine-tuned or chat versions, with varying parameter sizes (see \u00a74.1).",
            "reference_string": "[273403825 | Yang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 41,
            "citation_count": 607,
            "influential_citation_count": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.13528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "37907837",
                    "name": "Allyson Ettinger"
                }
            ],
            "abstract": "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
            "corpus_id": 199001173,
            "sentences": [
                {
                    "corpus_id": "199001173",
                    "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
                    "text": "Pre-training of NLP models with a language modeling objective has recently gained popularity as a precursor to task-specific fine-tuning. Pretrained models like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) have advanced the state of the art in a wide variety of tasks, suggesting that these models acquire valuable, generalizable linguistic competence during the pretraining process. However, though we have established the benefits of language model pre-training, we have yet to understand what exactly about language these models learn during that process. \n\nThis paper aims to improve our understanding of what language models (LMs) know about language, by introducing a set of diagnostics targeting a range of linguistic capacities, drawn from human psycholinguistic experiments. Because of their origin in psycholinguistics, these diagnostics have two distinct advantages: they are carefully controlled to ask targeted questions about linguistic capabilities, and they are designed to ask these questions by examining word predictions in context, which allows us to study LMs without any need for task-specific fine-tuning. \n\nBeyond these advantages, our diagnostics distinguish themselves from existing tests for LMs in two primary ways. First, these tests have been chosen specifically for their capacity to reveal insensitivities in predictive models, as evidenced by patterns that they elicit in human brain responses. Second, each of these tests targets a set of linguistic capacities that extend beyond the primarily syntactic focus seen in existing LM diagnosticswe have tests targeting commonsense/pragmatic inference, semantic roles and event knowledge, category membership, and negation. Each of our diagnostics is set up to support tests of both word prediction accuracy and sensitivity to distinctions between good and bad context completions. Although we focus on the BERT model here as an illustrative case study, these diagnostics are applicable for testing of any language model. This paper makes two main contributions. First, we introduce a new set of targeted diagnostics for assessing linguistic capacities in language models. 1 Second, we apply these tests to shed light on strengths and weaknesses of the popular BERT model.",
                    "score": 0.46379718298381034,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 572
                        },
                        {
                            "start": 575,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 1142
                        },
                        {
                            "start": 1145,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 2014
                        },
                        {
                            "start": 2015,
                            "end": 2055
                        },
                        {
                            "start": 2056,
                            "end": 2265
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 187,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 197,
                            "end": 219,
                            "matchedPaperCorpusId": "3626819"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Allyson Ettinger\n## Abstract\nPre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation.\n## Introduction\nPre-training of NLP models with a language modeling objective has recently gained popularity as a precursor to task-specific fine-tuning. Pretrained models like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) have advanced the state of the art in a wide variety of tasks, suggesting that these models acquire valuable, generalizable linguistic competence during the pretraining process. However, though we have established the benefits of language model pre-training, we have yet to understand what exactly about language these models learn during that process. \n\nThis paper aims to improve our understanding of what language models (LMs) know about language, by introducing a set of diagnostics targeting a range of linguistic capacities, drawn from human psycholinguistic experiments. Because of their origin in psycholinguistics, these diagnostics have two distinct advantages: they are carefully controlled to ask targeted questions about linguistic capabilities, and they are designed to ask these questions by examining word predictions in context, which allows us to study LMs without any need for task-specific fine-tuning. \n\nBeyond these advantages, our diagnostics distinguish themselves from existing tests for LMs in two primary ways. First, these tests have been chosen specifically for their capacity to reveal insensitivities in predictive models, as evidenced by patterns that they elicit in human brain responses. Second, each of these tests targets a set of linguistic capacities that extend beyond the primarily syntactic focus seen in existing LM diagnosticswe have tests targeting commonsense/pragmatic inference, semantic roles and event knowledge, category membership, and negation. Each of our diagnostics is set up to support tests of both word prediction accuracy and sensitivity to distinctions between good and bad context completions. Although we focus on the BERT model here as an illustrative case study, these diagnostics are applicable for testing of any language model. This paper makes two main contributions. First, we introduce a new set of targeted diagnostics for assessing linguistic capacities in language models. 1 Second, we apply these tests to shed light on strengths and weaknesses of the popular BERT model.",
            "reference_string": "[199001173 | Ettinger | 2019 | Citations: 607]"
        },
        {
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2168285763",
                    "name": "Giulio Starace"
                },
                {
                    "authorId": "2262444926",
                    "name": "Konstantinos Papakostas"
                },
                {
                    "authorId": "2067163164",
                    "name": "Rochelle Choenni"
                },
                {
                    "authorId": "31713235",
                    "name": "Apostolos Panagiotopoulos"
                },
                {
                    "authorId": "2168109752",
                    "name": "Matteo Rosati"
                },
                {
                    "authorId": "2262444923",
                    "name": "Alina Leidinger"
                },
                {
                    "authorId": "2262445370",
                    "name": "Ekaterina Shutova"
                }
            ],
            "abstract": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
            "corpus_id": 264590507,
            "sentences": [
                {
                    "corpus_id": "264590507",
                    "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
                    "text": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
                    "score": 0.4136987656015606,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: Probing LLMs for Joint Encoding of Linguistic Categories\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Giulio Starace, Konstantinos Papakostas, Rochelle Choenni, Apostolos Panagiotopoulos, Matteo Rosati, Alina Leidinger, Ekaterina Shutova\n## Abstract\nLarge Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.\n",
            "reference_string": "[264590507 | Starace et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Explainability for Large Language Models: A Survey",
            "venue": "ACM Transactions on Intelligent Systems and Technology",
            "year": 2023,
            "reference_count": 217,
            "citation_count": 461,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3639372",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.01029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "7315244",
                    "name": "Hanjie Chen"
                },
                {
                    "authorId": "145338224",
                    "name": "F. Yang"
                },
                {
                    "authorId": "47717322",
                    "name": "Ninghao Liu"
                },
                {
                    "authorId": "13689700",
                    "name": "Huiqi Deng"
                },
                {
                    "authorId": "22561596",
                    "name": "Hengyi Cai"
                },
                {
                    "authorId": "2237948548",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.",
            "corpus_id": 261530292,
            "sentences": [
                {
                    "corpus_id": "261530292",
                    "title": "Explainability for Large Language Models: A Survey",
                    "text": "Assistant models are typically trained in two stages. First, they undergo unsupervised pre-training on large amounts of raw text to learn general linguistic representations. This pre-training stage allows the models to acquire general language knowledge. Second, the models go through alignment fine-tuning via supervised and reinforcement learning. This aligns the models with specific end tasks and user preferences. Explainability research on these models focuses on determining whether their knowledge comes predominantly from the initial pre-training stage, wherein they acquire general language abilities, or from the subsequent alignment fine-tuning stage, wherein they are tailored to specific tasks and preferences. Understanding the source of the models' knowledge provides insight into how to improve and interpret their performance. \n\nA recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches. Complex fine-tuning and reinforcement learning techniques may be less crucial than previously believed. On the other hand, this study also indicates that data quality is more important compared to data quantity during instruction fine-tuning. Furthermore, Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models.",
                    "score": 0.4684431855460669,
                    "section_title": "Explaining the Role of Fine-tuning",
                    "char_start_offset": 56400,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 53
                        },
                        {
                            "start": 54,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 844
                        },
                        {
                            "start": 847,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1630
                        },
                        {
                            "start": 1631,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 1922
                        },
                        {
                            "start": 1923,
                            "end": 2059
                        },
                        {
                            "start": 2060,
                            "end": 2148
                        },
                        {
                            "start": 2149,
                            "end": 2285
                        },
                        {
                            "start": 2286,
                            "end": 2479
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90478515625
                }
            ],
            "relevance_judgement": 0.90478515625,
            "relevance_judgment_input_expanded": "# Title: Explainability for Large Language Models: A Survey\n# Venue: ACM Transactions on Intelligent Systems and Technology\n# Authors: Haiyan Zhao, Hanjie Chen, F. Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du\n## Abstract\nLarge language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.\n## Explaining the Role of Fine-tuning\nAssistant models are typically trained in two stages. First, they undergo unsupervised pre-training on large amounts of raw text to learn general linguistic representations. This pre-training stage allows the models to acquire general language knowledge. Second, the models go through alignment fine-tuning via supervised and reinforcement learning. This aligns the models with specific end tasks and user preferences. Explainability research on these models focuses on determining whether their knowledge comes predominantly from the initial pre-training stage, wherein they acquire general language abilities, or from the subsequent alignment fine-tuning stage, wherein they are tailored to specific tasks and preferences. Understanding the source of the models' knowledge provides insight into how to improve and interpret their performance. \n\nA recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches. Complex fine-tuning and reinforcement learning techniques may be less crucial than previously believed. On the other hand, this study also indicates that data quality is more important compared to data quantity during instruction fine-tuning. Furthermore, Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models.",
            "reference_string": "[261530292 | Zhao et al. | 2023 | Citations: 461]"
        },
        {
            "title": "Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203368550",
                    "name": "Qian Dong"
                },
                {
                    "authorId": "2249554788",
                    "name": "Yiding Liu"
                },
                {
                    "authorId": "144922928",
                    "name": "Qingyao Ai"
                },
                {
                    "authorId": "47039225",
                    "name": "Zhijing Wu"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "46399371",
                    "name": "Y. Liu"
                },
                {
                    "authorId": "2237948548",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "8093158",
                    "name": "Shaoping Ma"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various research domains, including the field of Information Retrieval (IR). However, the responses generated by off-the-shelf LLMs tend to be generic, i.e., cannot capture the distinctiveness of each document with similar content. This limits the performance of LLMs in IR because finding and distinguishing relevant documents from substantial similar documents is a typical problem in many IR tasks. To address this issue, we propose an unsupervised alignment method, namely Reinforcement Learning from Contrastive Feedback (RLCF), empowering LLMs to generate both high-quality and context-specific responses. Our approach constructs unsupervised contrastive feedback signals based on similar document groups, and adopts a reward function, named group-wise reciprocal rank, to optimize LLMs. We conduct extensive experiments to evaluate the effectiveness of RLCF.",
            "corpus_id": 263310649,
            "sentences": [
                {
                    "corpus_id": "263310649",
                    "title": "Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback",
                    "text": "Recently, LLMs are emerged and boost many natural language processing tasks. The architecture of LLMs, particularly the Transformer [55], leads to significant improvements in capturing textual semantics. This advancement empowers many influential models such as BERT [17] and GPT [43]. These models pave the way for subsequent advancements like GPT-2 [44] and GPT-3 [7], with increasing model sizes and capabilities. The training pipeline of LLMs also earned significant attention in recent years due to its pivotal role in enabling models like GPT to exhibit remarkable language understanding and generation capabilities. Pre-training is a cornerstone of training LLMs and involves training the model on a massive corpus to learn linguistic patterns and structures, leveraging the tasks such as masked language modeling [17], next token prediction [43] and etc. By utilizing large-scale pre-training, LLMs acquire a general understanding of language, making them available for various downstream tasks. Supervised Fine-Tuning (SFT) involves training LLMs on task-specific datasets with labeled examples. This stage adapts the generic linguistic knowledge acquired during pre-training to specific tasks, such as sentiment analysis [20], text classification [19,21], and dialogues [41]. Alignment technique facilitates LLMs in learning from the generated responses and environmental feedback, thereby aligning the capability with the desired attribute. The environment feedback could be from human [41] or other models [2]. This approach has shown promise in improving the helpfulness and harmlessness of LLMs.",
                    "score": 0.4053861015047501,
                    "section_title": "RELATED WORK 2.1 Large Language Models",
                    "char_start_offset": 5106,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 76
                        },
                        {
                            "start": 77,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1285
                        },
                        {
                            "start": 1286,
                            "end": 1451
                        },
                        {
                            "start": 1452,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1609
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 132,
                            "end": 136,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 351,
                            "end": 355,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 366,
                            "end": 369,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1231,
                            "end": 1235,
                            "matchedPaperCorpusId": "233236941"
                        },
                        {
                            "start": 1261,
                            "end": 1264,
                            "matchedPaperCorpusId": "235792270"
                        },
                        {
                            "start": 1280,
                            "end": 1284,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1497,
                            "end": 1501,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Y. Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma\n## Abstract\nLarge language models (LLMs) have demonstrated remarkable capabilities across various research domains, including the field of Information Retrieval (IR). However, the responses generated by off-the-shelf LLMs tend to be generic, i.e., cannot capture the distinctiveness of each document with similar content. This limits the performance of LLMs in IR because finding and distinguishing relevant documents from substantial similar documents is a typical problem in many IR tasks. To address this issue, we propose an unsupervised alignment method, namely Reinforcement Learning from Contrastive Feedback (RLCF), empowering LLMs to generate both high-quality and context-specific responses. Our approach constructs unsupervised contrastive feedback signals based on similar document groups, and adopts a reward function, named group-wise reciprocal rank, to optimize LLMs. We conduct extensive experiments to evaluate the effectiveness of RLCF.\n## RELATED WORK 2.1 Large Language Models\nRecently, LLMs are emerged and boost many natural language processing tasks. The architecture of LLMs, particularly the Transformer [55], leads to significant improvements in capturing textual semantics. This advancement empowers many influential models such as BERT [17] and GPT [43]. These models pave the way for subsequent advancements like GPT-2 [44] and GPT-3 [7], with increasing model sizes and capabilities. The training pipeline of LLMs also earned significant attention in recent years due to its pivotal role in enabling models like GPT to exhibit remarkable language understanding and generation capabilities. Pre-training is a cornerstone of training LLMs and involves training the model on a massive corpus to learn linguistic patterns and structures, leveraging the tasks such as masked language modeling [17], next token prediction [43] and etc. By utilizing large-scale pre-training, LLMs acquire a general understanding of language, making them available for various downstream tasks. Supervised Fine-Tuning (SFT) involves training LLMs on task-specific datasets with labeled examples. This stage adapts the generic linguistic knowledge acquired during pre-training to specific tasks, such as sentiment analysis [20], text classification [19,21], and dialogues [41]. Alignment technique facilitates LLMs in learning from the generated responses and environmental feedback, thereby aligning the capability with the desired attribute. The environment feedback could be from human [41] or other models [2]. This approach has shown promise in improving the helpfulness and harmlessness of LLMs.",
            "reference_string": "[263310649 | Dong et al. | 2023 | Citations: 5]"
        },
        {
            "title": "A Survey on Federated Fine-tuning of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 197,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297831289",
                    "name": "Yebo Wu"
                },
                {
                    "authorId": "2297767120",
                    "name": "Chunlin Tian"
                },
                {
                    "authorId": "2350520035",
                    "name": "Jingguang Li"
                },
                {
                    "authorId": "2350521999",
                    "name": "He Sun"
                },
                {
                    "authorId": "2220304337",
                    "name": "Kahou Tam"
                },
                {
                    "authorId": "2260286654",
                    "name": "Li Li"
                },
                {
                    "authorId": "2362594432",
                    "name": "Chengzhong Xu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, with fine-tuning playing a pivotal role in adapting them to specific downstream applications. Federated Learning (FL) offers a promising approach that enables collaborative model adaptation while ensuring data privacy, i.e., FedLLM. In this survey, we provide a systematic and thorough review of the integration of LLMs with FL. Specifically, we first trace the historical evolution of both LLMs and FL, while summarizing relevant prior surveys. We then present an in-depth analysis of the fundamental challenges encountered in deploying FedLLM. Following this, we conduct an extensive study of existing parameter-efficient fine-tuning (PEFT) methods and explore their applicability in FL. Furthermore, we introduce a comprehensive evaluation benchmark to rigorously assess FedLLM performance and discuss its diverse real-world applications across multiple domains. Finally, we identify critical open challenges and outline promising research directions to drive future advancements in FedLLM. We maintain an active \\href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{GitHub repository} tracking cutting-edge advancements. This survey serves as a foundational resource for researchers and practitioners, offering insights into the evolving landscape of federated fine-tuning for LLMs while guiding future innovations in privacy-preserving AI.",
            "corpus_id": 277065732,
            "sentences": [
                {
                    "corpus_id": "277065732",
                    "title": "A Survey on Federated Fine-tuning of Large Language Models",
                    "text": "The training of LLMs typically involves two distinct stages (Xin et al., 2024): pre-training and fine-tuning, as shown in Figure 4. 1) Pre-training involves training the model on extensive, publicly available corpora, such as text from books and web pages. During this phase, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts. \n\n2) Fine-tuning, the second stage, tailors the pre-trained model to specific tasks or domains (Ding et al., 2023). In this phase, the model is further trained on task-specific datasets, typically using supervised learning techniques, to optimize its performance for particular applications such as text generation, sentiment analysis, or question answering. Fine-tuning refines the model's capabilities, enabling it to specialize its general language understanding to meet the requirements of the target task. However, traditional methods typically involve centralizing data from multiple sources on a single server for fine-tuning (Huang et al., 2025), which raises significant privacy concerns. This challenge highlights the need for innovative fine-tuning techniques that can effectively preserve data privacy.",
                    "score": 0.40527966033469653,
                    "section_title": "Training of LLMs",
                    "char_start_offset": 9866,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 256
                        },
                        {
                            "start": 257,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 965
                        },
                        {
                            "start": 968,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1780
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 604,
                            "end": 623,
                            "matchedPaperCorpusId": "195069387"
                        },
                        {
                            "start": 1061,
                            "end": 1080,
                            "matchedPaperCorpusId": "257316425"
                        },
                        {
                            "start": 1599,
                            "end": 1619,
                            "matchedPaperCorpusId": "265067168"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8994140625
                }
            ],
            "relevance_judgement": 0.8994140625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Federated Fine-tuning of Large Language Models\n# Venue: arXiv.org\n# Authors: Yebo Wu, Chunlin Tian, Jingguang Li, He Sun, Kahou Tam, Li Li, Chengzhong Xu\n## Abstract\nLarge Language Models (LLMs) have achieved remarkable success across a wide range of tasks, with fine-tuning playing a pivotal role in adapting them to specific downstream applications. Federated Learning (FL) offers a promising approach that enables collaborative model adaptation while ensuring data privacy, i.e., FedLLM. In this survey, we provide a systematic and thorough review of the integration of LLMs with FL. Specifically, we first trace the historical evolution of both LLMs and FL, while summarizing relevant prior surveys. We then present an in-depth analysis of the fundamental challenges encountered in deploying FedLLM. Following this, we conduct an extensive study of existing parameter-efficient fine-tuning (PEFT) methods and explore their applicability in FL. Furthermore, we introduce a comprehensive evaluation benchmark to rigorously assess FedLLM performance and discuss its diverse real-world applications across multiple domains. Finally, we identify critical open challenges and outline promising research directions to drive future advancements in FedLLM. We maintain an active \\href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{GitHub repository} tracking cutting-edge advancements. This survey serves as a foundational resource for researchers and practitioners, offering insights into the evolving landscape of federated fine-tuning for LLMs while guiding future innovations in privacy-preserving AI.\n## Training of LLMs\nThe training of LLMs typically involves two distinct stages (Xin et al., 2024): pre-training and fine-tuning, as shown in Figure 4. 1) Pre-training involves training the model on extensive, publicly available corpora, such as text from books and web pages. During this phase, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts. \n\n2) Fine-tuning, the second stage, tailors the pre-trained model to specific tasks or domains (Ding et al., 2023). In this phase, the model is further trained on task-specific datasets, typically using supervised learning techniques, to optimize its performance for particular applications such as text generation, sentiment analysis, or question answering. Fine-tuning refines the model's capabilities, enabling it to specialize its general language understanding to meet the requirements of the target task. However, traditional methods typically involve centralizing data from multiple sources on a single server for fine-tuning (Huang et al., 2025), which raises significant privacy concerns. This challenge highlights the need for innovative fine-tuning techniques that can effectively preserve data privacy.",
            "reference_string": "[277065732 | Wu et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 17,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.05476",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.05476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217728327",
                    "name": "Anusuya Krishnan"
                }
            ],
            "abstract": "Deceptive text classification is a critical task in natural language processing that aims to identify deceptive o fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content.",
            "corpus_id": 260775882,
            "sentences": [
                {
                    "corpus_id": "260775882",
                    "title": "Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis",
                    "text": "During the training phase of a Transformer-based model, we harness the power of self-attention mechanisms and large-scale pre-training to develop a robust and versatile language model. Transformer models, such as XLNET (EXtreme Learning Network), DistilBERT (Distilled Bidirectional Encoder Representations from Transformers), BERT (Bidirectional Encoder Representations from Transformers), and RoBERTa (Robustly Optimized Bidirectional Encoder Representations from Transformers Approach), have revolutionized natural language processing tasks. The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns. After pre-training, the model undergoes fine-tuning on specific downstream tasks. This involves training the model on labeled task-specific data. The labeled data could include sentiment analysis, named entity recognition, text classification, or question answering, among others. During fine-tuning, the model adapts its pre-trained knowledge to the task at hand, refining its internal parameters and learning task-specific patterns. The training data is typically split into training and validation sets. The model is trained on the training set, and its performance is evaluated on the validation set to monitor progress and prevent overfitting. Various optimization techniques, such as stochastic gradient descent (SGD) and Adam optimization, are employed to update the model's parameters iteratively and minimize the loss function. Throughout the training process, hyperparameter tuning is often performed to find the optimal settings for the model architecture, learning rate, batch size, and regularization. This fine-tuning allows the model to generalize well, improve its accuracy, and perform effectively on unseen data.",
                    "score": 0.40248703568666333,
                    "section_title": "Transformer based model training",
                    "char_start_offset": 25115,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 912
                        },
                        {
                            "start": 915,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1389
                        },
                        {
                            "start": 1390,
                            "end": 1524
                        },
                        {
                            "start": 1525,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2258
                        },
                        {
                            "start": 2259,
                            "end": 2374
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89892578125
                }
            ],
            "relevance_judgement": 0.89892578125,
            "relevance_judgment_input_expanded": "# Title: Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis\n# Venue: arXiv.org\n# Authors: Anusuya Krishnan\n## Abstract\nDeceptive text classification is a critical task in natural language processing that aims to identify deceptive o fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content.\n## Transformer based model training\nDuring the training phase of a Transformer-based model, we harness the power of self-attention mechanisms and large-scale pre-training to develop a robust and versatile language model. Transformer models, such as XLNET (EXtreme Learning Network), DistilBERT (Distilled Bidirectional Encoder Representations from Transformers), BERT (Bidirectional Encoder Representations from Transformers), and RoBERTa (Robustly Optimized Bidirectional Encoder Representations from Transformers Approach), have revolutionized natural language processing tasks. The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns. After pre-training, the model undergoes fine-tuning on specific downstream tasks. This involves training the model on labeled task-specific data. The labeled data could include sentiment analysis, named entity recognition, text classification, or question answering, among others. During fine-tuning, the model adapts its pre-trained knowledge to the task at hand, refining its internal parameters and learning task-specific patterns. The training data is typically split into training and validation sets. The model is trained on the training set, and its performance is evaluated on the validation set to monitor progress and prevent overfitting. Various optimization techniques, such as stochastic gradient descent (SGD) and Adam optimization, are employed to update the model's parameters iteratively and minimize the loss function. Throughout the training process, hyperparameter tuning is often performed to find the optimal settings for the model architecture, learning rate, batch size, and regularization. This fine-tuning allows the model to generalize well, improve its accuracy, and perform effectively on unseen data.",
            "reference_string": "[260775882 | Krishnan | 2023 | Citations: 1]"
        },
        {
            "title": "Word Acquisition in Neural Language Models",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 56,
            "citation_count": 40,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00444/1986589/tacl_a_00444.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.02406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2087001989",
                    "name": "Tyler A. Chang"
                },
                {
                    "authorId": "24316216",
                    "name": "B. Bergen"
                }
            ],
            "abstract": "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.",
            "corpus_id": 238408423,
            "sentences": [
                {
                    "corpus_id": "238408423",
                    "title": "Word Acquisition in Neural Language Models",
                    "text": "We selected the random chance baseline to best reflect a language model's ability to predict a word with no access to any training data, similar to an infant's language-specific knowledge prior to any linguistic exposure. We selected minimum surprisal as our other bound to reflect how well a particular word can eventually be learned by a particular language model, analogous to an adult's understanding of a given word. \n\nFor each learning curve, we found the intersection between the fitted sigmoid and the cutoff surprisal value. We defined age of acquisition for a language model as the corresponding training step, on a log10 scale. Sample cutoffs and ages of acquisition are shown in Figure 1 and Figure 2.",
                    "score": 0.44012148732144496,
                    "section_title": "Learning curves and ages of acquisition",
                    "char_start_offset": 9128,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 421
                        },
                        {
                            "start": 424,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 713
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8984375
                }
            ],
            "relevance_judgement": 0.8984375,
            "relevance_judgment_input_expanded": "# Title: Word Acquisition in Neural Language Models\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Tyler A. Chang, B. Bergen\n## Abstract\nWe investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.\n## Learning curves and ages of acquisition\nWe selected the random chance baseline to best reflect a language model's ability to predict a word with no access to any training data, similar to an infant's language-specific knowledge prior to any linguistic exposure. We selected minimum surprisal as our other bound to reflect how well a particular word can eventually be learned by a particular language model, analogous to an adult's understanding of a given word. \n\nFor each learning curve, we found the intersection between the fitted sigmoid and the cutoff surprisal value. We defined age of acquisition for a language model as the corresponding training step, on a log10 scale. Sample cutoffs and ages of acquisition are shown in Figure 1 and Figure 2.",
            "reference_string": "[238408423 | Chang et al. | 2021 | Citations: 40]"
        },
        {
            "title": "Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models",
            "venue": "Conference on Computational Natural Language Learning",
            "year": 2020,
            "reference_count": 72,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.conll-1.36.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.conll-1.36, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1900104",
                    "name": "Steven Derby"
                },
                {
                    "authorId": "2068246439",
                    "name": "Paul Miller"
                },
                {
                    "authorId": "1922600242",
                    "name": "Barry Devereux"
                }
            ],
            "abstract": "Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.",
            "corpus_id": 226283774,
            "sentences": [
                {
                    "corpus_id": "226283774",
                    "title": "Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models",
                    "text": "Neural Language Modelling has recently gained popularity in NLP. A Neural Network Language Model (NNLM) is tasked with learning a conditional probability distribution over the occurrences of words in text (Mikolov et al., 2011). This language modelling objective requires a neural network with sufficient capacity to learn meaningful linguistic information such as semantic knowledge and syntactic structure. Due to their ability to learn these important linguistic phenomena, NNLMs have been successfully employed as an effective method for generative pretraining (Dai and Le, 2015) and transfer learning to other natural language tasks (Peters et al., 2018a;Howard and Ruder, 2018;Radford et al., 2018). As previously suggested by Bengio et al. (2003), Mnih and Hinton (2007) and Mnih and Teh (2012), the weights of the final fully-connected output layer, or output embeddings, which compute the conditional probability distribution over the lexicon, also constitute a legitimate set of embedding vectors representing word meaning, as is the case for the input embeddings. This commonality between the input and output layers of the NNLM has motivated researchers to tie these representations together during training, improving performance on language modelling tasks (Inan et al., 2016;Press and Wolf, 2017). Furthermore, such a procedure is intuitive, since both the input and output embeddings of the network would appear to be performing a similar task of encoding information about lexical content. As described by Inan et al. (2016), they clearly live in an identical semantic space in language models, unlike other machine learning models were the input and output embeddings have no direct link.\n\nOn the other hand, it would also be reasonable to assume that the output representations require highly task-specific features (Peters et al., 2018a,b;Devlin et al., 2019). Despite their utility in language modelling, in-depth analysis of these input and output vector representations remains limited. The goal of this work is to gain a deeper understanding of the aspects of language captured in these contrasting representations. Our two main contributions 1 are as follows:\n\n1. We perform an investigation to uncover both the broad types of semantic knowledge and fine-grained linguistic phenomena encoded within each set of word representations.",
                    "score": 0.4101728018448685,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 205,
                            "end": 227,
                            "matchedPaperCorpusId": "14850173"
                        },
                        {
                            "start": 565,
                            "end": 583,
                            "matchedPaperCorpusId": "7138078"
                        },
                        {
                            "start": 638,
                            "end": 660,
                            "matchedPaperCorpusId": "3626819"
                        },
                        {
                            "start": 660,
                            "end": 683,
                            "matchedPaperCorpusId": "40100965"
                        },
                        {
                            "start": 733,
                            "end": 753,
                            "matchedPaperCorpusId": "221275765"
                        },
                        {
                            "start": 755,
                            "end": 777,
                            "matchedPaperCorpusId": "577005"
                        },
                        {
                            "start": 782,
                            "end": 801,
                            "matchedPaperCorpusId": "6633369"
                        },
                        {
                            "start": 1290,
                            "end": 1311,
                            "matchedPaperCorpusId": "836219"
                        },
                        {
                            "start": 1859,
                            "end": 1879,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8984375
                }
            ],
            "relevance_judgement": 0.8984375,
            "relevance_judgment_input_expanded": "# Title: Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models\n# Venue: Conference on Computational Natural Language Learning\n# Authors: Steven Derby, Paul Miller, Barry Devereux\n## Abstract\nResearchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.\n## Introduction\nNeural Language Modelling has recently gained popularity in NLP. A Neural Network Language Model (NNLM) is tasked with learning a conditional probability distribution over the occurrences of words in text (Mikolov et al., 2011). This language modelling objective requires a neural network with sufficient capacity to learn meaningful linguistic information such as semantic knowledge and syntactic structure. Due to their ability to learn these important linguistic phenomena, NNLMs have been successfully employed as an effective method for generative pretraining (Dai and Le, 2015) and transfer learning to other natural language tasks (Peters et al., 2018a;Howard and Ruder, 2018;Radford et al., 2018). As previously suggested by Bengio et al. (2003), Mnih and Hinton (2007) and Mnih and Teh (2012), the weights of the final fully-connected output layer, or output embeddings, which compute the conditional probability distribution over the lexicon, also constitute a legitimate set of embedding vectors representing word meaning, as is the case for the input embeddings. This commonality between the input and output layers of the NNLM has motivated researchers to tie these representations together during training, improving performance on language modelling tasks (Inan et al., 2016;Press and Wolf, 2017). Furthermore, such a procedure is intuitive, since both the input and output embeddings of the network would appear to be performing a similar task of encoding information about lexical content. As described by Inan et al. (2016), they clearly live in an identical semantic space in language models, unlike other machine learning models were the input and output embeddings have no direct link.\n\nOn the other hand, it would also be reasonable to assume that the output representations require highly task-specific features (Peters et al., 2018a,b;Devlin et al., 2019). Despite their utility in language modelling, in-depth analysis of these input and output vector representations remains limited. The goal of this work is to gain a deeper understanding of the aspects of language captured in these contrasting representations. Our two main contributions 1 are as follows:\n\n1. We perform an investigation to uncover both the broad types of semantic knowledge and fine-grained linguistic phenomena encoded within each set of word representations.",
            "reference_string": "[226283774 | Derby et al. | 2020 | Citations: 4]"
        },
        {
            "title": "Probing Linguistic Information For Logical Inference In Pre-trained Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/21294/21043",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.01753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111435018",
                    "name": "Zeming Chen"
                },
                {
                    "authorId": "2106414716",
                    "name": "Qiyue Gao"
                }
            ],
            "abstract": "Progress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.",
            "corpus_id": 244896159,
            "sentences": [
                {
                    "corpus_id": "244896159",
                    "title": "Probing Linguistic Information For Logical Inference In Pre-trained Language Models",
                    "text": "models encode linguistic information essential to symbolic inference systems? (2) Do pre-trained language models acquire new linguistic information for inference during the fine-tuning process for the NLI task? For each task, we conducted probing experiments on multiple contextualized language models and compared results to several strong baselines.\n\nOur analysis shows that language models encode diverse types of linguistic information for inference. In particular, they encode more information on simple semantic phenomena than complex semantic phenomena. Our label-wise qualitative analysis revealed that the amount of information encoded by language models for each task is different across The Thirty-Sixth AAAI Conference on Artificial Intelligence  labels which justifies our previous findings. Moreover, we found that pre-trained language models can obtain some types of the missing linguistic information through finetuning for the NLI task. Overall, our findings show that pretrained language models can be potential linguistic knowledge bases supporting symbolic inference systems.\n\nContributions Our contributions are as follows: 1. Our work expands on prior probing studies by studying a wider range of linguistic information, including simple and complex semantic phenomena. 2. Our experiments allow classifier expressiveness to be analyzed in a more complex setting covering syntactic and semantic linguistic properties beyond prior works. 3. Our study provides insights into what types of new linguistic information pre-trained language models obtain during fine-tuning on large NLI datasets. This contributes to the interpretability of NLI models.",
                    "score": 0.5253884787265384,
                    "section_title": "Introduction",
                    "char_start_offset": 2217,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89599609375
                }
            ],
            "relevance_judgement": 0.89599609375,
            "relevance_judgment_input_expanded": "# Title: Probing Linguistic Information For Logical Inference In Pre-trained Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zeming Chen, Qiyue Gao\n## Abstract\nProgress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.\n## Introduction\nmodels encode linguistic information essential to symbolic inference systems? (2) Do pre-trained language models acquire new linguistic information for inference during the fine-tuning process for the NLI task? For each task, we conducted probing experiments on multiple contextualized language models and compared results to several strong baselines.\n\nOur analysis shows that language models encode diverse types of linguistic information for inference. In particular, they encode more information on simple semantic phenomena than complex semantic phenomena. Our label-wise qualitative analysis revealed that the amount of information encoded by language models for each task is different across The Thirty-Sixth AAAI Conference on Artificial Intelligence  labels which justifies our previous findings. Moreover, we found that pre-trained language models can obtain some types of the missing linguistic information through finetuning for the NLI task. Overall, our findings show that pretrained language models can be potential linguistic knowledge bases supporting symbolic inference systems.\n\nContributions Our contributions are as follows: 1. Our work expands on prior probing studies by studying a wider range of linguistic information, including simple and complex semantic phenomena. 2. Our experiments allow classifier expressiveness to be analyzed in a more complex setting covering syntactic and semantic linguistic properties beyond prior works. 3. Our study provides insights into what types of new linguistic information pre-trained language models obtain during fine-tuning on large NLI datasets. This contributes to the interpretability of NLI models.",
            "reference_string": "[244896159 | Chen et al. | 2021 | Citations: 9]"
        },
        {
            "title": "CALaMo: a Constructionist Assessment of Language Models",
            "venue": "CXGSNLP",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.03589",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.03589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "22233530",
                    "name": "Ludovica Pannitto"
                },
                {
                    "authorId": "3352951",
                    "name": "Aur\u00e9lie Herbelot"
                }
            ],
            "abstract": "This paper presents a novel framework for evaluating Neural Language Models\u2019 linguistic abilities using a constructionist approach. Not only is the usage-based model in line with the un- derlying stochastic philosophy of neural architectures, but it also allows the linguist to keep meaning as a determinant factor in the analysis. We outline the framework and present two possible scenarios for its application.",
            "corpus_id": 256627634,
            "sentences": [
                {
                    "corpus_id": "256627634",
                    "title": "CALaMo: a Constructionist Assessment of Language Models",
                    "text": "In our proposed methodology, CALaMo (Constructionist Assessment of Language Models), we incorporate the UB perspective across all three aspects: input, stability and systematicity. \n\nAs far as input is concerned, CALaMo differs from standard approaches by considering input data an important factor in determining the shape of the learner's grammatical knowledge. In traditional scenarios, the input only serves as a triggering factor and its features play little to no role in the analysis. From a UB perspective, instead, the relation between the abstract grammatical structure of the input and the acquired grammar, which then constrains the production of the learner, is strict. \n\nRegarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training. \n\nThe compositionality vs. productivity perspectives, finally, entail a different organization of linguistic knowledge: the mainstream compositionality perspective tends to set meaning aside, and treat the lexicon as an organized repository of meanings (it makes sense therefore to test NLM's capabilities on semantically nonsensical sentences or to extend the known rules to completely unknown lexical items). In the productivity perspective, instead, meaning is intrinsically part of the process and is treated as a systematic aspect of grammar, too.",
                    "score": 0.4351148373740425,
                    "section_title": "CALaMo",
                    "char_start_offset": 12727,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 183,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 682
                        },
                        {
                            "start": 685,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1132
                        },
                        {
                            "start": 1135,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1685
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89501953125
                }
            ],
            "relevance_judgement": 0.89501953125,
            "relevance_judgment_input_expanded": "# Title: CALaMo: a Constructionist Assessment of Language Models\n# Venue: CXGSNLP\n# Authors: Ludovica Pannitto, Aur\u00e9lie Herbelot\n## Abstract\nThis paper presents a novel framework for evaluating Neural Language Models\u2019 linguistic abilities using a constructionist approach. Not only is the usage-based model in line with the un- derlying stochastic philosophy of neural architectures, but it also allows the linguist to keep meaning as a determinant factor in the analysis. We outline the framework and present two possible scenarios for its application.\n## CALaMo\nIn our proposed methodology, CALaMo (Constructionist Assessment of Language Models), we incorporate the UB perspective across all three aspects: input, stability and systematicity. \n\nAs far as input is concerned, CALaMo differs from standard approaches by considering input data an important factor in determining the shape of the learner's grammatical knowledge. In traditional scenarios, the input only serves as a triggering factor and its features play little to no role in the analysis. From a UB perspective, instead, the relation between the abstract grammatical structure of the input and the acquired grammar, which then constrains the production of the learner, is strict. \n\nRegarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training. \n\nThe compositionality vs. productivity perspectives, finally, entail a different organization of linguistic knowledge: the mainstream compositionality perspective tends to set meaning aside, and treat the lexicon as an organized repository of meanings (it makes sense therefore to test NLM's capabilities on semantically nonsensical sentences or to extend the known rules to completely unknown lexical items). In the productivity perspective, instead, meaning is intrinsically part of the process and is treated as a systematic aspect of grammar, too.",
            "reference_string": "[256627634 | Pannitto et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 12,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.21030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "20441545",
                    "name": "J. I. Orlicki"
                }
            ],
            "abstract": "Recent advances in large language models (LLMs) have popularized the chain-of-thought (CoT) paradigm, in which models produce explicit reasoning steps in natural language. Although this approach improves interpretability and facilitates external auditing, it may not represent the most computationally efficient method for internal reasoning. In contrast, human cognition relies on implicit mental representations that recall past sensory and episodic information without requiring complete verbalization. In this paper, we propose a framework that integrates implicit mental representations into the internal reasoning processes of LLMs. Preliminary experiments indicate that incorporating an Implicit Memory Module (IMM) into a simple GPT model yields a reduction of between 35% and 57% in final training loss compared to a regular GPT baseline. The addition of an explicit interpretability channel (e.g., a chain-of-thought decoder) is straightforward to implement within this approach. We outline theoretical foundations, propose technical mechanisms to scale the memory module, and discuss how these ideas may lead to more efficient and robust reasoning, with optional future extensions for explicit auditability.",
            "corpus_id": 276725505,
            "sentences": [
                {
                    "corpus_id": "276725505",
                    "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs",
                    "text": "Several influential works have explored the nature of latent representations in language models. Petroni et al. [2019] demonstrated that LLMs can act as implicit knowledge bases, storing factual information in their latent spaces. Similarly, Wei et al. [2022a] investigated how different model architectures and pretraining objectives impact zero-shot generalization, showing that improved latent representations correlate with enhanced generalization capabilities. A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics. \n\nKey insights include: \n\n\u2022 Specialized Directions: LLMs develop latent directions that encode specific concepts or linguistic features. \n\n\u2022 Layerwise Abstraction: Lower layers capture syntactic details while higher layers integrate complex semantic and world knowledge. \n\n\u2022 Scaling Benefits: The quality of latent representations improves with increased model scale and optimized architectures. \n\n3 Proposed Framework",
                    "score": 0.39848136981393567,
                    "section_title": "Latent Representations in LLMs",
                    "char_start_offset": 2085,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 700
                        },
                        {
                            "start": 703,
                            "end": 724
                        },
                        {
                            "start": 727,
                            "end": 837
                        },
                        {
                            "start": 840,
                            "end": 971
                        },
                        {
                            "start": 974,
                            "end": 1096
                        },
                        {
                            "start": 1099,
                            "end": 1119
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 118,
                            "matchedPaperCorpusId": "202539551"
                        },
                        {
                            "start": 242,
                            "end": 260,
                            "matchedPaperCorpusId": "248118752"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                }
            ],
            "relevance_judgement": 0.890625,
            "relevance_judgment_input_expanded": "# Title: Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs\n# Venue: arXiv.org\n# Authors: J. I. Orlicki\n## Abstract\nRecent advances in large language models (LLMs) have popularized the chain-of-thought (CoT) paradigm, in which models produce explicit reasoning steps in natural language. Although this approach improves interpretability and facilitates external auditing, it may not represent the most computationally efficient method for internal reasoning. In contrast, human cognition relies on implicit mental representations that recall past sensory and episodic information without requiring complete verbalization. In this paper, we propose a framework that integrates implicit mental representations into the internal reasoning processes of LLMs. Preliminary experiments indicate that incorporating an Implicit Memory Module (IMM) into a simple GPT model yields a reduction of between 35% and 57% in final training loss compared to a regular GPT baseline. The addition of an explicit interpretability channel (e.g., a chain-of-thought decoder) is straightforward to implement within this approach. We outline theoretical foundations, propose technical mechanisms to scale the memory module, and discuss how these ideas may lead to more efficient and robust reasoning, with optional future extensions for explicit auditability.\n## Latent Representations in LLMs\nSeveral influential works have explored the nature of latent representations in language models. Petroni et al. [2019] demonstrated that LLMs can act as implicit knowledge bases, storing factual information in their latent spaces. Similarly, Wei et al. [2022a] investigated how different model architectures and pretraining objectives impact zero-shot generalization, showing that improved latent representations correlate with enhanced generalization capabilities. A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics. \n\nKey insights include: \n\n\u2022 Specialized Directions: LLMs develop latent directions that encode specific concepts or linguistic features. \n\n\u2022 Layerwise Abstraction: Lower layers capture syntactic details while higher layers integrate complex semantic and world knowledge. \n\n\u2022 Scaling Benefits: The quality of latent representations improves with increased model scale and optimized architectures. \n\n3 Proposed Framework",
            "reference_string": "[276725505 | Orlicki | 2025 | Citations: 0]"
        },
        {
            "title": "Testing learning hypotheses using neural networks by manipulating learning data",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.04593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2058467905",
                    "name": "Cara Su-Yi Leong"
                },
                {
                    "authorId": "2467508",
                    "name": "Tal Linzen"
                }
            ],
            "abstract": "Although passivization is productive in English, it is not completely general -- some exceptions exist (e.g. *One hour was lasted by the meeting). How do English speakers learn these exceptions to an otherwise general pattern? Using neural network language models as theories of acquisition, we explore the sources of indirect evidence that a learner can leverage to learn whether a verb can passivize. We first characterize English speakers' judgments of exceptions to the passive, confirming that speakers find some verbs more passivizable than others. We then show that a neural network language model can learn restrictions to the passive that are similar to those displayed by humans, suggesting that evidence for these exceptions is available in the linguistic input. We test the causal role of two hypotheses for how the language model learns these restrictions by training models on modified training corpora, which we create by altering the existing training corpora to remove features of the input implicated by each hypothesis. We find that while the frequency with which a verb appears in the passive significantly affects its passivizability, the semantics of the verb does not. This study highlight the utility of altering a language model's training data for answering questions where complete control over a learner's input is vital.",
            "corpus_id": 271039363,
            "sentences": [
                {
                    "corpus_id": "271039363",
                    "title": "Testing learning hypotheses using neural networks by manipulating learning data",
                    "text": "Neural network language models are systems that learn probability distributions over sequences of words given a text corpus.While they are not tuned or designed to predict linguistic acceptability, they can be used to model acceptability judgments through targeted syntactic evaluations (Lau et al., 2017;Linzen et al., 2016;Marvin and Linzen, 2018;Warstadt et al., 2020).Given a minimal pair such as (1), a neural network language model can be used to assign each sequence of words a probability score.A model that assigns a higher probability to the grammatical sentence (1a) than than the ungrammatical (1b) shows sensitivity to the underlying syntactic differences between the two sentences.Targeted syntactic evaluations have shown that neural network language models are sensitive to a variety of syntactic and semantic constraints (Linzen et al., 2016;Warstadt et al., 2020).\n\nBy using neural network language models as theories of acquisition, we can address an existing methodological limitation in acquisition studies with humans, as these models allowing for complete control of the input provided to the learner: specifically, We can train multiple models on corpora which differ in controlled and targeted ways, and compare how learners with the same initial state and learning goals, but different input, diverge in their behavior at the end of learning.\n\nWe present a case study using this method.We train neural network language models on approximately the same amount of linguistic data that humans are exposed to, and use these models to answer two questions: firstly, is a human-scale amount of linguistic input sufficient for a model to learn to make judgments that are similar to human judgments on passive exceptions?Secondly, what kinds of information might a learner be using from the linguistic input to learn to make those judgments?\n\nIn Experiment 1, we answer the first question in the affirmative.We show that neural network language models can learn to make acceptability judgments on exceptions to passivization that are similar (though not identical) to those of English speakers.Such behavior suggests that language models can make use of indirect evidence in the linguistic input to learn exceptions.\n\nTo answer the second question, we take inspiration from work that uses systematic interventions on a model's training corpus to test causal links between a model's input and its eventual inferences (e.g.",
                    "score": 0.4117620279291482,
                    "section_title": "The role of neural networks",
                    "char_start_offset": 4602,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 124,
                            "end": 372
                        },
                        {
                            "start": 372,
                            "end": 503
                        },
                        {
                            "start": 503,
                            "end": 695
                        },
                        {
                            "start": 695,
                            "end": 882
                        },
                        {
                            "start": 884,
                            "end": 1368
                        },
                        {
                            "start": 1370,
                            "end": 1412
                        },
                        {
                            "start": 1412,
                            "end": 1739
                        },
                        {
                            "start": 1739,
                            "end": 1859
                        },
                        {
                            "start": 1861,
                            "end": 1926
                        },
                        {
                            "start": 1926,
                            "end": 2112
                        },
                        {
                            "start": 2112,
                            "end": 2234
                        },
                        {
                            "start": 2236,
                            "end": 2439
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 287,
                            "end": 305,
                            "matchedPaperCorpusId": "1056628"
                        },
                        {
                            "start": 305,
                            "end": 325,
                            "matchedPaperCorpusId": "14091946"
                        },
                        {
                            "start": 325,
                            "end": 349,
                            "matchedPaperCorpusId": "52113185"
                        },
                        {
                            "start": 349,
                            "end": 371,
                            "matchedPaperCorpusId": "208527435"
                        },
                        {
                            "start": 838,
                            "end": 859,
                            "matchedPaperCorpusId": "14091946"
                        },
                        {
                            "start": 859,
                            "end": 881,
                            "matchedPaperCorpusId": "208527435"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88427734375
                }
            ],
            "relevance_judgement": 0.88427734375,
            "relevance_judgment_input_expanded": "# Title: Testing learning hypotheses using neural networks by manipulating learning data\n# Venue: arXiv.org\n# Authors: Cara Su-Yi Leong, Tal Linzen\n## Abstract\nAlthough passivization is productive in English, it is not completely general -- some exceptions exist (e.g. *One hour was lasted by the meeting). How do English speakers learn these exceptions to an otherwise general pattern? Using neural network language models as theories of acquisition, we explore the sources of indirect evidence that a learner can leverage to learn whether a verb can passivize. We first characterize English speakers' judgments of exceptions to the passive, confirming that speakers find some verbs more passivizable than others. We then show that a neural network language model can learn restrictions to the passive that are similar to those displayed by humans, suggesting that evidence for these exceptions is available in the linguistic input. We test the causal role of two hypotheses for how the language model learns these restrictions by training models on modified training corpora, which we create by altering the existing training corpora to remove features of the input implicated by each hypothesis. We find that while the frequency with which a verb appears in the passive significantly affects its passivizability, the semantics of the verb does not. This study highlight the utility of altering a language model's training data for answering questions where complete control over a learner's input is vital.\n## The role of neural networks\nNeural network language models are systems that learn probability distributions over sequences of words given a text corpus.While they are not tuned or designed to predict linguistic acceptability, they can be used to model acceptability judgments through targeted syntactic evaluations (Lau et al., 2017;Linzen et al., 2016;Marvin and Linzen, 2018;Warstadt et al., 2020).Given a minimal pair such as (1), a neural network language model can be used to assign each sequence of words a probability score.A model that assigns a higher probability to the grammatical sentence (1a) than than the ungrammatical (1b) shows sensitivity to the underlying syntactic differences between the two sentences.Targeted syntactic evaluations have shown that neural network language models are sensitive to a variety of syntactic and semantic constraints (Linzen et al., 2016;Warstadt et al., 2020).\n\nBy using neural network language models as theories of acquisition, we can address an existing methodological limitation in acquisition studies with humans, as these models allowing for complete control of the input provided to the learner: specifically, We can train multiple models on corpora which differ in controlled and targeted ways, and compare how learners with the same initial state and learning goals, but different input, diverge in their behavior at the end of learning.\n\nWe present a case study using this method.We train neural network language models on approximately the same amount of linguistic data that humans are exposed to, and use these models to answer two questions: firstly, is a human-scale amount of linguistic input sufficient for a model to learn to make judgments that are similar to human judgments on passive exceptions?Secondly, what kinds of information might a learner be using from the linguistic input to learn to make those judgments?\n\nIn Experiment 1, we answer the first question in the affirmative.We show that neural network language models can learn to make acceptability judgments on exceptions to passivization that are similar (though not identical) to those of English speakers.Such behavior suggests that language models can make use of indirect evidence in the linguistic input to learn exceptions.\n\nTo answer the second question, we take inspiration from work that uses systematic interventions on a model's training corpus to test causal links between a model's input and its eventual inferences (e.g.",
            "reference_string": "[271039363 | Leong et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model",
            "venue": "BlackboxNLP@EMNLP",
            "year": 2018,
            "reference_count": 19,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W18-5449.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W18-5449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1900104",
                    "name": "Steven Derby"
                },
                {
                    "authorId": "2068246439",
                    "name": "Paul Miller"
                },
                {
                    "authorId": "145840946",
                    "name": "B. Murphy"
                },
                {
                    "authorId": "2331230",
                    "name": "Barry Devereux"
                }
            ],
            "abstract": "Performance in language modelling has been significantly improved by training recurrent neural networks on large corpora. This progress has come at the cost of interpretability and an understanding of how these architectures function, making principled development of better language models more difficult. We look inside a state-of-the-art neural language model to analyse how this model represents high-level lexico-semantic information. In particular, we investigate how the model represents words by extracting activation patterns where they occur in the text, and compare these representations directly to human semantic knowledge.",
            "corpus_id": 53324448,
            "sentences": [
                {
                    "corpus_id": "53324448",
                    "title": "Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model",
                    "text": "Language modelling involves learning to predict the next word in a sequence of words, using large text corpora as the training input. Language models must therefore learn to represent information from the preceding context which is relevant for future word prediction, and, intuitively, this should include information about the syntactic structure of the context and the meanings of constituent words. Today's state-of-the-art language models make use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory cells (LSTMs) (Hochreiter and Schmidhuber, 1997) which can handle time series information by remembering salient information over latent variables (Mikolov et al., 2010). Because of their wide applicability, there has been much interest in developing a better understanding of the inner workings of RNN models, and, in particular, researchers have investigated how syntactic knowledge is encoded and processed by such networks (Dyer et al., 2016;Linzen et al., 2016;Jozefowicz et al., 2016;Mc-Coy et al., 2018;Gulordava et al., 2018). Karpathy et al. (2015) performed an in-depth analysis of the types of errors RNN's make, in order to understand how recurrent mechanisms can encode long-term dependency information. Linzen et al. (2016) present a more direct analysis by examining LSTM language models' ability to understand difficult long-range dependencies such as the form of a verb linked to a noun subject. Recently, researchers have started to study the semantic embeddings generated by these networks (Chrupa\u0142a et al., 2015), especially for those focused on encoding visual grounding (Kiela et al., 2017;Yoo et al., 2017). However, compared to syntax, there has been relatively less work on how LSTM networks represent lexical semantic knowledge. \n\nIn this work, we evaluate latent semantic knowledge present in the LSTM activation patterns produced before and after the word of interest. We evaluate whether these activations predict human similarity ratings, human-derived property knowledge, and brain imaging data.",
                    "score": 0.42837110955638397,
                    "section_title": "Introduction & Related Work",
                    "char_start_offset": 30,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1429
                        },
                        {
                            "start": 1430,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1771
                        },
                        {
                            "start": 1774,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2043
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 531,
                            "end": 565,
                            "matchedPaperCorpusId": "1915014"
                        },
                        {
                            "start": 664,
                            "end": 686,
                            "matchedPaperCorpusId": "17048224"
                        },
                        {
                            "start": 1027,
                            "end": 1050,
                            "matchedPaperCorpusId": "4460159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8828125
                }
            ],
            "relevance_judgement": 0.8828125,
            "relevance_judgment_input_expanded": "# Title: Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model\n# Venue: BlackboxNLP@EMNLP\n# Authors: Steven Derby, Paul Miller, B. Murphy, Barry Devereux\n## Abstract\nPerformance in language modelling has been significantly improved by training recurrent neural networks on large corpora. This progress has come at the cost of interpretability and an understanding of how these architectures function, making principled development of better language models more difficult. We look inside a state-of-the-art neural language model to analyse how this model represents high-level lexico-semantic information. In particular, we investigate how the model represents words by extracting activation patterns where they occur in the text, and compare these representations directly to human semantic knowledge.\n## Introduction & Related Work\nLanguage modelling involves learning to predict the next word in a sequence of words, using large text corpora as the training input. Language models must therefore learn to represent information from the preceding context which is relevant for future word prediction, and, intuitively, this should include information about the syntactic structure of the context and the meanings of constituent words. Today's state-of-the-art language models make use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory cells (LSTMs) (Hochreiter and Schmidhuber, 1997) which can handle time series information by remembering salient information over latent variables (Mikolov et al., 2010). Because of their wide applicability, there has been much interest in developing a better understanding of the inner workings of RNN models, and, in particular, researchers have investigated how syntactic knowledge is encoded and processed by such networks (Dyer et al., 2016;Linzen et al., 2016;Jozefowicz et al., 2016;Mc-Coy et al., 2018;Gulordava et al., 2018). Karpathy et al. (2015) performed an in-depth analysis of the types of errors RNN's make, in order to understand how recurrent mechanisms can encode long-term dependency information. Linzen et al. (2016) present a more direct analysis by examining LSTM language models' ability to understand difficult long-range dependencies such as the form of a verb linked to a noun subject. Recently, researchers have started to study the semantic embeddings generated by these networks (Chrupa\u0142a et al., 2015), especially for those focused on encoding visual grounding (Kiela et al., 2017;Yoo et al., 2017). However, compared to syntax, there has been relatively less work on how LSTM networks represent lexical semantic knowledge. \n\nIn this work, we evaluate latent semantic knowledge present in the LSTM activation patterns produced before and after the word of interest. We evaluate whether these activations predict human similarity ratings, human-derived property knowledge, and brain imaging data.",
            "reference_string": "[53324448 | Derby et al. | 2018 | Citations: 2]"
        },
        {
            "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
            "venue": "Findings",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 42,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-acl.100.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.10540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1409707585",
                    "name": "Qian Liu"
                },
                {
                    "authorId": "2111181262",
                    "name": "Dejian Yang"
                },
                {
                    "authorId": null,
                    "name": "Jiahui Zhang"
                },
                {
                    "authorId": "2148899355",
                    "name": "Jiaqi Guo"
                },
                {
                    "authorId": "2118870582",
                    "name": "Bin Zhou"
                },
                {
                    "authorId": "153249455",
                    "name": "Jian-Guang Lou"
                }
            ],
            "abstract": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.",
            "corpus_id": 236478283,
            "sentences": [
                {
                    "corpus_id": "236478283",
                    "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
                    "text": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.",
                    "score": 0.40839932213322183,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88134765625
                }
            ],
            "relevance_judgement": 0.88134765625,
            "relevance_judgment_input_expanded": "# Title: Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing\n# Venue: Findings\n# Authors: Qian Liu, Dejian Yang, Jiahui Zhang, Jiaqi Guo, Bin Zhou, Jian-Guang Lou\n## Abstract\nRecent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.\n",
            "reference_string": "[236478283 | Liu et al. | 2021 | Citations: 42]"
        },
        {
            "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323825389",
                    "name": "Jiyeon Kim"
                },
                {
                    "authorId": "2140191673",
                    "name": "Hyunji Lee"
                },
                {
                    "authorId": "2305829618",
                    "name": "Hyowon Cho"
                },
                {
                    "authorId": "2000091730",
                    "name": "Joel Jang"
                },
                {
                    "authorId": "2161436241",
                    "name": "Hyeonbin Hwang"
                },
                {
                    "authorId": "40981970",
                    "name": "Seungpil Won"
                },
                {
                    "authorId": "2321094282",
                    "name": "Youbin Ahn"
                },
                {
                    "authorId": "2323896861",
                    "name": "Dohaeng Lee"
                },
                {
                    "authorId": "2266468609",
                    "name": "Minjoon Seo"
                }
            ],
            "abstract": "In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.",
            "corpus_id": 273025776,
            "sentences": [
                {
                    "corpus_id": "273025776",
                    "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
                    "text": "In this work, we examine how large language models' ability to broadly integrate their parametric knowledge (measured by knowledge entropy) changes throughout pretraining and how these changes affect knowledge acquisition and forgetting in a continual learning setup. Our findings reveal a strong correlation between knowledge entropy and the model's capacity to acquire and retain knowledge. Models in the final stages of pretraining tend to exhibit narrower integration of memory vectors, leading to lower knowledge entropy, which negatively impacts both knowledge acquisition and retention. Interestingly, artificially increasing knowledge entropy by modifying the parameters of final-stage models tends to improve these capabilities. \n\nBased on our analysis, we suggest that models from the mid-stage of pretraining offer a good balance between knowledge acquisition, retention, and overall performance, making them a good choice for further training to introduce new knowledge.",
                    "score": 0.4525115886582381,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 25068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 982
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition\n# Venue: International Conference on Learning Representations\n# Authors: Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo\n## Abstract\nIn this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.\n## CONCLUSION\nIn this work, we examine how large language models' ability to broadly integrate their parametric knowledge (measured by knowledge entropy) changes throughout pretraining and how these changes affect knowledge acquisition and forgetting in a continual learning setup. Our findings reveal a strong correlation between knowledge entropy and the model's capacity to acquire and retain knowledge. Models in the final stages of pretraining tend to exhibit narrower integration of memory vectors, leading to lower knowledge entropy, which negatively impacts both knowledge acquisition and retention. Interestingly, artificially increasing knowledge entropy by modifying the parameters of final-stage models tends to improve these capabilities. \n\nBased on our analysis, we suggest that models from the mid-stage of pretraining offer a good balance between knowledge acquisition, retention, and overall performance, making them a good choice for further training to introduce new knowledge.",
            "reference_string": "[273025776 | Kim et al. | 2024 | Citations: 5]"
        },
        {
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281641042",
                    "name": "Florin Cuconasu"
                },
                {
                    "authorId": "120709579",
                    "name": "Giovanni Trappolini"
                },
                {
                    "authorId": "2281641625",
                    "name": "Nicola Tonellotto"
                },
                {
                    "authorId": "2260653973",
                    "name": "Fabrizio Silvestri"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".",
            "corpus_id": 270688478,
            "sentences": [
                {
                    "corpus_id": "270688478",
                    "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
                    "text": "Here, we illustrate the training processes of LLMs, which consist of at least three main steps: pretraining, which is done on the task of next token prediction, instruction fine-tuning, and preference alignment.\n\nPre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.\n\nInstruction Fine-Tuning.Instruction fine-tuning is the part of the training process of large language models aimed at enhancing their ability to follow specific directives and perform specialized tasks (Taori et al., 2023).This phase involves further training of the base model on curated datasets that include explicit instructions paired with corresponding responses.The objective is to teach the model how to interpret and execute various types of commands or queries effectively.During instruction fine-tuning, the model is exposed to a wide range of examples that illustrate how to respond to different types of prompts, from answering questions to generating summaries and more complex task-oriented interactions.The primary purpose of instruction fine-tuning is to improve the model's usability and performance in real-world applications.By aligning the model's behavior with the specific needs and expectations of users, instruction finetuning ensures that the model can handle diverse and nuanced tasks more effectively, enhancing its overall functionality and practical utility.This pro-cess might involve using an instruction template, where specific patterns and structures for providing instructions and expected responses are standardized to facilitate consistent and effective learning.It is worth noticing that this is still achieved with traditional supervised learning, unlike the next phase we are considering.",
                    "score": 0.4006667585124933,
                    "section_title": "LLM Training",
                    "char_start_offset": 6387,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 213,
                            "end": 226
                        },
                        {
                            "start": 226,
                            "end": 464
                        },
                        {
                            "start": 464,
                            "end": 667
                        },
                        {
                            "start": 667,
                            "end": 850
                        },
                        {
                            "start": 850,
                            "end": 911
                        },
                        {
                            "start": 911,
                            "end": 1002
                        },
                        {
                            "start": 1004,
                            "end": 1028
                        },
                        {
                            "start": 1028,
                            "end": 1227
                        },
                        {
                            "start": 1227,
                            "end": 1373
                        },
                        {
                            "start": 1373,
                            "end": 1487
                        },
                        {
                            "start": 1487,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1849
                        },
                        {
                            "start": 1849,
                            "end": 2092
                        },
                        {
                            "start": 2092,
                            "end": 2305
                        },
                        {
                            "start": 2305,
                            "end": 2433
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87890625
                }
            ],
            "relevance_judgement": 0.87890625,
            "relevance_judgment_input_expanded": "# Title: A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems\n# Venue: arXiv.org\n# Authors: Florin Cuconasu, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri\n## Abstract\nRetrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".\n## LLM Training\nHere, we illustrate the training processes of LLMs, which consist of at least three main steps: pretraining, which is done on the task of next token prediction, instruction fine-tuning, and preference alignment.\n\nPre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.\n\nInstruction Fine-Tuning.Instruction fine-tuning is the part of the training process of large language models aimed at enhancing their ability to follow specific directives and perform specialized tasks (Taori et al., 2023).This phase involves further training of the base model on curated datasets that include explicit instructions paired with corresponding responses.The objective is to teach the model how to interpret and execute various types of commands or queries effectively.During instruction fine-tuning, the model is exposed to a wide range of examples that illustrate how to respond to different types of prompts, from answering questions to generating summaries and more complex task-oriented interactions.The primary purpose of instruction fine-tuning is to improve the model's usability and performance in real-world applications.By aligning the model's behavior with the specific needs and expectations of users, instruction finetuning ensures that the model can handle diverse and nuanced tasks more effectively, enhancing its overall functionality and practical utility.This pro-cess might involve using an instruction template, where specific patterns and structures for providing instructions and expected responses are standardized to facilitate consistent and effective learning.It is worth noticing that this is still achieved with traditional supervised learning, unlike the next phase we are considering.",
            "reference_string": "[270688478 | Cuconasu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Clinical insights: A comprehensive review of language models in medicine",
            "venue": "PLOS Digital Health",
            "year": 2024,
            "reference_count": 248,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.11735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "147692873",
                    "name": "Nikita Neveditsin"
                },
                {
                    "authorId": "2171126372",
                    "name": "Pawan Lingras"
                },
                {
                    "authorId": "2103123095",
                    "name": "V. Mago"
                }
            ],
            "abstract": "This paper explores the advancements and applications of language models in healthcare, focusing on their clinical use cases. It examines the evolution from early encoder-based systems requiring extensive fine-tuning to state-of-the-art large language and multimodal models capable of integrating text and visual data through in-context learning. The analysis emphasizes locally deployable models, which enhance data privacy and operational autonomy, and their applications in tasks such as text generation, classification, information extraction, and conversational systems. The paper also highlights a structured organization of tasks and a tiered ethical approach, providing a valuable resource for researchers and practitioners, while discussing key challenges related to ethics, evaluation, and implementation.",
            "corpus_id": 271916426,
            "sentences": [
                {
                    "corpus_id": "271916426",
                    "title": "Clinical insights: A comprehensive review of language models in medicine",
                    "text": "During pre-training, a model learns general language representations from large text corpora in an unsupervised manner. For example, BERT [25] predicts masked tokens, while GPT-2 [19] predicts the next token in a sequence. The fine-tuning phase then adapts these pre-trained models to specific tasks (e.g., classification, summarization) using labeled data. Although effective, this approach often demands substantial task-specific datasets, which can limit its scalability for diverse downstream applications. \n\nIn contrast, zero-shot learning enables models to generalize to unseen tasks without requiring task-specific fine-tuning. Instead, the model leverages its pre-trained knowledge and interprets carefully designed prompts to perform tasks directly. For example, a model in a zero-shot setting can respond to a prompt such as \"Summarize the key findings of this medical report\" without being explicitly trained on clinical summarization datasets. \n\nIn-context learning enhances the capabilities of these models by enabling them to solve tasks using information provided within the input prompt. Unlike fine-tuning, ICL does not require updating the model's weights. Instead, the model temporarily \"learns\" from examples in the prompt. For instance, the model can be provided with a series of medical cases that include patient symptoms, relevant background knowledge, and corresponding diagnoses. When presented with a new case, such as a patient with specific symptoms (e.g., coughing up phlegm and blood), the model can infer the most likely diagnosis based on the examples and background knowledge, without explicit task-specific fine-tuning [26]. \n\nNevertheless, challenges remain, as highlighted by Mahowald et al. [27], who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines.",
                    "score": 0.4048053171953685,
                    "section_title": "Language models",
                    "char_start_offset": 4847,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 510
                        },
                        {
                            "start": 513,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 955
                        },
                        {
                            "start": 958,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1659
                        },
                        {
                            "start": 1662,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 2032
                        },
                        {
                            "start": 2033,
                            "end": 2249
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 138,
                            "end": 142,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1654,
                            "end": 1658,
                            "matchedPaperCorpusId": "258741378"
                        },
                        {
                            "start": 1729,
                            "end": 1733,
                            "matchedPaperCorpusId": "268551442"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8720703125
                }
            ],
            "relevance_judgement": 0.8720703125,
            "relevance_judgment_input_expanded": "# Title: Clinical insights: A comprehensive review of language models in medicine\n# Venue: PLOS Digital Health\n# Authors: Nikita Neveditsin, Pawan Lingras, V. Mago\n## Abstract\nThis paper explores the advancements and applications of language models in healthcare, focusing on their clinical use cases. It examines the evolution from early encoder-based systems requiring extensive fine-tuning to state-of-the-art large language and multimodal models capable of integrating text and visual data through in-context learning. The analysis emphasizes locally deployable models, which enhance data privacy and operational autonomy, and their applications in tasks such as text generation, classification, information extraction, and conversational systems. The paper also highlights a structured organization of tasks and a tiered ethical approach, providing a valuable resource for researchers and practitioners, while discussing key challenges related to ethics, evaluation, and implementation.\n## Language models\nDuring pre-training, a model learns general language representations from large text corpora in an unsupervised manner. For example, BERT [25] predicts masked tokens, while GPT-2 [19] predicts the next token in a sequence. The fine-tuning phase then adapts these pre-trained models to specific tasks (e.g., classification, summarization) using labeled data. Although effective, this approach often demands substantial task-specific datasets, which can limit its scalability for diverse downstream applications. \n\nIn contrast, zero-shot learning enables models to generalize to unseen tasks without requiring task-specific fine-tuning. Instead, the model leverages its pre-trained knowledge and interprets carefully designed prompts to perform tasks directly. For example, a model in a zero-shot setting can respond to a prompt such as \"Summarize the key findings of this medical report\" without being explicitly trained on clinical summarization datasets. \n\nIn-context learning enhances the capabilities of these models by enabling them to solve tasks using information provided within the input prompt. Unlike fine-tuning, ICL does not require updating the model's weights. Instead, the model temporarily \"learns\" from examples in the prompt. For instance, the model can be provided with a series of medical cases that include patient symptoms, relevant background knowledge, and corresponding diagnoses. When presented with a new case, such as a patient with specific symptoms (e.g., coughing up phlegm and blood), the model can infer the most likely diagnosis based on the examples and background knowledge, without explicit task-specific fine-tuning [26]. \n\nNevertheless, challenges remain, as highlighted by Mahowald et al. [27], who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines.",
            "reference_string": "[271916426 | Neveditsin et al. | 2024 | Citations: 5]"
        },
        {
            "title": "A systematic investigation of learnability from single child linguistic input",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283881086",
                    "name": "Yulu Qin"
                },
                {
                    "authorId": "2108329007",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "2373318",
                    "name": "B. Lake"
                }
            ],
            "abstract": "Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.",
            "corpus_id": 267627053,
            "sentences": [
                {
                    "corpus_id": "267627053",
                    "title": "A systematic investigation of learnability from single child linguistic input",
                    "text": "Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.",
                    "score": 0.4203048304480879,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86962890625
                }
            ],
            "relevance_judgement": 0.86962890625,
            "relevance_judgment_input_expanded": "# Title: A systematic investigation of learnability from single child linguistic input\n# Venue: arXiv.org\n# Authors: Yulu Qin, Wentao Wang, B. Lake\n## Abstract\nLanguage models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.\n",
            "reference_string": "[267627053 | Qin et al. | 2024 | Citations: 5]"
        },
        {
            "title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2021,
            "reference_count": 95,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/12788/26750",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.01819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2841761",
                    "name": "Vassilina Nikoulina"
                },
                {
                    "authorId": "1471888023",
                    "name": "Maxat Tezekbayev"
                },
                {
                    "authorId": "2051805778",
                    "name": "Nuradil Kozhakhmet"
                },
                {
                    "authorId": "2051799158",
                    "name": "Madina Babazhanova"
                },
                {
                    "authorId": "2907260",
                    "name": "Matthias Gall\u00e9"
                },
                {
                    "authorId": "2842549",
                    "name": "Z. Assylbekov"
                }
            ],
            "abstract": "There is an ongoing debate in the NLP community whether modern language models contain linguistic knowledge, recovered through so-called probes. In this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the rediscovery hypothesis. \nIn the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. This result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. This framework also provides a metric to measure the impact of linguistic information on the word prediction task. We reinforce our analytical results with various experiments, both on synthetic and on real NLP tasks in English.",
            "corpus_id": 232092403,
            "sentences": [
                {
                    "corpus_id": "232092403",
                    "title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics",
                    "text": "into the pretrained model's behavior.\n\n\u2022 We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training.\n\nSection 3 proposes an information-theoretic formulation of the rediscovery hypothesis and a measure of the dependence between the text and linguistic annotations. In particular, Theorem 1 states that neural language models 28 do need to discover linguistic information to perform at their best: when the language models are denied access to linguistic information, 27. Pruning was done through iterative magnitude pruning (aka Lottery Ticket Hypothesis) or 1-shot magnitude pruning 28. This is, language models which rely on intermediate representations of tokens. their performance is not optimal. The metric \u03c1, which naturally appears in the development of our information-theoretic framework proposes a measure of the relationship between a given linguistic property and the underlying text. We show that it is also a reliable predictor of how suboptimal a language model will be if it is denied access to a given linguistic property. Our proposed methods of estimating \u03c1 can be used in further studies to quantify the dependence between text and its per-token annotation.\n\nA number of previous studies (Tenney et al., 2019b;Conneau et al., 2018;Hewitt & Manning, 2019;Zhang et al., 2020) have shown some evidence in favour of rediscovery hypothesis, while many others (Voita & Titov, 2020;Pimentel et al., 2020b;Hewitt & Liang, 2019;Elazar et al., 2021) questioned the methodology of those studies. However, none of the above-mentioned works questioned the rediscovery hypothesis per se nor demonstrated any evidence for rejecting it. We believe that our work brings an additional understanding of this phenomenon. In addition, it is the first attempt to formalize the rediscovery hypothesis and provide a theoretical frame for it.",
                    "score": 0.4173279510664004,
                    "section_title": "Conclusion",
                    "char_start_offset": 52507,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1519,
                            "end": 1541,
                            "matchedPaperCorpusId": "108300988"
                        },
                        {
                            "start": 1541,
                            "end": 1562,
                            "matchedPaperCorpusId": "24461982"
                        },
                        {
                            "start": 1562,
                            "end": 1585,
                            "matchedPaperCorpusId": "106402715"
                        },
                        {
                            "start": 1685,
                            "end": 1706,
                            "matchedPaperCorpusId": "214693050"
                        },
                        {
                            "start": 1706,
                            "end": 1729,
                            "matchedPaperCorpusId": "215238965"
                        },
                        {
                            "start": 1729,
                            "end": 1750,
                            "matchedPaperCorpusId": "202538609"
                        },
                        {
                            "start": 1750,
                            "end": 1770,
                            "matchedPaperCorpusId": "227408471"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8671875
                }
            ],
            "relevance_judgement": 0.8671875,
            "relevance_judgment_input_expanded": "# Title: The Rediscovery Hypothesis: Language Models Need to Meet Linguistics\n# Venue: Journal of Artificial Intelligence Research\n# Authors: Vassilina Nikoulina, Maxat Tezekbayev, Nuradil Kozhakhmet, Madina Babazhanova, Matthias Gall\u00e9, Z. Assylbekov\n## Abstract\nThere is an ongoing debate in the NLP community whether modern language models contain linguistic knowledge, recovered through so-called probes. In this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the rediscovery hypothesis. \nIn the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. This result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. This framework also provides a metric to measure the impact of linguistic information on the word prediction task. We reinforce our analytical results with various experiments, both on synthetic and on real NLP tasks in English.\n## Conclusion\ninto the pretrained model's behavior.\n\n\u2022 We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training.\n\nSection 3 proposes an information-theoretic formulation of the rediscovery hypothesis and a measure of the dependence between the text and linguistic annotations. In particular, Theorem 1 states that neural language models 28 do need to discover linguistic information to perform at their best: when the language models are denied access to linguistic information, 27. Pruning was done through iterative magnitude pruning (aka Lottery Ticket Hypothesis) or 1-shot magnitude pruning 28. This is, language models which rely on intermediate representations of tokens. their performance is not optimal. The metric \u03c1, which naturally appears in the development of our information-theoretic framework proposes a measure of the relationship between a given linguistic property and the underlying text. We show that it is also a reliable predictor of how suboptimal a language model will be if it is denied access to a given linguistic property. Our proposed methods of estimating \u03c1 can be used in further studies to quantify the dependence between text and its per-token annotation.\n\nA number of previous studies (Tenney et al., 2019b;Conneau et al., 2018;Hewitt & Manning, 2019;Zhang et al., 2020) have shown some evidence in favour of rediscovery hypothesis, while many others (Voita & Titov, 2020;Pimentel et al., 2020b;Hewitt & Liang, 2019;Elazar et al., 2021) questioned the methodology of those studies. However, none of the above-mentioned works questioned the rediscovery hypothesis per se nor demonstrated any evidence for rejecting it. We believe that our work brings an additional understanding of this phenomenon. In addition, it is the first attempt to formalize the rediscovery hypothesis and provide a theoretical frame for it.",
            "reference_string": "[232092403 | Nikoulina et al. | 2021 | Citations: 8]"
        },
        {
            "title": "Modelling constructivist language acquisition through syntactico-semantic pattern finding",
            "venue": "Royal Society Open Science",
            "year": 2024,
            "reference_count": 96,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://royalsocietypublishing.org/doi/pdf/10.1098/rsos.231998",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11268157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187454476",
                    "name": "Jonas Doumen"
                },
                {
                    "authorId": "2972424",
                    "name": "Katrien Beuls"
                },
                {
                    "authorId": "150266649",
                    "name": "Paul Van Eecke"
                }
            ],
            "abstract": "The constructivist acquisition of language by children has been elaborately documented by researchers in psycholinguistics and cognitive science. However, despite the centrality of human-like communication in the field of artificial intelligence, no faithful computational operationalizations of the mechanisms through which children learn language exist to date. In this article, we fill part of this void by introducing a mechanistic model of the constructivist acquisition of language through syntactico-semantic pattern finding. Concretely, we present a methodology for learning grammars based on similarities and differences in the form and meaning of linguistic observations alone. The resulting grammars consist of form-meaning mappings of variable extent and degree of abstraction, called constructions, which facilitate both language comprehension and production. Applying our methodology to the CLEVR benchmark dataset, we provide a proof of concept that demonstrates the online, incremental, data-efficient, transparent and effective learning of item-based construction grammars from utterance\u2013meaning pairs.",
            "corpus_id": 271430405,
            "sentences": [
                {
                    "corpus_id": "271430405",
                    "title": "Modelling constructivist language acquisition through syntactico-semantic pattern finding",
                    "text": "In particular, we provide a fully operational model of how a communicatively adequate linguistic system can be captured in the form of a collection of learned form-meaning mappings.These mappings can cover syntactico-semantic patterns of variable extent and degree of abstraction.The emerged constructions provide a unique insight into the compositional and non-compositional aspects of the learned language, as a consequence of the pattern-finding processes implemented by the repair strategies.Through these pattern finding processes, non-compositional pairings between aspects of observed form and meaning are included in constructions, while compositional aspects are generalized over through the use of variable slots.Like constructions, grammatical categories also emerge during the language acquisition process.In the spirit of radical construction grammar [4], categories are construction-specific and functionally motivated.They are conceived as fine-grained abstractions over observed syntacticosemantic usage patterns, and are captured in the form of a dynamic and adaptive categorial network.\n\nFinally, the methodology and experiments presented in this paper pave the way for learning computationally tractable, large-scale, usage-based grammars that facilitate both language comprehension and production.The proposed learning algorithm supports online, interactive, incremental, transparent and data-efficient learning.The learner builds up its human-interpretable inventory of constructions and categories through the application of transparent syntacticosemantic generalization processes.Already after a single observation, the fragment of linguistic knowledge acquired by the learner can be successfully used for language comprehension and production.As more and more utterance-meaning pairs are observed, the linguistic knowledge of the learner quickly expands and becomes better fit for achieving their communication goals.As a result of the dynamics of rewarding successful construction applications and punishing competing ones, the grammar of the learner remains ever-adaptive to any changes in the task or environment.Owing to their online, interactive, incremental, transparent and data-efficient nature, the proposed mechanisms for learning computational construction grammars that facilitate both language comprehension and production can serve as an excellent basis for implementing the language acquisition ability of autonomous agents.",
                    "score": 0.4210332631940338,
                    "section_title": "Discussion and conclusion",
                    "char_start_offset": 68262,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 181,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 496
                        },
                        {
                            "start": 496,
                            "end": 723
                        },
                        {
                            "start": 723,
                            "end": 818
                        },
                        {
                            "start": 818,
                            "end": 933
                        },
                        {
                            "start": 933,
                            "end": 1104
                        },
                        {
                            "start": 1106,
                            "end": 1317
                        },
                        {
                            "start": 1317,
                            "end": 1432
                        },
                        {
                            "start": 1432,
                            "end": 1603
                        },
                        {
                            "start": 1603,
                            "end": 1767
                        },
                        {
                            "start": 1767,
                            "end": 1941
                        },
                        {
                            "start": 1941,
                            "end": 2140
                        },
                        {
                            "start": 2140,
                            "end": 2463
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86669921875
                }
            ],
            "relevance_judgement": 0.86669921875,
            "relevance_judgment_input_expanded": "# Title: Modelling constructivist language acquisition through syntactico-semantic pattern finding\n# Venue: Royal Society Open Science\n# Authors: Jonas Doumen, Katrien Beuls, Paul Van Eecke\n## Abstract\nThe constructivist acquisition of language by children has been elaborately documented by researchers in psycholinguistics and cognitive science. However, despite the centrality of human-like communication in the field of artificial intelligence, no faithful computational operationalizations of the mechanisms through which children learn language exist to date. In this article, we fill part of this void by introducing a mechanistic model of the constructivist acquisition of language through syntactico-semantic pattern finding. Concretely, we present a methodology for learning grammars based on similarities and differences in the form and meaning of linguistic observations alone. The resulting grammars consist of form-meaning mappings of variable extent and degree of abstraction, called constructions, which facilitate both language comprehension and production. Applying our methodology to the CLEVR benchmark dataset, we provide a proof of concept that demonstrates the online, incremental, data-efficient, transparent and effective learning of item-based construction grammars from utterance\u2013meaning pairs.\n## Discussion and conclusion\nIn particular, we provide a fully operational model of how a communicatively adequate linguistic system can be captured in the form of a collection of learned form-meaning mappings.These mappings can cover syntactico-semantic patterns of variable extent and degree of abstraction.The emerged constructions provide a unique insight into the compositional and non-compositional aspects of the learned language, as a consequence of the pattern-finding processes implemented by the repair strategies.Through these pattern finding processes, non-compositional pairings between aspects of observed form and meaning are included in constructions, while compositional aspects are generalized over through the use of variable slots.Like constructions, grammatical categories also emerge during the language acquisition process.In the spirit of radical construction grammar [4], categories are construction-specific and functionally motivated.They are conceived as fine-grained abstractions over observed syntacticosemantic usage patterns, and are captured in the form of a dynamic and adaptive categorial network.\n\nFinally, the methodology and experiments presented in this paper pave the way for learning computationally tractable, large-scale, usage-based grammars that facilitate both language comprehension and production.The proposed learning algorithm supports online, interactive, incremental, transparent and data-efficient learning.The learner builds up its human-interpretable inventory of constructions and categories through the application of transparent syntacticosemantic generalization processes.Already after a single observation, the fragment of linguistic knowledge acquired by the learner can be successfully used for language comprehension and production.As more and more utterance-meaning pairs are observed, the linguistic knowledge of the learner quickly expands and becomes better fit for achieving their communication goals.As a result of the dynamics of rewarding successful construction applications and punishing competing ones, the grammar of the learner remains ever-adaptive to any changes in the task or environment.Owing to their online, interactive, incremental, transparent and data-efficient nature, the proposed mechanisms for learning computational construction grammars that facilitate both language comprehension and production can serve as an excellent basis for implementing the language acquisition ability of autonomous agents.",
            "reference_string": "[271430405 | Doumen et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "27729635",
                    "name": "Eva Portelance"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "2288266788",
                    "name": "T. O'Donnell"
                }
            ],
            "abstract": "Semantic and syntactic bootstrapping posit that children use their prior knowledge of one linguistic domain, say syntactic relations, to help later acquire another, such as the meanings of new words. Empirical results supporting both theories may tempt us to believe that these are different learning strategies, where one may precede the other. Here, we argue that they are instead both contingent on a more general learning strategy for language acquisition: joint learning. Using a series of neural visually-grounded grammar induction models, we demonstrate that both syntactic and semantic bootstrapping effects are strongest when syntax and semantics are learnt simultaneously. Joint learning results in better grammar induction, realistic lexical category learning, and better interpretations of novel sentence and verb meanings. Joint learning makes language acquisition easier for learners by mutually constraining the hypotheses spaces for both syntax and semantics. Studying the dynamics of joint inference over many input sources and modalities represents an important new direction for language modeling and learning research in both cognitive sciences and AI, as it may help us explain how language can be acquired in more constrained learning settings.",
            "corpus_id": 270562766,
            "sentences": [
                {
                    "corpus_id": "270562766",
                    "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
                    "text": "Semantic and syntactic bootstrapping posit that children use their prior knowledge of one linguistic domain, say syntactic relations, to help later acquire another, such as the meanings of new words. Empirical results supporting both theories may tempt us to believe that these are different learning strategies, where one may precede the other. Here, we argue that they are instead both contingent on a more general learning strategy for language acquisition: joint learning. Using a series of neural visually-grounded grammar induction models, we demonstrate that both syntactic and semantic bootstrapping effects are strongest when syntax and semantics are learnt simultaneously. Joint learning results in better grammar induction, realistic lexical category learning, and better interpretations of novel sentence and verb meanings. Joint learning makes language acquisition easier for learners by mutually constraining the hypotheses spaces for both syntax and semantics. Studying the dynamics of joint inference over many input sources and modalities represents an important new direction for language modeling and learning research in both cognitive sciences and AI, as it may help us explain how language can be acquired in more constrained learning settings.",
                    "score": 0.46838463054435364,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models\n# Venue: arXiv.org\n# Authors: Eva Portelance, Siva Reddy, T. O'Donnell\n## Abstract\nSemantic and syntactic bootstrapping posit that children use their prior knowledge of one linguistic domain, say syntactic relations, to help later acquire another, such as the meanings of new words. Empirical results supporting both theories may tempt us to believe that these are different learning strategies, where one may precede the other. Here, we argue that they are instead both contingent on a more general learning strategy for language acquisition: joint learning. Using a series of neural visually-grounded grammar induction models, we demonstrate that both syntactic and semantic bootstrapping effects are strongest when syntax and semantics are learnt simultaneously. Joint learning results in better grammar induction, realistic lexical category learning, and better interpretations of novel sentence and verb meanings. Joint learning makes language acquisition easier for learners by mutually constraining the hypotheses spaces for both syntax and semantics. Studying the dynamics of joint inference over many input sources and modalities represents an important new direction for language modeling and learning research in both cognitive sciences and AI, as it may help us explain how language can be acquired in more constrained learning settings.\n",
            "reference_string": "[270562766 | Portelance et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257315251",
                    "name": "Jun Zhao"
                },
                {
                    "authorId": "2262654002",
                    "name": "Zhihao Zhang"
                },
                {
                    "authorId": "2262551692",
                    "name": "Yide Ma"
                },
                {
                    "authorId": "2257376355",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "2262686179",
                    "name": "Luhui Gao"
                },
                {
                    "authorId": "2257129989",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Brain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.",
            "corpus_id": 264591406,
            "sentences": [
                {
                    "corpus_id": "264591406",
                    "title": "Unveiling A Core Linguistic Region in Large Language Models",
                    "text": "Inspired by cognitive science research, this paper investigates whether specific functional regions exist within LLMs. We identify a core region specifically responsible for language processing within LLMs. This region occupies only about 1% of the model's parameters but plays a crucial role in maintaining the overall linguistic competence of the model. Invalid changes in the parameters of this region can severely impair the model's linguistic competence. We also observe a pronounced dimension dependence in the core region of linguistic competence. Surprisingly, in a large model like LLaMA-13B, which boasts 13 billion parameters, altering just one parameter could potentially inflict significant damage to its linguistic competence. This study further elucidates the relationship between linguistic competence and knowledge in large language models. We find that an improvement in linguistic competence does not necessarily imply an enhancement in knowledge level. This suggests the presence of a knowledge storage region in LLMs that operates independently of language processing. In summary, the findings of this paper shed new light on how the capabilities and knowledge are structured in large language models and help explain why the pre-training and fine-tuning processes of these large models differ significantly from their smaller predecessors.",
                    "score": 0.4084351913154989,
                    "section_title": "CONCLUSIONS",
                    "char_start_offset": 25015,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1361
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8623046875
                }
            ],
            "relevance_judgement": 0.8623046875,
            "relevance_judgment_input_expanded": "# Title: Unveiling A Core Linguistic Region in Large Language Models\n# Venue: arXiv.org\n# Authors: Jun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui, Luhui Gao, Xuanjing Huang\n## Abstract\nBrain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.\n## CONCLUSIONS\nInspired by cognitive science research, this paper investigates whether specific functional regions exist within LLMs. We identify a core region specifically responsible for language processing within LLMs. This region occupies only about 1% of the model's parameters but plays a crucial role in maintaining the overall linguistic competence of the model. Invalid changes in the parameters of this region can severely impair the model's linguistic competence. We also observe a pronounced dimension dependence in the core region of linguistic competence. Surprisingly, in a large model like LLaMA-13B, which boasts 13 billion parameters, altering just one parameter could potentially inflict significant damage to its linguistic competence. This study further elucidates the relationship between linguistic competence and knowledge in large language models. We find that an improvement in linguistic competence does not necessarily imply an enhancement in knowledge level. This suggests the presence of a knowledge storage region in LLMs that operates independently of language processing. In summary, the findings of this paper shed new light on how the capabilities and knowledge are structured in large language models and help explain why the pre-training and fine-tuning processes of these large models differ significantly from their smaller predecessors.",
            "reference_string": "[264591406 | Zhao et al. | 2023 | Citations: 6]"
        },
        {
            "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?",
            "venue": "Current Directions in Psychological Science",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://osf.io/j5ckf/download",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/09637214241268098?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/09637214241268098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290681639",
                    "name": "Sam Whitman McGrath"
                },
                {
                    "authorId": "2321226194",
                    "name": "Jacob Russin"
                },
                {
                    "authorId": "2260118854",
                    "name": "Ellie Pavlick"
                },
                {
                    "authorId": "48522350",
                    "name": "Roman Feiman"
                }
            ],
            "abstract": "Over the last decade, deep neural networks (DNNs) have transformed the state of the art in artificial intelligence. In domains such as language production and reasoning, long considered uniquely human abilities, contemporary models have proven capable of strikingly human-like performance. However, in contrast to classical symbolic models, neural networks can be inscrutable even to their designers, making it unclear what significance, if any, they have for theories of human cognition. Two extreme reactions are common. Neural network enthusiasts argue that, because the inner workings of DNNs do not seem to resemble any of the traditional constructs of psychological or linguistic theory, their success renders these theories obsolete and motivates a radical paradigm shift. Neural network skeptics instead take this inability to interpret DNNs in psychological terms to mean that their success is irrelevant to psychological science. In this article, we review recent work that suggests that the internal mechanisms of DNNs can, in fact, be interpreted in the functional terms characteristic of psychological explanations. We argue that this undermines the shared assumption of both extremes and opens the door for DNNs to inform theories of cognition and its development.",
            "corpus_id": 272661905,
            "sentences": [
                {
                    "corpus_id": "272661905",
                    "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?",
                    "text": "For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable. They also demonstrated a more direct causal link-targeted interventions designed to prevent the model from learning these high-level representations obstruct its acquisition of the corresponding grammatical capabilities. Other evidence suggests that higher level descriptions also play a causal role once a model is trained. Manipulating the nodes and weights that correspond to higher level interpretations elicits the expected changes in the models' behavior. For example, editing the weights that a model has used to produce \"The Eiffel Tower is in Paris\" to instead produce \"The Eiffel Tower is in Rome\" has related downstream effects, such as causing the edited model to give directions that refer to the \"nearby\" St. Peter's Basilica (Meng et al., 2022).",
                    "score": 0.42029512189168633,
                    "section_title": "Challenging the Black-Box Assumption",
                    "char_start_offset": 6550,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1104
                        },
                        {
                            "start": 1107,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1556
                        },
                        {
                            "start": 1557,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2018
                        },
                        {
                            "start": 2019,
                            "end": 2317
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86181640625
                }
            ],
            "relevance_judgement": 0.86181640625,
            "relevance_judgment_input_expanded": "# Title: How Can Deep Neural Networks Inform Theory in Psychological Science?\n# Venue: Current Directions in Psychological Science\n# Authors: Sam Whitman McGrath, Jacob Russin, Ellie Pavlick, Roman Feiman\n## Abstract\nOver the last decade, deep neural networks (DNNs) have transformed the state of the art in artificial intelligence. In domains such as language production and reasoning, long considered uniquely human abilities, contemporary models have proven capable of strikingly human-like performance. However, in contrast to classical symbolic models, neural networks can be inscrutable even to their designers, making it unclear what significance, if any, they have for theories of human cognition. Two extreme reactions are common. Neural network enthusiasts argue that, because the inner workings of DNNs do not seem to resemble any of the traditional constructs of psychological or linguistic theory, their success renders these theories obsolete and motivates a radical paradigm shift. Neural network skeptics instead take this inability to interpret DNNs in psychological terms to mean that their success is irrelevant to psychological science. In this article, we review recent work that suggests that the internal mechanisms of DNNs can, in fact, be interpreted in the functional terms characteristic of psychological explanations. We argue that this undermines the shared assumption of both extremes and opens the door for DNNs to inform theories of cognition and its development.\n## Challenging the Black-Box Assumption\nFor example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable. They also demonstrated a more direct causal link-targeted interventions designed to prevent the model from learning these high-level representations obstruct its acquisition of the corresponding grammatical capabilities. Other evidence suggests that higher level descriptions also play a causal role once a model is trained. Manipulating the nodes and weights that correspond to higher level interpretations elicits the expected changes in the models' behavior. For example, editing the weights that a model has used to produce \"The Eiffel Tower is in Paris\" to instead produce \"The Eiffel Tower is in Rome\" has related downstream effects, such as causing the edited model to give directions that refer to the \"nearby\" St. Peter's Basilica (Meng et al., 2022).",
            "reference_string": "[272661905 | McGrath et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Birth of a Transformer: A Memory Viewpoint",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 96,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.00802",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269602",
                    "name": "A. Bietti"
                },
                {
                    "authorId": "1387995815",
                    "name": "Vivien A. Cabannes"
                },
                {
                    "authorId": "3365029",
                    "name": "Diane Bouchacourt"
                },
                {
                    "authorId": "1681054",
                    "name": "H. J\u00e9gou"
                },
                {
                    "authorId": "52184096",
                    "name": "L. Bottou"
                }
            ],
            "abstract": "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.",
            "corpus_id": 258999187,
            "sentences": [
                {
                    "corpus_id": "258999187",
                    "title": "Birth of a Transformer: A Memory Viewpoint",
                    "text": "As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors. \n\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [5]. These \"in-context\" learning capabilities are often attributed to the transformer architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require \"global\" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model. \n\nIn order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head mechanism [14,36], namely a \"circuit\" of two attention heads that allows the transformer to predict b from a context [\u2022 \u2022 \u2022 , a, b, \u2022 \u2022 \u2022 , a], and which appears to be ubiquitous in transformer language models [36,48]. \n\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13,29,40].",
                    "score": 0.40720614748052253,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 361
                        },
                        {
                            "start": 364,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 924
                        },
                        {
                            "start": 927,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1781
                        },
                        {
                            "start": 1784,
                            "end": 2034
                        },
                        {
                            "start": 2035,
                            "end": 2215
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 497,
                            "end": 500,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1777,
                            "end": 1780,
                            "matchedPaperCorpusId": "253244237"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8583984375
                }
            ],
            "relevance_judgement": 0.8583984375,
            "relevance_judgment_input_expanded": "# Title: Birth of a Transformer: A Memory Viewpoint\n# Venue: Neural Information Processing Systems\n# Authors: A. Bietti, Vivien A. Cabannes, Diane Bouchacourt, H. J\u00e9gou, L. Bottou\n## Abstract\nLarge language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.\n## Introduction\nAs large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors. \n\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [5]. These \"in-context\" learning capabilities are often attributed to the transformer architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require \"global\" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model. \n\nIn order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head mechanism [14,36], namely a \"circuit\" of two attention heads that allows the transformer to predict b from a context [\u2022 \u2022 \u2022 , a, b, \u2022 \u2022 \u2022 , a], and which appears to be ubiquitous in transformer language models [36,48]. \n\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13,29,40].",
            "reference_string": "[258999187 | Bietti et al. | 2023 | Citations: 96]"
        },
        {
            "title": "Word learning and the acquisition of syntactic-semantic overhypotheses",
            "venue": "Annual Meeting of the Cognitive Science Society",
            "year": 2018,
            "reference_count": 23,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.04988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "24339276",
                    "name": "Jon Gauthier"
                },
                {
                    "authorId": "143643017",
                    "name": "R. Levy"
                },
                {
                    "authorId": "1763295",
                    "name": "J. Tenenbaum"
                }
            ],
            "abstract": "Children learning their first language face multiple problems of induction: how to learn the meanings of words, and how to build meaningful phrases from those words according to syntactic rules. We consider how children might solve these problems efficiently by solving them jointly, via a computational model that learns the syntax and semantics of multi-word utterances in a grounded reference game. We select a well-studied empirical case in which children are aware of patterns linking the syntactic and semantic properties of words --- that the properties picked out by base nouns tend to be related to shape, while prenominal adjectives tend to refer to other properties such as color. We show that children applying such inductive biases are accurately reflecting the statistics of child-directed speech, and that inducing similar biases in our computational model captures children's behavior in a classic adjective learning experiment. Our model incorporating such biases also demonstrates a clear data efficiency in learning, relative to a baseline model that learns without forming syntax-sensitive overhypotheses of word meaning. Thus solving a more complex joint inference problem may make the full problem of language acquisition easier, not harder.",
            "corpus_id": 44093987,
            "sentences": [
                {
                    "corpus_id": "44093987",
                    "title": "Word learning and the acquisition of syntactic-semantic overhypotheses",
                    "text": "This paper connects to a large body of recent work on computational models of word learning. Most relevant is the work of Abend et al. (2017) and Sadeghi and Scheutz (2018), who present models which jointly learn both word meanings and syntactic properties of language. Their models demonstrate how syntactic knowledge can be leveraged to more quickly infer the meanings of words. Our contribution is complementary: we introduce more complex structured probabilistic (a) Online accuracy curves (see Equation 5) estimated on a fixed test set after each training example. Shaded region represents bootstrap estimate of 95% CI.\n\n(b) Syntactic-semantic overhypotheses support rapid learning. The red line (left axis) plots the mean difference in R between runs of the overhypothesis model and runs of the base model as learning progresses. The gray line (right axis) tracks the average strength of the induced overhypothesis p(t = color | s = N/N) in the overhypothesis model. Shaded region represents bootstrapped 95% CI. Figure 5: Results from the computational simulation of syntactic-semantic overhypothesis learning. models supporting a class of overhypothesis linking syntactic properties of words to certain aspects of their meaning.\n\nOur model is an initial demonstration of how overhypotheses are a clear adaptive feature for a language learner, and how they can be tractably modeled by linking distinct computational tools. While there is naturally more to children's adjective learning than the phenomenon described in this paper, we believe the results support a general conclusion about the structure of the language learning problem. By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion.",
                    "score": 0.47458550864702626,
                    "section_title": "Discussion",
                    "char_start_offset": 23691,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 122,
                            "end": 141,
                            "matchedPaperCorpusId": "206866667"
                        },
                        {
                            "start": 146,
                            "end": 172,
                            "matchedPaperCorpusId": "19126641"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85498046875
                }
            ],
            "relevance_judgement": 0.85498046875,
            "relevance_judgment_input_expanded": "# Title: Word learning and the acquisition of syntactic-semantic overhypotheses\n# Venue: Annual Meeting of the Cognitive Science Society\n# Authors: Jon Gauthier, R. Levy, J. Tenenbaum\n## Abstract\nChildren learning their first language face multiple problems of induction: how to learn the meanings of words, and how to build meaningful phrases from those words according to syntactic rules. We consider how children might solve these problems efficiently by solving them jointly, via a computational model that learns the syntax and semantics of multi-word utterances in a grounded reference game. We select a well-studied empirical case in which children are aware of patterns linking the syntactic and semantic properties of words --- that the properties picked out by base nouns tend to be related to shape, while prenominal adjectives tend to refer to other properties such as color. We show that children applying such inductive biases are accurately reflecting the statistics of child-directed speech, and that inducing similar biases in our computational model captures children's behavior in a classic adjective learning experiment. Our model incorporating such biases also demonstrates a clear data efficiency in learning, relative to a baseline model that learns without forming syntax-sensitive overhypotheses of word meaning. Thus solving a more complex joint inference problem may make the full problem of language acquisition easier, not harder.\n## Discussion\nThis paper connects to a large body of recent work on computational models of word learning. Most relevant is the work of Abend et al. (2017) and Sadeghi and Scheutz (2018), who present models which jointly learn both word meanings and syntactic properties of language. Their models demonstrate how syntactic knowledge can be leveraged to more quickly infer the meanings of words. Our contribution is complementary: we introduce more complex structured probabilistic (a) Online accuracy curves (see Equation 5) estimated on a fixed test set after each training example. Shaded region represents bootstrap estimate of 95% CI.\n\n(b) Syntactic-semantic overhypotheses support rapid learning. The red line (left axis) plots the mean difference in R between runs of the overhypothesis model and runs of the base model as learning progresses. The gray line (right axis) tracks the average strength of the induced overhypothesis p(t = color | s = N/N) in the overhypothesis model. Shaded region represents bootstrapped 95% CI. Figure 5: Results from the computational simulation of syntactic-semantic overhypothesis learning. models supporting a class of overhypothesis linking syntactic properties of words to certain aspects of their meaning.\n\nOur model is an initial demonstration of how overhypotheses are a clear adaptive feature for a language learner, and how they can be tractably modeled by linking distinct computational tools. While there is naturally more to children's adjective learning than the phenomenon described in this paper, we believe the results support a general conclusion about the structure of the language learning problem. By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion.",
            "reference_string": "[44093987 | Gauthier et al. | 2018 | Citations: 10]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
            "score": 0.6403740176307853,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 612,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1567,
                    "end": 1588,
                    "matchedPaperCorpusId": "219315567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "259089351",
            "title": "Language acquisition: do children and language models follow similar learning stages?",
            "text": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.",
            "score": 0.5970661685834558,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "In the previous sections, we report general patterns during language model pre-training ( \u00a74), define ways to characterize learning curves ( \u00a75), and isolate specific features that predict the speed and stability of learning for individual tokens in context ( \u00a76). Our results contribute to ongoing work studying language model pre-training dynamics, with implications for robust model deployment. \n\nSequential learning. Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and Xia et al., 2023) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step. 10 Rather, they reflect some change in model processing that consistently occurs partially into pre-training (roughly step t \u0338 = 0). Because such a sudden change cannot be attributed to the specific examples observed (robust to random shuffling) or any change in the pre-training distribution at time t (the data is always i.i.d.), the primary remaining explanation is that the models' sudden \"learning\" at step t \u0338 = 0 is made possible by some systematic difference between models (and their optimizers) just before step t vs. at step 0. \n\nFramed from a potentially more interesting perspective, some types of language model \"learning\" appear to be dependent on previous learning and the linguistic abilities already present in the model. This aligns with previous work showing that language models acquire linguistic abilities in a systematic order during pre-training (Liu et al., 2021;Choshen et al., 2022), although not necessarily due to sequential dependencies. For example, Evanson et al. (2023) show that despite similar acquisition orders across models, different syntactic abilities are learned in parallel; performance for most individual abilities increases from the onset of pre-training. Our work provides evidence that there exist other capabilities or types of generalizations (e.g.",
            "score": 0.5796959075790518,
            "section_title": "Discussion",
            "char_start_offset": 27189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 674,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 1930,
                    "end": 1948,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 2041,
                    "end": 2062,
                    "matchedPaperCorpusId": "259089351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "276902722",
            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
            "text": "Large Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
            "score": 0.5515748139319179,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "244896159",
            "title": "Probing Linguistic Information For Logical Inference In Pre-trained Language Models",
            "text": "models encode linguistic information essential to symbolic inference systems? (2) Do pre-trained language models acquire new linguistic information for inference during the fine-tuning process for the NLI task? For each task, we conducted probing experiments on multiple contextualized language models and compared results to several strong baselines.\n\nOur analysis shows that language models encode diverse types of linguistic information for inference. In particular, they encode more information on simple semantic phenomena than complex semantic phenomena. Our label-wise qualitative analysis revealed that the amount of information encoded by language models for each task is different across The Thirty-Sixth AAAI Conference on Artificial Intelligence  labels which justifies our previous findings. Moreover, we found that pre-trained language models can obtain some types of the missing linguistic information through finetuning for the NLI task. Overall, our findings show that pretrained language models can be potential linguistic knowledge bases supporting symbolic inference systems.\n\nContributions Our contributions are as follows: 1. Our work expands on prior probing studies by studying a wider range of linguistic information, including simple and complex semantic phenomena. 2. Our experiments allow classifier expressiveness to be analyzed in a more complex setting covering syntactic and semantic linguistic properties beyond prior works. 3. Our study provides insights into what types of new linguistic information pre-trained language models obtain during fine-tuning on large NLI datasets. This contributes to the interpretability of NLI models.",
            "score": 0.5253884787265384,
            "section_title": "Introduction",
            "char_start_offset": 2217,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Previous work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.",
            "score": 0.5199418460785281,
            "section_title": "Related Work",
            "char_start_offset": 3961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 95,
                    "matchedPaperCorpusId": "267903883"
                },
                {
                    "start": 123,
                    "end": 144,
                    "matchedPaperCorpusId": "259089351"
                },
                {
                    "start": 422,
                    "end": 443,
                    "matchedPaperCorpusId": "222140842"
                },
                {
                    "start": 443,
                    "end": 460,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 942,
                    "end": 967,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1046,
                    "end": 1069,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1147,
                    "end": 1170,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1198,
                    "end": 1215,
                    "matchedPaperCorpusId": "254877112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Language models have received unprecedented attention in recent years due to impressive performance on natural language tasks (e.g. OpenAI, 2022;Google, 2023;Anthropic, 2023). However, these models are initialized as random word (token) generators, and it remains unclear how the models achieve complex linguistic abilities during pre-training. Previous work has investigated when syntactic, semantic, and reasoning abilities emerge (Liu et al., 2021;Evanson et al., 2023), quantified ages of acquisition for tokens averaged over contexts (Chang and Bergen, 2022b), and extracted learning curves for individual examples (Xia et al., 2023). However, features that influence individual learning curves have yet to be identified (e.g. n-gram probabilities and context lengths). Given any token in context, it is largely unknown when or how stably that token would be learned. \n\nFrom a scientific perspective, understanding when examples are learned by language models can provide insights into possible mechanisms for language acquisition. Regardless of their similarity to human language processing, language models are exemplars of how learning from language statistics alone (i.e. \"distributional\" learning) can lead to complex linguistic abilities (Chang and Bergen, 2022b;Warstadt and Bowman, 2023;Mahowald et al., 2023). Notably, despite smoothly decreasing corpus-level loss and independent and identically distributed (i.i.d.) data throughout pre-training, individual text examples exhibit learning curves with sudden decreases and increases in loss ( \u00a75 and Xia et al., 2023). This highlights the importance of examining individual example learning curves for pre-training dynamics research; aggregate curves often do not capture the fluctuations exhibited by individual examples. Our work seeks to characterize these fine-grained convergence patterns in terms of simpler distributional statistics. \n\nFrom a practical perspective, understanding language model learning curves can inform the pre-training and deployment of language models. Learning curve results might allow NLP practitioners to determine how much pre-training is necessary for different capabilities and what behaviors will remain stable after additional pre-training (e.g. \"continual learning\" on more recent data; Jin et al., 2022).",
            "score": 0.5100056535518896,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 451,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 451,
                    "end": 472,
                    "matchedPaperCorpusId": "259089351"
                },
                {
                    "start": 539,
                    "end": 564,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 1249,
                    "end": 1274,
                    "matchedPaperCorpusId": "238408423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "264451507",
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
            "text": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
            "score": 0.5082029162209087,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "270562766",
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "text": "where \u03b1 1 , \u03b1 2 are constants, here both equal to 1. Since the syntactic and semantic objectives are interdependent, during learning they will affect the joint model's learning trajectory by mutually constraining updates to the grammar model and semantic encoder.On the one hand, the semantic objective will push the grammar model to favor rules which derive trees containing constituents that can be visually represented.On the other, the syntactic objective will determine the distribution over constituents being mapped to semantic space.We illustrate the model and this relation between objectives in Figure 3.In using our model as a cognitive model, we are making the following assumptions about the acquisition of syntactic and semantic knowledge: (1) we start with the prior knowledge that there are such things as syntactic categories (though what they represent and how many there are is unknown); (2) grammatical structure can be represented by a PCFG; (3) meaning is grounded in visual representations.The first assumption is feasable under most linguistic theories and theories of acquisition.The second and third, are likely simplifications of children's learning environment: PCFGs may not having enough explanatory power for natural language (Shieber, 1985;Huybregts, 1984), and children ground language in embodied experiences going way beyond still images.By definition, a model is a simplified exemplification of a process.We acknowledge that our model does not capture all the complexities present in a child's naturalistic learning environment.However, we are using our model to study specific research questions and not language learning as a whole.We believe that this joint-learning model can demonstrate how joint learning objectives for language can aid language acquisition overall and explain bootstrapping phenomena, like syntactic and semantic bootstrapping.",
            "score": 0.5032513309647553,
            "section_title": "The semantic objective",
            "char_start_offset": 23848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1441
                },
                {
                    "start": 1441,
                    "end": 1564
                },
                {
                    "start": 1564,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 1257,
                    "end": 1272,
                    "matchedPaperCorpusId": "222277837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "249017999",
            "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
            "text": "Large-scale language models pretrained jointly on text from many different languages (Delvin, 2019;Lample and Conneau, 2019;Lin et al., 2021) perform very well on various languages and on crosslingual transfer between them (e.g., Kondratyuk and Straka, 2019;Pasini et al., 2021). Due to this success, there has been a great deal of interest in uncovering what these models learn from the multilingual pretraining signal ( \u00a76). However, these works analyze a single model artifact: the final training checkpoint at which the model is considered to be converged. Recent work has also studied monolingual models by expanding the analysis to multiple pretraining checkpoints to see how model knowledge changes across time (Liu et al., 2021). \n\nWe analyze multilingual training checkpoints throughout the pretraining process in order to identify when multilingual models obtain their inlanguage and cross-lingual abilities. The case of multilingual language models is particularly interesting, as the model learns both to capture individual languages and to transfer between them just from unbalanced multitask language modeling for each language. \n\nSpecifically, we retrain a popular multilingual model, XLM-R (Conneau et al., 2020a), and run a suite of linguistic tasks covering 59 languages on checkpoints from across the pretraining process. 1 his suite evaluates different syntactic and semantic skills in both monolingual and cross-lingual transfer settings. While our analysis primarily focuses on the knowledge captured in model output representations over time, we also consider how the performance of internal layers changes during pretraining for a subset of tasks. \n\nOur analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics. In comparison, the order in which the model learns to transfer linguistic information between specific languages can vary wildly.",
            "score": 0.49487521243134003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2253
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 258,
                    "matchedPaperCorpusId": "102351547"
                },
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "231878544"
                },
                {
                    "start": 718,
                    "end": 736,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1206,
                    "end": 1229,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "270045675",
            "title": "Filtered Corpus Training (FiCT) Shows that Language Models Can Generalize from Indirect Evidence",
            "text": "While targeted syntactic evaluations give an insight into a model's linguistic competence, it does not show how a model acquires this notion of grammaticality. In this paper we focus on two kinds of linguistic generalization. Structural generalization (Hupkes et al., 2023) asks: can language models make grammaticality judgments in syntactically more complex constructions than seen during training? One line of work approaches this question from a fine-tuning perspective: by fine-tuning a model on a particular set of constructions we can measure the impact that this has on other linguistic constructions (Prasad et al., 2019;Weber et al., 2024). Lexical generalization asks whether models can generalize a seen construction to new lexical items that it has not seen in that construction (Kim and Linzen, 2020). \n\nIn order to gain a causal perspective on how the training data influences model performance, we retrain models from scratch on filtered corpora. This methodology has been deployed in earlier work to investigate how LMs learn the licensing conditions of negative polarity items from different contexts (Jumelet et al., 2021;Weber et al., 2021). Warstadt (2022) investigates the poverty of the stimulus debate through the lens of filtered corpora, focusing on the phenomenon of subject auxiliary inversion. Finally, Misra and Mahowald (2024) investigate rare adjective-noun constructions and manipulate training corpora to investigate how models acquire an understanding of rare constructions. Whereas most of these focus on a particular linguistic construction, our work applies the approach to a wide range of phenomena.",
            "score": 0.49381475208088604,
            "section_title": "Linguistic Generalization",
            "char_start_offset": 6642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 273,
                    "matchedPaperCorpusId": "264490385"
                },
                {
                    "start": 609,
                    "end": 630,
                    "matchedPaperCorpusId": "202734477"
                },
                {
                    "start": 630,
                    "end": 649,
                    "matchedPaperCorpusId": "270370801"
                },
                {
                    "start": 792,
                    "end": 814,
                    "matchedPaperCorpusId": "222290851"
                },
                {
                    "start": 1119,
                    "end": 1141,
                    "matchedPaperCorpusId": "235248063"
                },
                {
                    "start": 1141,
                    "end": 1160,
                    "matchedPaperCorpusId": "231719146"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "270620163",
            "title": "Probing the Emergence of Cross-lingual Alignment during LLM Training",
            "text": "Concurrently, there is a rising interest in understanding the training dynamics of LLMs.Works that mainly examine monolingual English models report a steady trend in the acquisition of linguistic knowledge.Both Xia et al. (2023) and Choshen et al. (2022) argue that language acquisition undergoes the same order of phase transitions consistently across model scales, training objectives and random seeds.Chen et al. (2024) find that the emergence of syntactic structure in the attention scores of Transformer-based LMs is essential for grammar acquisition in LMs, but does not account for semantic knowledge acquisition.For multilingual training, Choenni et al. (2023) examine how data size and language variance affect the performance during fine-tuning.The experiments presented by Blevins et al. (2022) are the most reminiscent of      our work.They focus on the inconsistency between the emergence of in-language and cross-language abilities for encoder LMs, whereas we study the dynamics of neuron overlaps and the corresponding impact on downstream performance in autoregressive LMs.",
            "score": 0.4928935837927739,
            "section_title": "Knowledge Acquisition during Pre-training.",
            "char_start_offset": 21398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 88,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 233,
                    "end": 254,
                    "matchedPaperCorpusId": "237491997"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 647,
                    "end": 668,
                    "matchedPaperCorpusId": "258832559"
                },
                {
                    "start": 784,
                    "end": 805,
                    "matchedPaperCorpusId": "249017999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "218571018",
            "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
            "text": "This work addresses multiple open questions about syntactic evaluations and their relationship to other language model assessments. Our results dissociate model perplexity and performance in syntactic generalization tests, suggesting that the two metrics capture complementary features of language model knowledge. In a controlled evaluation of different model classes and datasets, we find model architecture plays a more important role than training data scale in yielding correct syntactic generalizations. Our circuit-level analysis reveals consistent failure on Licensing but inconsistent behavior on other circuits, suggesting that different syntactic circuits make use of different underlying processing capacities. In addition to the insight these results provide about neural NLP systems, they also bear on questions central to cognitive science and linguistics, putting lower bounds on what syntactic knowledge can be acquired from string input alone. \n\nTargeted syntactic evaluation is just one in a series of complementary methods being developed to assess the learning outcomes of neural language processing models. Other methods include classifying sentences as grammatical or ungrammatical (Warstadt et al., 2019b), decoding syntactic features from a model's internal state (Belinkov et al., 2017;Giulianelli et al., 2018), or transfer learning to a strictly syntactic task such as parsing or POS tagging (Hewitt and Manning, 2019). As each task brings an explicit set of assumptions, complementary assessment methods can collectively provide greater insight into models' learning outcomes. \n\nAlthough this paper, together with Warstadt et al. (2020), report what is to our knowledge the largestscale targeted syntactic evaluations to date, we emphasize that they are only first steps toward a comprehensive understanding of the syntactic capabilities of contemporary language models. This understanding will be further advanced by new targeted-evaluation test suites covering a still wider variety of syntactic phenomena, additional trained models with more varied hyperparameters and randomization seeds, and new architectural innovations. Humans develop extraordinary grammatical capabilities through exposure to natural linguistic input. It remains to be seen to just what extent contemporary artificial systems do the same.",
            "score": 0.48971318895126115,
            "section_title": "Discussion",
            "char_start_offset": 25249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2156
                },
                {
                    "start": 2157,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 1205,
                    "end": 1229,
                    "matchedPaperCorpusId": "44072099"
                },
                {
                    "start": 1289,
                    "end": 1312,
                    "matchedPaperCorpusId": "7100502"
                },
                {
                    "start": 1312,
                    "end": 1337,
                    "matchedPaperCorpusId": "52090220"
                },
                {
                    "start": 1420,
                    "end": 1446,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1643,
                    "end": 1665,
                    "matchedPaperCorpusId": "208527435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "53633715",
            "title": "Language Models Learn POS First",
            "text": "A glut of recent research shows that language models capture linguistic structure. Such work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
            "score": 0.48946290874603027,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "265520130",
            "title": "Contextualized word senses: from attention to compositionality",
            "text": "Our work, like all of the aforementioned, make use of empirical distributions learnt from corpora to either validate or not a semantic hypothesis and contribute to the design of computational architectures that better model the semantic interpretation process in a more transparent way. By contrast, other linguisticbased approaches thoroughly explore the results of the neural models to find some glimmer of implicit linguistic knowledge in their black boxes. This task is carried out by defining specific tests to analyze whether neural-based language models can learn syntactic regularities and are provided with compositional abilities (De-Dios-Flores and Garcia, 2022; Kim and Linzen, 2020;Linzen and Leonard, 2018). Linzen (2018) states that the role of linguists would be to clearly delineate the linguistic capabilities that can be expected of large language models, by constructing controlled experimental tests that can determine whether those desiderata have been met. This new paradigm of linguistic research is of great interest and can be seen as very different but complementary to the one we follow. \n\nIn the present work, we will implement a distributional architecture to check whether selectional preferences can dynamically build the semantic meaning of composite expressions.",
            "score": 0.4893622354868964,
            "section_title": "Symbolic and Compositional Approaches",
            "char_start_offset": 18109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "222290851"
                },
                {
                    "start": 695,
                    "end": 720,
                    "matchedPaperCorpusId": "49316999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "257505035",
            "title": "The Life Cycle of Knowledge in Big Language Models: A Survey",
            "text": "Only a few studies consider knowledge in large-scale pretrained language models. Chiang et al. (2020) first systematically investigate the knowledge acquisition process during the training of ALBERT (Lan et al., 2020). Specifically, they study the syntactic knowledge, semantic knowledge, and world knowledge development during pre-training, and find that the learning process varies across knowledge, and having more pre-trained steps could not necessarily increase the knowledge in PLMs. P\u00e9rez-Mayos et al. (2021) investigate the effect of the size of the pre-trained corpus on the syntactic ability of the RoBERTa (Liu et al., 2019c) model, and find that models pre-trained on more data typically contain more syntactic knowledge and perform better in related downstream tasks. Liu et al. (2021c) also investigate the knowledge acquisition process of RoBERTa (Liu et al., 2019c) on various knowledge. And find that compared with linguistic knowledge which can be learned quickly and robustly, world knowledge is learned slowly and domain-sensitive.",
            "score": 0.4872958474794318,
            "section_title": "Learning from Text Data",
            "char_start_offset": 15509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1051
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 101,
                    "matchedPaperCorpusId": "222140842"
                },
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 490,
                    "end": 515,
                    "matchedPaperCorpusId": "237433614"
                },
                {
                    "start": 617,
                    "end": 636,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 781,
                    "end": 799,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 862,
                    "end": 881,
                    "matchedPaperCorpusId": "198953378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
            "score": 0.4865843250472737,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "235761293",
            "title": "Tracking Child Language Development With Neural Network Language Models",
            "text": "Starting from the assumption that these parse trees contain sufficient syntactic information to assess language development, figuring out what structures to focus on is left to the machine. \n\nThe ability demonstrated by this approach to produce scores that track language development almost as accurately as with IPSyn, but without the expertise that went into the design of IPSyn, raises the important question of whether computational models of language can learn to measure syntactic development in children from only language data, without any given knowledge about the language acquisition process. This question is not whether a computational model can perform the steps necessary for IPSyn scoring, as in the work of Sagae et al. (2005), or whether a computational model can learn IPSyn scoring from examples, as in the work of Lubetich and Sagae, but whether a computational model derived from child language samples alone can encode its own metric that tracks language development over time as accurately as, or even more accurately than an expertly designed metric like IPSyn. In other words, if the goal is not to model an existing language development metric, but to model the language itself and how it changes over time in individual children, will the resulting model encode a usable language development metric? We investigate this question by creating such a model using neural networks. We base our approach on language modeling using a type of recurrent neural network, but unlike typical language models used in natural language processing that are trained to predict tokens in a string, we additionally have our model sort child language samples chronologically during training. This sorting consists of scoring different language samples produced at different times such that the score for the sample produced later is higher than the score for the sample produced earlier. This is intended to require the model to learn how utterances produced at different stages of development differ. Once the model is trained, it can be used to score a language sample, in the same way one would use existing metrics like IPSyn or MLU. Unlike previous work on automated assessment of child language development, this process does not use a syntactic parser or any information about how to measure language development, such as existing metrics.",
            "score": 0.4846147277457794,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 724,
                    "end": 743,
                    "matchedPaperCorpusId": "7000155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "270562766",
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "text": "Though linguistic bootstrapping can extend to all levels of linguistic representation, from phonology to pragmatics, we will concentrate on the two types which have been most discussed, semantic bootstrapping and syntactic bootstrapping.\n\nBroadly, in semantic bootstrapping proposals, children are said to use their knowledge of semantics and meaning to bootstrap syntactic knowledge.The most famous formulation comes from Pinker, 1984Pinker, , 2009. .In this version, semantic bootstrapping is envisioned as an early language learning strategy which allows children to learn syntactic primitives such as syntactic categories (noun, verb, adjective...), by mapping them to early perceptual or cognitive categories1 (individual, action, state...).For example, children may induce a syntactic category like noun by noticing that there are words which name the semantic perceptual categories of persons or things.Once categories are learnt, a more symbiotic exchange would then emerge between syntactic and semantic knowledge to acquire new structures and word meanings.There is indirect empirical evidence which supports this hypothesis, such as the observation that children's first nouns correspond to physical objects, first verbs to actions, and first adjectives to perceptually salient attributes.Pinker describes semantic bootstrapping as a form of distributional learning, stating that \"children always give priority to distributionally based analyses, and [semantic bootstrapping] is intended to explain how the child knows which distributional contexts are the relevant ones to examine.\"(Pinker, 2009, p.42-43)In this view, language learning can be thought of as a probabilistic mapping problem.Semantic bootstrapping is then a theory that tries to explain how children use aspects of meaning to acquire core pieces of syntactic structure.\n\nSyntactic bootstrapping proposals instead describe processes which involve the use of syntactic knowledge to bootstrap new meanings and semantic knowledge.The proposal and first full description of this theory is associated with Gleitman, 1990.",
            "score": 0.48362189841642356,
            "section_title": "Linguistic bootstrapping debates",
            "char_start_offset": 1204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 239,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 746
                },
                {
                    "start": 746,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1300
                },
                {
                    "start": 1300,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1846
                },
                {
                    "start": 1848,
                    "end": 2003
                },
                {
                    "start": 2003,
                    "end": 2092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478759765625
        },
        {
            "corpus_id": "247594916",
            "title": "How does the pre-training objective affect what large language models learn about linguistic properties?",
            "text": "the LM's encodings for a single linguistic feature such as grammatical information. It has been found through probing that BERT encodes syntactic (Tenney et al., 2019;Liu et al., 2019;Miaschi and Dell'Orletta, 2020;Hewitt and Manning, 2019;Jawahar et al., 2019) and semantic information (Ettinger, 2020;Jawahar et al., 2019;Tenney et al., 2019). However, Hall Maudslay and Cotterell (2021) argue that BERT's syntactic abilities may have been overestimated.\n\nIn this paper, we hypothesize that linguistically motivated objectives (e.g. MLM) should help BERT to acquire better linguistic knowledge compared to using non-linguistically motivated objectives, i.e. tasks that are hard for humans to guess the association between the input and the label to be predicted. To this end, we seek to answer the following research question: How does the pretraining objective affect what LMs learn about the English language?\n\nOur findings challenge the MLM status quo, showing that pre-training with non-linguistically informative objectives ( \u00a72) results in models with comparable linguistic capabilities, as measured by standard probing benchmarks ( \u00a73). These surprising results ( \u00a74) suggest that careful analysis of how LMs learn is critical to further improve language modeling ( \u00a75).",
            "score": 0.47974137001450395,
            "section_title": "Introduction",
            "char_start_offset": 1845,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "271039757",
            "title": "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations",
            "text": "Standard pre-training objectives, e.g. with denoising objectives (Raffel et al., 2020), encourage models to acquire syntactic knowledge but provide little information about syntactic transformations, which are central to many syntactic and semantic seq2seq tasks.Our research hypothesis is that intermediate pre-training to perform transformations of syntax trees encourages the model to (i) strengthen its representations of the syntactic categories to which transformations can be applied (e.g.sub-jects, objects) and (ii) acquire reusable dynamics of transformations that are useful for downstream applications.By providing an explicit description of the transformation as a prefix, different transformations the model has learned during pre-training can be 'activated' by the right choice of prefix.For this reason, we fine-tune the model with a prefix of tunable embeddings to make it easy to leverage these transformations on downstream tasks similar to SIP (Lindemann et al., 2023b).\n\nIn addition to learning about transformations of trees, we also want the model to incorporate knowledge about the syntax of the underlying language (i.e.English, in this case).Hence, we do not provide syntax trees to the model during training, which also enables us to perform inference and fine-tuning without a parser.",
            "score": 0.47762067747160086,
            "section_title": "Strengthening Structural Inductive Bias",
            "char_start_offset": 8036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 803
                },
                {
                    "start": 803,
                    "end": 990
                },
                {
                    "start": 992,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1168
                },
                {
                    "start": 1168,
                    "end": 1312
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 86,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "218487842",
            "title": "Emergence of Syntax Needs Minimal Supervision",
            "text": "Accordingly, probes were conceived to test if the model learned parts of speech (Saphra and Lopez, 2018), morphology (Belinkov et al., 2017;Peters et al., 2018a), or syntactic information. Tenney et al. (2019) evaluate contextualized word representations on syntactic and semantic sequence labeling tasks. Syntactic knowledge can be tested by extracting constituency trees from a network's hidden states (Peters et al., 2018b) or from its word representations (Hewitt and Manning, 2019). Other syntactic probe sets include the work of Conneau et al. (2018) and Marvin and Linzen (2018). \n\nDespite the vivid interest for the topic, no consensus seems to unfold from the experimental results. Two competing opinions emerge:  et al. (2018). \n\n\u2022 The language model training objective does not allow to learn compositional syntax from a corpus alone, no matter what amount of training data the model was exposed to. Syntax learning can only be achieved with taskspecific guidance, either as explicit supervision, or by restricting the hypothesis space to hierarchically structured models (Dyer et al., 2016;Marvin and Linzen, 2018;Chowdhury and Zamparelli, 2018;van Schijndel et al., 2019;Lake and Baroni, 2017). \n\nMoreover, some shortcomings of the above probing methods make it more difficult to come to a conclusion. Namely, it is not trivial to come up with minimal pairs of naturally occurring sentences that are equally likely. Furthermore, assigning a (slightly) higher probability to one sentence does not reflect the nature of knowledge behind a grammaticality judgment. Diagnostic classifiers may do well on a linguistic task because they learn to solve it, not because their input contains a hierarchical structure (Hewitt and Liang, 2019). In what follows, we present our assessment on how the difficulty of creating a linguistic probing data set is interconnected with the theoretical problem of learning a model of syntactic competence.",
            "score": 0.4766629738020294,
            "section_title": "Language models and syntax",
            "char_start_offset": 5770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 104,
                    "matchedPaperCorpusId": "53633715"
                },
                {
                    "start": 117,
                    "end": 140,
                    "matchedPaperCorpusId": "7100502"
                },
                {
                    "start": 140,
                    "end": 161,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 189,
                    "end": 209,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 460,
                    "end": 486,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 535,
                    "end": 556,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 561,
                    "end": 585,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1083,
                    "end": 1102,
                    "matchedPaperCorpusId": "1949831"
                },
                {
                    "start": 1102,
                    "end": 1126,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1126,
                    "end": 1157,
                    "matchedPaperCorpusId": "52010508"
                },
                {
                    "start": 1157,
                    "end": 1184,
                    "matchedPaperCorpusId": "202540362"
                },
                {
                    "start": 1184,
                    "end": 1206,
                    "matchedPaperCorpusId": "46761158"
                },
                {
                    "start": 1721,
                    "end": 1745,
                    "matchedPaperCorpusId": "202538609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "14602503",
            "title": "Children as Models for Computers: Natural Language Acquisition for Machine Learning",
            "text": "Most of the research within the field of GI has focused on learning syntax, and tends to omit any semantic information. However, do children learn their native language independent of meaning? What is the role of semantics in language learning? As linguistic and cognitive studies suggest, semantic and pragmatic information is also available to the child. Moreover, semantics and context seem to play an especially important role in the 2-word stage of child linguistic development. In this stage, context is important to understand the meaning of 2-word sentences and, thanks to the shared context, child and adult can communicate with each other although their grammars are different. \n\nTaking into account that formal language learning is a hard problem, and taking into account the evidence of semantic learning in the first stages of natural language acquisition, we claim that semantic information can simplify the learning problem, and can make learning easier. \n\nThe first attempt to incorporate semantics in the field of GI has been made by Angluin and Becerra-Bonache in [25][26][27]. Inspired by the two-word stage of children's language acquisition, they proposed a computational model that takes into account semantics for language learning. In contrast to other approaches, their model does not rely on a complex syntactic mechanism; in that way, they try to represent the fact that, although the child and adult grammars are different, the semantic situation allows communication. This model also tries to give an account of the meaning-preserving corrections given to the child during the first stages of language acquisition (child's erroneous utterances are corrected by her parents based on the meaning that the child intends to express). This model has allowed them to investigate aspects of the roles of semantics and corrections in the process of learning to understand and speak a natural language. \n\nHow children acquire and use natural language is a fundamental problem that has attracted the attention of researchers for several decades. Besides obtaining a better understanding of natural language acquisition, interest in studying formal models of language learning stems also from the numerous practical applications of language learning by machines. In this paper we have proposed some ideas for simulating, in GI, the process of natural language acquisition.",
            "score": 0.4764439415456143,
            "section_title": "Syntax or Semantics?",
            "char_start_offset": 21809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1922
                },
                {
                    "start": 1925,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2390
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71533203125
        },
        {
            "corpus_id": "4852738",
            "title": "Modeling Coevolution between Language and Memory Capacity during Language Origin",
            "text": ". [62]) entangled lexical and syntactic knowledge, and the setting of single-individual generations made them hard to disentangle cultural and natural selections. In addition, due to the focus on the evolutions of grammatical constructions, artificial agents in many language game models (e.g. [40]) directly recruited relevant learning mechanisms to handle language materials, without manipulating their levels. This made these models unable to trace the coevolution between language and cognitive abilities during language origin, which manifests primarily in terms of level change in relevant cognitive abilities and linguistic understandability.\n\nFourth, the acquired linguistic knowledge in the expected model should not be limited to lexical items. This criterion makes sure that the emergent language in the model is distinct from lexicon-like communication systems as in some nonhuman species, and avoids showing a superficial correlation between language and memory (i.e., an increasing number of lexical items certainly requires an increase in memory capacity for storage). The outcome of the coevolution between language and memory capacity is not merely an increase in memory capacity for keeping relevant linguistic knowledge, but also a transition of linguistic knowledge to accommodate limited capacity. Such transition is collectively induced by individual learning mechanisms and acquired linguistic knowledge. It resolves the conflict between unlimited expressions and limited storing capacities, and allows language users to encode an unlimited number of meanings using a limited number of expressions. It is not evident in most animal communication systems, and cannot be observed in lexical evolution models without grammar learning mechanisms.\n\nConsidering these, we adopt the lexicon-syntax coevolution model [63,64] in our study of language-memory coevolution. The model was first designed to simulate a collective acquisition of lexical items and constituent word orders out of a holistic protolanguage in a multiagent population. In the model, artificial agents are equipped with both STM units to store exchanged meaning-utterance instances, and LTM units to record lexical and syntactic knowledge extracted from these instances. In addition, they apply domain-general abilities such as pattern extraction and sequential learning to acquire lexical and syntactic knowledge. These abilities resemble those used by language learning children [65] or early hominins [8,66]. The initial holistic language resembles the hypothesized",
            "score": 0.47494123770702135,
            "section_title": "Selecting models to simulate language-memory coevolution",
            "char_start_offset": 12978,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2,
                    "end": 6,
                    "matchedPaperCorpusId": "6567063"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "62696833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45166015625
        },
        {
            "corpus_id": "44093987",
            "title": "Word learning and the acquisition of syntactic-semantic overhypotheses",
            "text": "This paper connects to a large body of recent work on computational models of word learning. Most relevant is the work of Abend et al. (2017) and Sadeghi and Scheutz (2018), who present models which jointly learn both word meanings and syntactic properties of language. Their models demonstrate how syntactic knowledge can be leveraged to more quickly infer the meanings of words. Our contribution is complementary: we introduce more complex structured probabilistic (a) Online accuracy curves (see Equation 5) estimated on a fixed test set after each training example. Shaded region represents bootstrap estimate of 95% CI.\n\n(b) Syntactic-semantic overhypotheses support rapid learning. The red line (left axis) plots the mean difference in R between runs of the overhypothesis model and runs of the base model as learning progresses. The gray line (right axis) tracks the average strength of the induced overhypothesis p(t = color | s = N/N) in the overhypothesis model. Shaded region represents bootstrapped 95% CI. Figure 5: Results from the computational simulation of syntactic-semantic overhypothesis learning. models supporting a class of overhypothesis linking syntactic properties of words to certain aspects of their meaning.\n\nOur model is an initial demonstration of how overhypotheses are a clear adaptive feature for a language learner, and how they can be tractably modeled by linking distinct computational tools. While there is naturally more to children's adjective learning than the phenomenon described in this paper, we believe the results support a general conclusion about the structure of the language learning problem. By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion.",
            "score": 0.47458550864702626,
            "section_title": "Discussion",
            "char_start_offset": 23691,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 141,
                    "matchedPaperCorpusId": "206866667"
                },
                {
                    "start": 146,
                    "end": 172,
                    "matchedPaperCorpusId": "19126641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "232046168",
            "title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks",
            "text": "Abstraction and composition are two important themes in the study of human languages, made possible by different linguistic representations. Although treatments in different linguistic traditions vary, representations at the lexical, syntactic, and semantic levels are a common feature in nearly all theoretical studies of human language, and until relatively recently, these representations are explicitly \"materialized\" in language processing pipelines (for example, semantic role labeling takes as input a syntactic parse). \n\nHowever, with the advent of pretrained transformer models, these intermediate representations no longer have any explicit \"reality\": while various studies have found evidence of syntactic and semantic knowledge in these models (Tenney et al., 2019), it is no longer possible to isolate, for example, a subject-verb relation in a specific part of the model. With transformers, the only input to the model is the surface form of text combined with supplemental embeddings (e.g., positional embeddings, and in the case of BERT, segment embeddings). \n\nWhat are the consequences of this exclusive focus on the surface form of text? Some might say, nothing, as bigger models, better pretraining objectives, etc. will lead us to models that are capable of reasoning (Brown et al., 2020). We believe this to be an untenable position and present a case study in simple arithmetic tasks where having the right representation is the difference between a nearly-impossible-to-learn task and an easy-to-learn task. Our work shows that it is possible to \"inject\" representations into transformer models by simple manipulations of the input sequence (in our case, explicitly enumerating the semantics of digit positions), and that doing so makes it possible for off-the-shelf models to easily perform simple arithmetic, whereas it is nearly impossible without our manipulations. \n\nWhile we present only a case study, our findings have broader implications for various language analysis tasks: First, although end-to-end training enabled by neural networks are a powerful tool, having the right representation is crucial also. \n\nSecond, we demonstrate a simple way in which representations can be \"injected\" into transformer models in a completely transparent manner, without any need to re-pretrain.",
            "score": 0.472194096279784,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 777,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1288,
                    "end": 1308,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "3062627",
            "title": "An Incremental Bayesian Model for Learning Syntactic Categories",
            "text": "An important open problem in cognitive science and artificial intelligence is how children successfully learn their native language despite the lack of explicit training. A key challenge in the early stages of language acquisition is to learn the notion of abstract syntactic categories (e.g., nouns, verbs, or determiners), which is necessary for acquiring the syntactic structure of language. Indeed, children as young as two years old show evidence of having acquired a good knowledge of some of these abstract categories (Olguin and Tomasello, 1993); by around six years of age, they have learned almost all syntactic categories (Kemp et al., 2005). Computational models help to elucidate the kinds of learning mechanisms that may be capable of achieving this feat. Such studies shed light on the possible cognitive mechanisms at work in human language acquisition, and also on potential means for unsupervised learning of complex linguistic knowledge in a computational system. \n\nLearning the syntactic categories of words has been suggested to be based on the morphological and phonological properties of individual words, as well as on the distributional information about the contexts in which they appear. Several computational models have been proposed that draw on one or more of the above-mentioned properties in order to group words into discrete unlabeled categories. Most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories such as nouns and verbs; hence, they do not necessarily incorporate the types of learning mechanisms used by children (Sch\u00fctze, 1993;Redington et al., 1998;Clark, 2000;Mintz, 2003;Onnis and Christiansen, 2005). For example, in contrast to the above models, children acquire their knowledge of syntactic categories incrementally, processing the utterances they hear one at a time. Moreover, children appear to be sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. \n\nWe thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Incremental processing also allows a model to incorporate its partially-learned knowledge of categories, letting the model bootstrap its development.",
            "score": 0.47190319989814017,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 553,
                    "matchedPaperCorpusId": "144852907"
                },
                {
                    "start": 633,
                    "end": 652,
                    "matchedPaperCorpusId": "19376934"
                },
                {
                    "start": 1628,
                    "end": 1643,
                    "matchedPaperCorpusId": "9197677"
                },
                {
                    "start": 1643,
                    "end": 1666,
                    "matchedPaperCorpusId": "2596605"
                },
                {
                    "start": 1666,
                    "end": 1678,
                    "matchedPaperCorpusId": "1747915"
                },
                {
                    "start": 1678,
                    "end": 1690,
                    "matchedPaperCorpusId": "6345319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "258686446",
            "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
            "text": "Natural language is rich and diverse. It is not only a system of rules and symbols, but also a way of conveying and interpreting meaning [32]. To understand and produce language, one needs not only to master the technical rules of grammar and knowledge of vocabulary, but also to have sufficient factual information and to be able to reason logically and contextually. Therefore, autoregressive language models, which are able to generate coherent English text, must have acquired some degree of these capabilities as well. For example, consider the following incomplete sentence: Jack was hungry, so he went looking for \u27e8 \u27e9 To complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates people to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word that fits the syntactic and semantic constraints of the sentence (such as \"a snack\"), and that is plausible given the situation and the background knowledge. \n\nAn example that illustrates the need for reasoning is: \n\nLily wanted to get either a cat or a dog. Her mother didn't let her get a dog so instead she \u27e8 \u27e9 \n\nTo complete this sentence, the language model needs to invoke reasoning: it needs to apply the principle of disjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat. It also needs to choose a words that expresses Lily's intention or action that is coherent with the tone and style of the text. Language models have been shown to exhibit a range of emergent abilities, such as summarization, arithmetic, translation, and commonsense reasoning, as they are scaled up in size and trained on diverse and large corpora [24,4,5,21]. These abilities suggest that language models are not only learning the surface patterns of language, but also acquiring some degree of semantic and logical understanding of the world and the text. However, it is not clear at what scale these abilities emerge, and how they depend on the model architecture and the data distribution. \n\nPerhaps the most fundamental ability for a language model is to produce coherent and fluent English text, which, as we discussed above, requires not only grammatical and lexical knowledge, but also factual information and contextual reasoning.",
            "score": 0.4713721141925925,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 141,
                    "matchedPaperCorpusId": "56798209"
                },
                {
                    "start": 1744,
                    "end": 1748,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1748,
                    "end": 1750,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "267523425",
            "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
            "text": "The combination of pretraining (optimization for next-token prediction), post-training (finetuning), and prompt and inference design (e.g., reasoning instruction and inference time) enables LLMs to perform tasks once thought to exclusively require human intelligence (Box 1). This process equips LLMs with intricate linguistic knowledge, including syntactic rules and semantic relations-referred to as formal linguistic competence 16 . It allows LLMs to transcend mere memorization, enabling them to tackle complex, context-dependent tasks in language processing and generation 16,22 . It also empowers LLMs to infer underlying task structures and generate contextually appropriate responses-a form of instrumental knowledge that enhances their ability to solve nontrivial tasks in diverse contexts 23 . \n\nYet, at their core, LLMs have no minds 24 but are autoregressive statistical models that manipulate language-a task that is fundamentally different from that of humans, thus producing a kind of singular, ungrounded intelligence that fundamentally differs from  25 . \n\nIn contrast, tokens in LLMs lack real-world referents, meaning, and experience (cf. symbols in symbolic systems) 26 . LLMs manipulate tokens based solely on statistical patterns learned during training, devoid of real-world grounding, much like how Church encoding in lambda calculus represents data and operators purely through abstract functions defined by their input-output relationships-without any inherent meaning 27 . This lack of grounding detaches the models from the very physical and social realities they simulate, hindering them from obtaining functional linguistic competence-the use of language to achieve goals in the world 16 -and from acquiring the worldly knowledge necessary to approximate world models-representations of the real world that are structure-preserving and behaviorally efficacious 23,28 , such as cognitive maps, body schemas, or spatial schemas 29 . \n\nThus, while LLMs manifest proficient language use that contrasts with simply searching prerecorded strings of text (a lookup table of all possible conversations, as in Block's hypothetical Blockhead machine), they are not true language users: they do not possess intrinsic meaning, communicative intentions, or other internal states essential to human language users 30 .",
            "score": 0.4708811911415215,
            "section_title": "Token prediction as human intelligence fallacy",
            "char_start_offset": 8498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1960
                },
                {
                    "start": 1963,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 433,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 799,
                    "end": 801,
                    "matchedPaperCorpusId": "263780995"
                },
                {
                    "start": 845,
                    "end": 847,
                    "matchedPaperCorpusId": "55303721"
                },
                {
                    "start": 1067,
                    "end": 1069,
                    "matchedPaperCorpusId": "268608487"
                },
                {
                    "start": 1495,
                    "end": 1497,
                    "matchedPaperCorpusId": "14181275"
                },
                {
                    "start": 1715,
                    "end": 1717,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1891,
                    "end": 1894,
                    "matchedPaperCorpusId": "263780995"
                },
                {
                    "start": 1894,
                    "end": 1896,
                    "matchedPaperCorpusId": "254854575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "258959069",
            "title": "Syntax and Semantics Meet in the \u201cMiddle\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity",
            "text": "and pragmatics, they do not investigate the interaction between the meanings associated with syntactic forms and those of individual lexical items.\n\nThus, we not only need to evaluate syntax and utilization of semantic knowledge, but we also need to understand how interactions of meaning at different linguistic levels-i.e. morphological, lexical, phrasal-may alter model behavior. Exploring phenomena within the syntax-semantics interface is a compelling approach as it gives us access to specific aspects of semantics while allowing precise control over syntactic form between levels.\n\nIn this work, we probe the syntax-semantics interface of several language models, focusing on the semantic notion of agentivity. We do this by prompting models to label nouns in isolation or in context as either agents or patients from a curated test set of noun-verb-adverb combinations that dis-(to) this author easily). Regardless, given that the agent and patient roles as defined by Dowty (1991) are prototypes on a scale, this author in the recipient reading is closer to the patient role. play the alternation shown in example (1). We then compare the performance of LMs to both human judgements and corpus statistics.\n\nProbing for LMs for their knowledge of agentivity in syntactic constructions as in (1) and (2) is a particularly insightful case study as it allows us to explore three interconnected questions in a highly controlled syntactic setting: I. Do models display sensitivity to aspects of word-level semantics independent of syntactic context, and is such sensitivity aligned with human judgements? ( \u00a73.1) II. Can models employ lexical semantics to determine the appropriate semantics of a sentence where the syntax is ambiguous between readings (as in 1)? ( \u00a73.2) III. Can models determine the semantics of a sentence from syntax, disregarding lexical semantics when necessary (as in 2)? ( \u00a73.3)\n\nAdditionally, the relatively infrequent pairings of semantic function and syntactic form of sentences such as (1b) are also interesting from a learnability and acquisition perspective for both LMs and humans. How both come to process and acquire exceptions to a general \"rule\" has been a topic of debate since early connectionist models (Rumelhart and McClelland, 1986). Hence",
            "score": 0.4708012978167978,
            "section_title": "Introduction",
            "char_start_offset": 3961,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "53633715",
            "title": "Language Models Learn POS First",
            "text": "A glut of recent research shows that language models capture linguistic structure. Linzen et al. (2016) found that LSTM-based language models may encode syntactic information sufficient to favor verbs which match the number of their subject nouns. Liu et al. (2018) suggested that the high performance of LSTMs may depend on the linguistic structure of the input data, as performance on several artificial tasks was higher with natural language data than with artificial sequential data.\n\nSuch work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
            "score": 0.47057161468340075,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "198192195",
            "title": "A Single Paradigm for Implicit and Statistical Learning",
            "text": "In the second experiment, we investigated in greater depth the point at which participants derived explicit knowledge of the syntactic structure of the language and their use of this knowledge in order to guide their acquisition of learning the word-referent mappings in the language. Experiment 1 tested how explicit knowledge of structure could affect learning, and whether awareness, as measured by retrospective verbal reports, affected performance during the acquisition of the language structure. In Experiment 2, we determined more precisely when explicit knowledge of structure emerged during training, and how this affected learning. In Experiment 2, we asked participants to report, on a trial by trial basis, what each classification decision was based on, with response options ranging from implicit to explicit sources of knowledge. Determining whether explicit knowledge preceded or followed an enhanced ability to acquire the language addresses the relation between vocabulary and syntax acquisition, and, more broadly, the relation between implicit and statistical learning.",
            "score": 0.4701841062626486,
            "section_title": "Introduction",
            "char_start_offset": 7049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1090
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "261530292",
            "title": "Explainability for Large Language Models: A Survey",
            "text": "Assistant models are typically trained in two stages. First, they undergo unsupervised pre-training on large amounts of raw text to learn general linguistic representations. This pre-training stage allows the models to acquire general language knowledge. Second, the models go through alignment fine-tuning via supervised and reinforcement learning. This aligns the models with specific end tasks and user preferences. Explainability research on these models focuses on determining whether their knowledge comes predominantly from the initial pre-training stage, wherein they acquire general language abilities, or from the subsequent alignment fine-tuning stage, wherein they are tailored to specific tasks and preferences. Understanding the source of the models' knowledge provides insight into how to improve and interpret their performance. \n\nA recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches. Complex fine-tuning and reinforcement learning techniques may be less crucial than previously believed. On the other hand, this study also indicates that data quality is more important compared to data quantity during instruction fine-tuning. Furthermore, Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models.",
            "score": 0.4684431855460669,
            "section_title": "Explaining the Role of Fine-tuning",
            "char_start_offset": 56400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2285
                },
                {
                    "start": 2286,
                    "end": 2479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "270562766",
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "text": "Semantic and syntactic bootstrapping posit that children use their prior knowledge of one linguistic domain, say syntactic relations, to help later acquire another, such as the meanings of new words. Empirical results supporting both theories may tempt us to believe that these are different learning strategies, where one may precede the other. Here, we argue that they are instead both contingent on a more general learning strategy for language acquisition: joint learning. Using a series of neural visually-grounded grammar induction models, we demonstrate that both syntactic and semantic bootstrapping effects are strongest when syntax and semantics are learnt simultaneously. Joint learning results in better grammar induction, realistic lexical category learning, and better interpretations of novel sentence and verb meanings. Joint learning makes language acquisition easier for learners by mutually constraining the hypotheses spaces for both syntax and semantics. Studying the dynamics of joint inference over many input sources and modalities represents an important new direction for language modeling and learning research in both cognitive sciences and AI, as it may help us explain how language can be acquired in more constrained learning settings.",
            "score": 0.46838463054435364,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "277740884",
            "title": "Linguistic Interpretability of Transformer-based Language Models: a systematic review",
            "text": "This is a research area that was sparked by work such as the one by Hewitt and Manning [56], which was able to uncover partial syntactic dependencies from pairs of contextual embeddings in BERT [36] -a popular Transformer language model-, correctly deducing from them the depth of each word in a parse tree as well as the distance between each pair of words. This work demonstrated that syntactic structure may have been indirectly encoded in Transformerbased models simply from their exposure to raw text during pre-training. This led to a number of studies that have attempted to discover whether linguistic information of other types (morphological, semantic, etc.) may have been acquired as well by these models during their training. This is a controversial topic, as many works [15,16] claim that Transformer-based models may simply be learning statistical correlations between co-occurring individual tokens, without arriving at a true generalization of any linguistic relationship between them, likening these models to 'stochastic parrots' [15]. Regardless of this view, some research papers have attempted to analyze the models' internal parameters to discover a sort of generalization capability that can be approximated -although not exactly matched-to humans' knowledge of language. \n\nThe motivation for uncovering a supposed linguistic competence in modern language models is varied. Other than the knowledge that it provides on the internal implementations of these models, it also brings some clear practical advantages. For instance, while it is only feasible to train a Transformer-based model for a specific language if there exists an immensely large text corpora available for it [184] -which becomes a barrier for low-resourced languages-, having a knowledge on how existing models may process linguistic information on their respective languages could become a source of vital data for attempting to rework one of those models -via various techniques, such as knowledge injection [75,130]-to support a minority language. Also, by providing an insight into some aspects of the pre-training of these models, we might also be able to understand and even manually control some parts of it, something that could help alleviate the environmental and budgeting issues that are commonly associated with the pre-training from scratch of ever-larger models [15].",
            "score": 0.46837505748456276,
            "section_title": "Introduction",
            "char_start_offset": 3989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2375
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1701,
                    "end": 1706,
                    "matchedPaperCorpusId": "226289695"
                },
                {
                    "start": 2003,
                    "end": 2007,
                    "matchedPaperCorpusId": "218870140"
                },
                {
                    "start": 2007,
                    "end": 2011,
                    "matchedPaperCorpusId": "218470133"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "241583291",
            "title": "Do pretrained transformers infer telicity like humans?",
            "text": "Large pretrained-language models (ELMo: Peters et al., 2018a, BERT: Devlin et al., 2019, RoBERTa Liu et al., 2019 keep achieving new states of the art in a variety of NLP tasks, leading to a growing interest in exploring what has been acquired by the pretraining objectives.\n\nMany recent works utilize probes: shallow, usually supervised classifiers that try to determine which linguistic phenomena are predictable from the pretrained representations. The linguistic properties studied include syntactic relationships (Hewitt and Manning, 2019), morphological information , and semantic knowledge and entailment (Peters et al., 2018b;Goodwin et al., 2020), and the representations studied include sentence embeddings (Adi et al., 2017;Conneau et al., 2018;Ettinger et al., 2018) and token/word-level embeddings (Kim et al., 2019;Ethayarajh, 2019). Recent work has also explored unsupervised analysis of attention heads instead of supervised training (Clark et al., 2019;Kovaleva et al., 2019;Zhao and Bethard, 2020).\n\nAnother approach to understanding pretrained language models is to test their behavior on psycholinguistic tasks. Stimuli in psycholinguistic tasks are typically designed to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016;Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018;Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)'s work on semantic and pragmatic aspect).\n\nOur work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property:",
            "score": 0.46756515528860954,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 60,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 612,
                    "end": 634,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 634,
                    "end": 655,
                    "matchedPaperCorpusId": "218581294"
                },
                {
                    "start": 717,
                    "end": 735,
                    "matchedPaperCorpusId": "6771196"
                },
                {
                    "start": 735,
                    "end": 756,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 756,
                    "end": 777,
                    "matchedPaperCorpusId": "49363457"
                },
                {
                    "start": 811,
                    "end": 829,
                    "matchedPaperCorpusId": "135465591"
                },
                {
                    "start": 829,
                    "end": 846,
                    "matchedPaperCorpusId": "202120592"
                },
                {
                    "start": 950,
                    "end": 970,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 970,
                    "end": 992,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 992,
                    "end": 1015,
                    "matchedPaperCorpusId": "220045413"
                },
                {
                    "start": 1491,
                    "end": 1512,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 1512,
                    "end": 1535,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1560,
                    "end": 1581,
                    "matchedPaperCorpusId": "52156878"
                },
                {
                    "start": 1603,
                    "end": 1625,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 1662,
                    "end": 1687,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1687,
                    "end": 1703,
                    "matchedPaperCorpusId": "218571018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "270562766",
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "text": "Both semantic and syntactic bootstrapping theories are predicated on children learning something first to bootstrap the acquisition of something later.The empirical results which exist in both direction may tempt us to believe that these are different learning strategies, where one may possibly precede the other (i.e.children learn the meanings of simple nouns first or maybe they learn basic syntactic relations first).However, another possibility is that they are both contingent on a more general learning strategy for language acquisition.\n\nIn this paper, we make the following theoretical proposal: linguistic bootstrapping follows from joint learning over multiple levels of linguistic representation, via simultaneous access to multiple input modalities.We argue that neither syntactic nor semantic bootstrapping are independent learning strategies, as they have previously been presented, but both learning effects which arise as a consequence of a probabilistic joint learning strategy over both syntactic and semantic levels of representation for language.Furthermore, we propose that no prior linguistic knowledge is necessary beyond a bias towards learning abstract categories, to acquire both syntactic and semantic representations.\n\nLearning syntactic structures can facilitate learning to meanings, and conversely, learning meanings can facilitate learning syntactic structures.In a joint inference process over both syntactic and semantic representations, each hypothesis space can constrain the other and help learners to simultaneously acquire syntax and semantics.\n\nIn the sections which follow, we will use a neural visual grammar induction model to show how constraints during joint inference on one linguistic domain can affect another and lead to better generalisation in completely novel contexts2 .Our model learns grounded representations of both syntactic structure and semantic meanings from sentence-image pairs using a statistical learning algorithm.We ask the following questions: 1. (analogous to semantic bootstrapping) Can access to visual-grounding and the ability to learn semantic representations in a joint learning setting facilitate learning grammars that generalize better to unseen contexts?2. (analogous to syntactic bootstrapping) Can access to linguistic structure and the ability to learn grammar in a joint learning setting facilitate learning and interpreting novel words and contexts?",
            "score": 0.4640482399285061,
            "section_title": "Our proposal",
            "char_start_offset": 9416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 545
                },
                {
                    "start": 547,
                    "end": 763
                },
                {
                    "start": 763,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1247
                },
                {
                    "start": 1249,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1585
                },
                {
                    "start": 1587,
                    "end": 1825
                },
                {
                    "start": 1825,
                    "end": 1982
                },
                {
                    "start": 1982,
                    "end": 2235
                },
                {
                    "start": 2235,
                    "end": 2435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "199001173",
            "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
            "text": "Pre-training of NLP models with a language modeling objective has recently gained popularity as a precursor to task-specific fine-tuning. Pretrained models like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) have advanced the state of the art in a wide variety of tasks, suggesting that these models acquire valuable, generalizable linguistic competence during the pretraining process. However, though we have established the benefits of language model pre-training, we have yet to understand what exactly about language these models learn during that process. \n\nThis paper aims to improve our understanding of what language models (LMs) know about language, by introducing a set of diagnostics targeting a range of linguistic capacities, drawn from human psycholinguistic experiments. Because of their origin in psycholinguistics, these diagnostics have two distinct advantages: they are carefully controlled to ask targeted questions about linguistic capabilities, and they are designed to ask these questions by examining word predictions in context, which allows us to study LMs without any need for task-specific fine-tuning. \n\nBeyond these advantages, our diagnostics distinguish themselves from existing tests for LMs in two primary ways. First, these tests have been chosen specifically for their capacity to reveal insensitivities in predictive models, as evidenced by patterns that they elicit in human brain responses. Second, each of these tests targets a set of linguistic capacities that extend beyond the primarily syntactic focus seen in existing LM diagnosticswe have tests targeting commonsense/pragmatic inference, semantic roles and event knowledge, category membership, and negation. Each of our diagnostics is set up to support tests of both word prediction accuracy and sensitivity to distinctions between good and bad context completions. Although we focus on the BERT model here as an illustrative case study, these diagnostics are applicable for testing of any language model. This paper makes two main contributions. First, we introduce a new set of targeted diagnostics for assessing linguistic capacities in language models. 1 Second, we apply these tests to shed light on strengths and weaknesses of the popular BERT model.",
            "score": 0.46379718298381034,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 187,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 197,
                    "end": 219,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "273653986",
            "title": "Generative linguistics contribution to artificial intelligence: Where this contribution lies?",
            "text": "Thus, the syntactic or semantic competence learned by any language model, involving DNNs or RNNs, is a result of training it on massive amounts of data, having the ability to learn in the same way humans acquire language provided that they are exposed to sufficient and efficient linguistic input necessary for language acquisition to take place (Cook 1983;Chomsky 1957, et seq;White 2003;Shormani 2023). There is also much involvement of generic architectural properties and features in the same way hierarchical structures including Tree markers or Phrase markers represent how a piece of language is derived and processed, and the mental properties and capacities involved in processing it (Chomsky 1957(Chomsky , 1965(Chomsky , 2013)).",
            "score": 0.46339009352089267,
            "section_title": "LLMs' language processing/interpreting",
            "char_start_offset": 53318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 739
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 357,
                    "matchedPaperCorpusId": "145380520"
                },
                {
                    "start": 706,
                    "end": 721,
                    "matchedPaperCorpusId": "143496935"
                },
                {
                    "start": 721,
                    "end": 738,
                    "matchedPaperCorpusId": "60269447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Our work provides evidence that there exist other capabilities or types of generalizations (e.g. non-syntactic abilities or even more fine-grained syntactic sub-abilities) that can only be learned after others, or at least only once the model reaches some particular state. Isolating these sequential dependencies is an exciting direction for future work. \n\nN-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; Chang and Bergen, 2022b), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; Liu et al., 2021). \n\nRobust model deployment. Our work also has implications for robust model deployment. High token frequencies and n-gram probabilities are by far the most influential predictors of early and stable learning in language models ( \u00a76.2, with marginal additional effects of context lengths and likelihoods). As language models are deployed in do-mains with highly-specific vocabulary terms (e.g. healthcare, law, and finance; Yang et al., 2024), the accurate prediction of infrequent domain-specific terms during text generation is likely to require extensive pre-training (late acquisition, likely mitigated by large pre-training datasets). Such domainspecific text generation is also likely to be unstable across models and pre-training steps (high variability, potentially more difficult to mitigate).",
            "score": 0.4590501652219641,
            "section_title": "Discussion",
            "char_start_offset": 29451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 1112,
                    "end": 1136,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1370,
                    "end": 1387,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1811,
                    "end": 1829,
                    "matchedPaperCorpusId": "258331833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "Most work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions. \n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset. We show similar trajectories on the MultiBERTs (Sellam et al., 2022) reproductions (Appendix F). \n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other. We might compare this to the \"checkmate in one\" BIG-Bench task, a known breakthrough behavior in autoregressive language models (Srivastava et al., 2022). Only at a large scale can models accurately identify checkmate moves, but further exploration revealed that the model was progressing in a linear fashion at offering consistently valid chess moves before that point.",
            "score": 0.45576542478575116,
            "section_title": "THE SYNTAX ACQUISITION PHASE",
            "char_start_offset": 11881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 1307,
                    "end": 1328,
                    "matchedPaperCorpusId": "235683534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "268514497",
            "title": "Acquiring a language vs. inducing a grammar",
            "text": "We can call the function mapping the grammar onto the data F 1 . The learner then performs a 'reverse inference', attempting to get back from the data to the grammar which generated it, according to some function F 2 . Speaking loosely, we can view F 2 as the inverse of F 1 . Grammar induction is successful when, after training, \n\nWhat is crucial for our purposes is that models with this sort of structure assume that the grammar is the sole influence on the nature of the elements in the training data. This is precisely the structure of grammar induction in experimental models, wherein the strings from which the system learns are indeed products of the target grammar. But even for corpus models, it is assumed that the system's goal is to try and identify the grammar which generates the strings it encounters, the training data, and so this structure is taken for granted. \n\nHowever, as noted, the real language-learning situation is quite unlike this. The grammar of competent speakers plays a role in generating data for learners, but is far from the only factor, and nor is it sufficient on its own. A more realistic learning scenario would look more like this, with unlabeled arrows indicating a causal/explanatory dependence relation: G 1 does not, unlike in the previous, and simplified, diagram based on experimental grammar induction models, generate linguistic data. Rather, G 1 generates structured mental representations. These representations are not public, elements of linguistic behavior, but private, of their criterion for successful language learning: ''the learner must not merely uncover the phonological and syntactic rules underlying language structure, but must also master whatever other regularities determine which sentences in the corpus are generated, whether these regularities are pragmatic, semantic, or due to the influence of world knowledge. Thus, this criterion requires that the learner acquire not merely the language, but much else besides''. (p. 140) If we retain the assumption that our specifically linguistic capacities are distinct from these other cognitive systems, this gives us strong reason to doubt that a grammar induction model trained on natural language data will identify the grammar of natural language, as opposed to some hybrid system which reflects all of these additional confounds and constraints.",
            "score": 0.45445788857870506,
            "section_title": "Structures and strings",
            "char_start_offset": 21425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "277043146",
            "title": "Do Construction Distributions Shape Formal Language Learning In German BabyLMs?",
            "text": "One of the most contentious issues in language acquisition is the relationship between the input that the learner receives and the resulting linguistic system. Child-directed speech (or CDS) is structurally simple and, especially in the first three years of life, abounds with questions, imperatives, and fragmentary utterances, but features very few complex sentences (Cameron-Faulkner et al., 2003;Bunzeck and Diessel, 2024). This distribution of utterancelevel constructions is conducive to the functional side of language acquisition: caregivers talk in this way to elicit responses, steer behavior or establish joined attention. But how do children acquire fullfledged, formal grammatical knowledge from such supposedly skewed input? Generativist approaches see this kind of stimulus as too impoverished to kickstart formal linguistic development and assume an underlying, innate grammatical endowment that enables it (cf. Chomsky, 1965;Crain and Pietroski, 2001;Thomas, 2002;Berwick et al., 2011), whereas usage-based scholars argue that domain-general learning mechanisms are sufficient to develop a full-fledged mental linguistic system from this input, which is rich enough (cf. MacWhinney, 2004;Tomasello, 2005). \n\nThe connectionist \"renaissance\", fueled by the increased adoption of deep learning and transformer language models, has opened up new avenues of addressing such questions (Warstadt and Bowman, 2022). Developmental AI (Dupoux, 2018) and the BabyLM community (Warstadt et al., 2023;Hu et al., 2024;Charpentier et al., 2025) have demonstrated that supposedly tabula rasa learners can indeed acquire syntactic structures, the formal side of language, successfully from very little data (Huebner et al., 2021), but without tapping into the functional side of language (Mahowald et al., 2024). This makes them ideal testbeds for the aforementioned issue: is the construction distribution found in child-directed speech really too impoverished to learn formal linguistic capabilities, and are other, more complex registers, e.g. book text (Cameron-Faulkner and Noble, 2013;Noble et al., 2018), a richer and more beneficial form of input?",
            "score": 0.4543263931730751,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 400,
                    "matchedPaperCorpusId": "15297785"
                },
                {
                    "start": 400,
                    "end": 426,
                    "matchedPaperCorpusId": "274764276"
                },
                {
                    "start": 928,
                    "end": 942,
                    "matchedPaperCorpusId": "238655569"
                },
                {
                    "start": 968,
                    "end": 981,
                    "matchedPaperCorpusId": "48086919"
                },
                {
                    "start": 981,
                    "end": 1002,
                    "matchedPaperCorpusId": "2799578"
                },
                {
                    "start": 1188,
                    "end": 1205,
                    "matchedPaperCorpusId": "25820226"
                },
                {
                    "start": 1205,
                    "end": 1221,
                    "matchedPaperCorpusId": "15476748"
                },
                {
                    "start": 1442,
                    "end": 1456,
                    "matchedPaperCorpusId": "3268317"
                },
                {
                    "start": 1482,
                    "end": 1505,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 1707,
                    "end": 1729,
                    "matchedPaperCorpusId": "241583340"
                },
                {
                    "start": 1788,
                    "end": 1811,
                    "matchedPaperCorpusId": "268551442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.240478515625
        },
        {
            "corpus_id": "250113885",
            "title": "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
            "text": "The increasing capacities of neural language models in recent years have led to a surge in research into their representation of language on a finegrained linguistic level (Alishahi et al., 2019;Tenney et al., 2019a;Rogers et al., 2020, i.a.). A common approach to examining language models is to consider them as 'psycholinguistic subjects'; by testing hypotheses derived from psycholinguistics we are able to determine to what extent language models process language similarly to humans (Futrell et al., 2019;Ettinger, 2020;Davis and van Schijndel, 2020;Lakretz et al., 2021). \n\nTo assess the linguistic knowledge of LMs, a range of tools have been deployed. For instance, by training auxiliary diagnostic classifiers on top of a model's internal states (Hupkes et al., 2018), we can probe whether these states encode certain linguistic properties such as POS tags (Tenney et al., 2019b), syntactic dependencies (Hewitt and Manning, 2019;White et al., 2021), or constructional information (Madabushi et al., 2020;Li et al., 2022). Another common approach is the usage of Targeted Syntactic Evaluations, in which the LM's output probabilities are compared on a minimally different pair of a grammatical and ungrammatical sentence (Linzen et al., 2016;Marvin and Linzen, 2018;Gauthier et al., 2020;Hu et al., 2020). This procedure makes it possible to investigate a model's knowledge of specific linguistic phenomena without probing the model's internal representations, such as negative polarity items (Warstadt et al., 2019;Jumelet et al., 2021), subject-verb agreement (Gulordava et al., 2018;Lakretz et al., 2019), and argument binding (Warstadt et al., 2020).",
            "score": 0.45349934047951546,
            "section_title": "Structural Sensitivity of Neural LMs",
            "char_start_offset": 8900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1664
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 216,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 489,
                    "end": 511,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 511,
                    "end": 526,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 526,
                    "end": 556,
                    "matchedPaperCorpusId": "222290667"
                },
                {
                    "start": 556,
                    "end": 577,
                    "matchedPaperCorpusId": "233714186"
                },
                {
                    "start": 756,
                    "end": 777,
                    "matchedPaperCorpusId": "5013113"
                },
                {
                    "start": 867,
                    "end": 889,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 914,
                    "end": 940,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 940,
                    "end": 959,
                    "matchedPaperCorpusId": "235097306"
                },
                {
                    "start": 991,
                    "end": 1015,
                    "matchedPaperCorpusId": "226281476"
                },
                {
                    "start": 1015,
                    "end": 1031,
                    "matchedPaperCorpusId": "247084163"
                },
                {
                    "start": 1252,
                    "end": 1276,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1276,
                    "end": 1298,
                    "matchedPaperCorpusId": "220060899"
                },
                {
                    "start": 1298,
                    "end": 1314,
                    "matchedPaperCorpusId": "218571018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "259089351",
            "title": "Language acquisition: do children and language models follow similar learning stages?",
            "text": "The present study fuels a long-lasting debate on the acquisition of language. While \"empiricists\" argue that language can be acquired with a statistical approach (Clark, 2002;Kolodny et al., 2015;Chater and Christiansen, 2018;McCauley and Christiansen, 2019), \"nativists\" maintain that this ability depends on a core and innate operation, spe-cific to humans (Chomsky, 1959(Chomsky, , 1971)). The present study shows how modern language models may contribute to resolving this debate, by systematically studying which components of a model (e.g. architecture) or properties of the training data (e.g., frequency of sentence structures) contribute to shape the trajectory of language acquisition. Claims about an innate Universal Grammar could be understood as an inductive bias of a language model, implemented in its architecture and dynamics, which tightly constrains learning trajectories across models. If this bias is hierarchical (rather than linear) then this could lead to learning trajectories that follow the structure of the syntactic tree, consistently with the hypothesis of three linguistic stages presented by Friedmann et al. (2021) in humans and what we find in this study in language models. Indeed, neural language models have been previously shown to have a weak inductive bias towards hierarchical processing (Mc-Coy et al., 2020;Kharitonov and Chaabouni, 2020), which can partially explain our results. \n\nThis result echos the recent observation that syntactic trees spontaneously emerge in the middle layers of neural language models (Hewitt and Manning, 2019). Together, these elements thus suggest that modern neural networks provide fruitful models of language acquisition and could reconcile or settle the confronting theories of language acquisition (Warstadt and Bowman, 2022).",
            "score": 0.45309540439347634,
            "section_title": "Fueling the debate between nativism versus empiricism",
            "char_start_offset": 22353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "813498"
                },
                {
                    "start": 226,
                    "end": 258,
                    "matchedPaperCorpusId": "58632948"
                },
                {
                    "start": 1557,
                    "end": 1583,
                    "matchedPaperCorpusId": "106402715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "273025776",
            "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
            "text": "In this work, we examine how large language models' ability to broadly integrate their parametric knowledge (measured by knowledge entropy) changes throughout pretraining and how these changes affect knowledge acquisition and forgetting in a continual learning setup. Our findings reveal a strong correlation between knowledge entropy and the model's capacity to acquire and retain knowledge. Models in the final stages of pretraining tend to exhibit narrower integration of memory vectors, leading to lower knowledge entropy, which negatively impacts both knowledge acquisition and retention. Interestingly, artificially increasing knowledge entropy by modifying the parameters of final-stage models tends to improve these capabilities. \n\nBased on our analysis, we suggest that models from the mid-stage of pretraining offer a good balance between knowledge acquisition, retention, and overall performance, making them a good choice for further training to introduce new knowledge.",
            "score": 0.4525115886582381,
            "section_title": "CONCLUSION",
            "char_start_offset": 25068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 982
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "226289695",
            "title": "When Do You Need Billions of Words of Pretraining Data?",
            "text": "NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.",
            "score": 0.45215723126860907,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "236340884",
            "title": "Ideal Words",
            "text": "The present paper makes an attempt at piecing together the various arguments and ideas about competence acquisition that we have related above. Our position is that linguistic competence is the result of cognitive processes but that it does not preclude the formal definition of an intensional semantics over incomplete models, dependent on a speaker's exposure to performance data. That is, following Partee [47], competence is not super-competence. We will explore what this means in terms of the formalisation of a model.\n\nOur hypothesis, as stated in Sect. 1, is that the acquisition of semantic (and syntactic) competence should be derivable from performance data. The formalisation of competence should have the same components as that of performance, so that performance can be seen as 'incomplete' or 'degraded' competence rather than a fully different type of linguistic object. We have seen that semantic competence can refer to various notions. One relates to the knowledge of core lexical relations [32], another to the ability to retrieve the extension of a term [46], yet another to the 'acceptable' use of a term [32,35,50]. We endeavour in this paper to find a common formalisation underlying these three notions, whilst at the same time acknowledging that they may not emerge jointly (and consequently not fail jointly). The limitations of speakers' competence that we presented in Sect. 2.2 (e.g. not knowing the extensional difference between elms and beeches) should be explicable in terms of the very nature of the performance data they were exposed to. A consequence of our approach is that Katz and Fodor's lexical relations should be discoverable from performance data rather than assumed to be innate, and they should be tightly bound to the state of the syntax-semantics interface in the learner. We will cover this in Sect. 4.3.\n\nThe currently most popular approach to learning meaning from performance data is distributional semantics (henceforth DS-for introductions to the topic, see [6,18,40]). DS is a corpus-driven technique to acquire lexical meaning, in the tradition of distributionalists such as Harris [26]. By virtue of being corpus-driven, DS is usually considered a representation of performance, to be distinguished from the type of lexical relations that might be",
            "score": 0.45161279942389104,
            "section_title": "How to Position this Paper",
            "char_start_offset": 12499,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "9860676"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "9860676"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "142943379"
                },
                {
                    "start": 2014,
                    "end": 2017,
                    "matchedPaperCorpusId": "146120618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46875
        },
        {
            "corpus_id": "257666138",
            "title": "A deep hierarchy of predictions enables online meaning extraction in a computational model of human speech comprehension",
            "text": "Especially if trained and evaluated on tasks involving predicting the next input [20,21], e.g., a word, it is virtually impossible for such models to capture the abstract processing necessary for human language comprehension extending beyond linguistic forms and across cognitive domains [24,25]. A key aspect of speech understanding consists of applying structured internal knowledge to extract relevant information from the input signal. How and what internal knowledge is deployed depends on the listener's behavioral goal, which can range from \"understanding the message intended by the speaker\" during a conversation to simply \"predicting the next word\" during an experimental task. A language model exploiting built-in linguistic as well as nonlinguistic knowledge, and driven by a behavioral goal, may hence be more powerful and polyvalent than one based on recognition and short-range prediction. \n\nHere, we propose a computational framework in which the use of linguistic and nonlinguistic contextual knowledge allows the incremental extraction of multilevel information from the continuous speech signal. The model achieves single-sentence understanding by assigning appropriate values to semantic roles and making reasonable judgements about the nonlinguistic context in which the sentence takes place. Such a process relies on a probabilistic generative model that uses its linguistic and nonlinguistic knowledge to incrementally compose sentences. The generative model has a top context level that determines second-level semantic roles, which are translated into a third-level lemma sequence via linearized syntax rules. Each lemma produces a sequence of continuous, bottom-level spectro-temporal patterns via two intermediate hierarchies, integrating a syllable model [26] that was adapted from a biophysically plausible model of birdsong recognition [27,28]. Importantly, context and semantic states are maintained throughout the sentence but interact at the lemma rate, allowing the inverse model to modify previous estimates of these states with incoming evidence. During model inversion, top-down and bottom-up messages alternate at timescales of corresponding hierarchies, providing a possible solution to the \"now-or-never\" bottleneck [1] that is also consistent with the predictive coding hypothesis of perception [29][30][31].",
            "score": 0.45035944292280417,
            "section_title": "Introduction",
            "char_start_offset": 2075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 292,
                    "end": 295,
                    "matchedPaperCorpusId": "222144329"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "219873864"
                },
                {
                    "start": 1866,
                    "end": 1870,
                    "matchedPaperCorpusId": "15536479"
                },
                {
                    "start": 1870,
                    "end": 1873,
                    "matchedPaperCorpusId": "1093237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "251822540",
            "title": "When Do Discourse Markers Affect Computational Sentence Understanding?",
            "text": "Human language learning is a complex process involving many capabilities. Humans acquire vocabulary, order words in accepted sequences to form meaningful messages and consider the physical and social context to convey intentions and understand those of others. Computational systems -or models -working with natural language now aim to acquire such knowledge and proficiency with a single pretraining stage, during which they learn to perform a task requiring linguistic capabilities such as masked word prediction. Natural language processing (NLP) practitioners provide a randomly initialized model with a large number of texts in natural language and let it predict output for a predefined training task. For example, when the model is pretrained on the masked word prediction task, it tries to predict which word is hidden behind the mask in a given sentence: The lioness hunts the [MASK] \u2192 zebra. The correct answers are used to optimize the model and consequently improve its capabilities. By doing so, the model looks for patterns and important information in the input to adequately perform the pretraining task. These pretrained models can then be reused for other tasks. For example, NLP models can be used to tag the parts of speech in a sentence (Manning 2011;Plank, S\u00f8gaard, and Goldberg 2016), correct language-specific grammar errors (Heyman et al. 2018;Allein, Leeuwenberg, and Moens 2020), locate the correct answer to questions (Choi et al. 2018;Cartuyvels, Spinks, and Moens 2020) or even detect offensive or incorrect information (Ghadery and Moens 2020;Allein, Augenstein, and Moens 2021). To achieve high performance on these tasks, pretrained computational models need to acquire in-depth knowledge and understanding of syntax, semantics, pragmatics, and discourse. Given the scope of this volume, we focus on a special type of discourse markers, discourse connectives, and evaluate how they are leveraged by state-of-the-art NLP models to comprehend the discourse contained within a sentence. More specifically, we assess whether the presence or absence of discourse connectives affect a model's discourse understanding and compare the ease of comprehension between different connective types. Drawing parallels with the literature on human processing, this chapter also sheds light on the differences between humans and computers in terms of discourse comprehension.",
            "score": 0.45020398171906106,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 1258,
                    "end": 1272,
                    "matchedPaperCorpusId": "13232120"
                },
                {
                    "start": 1272,
                    "end": 1306,
                    "matchedPaperCorpusId": "5632184"
                },
                {
                    "start": 1349,
                    "end": 1369,
                    "matchedPaperCorpusId": "86616123"
                },
                {
                    "start": 1446,
                    "end": 1464,
                    "matchedPaperCorpusId": "52057510"
                },
                {
                    "start": 1464,
                    "end": 1499,
                    "matchedPaperCorpusId": "227231612"
                },
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "218571400"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "220045413",
            "title": "How does BERT\u2019s attention change when you fine-tune? An analysis methodology and a case study in negation scope",
            "text": "As large-scale pre-trained language models such as BERT and ELMo have achieved high performance in a variety of natural language processing tasks (Peters et al., 2018a;Radford et al., 2018;Devlin et al., 2019), a growing body of research is devoted to understanding what linguistic properties these language models have acquired. Recent work uses probes, which are supervised models trained to predict linguistic properties including morphology (Belinkov et al., 2017), syntax (Hewitt and Manning, 2019) and semantics (Peters et al., 2018b), etc. (See Belinkov and Glass (2019) for a complete survey.) A good probing performance is considered as evidence that the language models have learned the linguistic knowledge. \n\nWhat is not yet well understood is how this encoded linguistic knowledge changes when a pretrained language model is fine-tuned for a downstream task. Peters et al. (2019) applies a supervised probe both before and after fine-tuning BERT, and suggests that fine-tuning makes the internal representation task-sensitive. But with supervised probes it can be difficult to disentangle what was learned by the probe from what was present in the internal representation (Hewitt and Liang, 2019). \n\nRecent studies have thus turned to unsupervised probes that require no additional training of the model and instead look directly at the attention mechanism, i.e., how much to care about other words when computing the next version of the current word. Clark et al. (2019) inspected pretrained transformers and found several syntactic properties encoded in an intuitive way, where the maximum attention from a dependent is on its syntactic head. But only the pretrained models were considered, not what happened to these intuitive encodings after fine-tuning to a downstream task. \n\nWe argue that if some interpretable encoding of linguistic knowledge is a good explanation of a model, rather than showing it in the pretrained model, it is more important to show it will be enhanced by fine-tuning on a task where that linguistic knowledge is necessary. If the encoding is not enhanced by such fine-tuning, then the model must be using some other mechanism to encode that linguistic knowledge.",
            "score": 0.45019851191092497,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1792
                },
                {
                    "start": 1795,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 168,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 168,
                    "end": 189,
                    "matchedPaperCorpusId": "23678406"
                },
                {
                    "start": 189,
                    "end": 209,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 445,
                    "end": 468,
                    "matchedPaperCorpusId": "7100502"
                },
                {
                    "start": 477,
                    "end": 503,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 518,
                    "end": 540,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 552,
                    "end": 577,
                    "matchedPaperCorpusId": "56657817"
                },
                {
                    "start": 872,
                    "end": 892,
                    "matchedPaperCorpusId": "76666127"
                },
                {
                    "start": 1185,
                    "end": 1209,
                    "matchedPaperCorpusId": "202538609"
                },
                {
                    "start": 1465,
                    "end": 1484,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "276937524",
            "title": "BAMBI: Developing Baby Language Models for Italian",
            "text": "The cognitive implausibility of Large LMs is a recurring topic in the Natural Language Processing (NLP) literature. Numerous authors highlighted semantic (Bisk et al. 2020;Bender & Koller 2020;Merrill et al. 2021), syntactic (Pater 2019;Dupre 2021;Zhou et al. 2023), and cognitive (Bishop 2021;Borji 2023;Katzir 2023;Mahowald et al. 2024) shortcomings in LMs. These criticisms are countered by an equally substantial body of responses. Regarding grounding and a priori constraints, several authors argue that linguistic systems can articulate sophisticated semantic content without relying on grounding (Gastaldi 2021;Capone 2021;Abdou et al. 2021;Piantadosi & Hill 2022;S\u00f8gaard 2022S\u00f8gaard , 2023;;Patel & Pavlick 2022). Others highlight the ability of models to acquire the grammar of natural-historical languages as a by-product of pre-training (Goldberg 2019;Linzen & Baroni 2021;Piantadosi 2023), framing this as one of many emerging capacities exhibited by LMs (Wei et al. 2022). Some even suggest that, if certain plausibility criteria are met, LMs could serve as useful models for studying language and its acquisition (Warstadt & Bowman 2022;Connell & Lynott 2024;Lenci 2023;Cai et al. 2024). In the debate, none of the contributions appear conclusive. It seems plausible that LMs are capable of autonomously organizing semantic and syntactic content leveraging only next token prediction. At the same time, the functioning of these systems does not rule out the possibility that incorporating features of human ontogenetic development into pretraining strategies could improve model learning. \n\nA key aspect in child language acquisition, and the focus of the present work, is the quantity and quality of linguistic input. The increasing size of datasets, while advancing model performance, raises concerns about environmental sustainability, availability of training resources, and limitations for the future development of LMs (Villalobos et al. 2022).",
            "score": 0.44917822392708345,
            "section_title": "A QUICK DIVE INTO LMS' COGNITIVE PLAUSIBILTY",
            "char_start_offset": 2271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 193,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 281,
                    "end": 294,
                    "matchedPaperCorpusId": "230507889"
                },
                {
                    "start": 305,
                    "end": 317,
                    "matchedPaperCorpusId": "266331665"
                },
                {
                    "start": 317,
                    "end": 338,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 603,
                    "end": 618,
                    "matchedPaperCorpusId": "219405055"
                },
                {
                    "start": 671,
                    "end": 683,
                    "matchedPaperCorpusId": "253155329"
                },
                {
                    "start": 699,
                    "end": 720,
                    "matchedPaperCorpusId": "251647156"
                },
                {
                    "start": 863,
                    "end": 884,
                    "matchedPaperCorpusId": "216080851"
                },
                {
                    "start": 1127,
                    "end": 1151,
                    "matchedPaperCorpusId": "251623202"
                },
                {
                    "start": 1151,
                    "end": 1173,
                    "matchedPaperCorpusId": "269465778"
                },
                {
                    "start": 1184,
                    "end": 1199,
                    "matchedPaperCorpusId": "257505538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279052734375
        },
        {
            "corpus_id": "273025776",
            "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
            "text": "Dynamics of Knowledge in Language Models Recent studies have shown that language models embed world knowledge within their parameters and integrate this knowledge to generate responses (Yang, 2024;Petroni et al., 2019;Wang et al., 2021). Thereby, various research efforts aim to understand these dynamics of knowledge in language models (how they learn, store, and engage their parametric knowledge) during inference and training phases. Several studies have focused on investigating the inference process : Geva et al. (2023) analyzes the role of different layers in language models. Allen-Zhu & Li (2024b) demonstrates that model parameters have a limited knowledge capacity. Some studies suggest key-value memory (Geva et al., 2021;Meng et al., 2022;Dai et al., 2022a). Other research focuses on the pretraining phase. Liu et al. (2021) studies the sequence that language models learn various types of knowledge. Allen-Zhu & Li (2024a) examines strategies to enhance knowledge storage and extraction. Teehan et al. analyzes internal structural changes. Sun & Dredze (2024) investigates the interaction between pretraining and finetuning. Chang et al. (2024) analyzes patterns of knowledge acquisition specifically during the pretraining process, addressing the question of how language models acquire knowledge during pretraining. While their study shares similarities with ours in investigating knowledge acquisition behavior during LLM training, our work takes a different focus. We aim to understand why LLMs encounter increasing difficulty in acquiring new knowledge as pretraining progresses, exploring the underlying reasons behind the challenges faced by later-stage models in learning new knowledge. To the best of our knowledge, our work is the first to explore how the behavior of language models in integrating their knowledge evolves throughout the pretraining phase, and to analyze how these changes affect model performance in terms of knowledge acquisition and forgetting in continual knowledge learning. \n\nEntropy in Natural Language Processing In information theory, entropy quantifies the value of information, where predictable (certain) events have low entropy and unpredictable (uncertain) events have high entropy (Lairez, 2022;Majenz, 2018). In natural language processing (NLP), entropy is used in various ways to measure the certainty of language models.",
            "score": 0.4483924175418692,
            "section_title": "RELATED WORK",
            "char_start_offset": 4974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 2022
                },
                {
                    "start": 2025,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2382
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 218,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 218,
                    "end": 236,
                    "matchedPaperCorpusId": "235313524"
                },
                {
                    "start": 506,
                    "end": 526,
                    "matchedPaperCorpusId": "258417932"
                },
                {
                    "start": 716,
                    "end": 735,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 735,
                    "end": 753,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 753,
                    "end": 771,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 822,
                    "end": 839,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 916,
                    "end": 938,
                    "matchedPaperCorpusId": "262825178"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "226283946",
            "title": "Acquiring language from speech by learning to remember and predict",
            "text": "How children acquire language from the environment is one of the fundamental mysteries of cognitive science. Much theoretical, experimental, and computational research into this question has focused on acquiring abstractions over lowerorder symbols, such acquiring morphemes from phoneme sequences or syntactic structures from word sequences (Chomsky, 1965;Gold, 1967;Elman, 1991;Saffran et al., 1996;Albright, 2002;Klein and Manning, 2004;Goldwater et al., 2009;Christodoulopoulos et al., 2012, inter alia). Children, however, do not get symbolic input; symbolic representations at any level of granularity constitute abstractions inferred from highly variable, noisy, and information-rich perceptual signals like audition and vision. This work joins a growing computational literature exploring the kinds of architectures and learning objectives that best support acquisition of linguistic representations directly from the speech signal without supervision (Versteegh et al., 2015;Dunbar et al., 2017). Such models can be used to test questions about language acquisition under more realistic assumptions about the input signal, especially to the extent that they reflect known constraints on human cognition (Shain and Elsner, 2019;Begu\u0161, 2020).\n\nThis study uses computational modeling to examine two influential and possibly complementary ideas about how people learn abstract representations, including language, from data: learning to remember, and learning to predict. Both hypotheses have been advocated by prior work in language acquisition, cognitive neuroscience, and computational modeling, yet their relative contributions to language learning are not yet clear. Our model permits precise manipulation of memory and prediction pressures during acquisition, allowing direct comparison of these hypotheses.\n\nIn so doing, we implement several constraints on real-time language processing that have not been simultaneously present in prior modeling of this domain: (1) we jointly segment and label the speech signal without supervision; (2) the learning objective is applied incrementally during real-time processing using only locally available feedback;\n\n(3) the encoded signal is segmental, sparse, and hierarchically organized; (4) segments are represented featurally as patterns of activation, rather than discrete category symbols; and (5) the system is optimized by modeling its own state at multiple timescales, rather than by modeling the",
            "score": 0.4471494066527034,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 1212,
                    "end": 1236,
                    "matchedPaperCorpusId": "174800654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "225299339",
            "title": "Identifying English Language Use and Communication Challenges Facing \u201cEntry-Level\u201d Workplace Immigrants in Canada",
            "text": "Models of language ability explain what and how language is acquired and used, and the review of these models can help to understand newcomers' English language use and communication challenges in their workplace settings. Definitions of language ability have been conceptualized by looking at what kinds of abilities need to be included in communication. In the beginning, scholars and researchers used the term \"communicative competence.\" Chomsky (1965) viewed that communicative competence was related to an internal state of linguistic knowledge, such as knowledge of lexical, morphological, syntactic, semantic, phonological, and phonetic rules. This view communication challenges, previous research studies that investigated immigrants' language use and communication challenges are reviewed in the following section.",
            "score": 0.44690442354666726,
            "section_title": "English Language Ability",
            "char_start_offset": 2381,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1343994140625
        },
        {
            "corpus_id": "53246965",
            "title": "Aligning Developmental and Processing Accounts of Implicit and Statistical Learning",
            "text": "operationalized in an experimental setting: Implicit learning has used serial reaction-time tasks (e.g., Nissen & Bullemer, 1987) and AGL tasks (e.g., Reber, 1967), while statistical learning has typically used word segmentation tasks like those mentioned above (e.g., Aslin et al., 1998).\n\nEven though, as we have mentioned, each strand approaches the issue from a different perspective, many researchers from these fields share the goal of wanting to better understand the process by which we acquire syntactic knowledge in the face of a complex linguistic environment. Remarkable progress has been made in this regard, with a clear consensus that even infant learners possess the capabilities to exploit the predictability of the underlying structure of the input. However, neither the results from the infant studies used by the statistical learning literature nor the AGL studies used by the implicit learning literature can, at present, be used to fully explain how children's syntax becomes adult-like. This is because the findings from AGL studies can explain how learners unconsciously track the regularities of a grammar's surface structure, but not necessarily how this ability can be used to build knowledge about a grammar with a complex, hierarchical structure. Relatedly, although the findings from the statistical learning field provide compelling evidence that infants can use statistics to learn simple syntactic regularities, many of the studies have used artificial languages, meaning that assumptions about grammar learning are made based on learning regularities from an artificial speech stream. While there are advantages to using artificial languages (e.g., their use allows for a higher level of control over the input), they are, unfortunately, unable to fully capture the intricacies and complexities of natural language. This makes it difficult to scale these findings up to syntax learning in the real world. Furthermore, the findings from neither field are able to explain how young children make use of semantic information in the input during the formation of abstract syntactic categories, which must be part of the solution, since learning a grammar is essentially a process of learning to use syntactic structure to express meaning.\n\nOne way of reconciling these issues is to use an alternative method called the structural priming paradigm which exploits the tendency for speakers to re-use the syntactic structure of the sentences that they have recently",
            "score": 0.4459533463453411,
            "section_title": "Introduction",
            "char_start_offset": 9658,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 129,
                    "matchedPaperCorpusId": "54370482"
                },
                {
                    "start": 151,
                    "end": 163,
                    "matchedPaperCorpusId": "145282756"
                },
                {
                    "start": 269,
                    "end": 288,
                    "matchedPaperCorpusId": "14144332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "5926515",
            "title": "A Model of Early Syntactic Development",
            "text": "In this paper, I present a model that attempts to explain the regularities in children's early syntactic development. The model is called AMBER, an acronym for Acquisition Model Based on Error Recovery. As its name implies, AMBER learns language by comparing its own utterances to those of adults and attempting to correct any errors. The model is implemented as an adaptive production system -a formalism well-suited to modeling the incremental nature of human learning. AMEER focuses on issues such as the omission of content words, the occurrence of telegraphic speech, and the order in which function words are mastered. Before considering AMBER in detail, I will first review some major features of child language, and discuss some earlier models of these phenomena. \n\nChildren do not learn language in an all.or.none fashion. They begin their linguistic careers uttering one word at a time, and slowly evolve through a number of stages, each containing more adult-like speech than the one before. Around the age of one year, the child begins to produce words in isolation, and continues this strategy for some months. At approximately 18 months, the child begins to combine words into meaningful sequences. In order-based languages such as English, the child usually follows the adult order. Initially only pairs of words are produced, but these are followed by three-word and later by four-word utterances. The simple sentences occurring in this stage consist almost entirely of content words, while grammatical morphemes such as tense endings and prepositions are largely absent. \n\nDuring the period from about 24 to 40 months, the child masters the grammatical morphemes which were absent during the previous stage. These \"function words\" are learned gradually; the time between the initial production of a morpheme and its mastery may be as long as 16 months. Brown (1973) has examined the order in which 14 English morphemes are acquired, finding the order of acquisition to be remarkably consistent across children. In addition, those morphemes with simpler meanings and involved in fewer transformations are learned earlier than more complex ones. These findings place some strong constraints on the learning mechanisms one postulates for morpheme acquisition. Now that we have reviewed some of the major aspects of child language, let us consider the earlier attempts at modeling these phenomena.",
            "score": 0.4458458046670671,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2410
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455322265625
        },
        {
            "corpus_id": "256627634",
            "title": "CALaMo: a Constructionist Assessment of Language Models",
            "text": "All theories of language use and development recognize that at the root of human linguistic ability is the capacity to handle symbolic structures. But they disagree on the specific content of speakers' linguistic knowledge, the mode of acquisition of such content, and the extent to which linguistic productivity is affected by this stored knowledge (Bannard et al., 2009a). Theories diverge with respect to three aspects: input, stability and systematicity. The perspective taken on each of these aspects has consequences for the conclusions drawn from NLMs' responses to the evaluation setting. In the following, we consider each aspect in turn and specifically highlight how the evaluation of computational models becomes biased due to a lack of explicitness in relating experimental and theoretical aspects of the research question. \n\nInput. One of the main arguments introduced by nativist frameworks is the poverty of the stimulus: the input children are exposed to is underdetermined and does not explain acquisitional generalizations observed in learners (Crain and Pietroski, 2001). Such theories assume that children navigate a hypotheses space defined by innate constraints (Eisenbei\u00df, 2009). Constructionist approaches, instead, posit that language emerges from the input through domain-general mechanisms: this implies that the input is shaped and skewed in a specific way in order to enhance learnability (Boyd and Goldberg, 2009). A well established line of research has shown how children are proficient statistical learners (G\u00f3mez and Gerken, 2000;Romberg and Saffran, 2010;Christiansen, 2019). The emergence of language-like structure from purely linear signal has also been shown in recent experiments such as (Cornish et al., 2017), which demonstrated how important aspects of the sequential structure of language may derive from adaptations to the cognitive limitations of human learners and users (Christiansen and Chater, 2016b). The crucial difference between the nativist and the non-nativist approach here is how strict the relation between the received input and the acquired linguistic structure is: if we commit to a view in which the input only serves as a trigger of an almost pre-determined cognitive structure, we are naturally driving our attention far from the features of the input and primarily to the features of the structure. On the other hand, deriving the linguistic structure from the input structure itself requires investigating the two aspects together.",
            "score": 0.4452132192815228,
            "section_title": "Nativist vs. non-nativist approaches to language acquisition",
            "char_start_offset": 4521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2365
                },
                {
                    "start": 2366,
                    "end": 2499
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 373,
                    "matchedPaperCorpusId": "6227768"
                },
                {
                    "start": 1419,
                    "end": 1444,
                    "matchedPaperCorpusId": "113403469"
                },
                {
                    "start": 1541,
                    "end": 1565,
                    "matchedPaperCorpusId": "15933380"
                },
                {
                    "start": 1565,
                    "end": 1591,
                    "matchedPaperCorpusId": "11807455"
                },
                {
                    "start": 1591,
                    "end": 1610,
                    "matchedPaperCorpusId": "4740202"
                },
                {
                    "start": 1729,
                    "end": 1751,
                    "matchedPaperCorpusId": "16442010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330322265625
        },
        {
            "corpus_id": "261531236",
            "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
            "text": "One of the most captivating aspects of pre-trained language models (PLMs) is their capacity to acquire a wide range of knowledge across different domains, while being trained primarily through masked language modelling, a task requiring models to predict masked tokens in their input (Tenney et al., 2019;Petroni et al., 2019). The diverse abilities of PLMs can be categorised into two broad types: formal linguistic abilities and functional linguistic abilities. Formal linguistic abilities refer to the understanding of language rules and patterns, which PLMs, for example, BERT (Devlin et al., 2019) are known to excel at (Tenney et al., 2019;Petroni et al., 2019). The latter category includes a range of abilities akin to human cognition that are necessary for real-world language use and comprehension, such as commonsense knowledge and social awareness. While PLMs excel at formal linguistic abilities, they have faced challenges in developing functional linguistic abilities (Mahowald et al., 2023). \n\nThe introduction of Large Language Models (LLMs), which are typically generative PLMs scaled up to billions of parameters and trained on vast, web-scale data corpora, is changing this landscape (Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023a,b). Recent works indicate that LLMs exhibit emergent abilities, as measured by their above random performance without explicit training on tasks, including those tasks that explicitly require some form of reasoning. An emergent ability was first defined as an ability to solve a task which is absent in smaller models, but present in LLMs. This definition, introduced approximately concurrently by two works (Wei et al., 2022b;Srivastava et al., 2023), is based on the more general definition of emergence in physics: \"Emergence is when quantitative changes in a system result in qualitative changes in behaviour\" (Anderson, 1972). Emergent abilities are implied due to LLMs' capacity to perform above the random baseline on the corresponding tasks without explicit training on those same tasks.",
            "score": 0.4439982908604859,
            "section_title": "Introduction, Motivation and Context",
            "char_start_offset": 39,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 305,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 305,
                    "end": 326,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 581,
                    "end": 602,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 625,
                    "end": 646,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 646,
                    "end": 667,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1224,
                    "end": 1247,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1677,
                    "end": 1696,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1696,
                    "end": 1720,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1883,
                    "end": 1899,
                    "matchedPaperCorpusId": "2221052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "267523425",
            "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
            "text": "For instance, in linguistic processing, layers of a language model can be probed to understand how they internally represent different syntactic and semantic structures; early layers tend to represent low-level syntactic features (e.g., part-of-speech tags, which are labels assigned to each word to indicate their grammatical function, such as noun, verb, and preposition), while later layers encode more complex semantic relationships 111 . This helps map specific nodes or attention heads to linguistic tasks, shedding light on how human language processing might work 17 . \n\nHowever, while probing techniques offer insights into the internal representations of language models, they come with limitations. Perhaps the most critical is the lack of causality: high performance in classification or decoding may not reflect what the model uses functionally for its primary tasks, but the probe's capacity to extract information (e.g., superficial correlations). Conversely, just because a feature is not captured by the probing classifier does not mean it is not encoded somewhere in the model. Thus, experimental techniques are needed to bring stronger evidence by manipulating model inputs and model architecture 110 . \n\nOne key experimental method involves manipulating the input data fed into language models-analogous to controlled rearing in animal studies-to observe how different training conditions affect model behavior and performance. Just as newborn chicks' visual experiences can be manipulated (e.g., slow or fast object motion) to reveal the core learning algorithms that support object perception, input manipulations in language models help assess which specific types of input are necessary for learning. For example, by removing instances of specific constructions from the input corpus, like AANN phrases (Article + Adjective + Numeral + Noun; \"a beautiful five days\"), one can ask whether the model is able to infer such structures from related ones. A recent study shows that exposure to simple noun phrases (\"a few days\")-but not counterfactual versions of the AANN construction, like ANAN (e.g., \"a five beautiful days\") and NAAN (e.g., \"five beautiful a days\")-provides scaffolding for generalization across linguistic constructions 112 , akin to how structured sensory data helps newborn animals learn complex visual tasks.",
            "score": 0.44359536777086084,
            "section_title": "Using language models to model cognitive processes",
            "char_start_offset": 51863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 574,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "236924832"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "269292902",
            "title": "What Do Transformers Know about Government?",
            "text": "Many contemporary linguistic theories, such as Construction Grammar (Fillmore and Kay, 1996;Goldberg, 2006), view form-function pairs, called constructions, as fundamental elements of language.In this paper, our interest in constructions is driven by both theoretical and practical considerations.From the theoretical perspective, describing linguistic phenomena in terms of constructions provides a powerful and convenient means of capturing the complex interactions at the interface between lexical semantics and syntax.\n\nIn practical terms, constructions can provide a way of tracking language learners' progress.Constructions can be seen as describing the linguistic knowledge of a native speaker, and thus as constituting the knowledge needed to master a language-mastery of a language is mastery of its constructions.If we had a description of the constructions of a language, we could follow a learner's progress in terms of the proportion of constructions s/he has mastered so far; plan learning paths in terms of constructions of increasing complexity, etc.\n\nThe present study focuses on one of the most fundamental types of constructions: patterns of government. 1 A verb has several syntactic dependents (arguments) in a sentence; they are analyzed as complements vs. adjuncts.One may say \"I listened to many songs on a trip through Europe.\"Here the verb listen has two prepositional phrase (PP) modifiers: A. \"to many songs\", and B. \"on a trip\".Phrase A is a complement-it is semantically required by the verb; phrase B is an adjunct-it is optional, in that many actions can take place while \"on a trip,\" nothing binds this PP to this verb in particular.However, in \"I went on a trip,\" the same PP is a complement of went.Government is about which complements a verb governs: about its valencythe patterns of complements that a verb expects in a sentence.Our linguistic competency informs us that one listens to a sound / person / hunch / etc., while one goes on a trip / rampage / quest / etc.\n\nThe high performance of pre-trained transformer Language Models (LM) on many NLP tasks has stimulated an interest in investigating the inner representations of these models to find out how linguistic knowledge is encoded inside them, and whether it agrees with linguistic theories in general, and theories of grammar in particular.",
            "score": 0.4434164005611089,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 1066
                },
                {
                    "start": 1068,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1666
                },
                {
                    "start": 1666,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 1867
                },
                {
                    "start": 1867,
                    "end": 2006
                },
                {
                    "start": 2008,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "277467536",
            "title": "Implicit In-Context Learning: Evidence from Artificial Language Experiments",
            "text": "Humans can acquire complex patterns implicitly through brief exposure, known as implicit learning, as evidenced by decades of artificial grammar and language-learning experiments in cognitive science (e.g. Reber, 1967;Saffran et al., 1996;G\u00f3mez & Gerken, 2000). Recent advancements in large language models (LLMs) have demonstrated similar remarkable linguistic capabilities, achieving human-like performance on diverse language tasks through in-context learning (ICL) -the ability to rapidly generalize from limited examples without explicit training updates (Brown et al., 2020;Webb et al., 2023). However, despite the striking parallels in outcomes, the cognitive and computational mechanisms underlying such rapid, example-based generalization in LLMs remain largely unknown. \n\nExisting research on in-context learning in LLMs is fragmented and primarily applicationoriented, often relying on heuristic methods such as prompt engineering or intuition-driven experimentation (e.g. Mavromatis et al., 2023;Ye et al., 2022). These approaches provide practical insights but fall short of elucidating underlying cognitive or computational processes. Concurrently, another line of research attempts to link transformer-based models to cognitive models, leveraging training or fine-tuning paradigms to explore representational similarities and cognitive plausibility (Goldstein et al., 2022;Binz & Schulz, 2023). However, these studies predominantly investigate learning mechanisms at the training or fine-tuning stages. Crucially, the process by which pre-trained models rapidly acquire structured linguistic knowledge during inference -when presented with limited examplesremains notably understudied. \n\nGiven this knowledge gap, the present study adopts an explicitly exploratory approach, systematically investigating whether LLMs exhibit implicit learning capacities analogous to human learners at the inferencing level. Our approach is firmly grounded in well-established cognitive science paradigm. Specifically, we adapt three classical artificial language learning experiments that have provided robust insights into human implicit learning across different linguistic domains: morphology (Schuler et al., 2016;Schuler, 2017), morphosyntax (Valian & Coulson, 1988), and finite-state syntax (Alamia et al., 2020).",
            "score": 0.44297902868326416,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 218,
                    "matchedPaperCorpusId": "145282756"
                },
                {
                    "start": 218,
                    "end": 239,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 239,
                    "end": 260,
                    "matchedPaperCorpusId": "15933380"
                },
                {
                    "start": 560,
                    "end": 580,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 580,
                    "end": 598,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 984,
                    "end": 1008,
                    "matchedPaperCorpusId": "264829036"
                },
                {
                    "start": 1364,
                    "end": 1388,
                    "matchedPaperCorpusId": "247316687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "266177075",
            "title": "Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting",
            "text": "Another big issue in balancing the model and additional knowledge is catastrophic forgetting (French, 1999;Kemker et al., 2018), where additional training causes a significant loss of important knowledge embedded in the original model. To tackle this problem, we exploit multiple optimizers and quantitatively observe the balance of capabilities to predict semantic and syntactic features. To the best of our knowledge, our study is the first to analyze how both the types of additional syntactic training and its optimizations affect downstream tasks. \n\nOur contributions in this paper are as follows: \n\n\u2022 Designed additional syntactic training with four syntactic tasks that can help a language model solve downstream tasks from different syntactic perspectives. \n\n\u2022 Exploited optimization functions that prevent catastrophic forgetting during the additional syntactic training, and discovered the tradeoff between retention of original knowledge and enhancement with additional knowledge. \n\n\u2022 Demonstrated that the syntactically enhanced models improved performances on CoLA, RTE, MRPC and key phrase extraction tasks, with qualitative and quantitative discussions to find the syntactic perspectives that contribute to each task.",
            "score": 0.44169900133248063,
            "section_title": "Introduction",
            "char_start_offset": 2130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1232
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 107,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 107,
                    "end": 127,
                    "matchedPaperCorpusId": "22910766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "218470495",
            "title": "Reinforcement learning of minimalist grammars",
            "text": "In this contribution we have outlined an algorithm for effectively learning the syntax and semantics of English declarative sentences. Such sentences are presented to a cognitive agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as formulas of first order predicate logic. This representation allows for the application of compositional semantics via lambda calculus [12]. For the description of syntactic categories we use Stabler's minimalist grammar [48,52], (MG) a powerful computational implementation of Chomsky's recent Minimalist Program for generative linguistics [11]. Despite the controversy between Chomsky and Skinner [10], we exploit reinforcement learning [47,56] as training paradigm. Since MG codifies universal linguistic competence through the five inference rules (5 -9), thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach could potentially unify generative grammar and reinforcement learning, hence resolving the abovementioned dispute. \n\nMinimalist grammar can be learned from linguistic dependency structures [8,28,31,51] by positive examples, which is supported by psycholinguistic findings on early human language acquisition [16,44,57]. However, as Pinker [44] has emphasized, learning through positive examples alone, could lead to undesired overgeneralization. Therefore, reinforcement learning that might play a role in children language acquisition as well [40,55], could effec-tively avoid such problems. The required dependency structures are directly provided by the semantics in the training UMPs. Thus, our approach is explicitly semantically driven, in contrast to the algorithm in [28] that regards dependencies as latent variables for EM training. \n\nAs a proof-of-concept we suggested an algorithm for simple English declarative sentences. We also have evidence that it works for German and French as well and hopefully for other languages also. Our approach will open up an entirely new avenue for the further development of speech-controlled cognitive user interfaces [6,58,64].",
            "score": 0.4416897667222396,
            "section_title": "Discussion",
            "char_start_offset": 31929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "14181275"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "17432618"
                },
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "41921408"
                },
                {
                    "start": 1122,
                    "end": 1125,
                    "matchedPaperCorpusId": "17754102"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "1364249"
                },
                {
                    "start": 1128,
                    "end": 1131,
                    "matchedPaperCorpusId": "2998847"
                },
                {
                    "start": 1131,
                    "end": 1134,
                    "matchedPaperCorpusId": "404374"
                },
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "8041811"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "2623654"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "145213434"
                },
                {
                    "start": 1481,
                    "end": 1484,
                    "matchedPaperCorpusId": "24468447"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "1364249"
                },
                {
                    "start": 2101,
                    "end": 2104,
                    "matchedPaperCorpusId": "52179460"
                },
                {
                    "start": 2104,
                    "end": 2107,
                    "matchedPaperCorpusId": "7610049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "237150106",
            "title": "Child-directed speech is optimized for syntax-free semantic inference",
            "text": "semantic inference, and it is consistent with neurobiological evidence that the syntactic processing system matures only quite late in childhood 41 , at an age where children have fully acquired the meaning of causative verbs 82 .\n\nIt should be noted that the raw utterances in our studies are maximally syntax-free when processed by the model, even excluding the information of word order (the Word2Vec algorithm dissolves the order of contextual words in its predictive training; see more detail in Methods). In other words, the raw learning materials are simple word co-occurrences that entail no explicit syntactic information of any kind. Hence, the causal inference via statistical learning in our results of the \"raw\" layer can be fully accounted for by syntax-free information as the learning resources. Although the syntax-free condition is of theoretical interest and causative semantics indeed emerges from this learning condition, word order may be considered so fundamental in language learning (and processing) that disregarding it may seem unnatural. Our goal here, however, was to test whether generalizing causative semantics is even possible without presupposing knowledge about word order. To simulate a learner who attends to order, future studies may incorporate word order in the near-syntax-free training of raw utterances. Here, models that are sensitive to sequential information (e.g., BERT 83 ) would be useful. At the same time, however, such models require long sequences with closed contexts and may not realistically mirror the short individual chunks characteristic of child input and rapid conversation.\n\nWe end by pointing out two limitations of our studies. First, while we simulate possible mechanisms, we do not directly test how children actually perceive and process the raw utterances. It has been shown that syntactic knowledge may emerge from raw input 31,33,78 , which can facilitate semantic inference 34,35 . As much as we recognize such emergence of syntax, it does not contradict our findings that raw input is sufficient for semantic inference, as our model is not informed of the potential syntactic knowledge. In fact, both semantic inference and syntactic abstraction, if any, can be the end products of distributional learning from raw input alone, and these two processes do not necessarily depend on each other. That said, when both processes take place extensively, there is",
            "score": 0.4404755303567528,
            "section_title": "Discussion",
            "char_start_offset": 16800,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 228,
                    "matchedPaperCorpusId": "13092653"
                },
                {
                    "start": 1895,
                    "end": 1898,
                    "matchedPaperCorpusId": "11997491"
                },
                {
                    "start": 1898,
                    "end": 1901,
                    "matchedPaperCorpusId": "16525837"
                },
                {
                    "start": 1901,
                    "end": 1903,
                    "matchedPaperCorpusId": "3805955"
                },
                {
                    "start": 1949,
                    "end": 1951,
                    "matchedPaperCorpusId": "8139339"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34375
        },
        {
            "corpus_id": "234684869",
            "title": "The Unifying Theory of Bootstrapping: The Collaboration of Multiple Language Acquisition Mechanisms",
            "text": "First language acquisition has been under keen investigation by linguists around the world. Indian philosophers and scientists pondered, for more than twelve centuries, if language is bestowed by God (innate) or learned (Matilal, 1992). The debate still lingers: if language acquisition is an innate or acquired ability or both, and the interrelation among linguistic input, children's neurological development, and their congenital capabilities. However, despite decades of research, the understanding of how the essential components interact and consummate to the acquisition of language remains fragmentary. \n\nThe central question is a chicken-or-egg dilemma-if syntax comes before semantics or vise versa, and if \n\nPublished by SCHOLINK INC. there are any other components in play. Many theorists have developed hypotheses in an attempt to disentangle the problem, including prosodic bootstrapping (I), pragmatic bootstrapping (II), semantic bootstrapping (III), and syntactic bootstrapping (IV). Many theories are considered mutually antagonistic, feuding over their plausibility and credibility. The most notable one is the wrangle between semantic bootstrapping and syntactic bootstrapping (cf. IV). \n\nThe wrangle pivots on Universal Grammar (UG), a natural instinct definitive for linguistic development. \n\nIn syntactic bootstrapping, it is posited that children have an innate sense of syntax and can link syntactic categories to semantic categories; in semantic bootstrapping, this sense is reversed-children can link semantic categories to syntactic categories without external instructral input. Whether UG exists and in what form determines how we acquire language. Despite its promise, unsupportive evidence surfaces and calls for a theory in which UG is not a pre-determining factor. In our theory, children acquire language using instincts unlimited to language, such as pattern recognition. \n\nIn this paper, we tear apart the theories' incompatibilities and review the problem from a holistic perspective: how the bootstrapping hypotheses work in tandem during the developmental period and ultimately lead to the mastery of a language. Imagine a piano.",
            "score": 0.4402444172544154,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251708984375
        },
        {
            "corpus_id": "227230886",
            "title": "How Relevant Are Selectional Preferences for Transformer-based Language Models?",
            "text": "The recent emergence of contextual embedding models such as BERT, and their subsequent application and improvement they brought on many NLP tasks, has been followed by extensive research on whether these embeddings accommodate an implicit understanding of linguistic and semantic knowledge. A great deal of research has been focused on the syntactic knowledge learned by BERT; Goldberg (2019) has found that BERT (a Transformer model) is more robust in syntactic tasks than a simple LSTM architecture (a Recurrent Neural Network), and with a series of probing tasks on different datasets he proved that there is some syntactic knowledge beyond semantic and contextual relations. BERT was especially more successful in such tasks compared to other contextual embedding models, because of its bi-directional architecture (Wolf, 2019). Further research on learned syntactic information showed that BERT captures phrase-level information in the lower layers, and learns more sophisticated relations in higher layers (Jawahar et al., 2019). Coenen et al. (2019) found that the attention matrices output by bert-base-uncased contain syntactic representations, with certain directions in space representing specific relations, and they were also able to locate similar sub-spaces for semantic relations. Petroni et al. (2019) report that BERT contains enough relational knowledge to compete with knowledge-based methods on tasks such as open-type questions, which leads them to the conclusion that the model has acquired a certain level of semantic knowledge. And Ettinger (2020) presents a number of experiments in which in many cases BERT makes good predictions with regard to semantic fit, such as hypernyms and subject-object nouns. \n\nHowever, McCoy et al. (2019) question the ability of BERT-and similar pretrained models-to truly capture deep linguistic structures and semantic information, as past bibliography has suggested. Tenney et al. (2019) also investigated pretrained models on their performance on both syntactic and semantic phenomena, and concluded that simple syntactic phenomena were successfully identified, but phenomena which mostly relied on semantic relations were not as easily learned. Ettinger (2020) has also pointed out that BERT performance on predictions dropped in cases of true/false statements and negations.",
            "score": 0.4401990235588667,
            "section_title": "Interpretability of Neural Models",
            "char_start_offset": 10659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 1036,
                    "end": 1056,
                    "matchedPaperCorpusId": "174802633"
                },
                {
                    "start": 1297,
                    "end": 1318,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1741,
                    "end": 1760,
                    "matchedPaperCorpusId": "59599752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "238408423",
            "title": "Word Acquisition in Neural Language Models",
            "text": "We selected the random chance baseline to best reflect a language model's ability to predict a word with no access to any training data, similar to an infant's language-specific knowledge prior to any linguistic exposure. We selected minimum surprisal as our other bound to reflect how well a particular word can eventually be learned by a particular language model, analogous to an adult's understanding of a given word. \n\nFor each learning curve, we found the intersection between the fitted sigmoid and the cutoff surprisal value. We defined age of acquisition for a language model as the corresponding training step, on a log10 scale. Sample cutoffs and ages of acquisition are shown in Figure 1 and Figure 2.",
            "score": 0.44012148732144496,
            "section_title": "Learning curves and ages of acquisition",
            "char_start_offset": 9128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "258865433",
            "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
            "text": "In this paradigm, LLMs are finetuned to learn how to follow instructions on a few tasks the model is capable of and are subsequently expected to generalize and follow instructions for novel tasks by utilizing their pre-training knowledge (residing in their parameters). \n\nHowever, it is not clear to what extent this as- (3) The evaluation on the application of acquired knowledge is estimated through the performance on the test split, after finetuning M \u03b8 on the downstream task. \n\nsumption holds in practice, giving rise to a central question: how much of parametric knowledge will get applied in downstream tasks? If the causal link between \"identifiable knowledge\" and its practical application in downstream tasks is not established (Kulmizev and Nivre, 2021), the mere presence of knowledge within a model's parameters does not necessarily guarantee its utilization in such tasks. This raises questions about the assertion of pretrained language models (PLMs) as differentiable knowledge bases (Hao et al., 2022) and their overall capabilities. For instance, as demonstrated by Qin et al. (2023), ChatGPT's performance lags behind its foundational model, GPT-3.5, in areas including commonsense and logical reasoning tasks. \n\nPrevious studies have investigated this question within linguistic domains and have demonstrated that although PLMs have the capacity to encode linguistic knowledge, they may not effectively employ it in downstream tasks. For example, McCoy et al. (2019) illustrates that PLMs employ syntactic heuristics to solve NLI even though they are able to represent proper linguistic hierarchies (Tenney et al., 2019a), even after finetuning (Merchant et al., 2020;Zhou and Srikumar, 2022). Warstadt et al. (2020) provide evidence that RoBERTa requires data inoculation or pre-training with extensive data in order to effectively utilize its hierarchical linguistic knowledge. In a more recent study, Lover-ing et al. (2021) demonstrate that the quantity of \"evidence\" presented in the finetuning dataset influences the features that PLMs rely on during the finetuning process. Specifically, the model may resort to lexical heuristics when the finetuning signal toward linguistic features is insufficient.",
            "score": 0.4389642824978919,
            "section_title": "Introduction",
            "char_start_offset": 1732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 1001,
                    "end": 1019,
                    "matchedPaperCorpusId": "249525582"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1620,
                    "end": 1642,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1666,
                    "end": 1689,
                    "matchedPaperCorpusId": "216914339"
                },
                {
                    "start": 1689,
                    "end": 1713,
                    "matchedPaperCorpusId": "235658872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "262127677",
            "title": "Artificial Neural Network Language Models Predict Human Brain Responses to Language Even After a Developmentally Realistic Amount of Training",
            "text": "For example, Hart and Risley (1992) estimated that children are exposed to 3-11 million words each year, so by the time they turn 10 and possess adult-like linguistic competence, they are exposed to 30-110 million words. In contrast to a human child, who can learn a language from only ~100 million words (or less), many current models get orders of magnitude more training data (20,000 human years' worth for some models; Warstadt & Bowman, 2022). More recently, Gilkerson et al. (2017) estimated that by age of 10, the amount of language exposure is around 20 million words, and Frank (2023) put this estimate at between 9 and 110 million words (extrapolating from their estimate of between 200 and 400 million by age 20). Here, we ask whether this extensive amount of training is necessary for the models to acquire representations that are predictive of human brain responses during language sentence comprehension. \n\nPrior studies on the effects of training data on the models' linguistic ability found that even with limited amounts of training data, models achieve considerable proficiency (Warstadt & Bowman, 2022). For example, Hu et al. (2020) and Zhang et al. (2020) report impressive syntactic generalizations in a BERT model (Devlin et al., 2018) trained on only millions of tokens (see also Huebner & Willits, 2021;Pannitto & Herbelot, 2020, for related evidence from a RoBERTa model trained on 5 million words of child-directed speech). P\u00e9rez-Mayos et al. (2021) find that a RoBERTa model (Liu et al., 2019) trained on 100 million words performs similarly to a model trained on 1 billion words on several syntactic benchmarks. These findings suggest that massive amounts of training may not be necessary for models to acquire certain aspects of linguistic competence. However, it is not known whether models trained on limited amounts of data can also predict human neural and behavioral responses to language. \n\nHere, we evaluate how the amount of training data affects model-to-human alignment.",
            "score": 0.43820800070307153,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "143528292"
                },
                {
                    "start": 464,
                    "end": 487,
                    "matchedPaperCorpusId": "4447742"
                },
                {
                    "start": 581,
                    "end": 593,
                    "matchedPaperCorpusId": "778955"
                },
                {
                    "start": 1238,
                    "end": 1259,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1305,
                    "end": 1329,
                    "matchedPaperCorpusId": "241583173"
                },
                {
                    "start": 1329,
                    "end": 1354,
                    "matchedPaperCorpusId": "222272341"
                },
                {
                    "start": 1452,
                    "end": 1477,
                    "matchedPaperCorpusId": "237433614"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "56487807",
            "title": "Speech gestural interpretation by applying word representations in robotics",
            "text": "The models used in this research present different approaches to the language acquisition process. With some, the contexts of the words are managed from examples, while others start from the exact definitions to compare the meanings of the words. If we look closely at human learning, at the first stage we begin to acquire information about the concepts of a sentence without getting to know its structure [40]. At school, a metalinguistic awareness is acquired that makes it possible to separate meaning from form. Finally, at a more advanced stage of language acquisition, the multiple meanings of words and the ambiguity that this entails, acquire the notion of context. These processes can be approximated in robotic interfaces by establishing the semantic information of words through word representations. \n\nIt seems that models based on lexical learning have more properties in common with this first process of semantic learning of language by humans related to linguistic immersion, which is not based on any previous knowledge. They take advantage of a massive amount of textual information by extracting their own relationships -less accurately but with more realistic levels of coverage. In this way, they manage similarity as well as proximity between the different contexts of two words. In contrast, models based on expert knowledge are generated through previous training in an academic environment. The way these models manage information is similar to the process a linguist would use to compare the meaning of words. They are the product of in-depth language analysis and further elaboration, so in principle they are expected to offer higher precision values in decreasing coverage -bearing in mind the manual limitation of design -and efficiency.",
            "score": 0.43763926520052754,
            "section_title": "Semantic approaches",
            "char_start_offset": 14870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "26642806"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "233296049",
            "title": "Cetacean Translation Initiative: a roadmap to deciphering the communication of sperm whales",
            "text": "All human infants undergo similar stages during acquisition of language in the first years of life, regardless of the language in their environment. For example, the babbling period during which language-acquiring infants produce and repeat basic syllables (such as [da] or [ba]) or reduced handshapes and movements in sign languages ( 120) is a well-documented developmental stage during the first 6-13 months (121). Another well-documented concept in language acquisition is the critical period: if children are deprived of primary linguistic inputs in their first years, acquisition is not complete, often resulting in severe linguistic impairments (Friedmann 2015). The study of the developmental stages in language acquisition has yielded insights into how humans learn to discretize the acoustic speech stream into mental units, analyze meaning, and in turn produce language. In human language, for example, syllables that are produced first during language acquisition (e.g. [ma] or [ba]) are also most common in the world's languages, most stable, and easiest to produce. Similarly, morphological and syntactic constructions that are acquired first are the most basic (122). \n\nThere are currently several known parallels in the developmental stages between human language and animal communication. Acquisition of birdsong in some species, for example, involves the presence of babbling as well as the critical period (123). These parallels likely stem from common neural and genetic mechanisms behind human speech and animal vocalizations (124). However, in cetacean research, existing data on the vocalizations of non-adult whales in their natural setting are limited. Continuous and longitudinal data acquisition capabilities are required to record vocalizations of calf-mother pairs and collect behavioral data on their interactions as calves mature. Such data will provide insights into the order of acquisition of coda types, leading to insights into the articulatory effort of the vocalization as well as identification of the most basic structural building blocks and their functions.",
            "score": 0.4375382504710529,
            "section_title": "Language acquisition",
            "char_start_offset": 29513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 416,
                    "matchedPaperCorpusId": "41964944"
                },
                {
                    "start": 1425,
                    "end": 1430,
                    "matchedPaperCorpusId": "1505871"
                },
                {
                    "start": 1547,
                    "end": 1552,
                    "matchedPaperCorpusId": "924972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.168212890625
        },
        {
            "corpus_id": "250390800",
            "title": "Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention",
            "text": "Prior work has shown that a remarkable amount of linguistic structure emerges in the representations learned by large transformers self-supervised on natural language (Linzen and Baroni, 2021;Manning et al., 2020;Tenney et al., 2019), and that transformers can learn to approximate a compositional process for solving math problems (Russin et al., 2021). These findings may cast doubt on the idea that injecting explicit linguistic structure will aid these models in producing the kinds of system-atic behavior observed in human language learners. However, given their poor systematic generalization performance observed on tasks like SCAN (Lake and Baroni, 2018), and their reliance on certain syntactic heuristics that lead to predictable failures on challenging sentences (McCoy et al., 2019;Linzen and Baroni, 2021), it stands to reason that these models may benefit from access to explicit linguistic knowledge (Sachan et al., 2021). \n\nSome work has attempted to incorporate linguistically-informed labels such as part-ofspeech tags or syntactic parses into the inputs or training regiments of deep learning models (Sachan et al., 2021;Sennrich and Haddow, 2016;Strubell et al., 2018), showing some improvements on machine translation (Sennrich and Haddow, 2016) and semantic role labeling (Strubell et al., 2018). A number of methods have been used to inject linguistic knowledge into these models, including the use of graph neural networks (Marcheggiani and Titov, 2017;Sachan et al., 2021) and multi-task learning (Strubell et al., 2018). In this work, we develop a novel approach that attempts to establish an explicit link between linguistic structure and the attention mechanism of transformers to improve their systematic generalization capabilities.",
            "score": 0.4370158977227405,
            "section_title": "Utilizing Linguistic Knowledge",
            "char_start_offset": 5487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 192,
                    "matchedPaperCorpusId": "216080851"
                },
                {
                    "start": 192,
                    "end": 213,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 332,
                    "end": 353,
                    "matchedPaperCorpusId": "234778107"
                },
                {
                    "start": 650,
                    "end": 663,
                    "matchedPaperCorpusId": "46761158"
                },
                {
                    "start": 795,
                    "end": 819,
                    "matchedPaperCorpusId": "216080851"
                },
                {
                    "start": 1167,
                    "end": 1189,
                    "matchedPaperCorpusId": "5068376"
                },
                {
                    "start": 1295,
                    "end": 1318,
                    "matchedPaperCorpusId": "5068376"
                },
                {
                    "start": 1448,
                    "end": 1478,
                    "matchedPaperCorpusId": "16839291"
                },
                {
                    "start": 1523,
                    "end": 1546,
                    "matchedPaperCorpusId": "5068376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "272910877",
            "title": "Data Proportion Detection for Optimized Data Management for Large Language Models",
            "text": "The pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts. \n\nIn the pre-training of large language models (LLMs), the goal is to predict the next token in a sequence by minimizing the negative log-likelihood of the predicted token given its context. The loss function for a sequence  0 ,  1 , . . . ,   is: \n\nThis function measures how well the model predicts each token given the preceding tokens, and by minimizing this loss, the model learns to generate contextually appropriate sequences. Attention mechanisms, particularly Transformer architectures, are employed to capture long-range dependencies in the data. \n\nScaling Laws and Data Quality: Scaling Laws describe the relationship between the size of the model, the amount of data, and performance [11,12]. As the number of parameters in the model increases, or as more data is introduced, the model's performance improves, but with diminishing returns at a certain point. This creates a challenge where simply increasing model size or dataset size does not guarantee proportionate gains in performance. However, high-quality data can break Scaling Laws by boosting model efficiency beyond what is expected from quantity alone [21]. \n\nOne key factor is data quality-carefully curated, diverse, and representative datasets lead to more efficient learning. High-quality data allows models to generalize better, even with fewer parameters or less training data. Optimal data proportioning across domains also plays a critical role in this process. For example, selecting the right mix of domain-specific and general data ensures that the model is not only learning from the most relevant examples but also retains a broad understanding of language use cases.",
            "score": 0.43690237587476766,
            "section_title": "PRELIMINARY 2.1 Pre-training Stage in LLMs",
            "char_start_offset": 3427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 1867,
                    "end": 1871,
                    "matchedPaperCorpusId": "250113273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "10250712",
            "title": "Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World",
            "text": "Logical Semantics with Perception (LSP) is a model for grounded language acquisition. LSP accepts as input a natural language statement and an environment and outputs the objects in the environment denoted by the statement. The LSP model has three components: perception, parsing and evaluation (see Figure 2). The perception component constructs logical knowledge bases from low-level featurebased representations of environments. The parsing component semantically parses natural language into lambda calculus queries against the constructed knowledge base. Finally, the evaluation component deterministically executes this query against the knowledge base to produce LSP's output.\n\nThe output of LSP can be either a denotation or a grounding. A denotation is the set of entity referents for the phrase as a whole, while a grounding is the set of entity referents for each component of the phrase. The distinction between these two outputs is shown in Figure 1b. In this example, the denotation is the set of \"things to the right of the blue mug,\" which does not include the blue mug itself. On the other hand, the grounding includes both the referents of \"things\" and \"blue mug.\" Only denotations are used during training, so we ignore groundings in the following model description. However, groundings are used in our evaluation, as they are a more complete description of the model's understanding.\n\nFormally, LSP is a linear model f that predicts a denotation \u03b3 given a natural language statement z in an environment d. As shown in Figure 3, the structure of LSP factors into perception (f per ), semantic parsing (f prs ) and evaluation (f eval ) components using several latent variables:\n\nLSP assumes access to a set of predicates that take either one argument, called categories (c \u2208 C) or two arguments, called relations (r \u2208 R). 2 These predicates are the interface between LSP's perception and parsing components. The perception function f per takes an environment d and produces a log- Figure 3: Factor graph of LSP. The environment d and language z are given as input, from which the model predicts a logical knowledge base \u0393, logical form , syntactic tree t and denotation \u03b3.\n\nical knowledge base \u0393",
            "score": 0.436696530759285,
            "section_title": "Logical Semantics with Perception",
            "char_start_offset": 7055,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "249684834",
            "title": "Effects of Explicit Connected Speech Instruction on Libyan EFL Learners\u2019 Perceptions of Listening Comprehension Problems in Speech Recognition, Parsing and Utilisation",
            "text": "Language is structured according to a set of rules that tell the user how to proceed from a particular string of words to an interpretation of the meaning of the word string. In learning to comprehend a language, learners acquire a great many rules that encode the various linguistic patterns in the language and relate these patterns to meaning interpretations (Anderson, 2000). Therefore, the problems that learners may encounter during the parsing phase include syntactic and semantic problems. Some instances of syntactic problems are listeners' quickly forgetting what is heard, failing to form a mental representation of the words heard, and not understanding the subsequent parts of input because of earlier problems (Goh, 2000). Meanwhile, semantic problems occur when learners are distracted with the meaning of certain words that may be redundant, hence missing other parts of the message because of the limited processing capacity of their short-term memory and shallow processing (Goh, 1999).",
            "score": 0.4364926694872757,
            "section_title": "Parsing Phase",
            "char_start_offset": 10891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 1004
                }
            ],
            "ref_mentions": [
                {
                    "start": 724,
                    "end": 735,
                    "matchedPaperCorpusId": "55851928"
                },
                {
                    "start": 992,
                    "end": 1003,
                    "matchedPaperCorpusId": "146633139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "252907248",
            "title": "Transparency Helps Reveal When Language Models Learn Meaning",
            "text": "Bender and Koller (2020) initiated the discussion on the possibility of a learner acquiring meaning from training on linguistic forms alone. From first principles, they argued for its impossibility. Empirically, Traylor et al. (2021) also found that LMs cannot well-represent lexical-level symbols when the pretraining data is distributionally constrained to supply relevant signals. Merrill et al. (2021), on the other hand, proved theoretically that it is possible to emulate the meaning of strongly transparent languages with assertion oracle access. We showed in this work that, empirically, LMs also attain the capability. Patel and Pavlick (2022) is also conceptually similar to our work, discovering that the internal representation of LMs is to a large extent isomorphic to the conceptual spaces of directions and colors. They adopted in-context learning (Brown et al., 2020;i.a.) to elicit the isomorphism, while we used the more traditional probing paradigm. \n\nAnother line of work has inspected the extent to which pretrained LMs encode various types of semantic information. Some have examined the representation of lexical semantics: Gar\u00ed Soler and Apidianaki (2021) found that BERT repre-sentations reflect polysemy levels, and Vuli\u0107 et al. (2020) showed that they also capture abundant typelevel lexical knowledge. On the other hand, Ettinger (2020) and Ravichander et al. (2020) have discovered that pretrained LMs do not satisfactorily encode negation and hypernymy, respectively. Moving beyond the lexical level, Wu et al. (2021) demonstrated that pretrained BERT and RoBERTa models less readily surface semantic dependency information than syntactic dependencies, while Li et al. (2021) identified evidence of dynamic semantics representation in these models.",
            "score": 0.43645844540513323,
            "section_title": "Related Work",
            "char_start_offset": 37872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1778
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 233,
                    "matchedPaperCorpusId": "236459821"
                },
                {
                    "start": 384,
                    "end": 405,
                    "matchedPaperCorpusId": "233346957"
                },
                {
                    "start": 628,
                    "end": 652,
                    "matchedPaperCorpusId": "251647156"
                },
                {
                    "start": 1242,
                    "end": 1261,
                    "matchedPaperCorpusId": "222290596"
                },
                {
                    "start": 1369,
                    "end": 1394,
                    "matchedPaperCorpusId": "227230677"
                },
                {
                    "start": 1531,
                    "end": 1547,
                    "matchedPaperCorpusId": "228083871"
                },
                {
                    "start": 1689,
                    "end": 1705,
                    "matchedPaperCorpusId": "235294296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "227231219",
            "title": "Priorless Recurrent Networks Learn Curiously",
            "text": ".\n\nAlongside the observation that children learn their native languages efficiently from limited, noisy input, these shared principles have been been used to support the view that the human language faculties are innately adapted to learn and process one specific set of possible languages. Or, to put it another way, there are structures that are essentially impossible for these innate faculties to handle.\n\nRecently, however, domain-general neural sequence models, lacking explicit linguistic inductive biases, have demonstrated impressive performance on a range of practical language processing applications. This, in turn, has led to increasing interest in the question of how effective these architectures are as models of general human linguistic behaviour. Whereas our interest here is in their behaviour when trained on unnatural structures, most research has so far been interested in their ability to successfully learn the structure of natural languages.\n\nFor example, Gulordava et al. (2018) examined the extent to which a Long Short Term Memory (Hochreiter and Schmidhuber, 1997) network trained on raw text learned about agreement phenomena across a number of languages. They found that, given the right training data, the model was able to predict long-distance agreement on both natural and semantically nonsensical but syntactically valid sentences. Similarly, Futrell et al. (2019) investigated the ability of such models to track syntactic state within subordinations and garden path sentences. Treating their networks as psycholinguistic subjects, they found that given enough data the models responded to subtle cues in a manner comparable to human subjects.\n\nThese results suggest that although an LSTM handles its input in a purely sequential manner, it is able, nonetheless, to learn about phenomena that have traditionally been represented within hierarchical data structures. Frank and colleagues (Frank, 2009;Frank and Bod, 2011;Frank et al., 2012;Frank et al., 2015) suggest that, in fact, such sequential models have advantages over hierarchical alternatives as cognitive models of language processing. Using evidence from reading time and ERP studies, they argue that the ability to accurately model the probabilities of words in context is more important in a psycholinguistic model than the particular architecture employed.\n\nGiven these successes in language learning, Pater (2019) has proposed a complementary role for neural networks alongside generative linguistics. He argues that these architectures may supply the theory",
            "score": 0.43644085022875867,
            "section_title": "Introduction",
            "char_start_offset": 2163,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 981,
                    "end": 1004,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1059,
                    "end": 1093,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1379,
                    "end": 1400,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 1924,
                    "end": 1937,
                    "matchedPaperCorpusId": "10187795"
                },
                {
                    "start": 1937,
                    "end": 1957,
                    "matchedPaperCorpusId": "952654"
                },
                {
                    "start": 1957,
                    "end": 1976,
                    "matchedPaperCorpusId": "11969171"
                },
                {
                    "start": 1976,
                    "end": 1995,
                    "matchedPaperCorpusId": "778955"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "246298665",
            "title": "Unifying Physical Interaction, Linguistic Communication, and Language Acquisition of Cognitive Agents by Minimalist Grammars",
            "text": "This representation allows for the application of semantics via lambda calculus (Church, 1936). For the description of syntactic categories we use Stabler's minimalist grammar (Stabler, 1997;Stabler and Keenan, 2003), (MG) a powerful computational implementation of Chomsky's recent Minimalist Program for generative linguistics (Chomsky, 1995). Despite the controversy between Chomsky and Skinner (Chomsky, 1995), we exploit reinforcement learning (Skinner, 2015;Sutton and Barto, 2018) as training paradigm. Since MG codifies universal linguistic competence through the five inference rules (10-11), thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach could potentially unify generative grammar and reinforcement learning, hence resolving the abovementioned dispute. \n\nMinimalist grammar can be learned from linguistic dependency structures (Kobele et al., 2002;Stabler et al., 2003;Klein and Manning, 2004;Boston et al., 2010) by positive examples, which is supported by psycholinguistic findings on early human language acquisition (Pinker, 1995;Ellis, 2006;Tomasello, 2006). However, as Pinker (1995) has emphasized, learning through positive examples alone, could lead to undesired overgeneralization. Therefore, reinforcement learning that might play a role in children language acquisition as well (Moerk, 1983;Sundberg et al., 1996), could effectively avoid such problems. The required dependency structures are directly provided by the semantics in the training UMPs. Thus, our approach is explicitly semantically driven, in contrast to the algorithm of Klein and Manning (2004) that regards dependencies as latent variables for EM training. \n\nAs a proof-of-concept we suggested an algorithm for simple English declarative sentences. We also have evidence that it works for German and French as well and hopefully for other languages also. Our approach will open up an entirely new avenue for the further development of speech-controlled cognitive user interfaces (Young, 2010;Baranyi et al., 2015).",
            "score": 0.43638086918285407,
            "section_title": "DISCUSSION",
            "char_start_offset": 73157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 94,
                    "matchedPaperCorpusId": "14181275"
                },
                {
                    "start": 176,
                    "end": 191,
                    "matchedPaperCorpusId": "26506696"
                },
                {
                    "start": 191,
                    "end": 216,
                    "matchedPaperCorpusId": "17432618"
                },
                {
                    "start": 891,
                    "end": 912,
                    "matchedPaperCorpusId": "2998847"
                },
                {
                    "start": 933,
                    "end": 957,
                    "matchedPaperCorpusId": "1364249"
                },
                {
                    "start": 957,
                    "end": 977,
                    "matchedPaperCorpusId": "17754102"
                },
                {
                    "start": 1084,
                    "end": 1098,
                    "matchedPaperCorpusId": "9448687"
                },
                {
                    "start": 1098,
                    "end": 1110,
                    "matchedPaperCorpusId": "8041811"
                },
                {
                    "start": 1110,
                    "end": 1126,
                    "matchedPaperCorpusId": "2623654"
                },
                {
                    "start": 1140,
                    "end": 1153,
                    "matchedPaperCorpusId": "9448687"
                },
                {
                    "start": 1354,
                    "end": 1367,
                    "matchedPaperCorpusId": "145213434"
                },
                {
                    "start": 1367,
                    "end": 1389,
                    "matchedPaperCorpusId": "24468447"
                },
                {
                    "start": 1612,
                    "end": 1636,
                    "matchedPaperCorpusId": "1364249"
                },
                {
                    "start": 2022,
                    "end": 2035,
                    "matchedPaperCorpusId": "7003440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55322265625
        },
        {
            "corpus_id": "235097396",
            "title": "What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity",
            "text": "The set of considered linguistic features is based on the ones described in Brunato et al. (2020) which are acquired from raw, morpho-syntactic and syntactic levels of annotation for a total of 78 features that can be categorised in 9 groups corresponding to different linguistic phenomena. A summary of the linguistic features is reported in Table 1, while the whole list is provided in Appendix A. \n\nAs shown in Table, these features model linguistic phenomena ranging from raw text one, to morpho-syntactic information and inflectional properties of verbs, to more complex aspects of sentence structure modeling global and local properties of the whole parsed tree and of specific subtrees, such as the order of subjects and objects with respect to the verb, the distribution of UD syntactic relations, also including features referring to the use of subordination and to the structure of verbal predicates. \n\nAll these features have been shown to play a highly predictive role when leveraged by traditional learning models on a variety of classification problems, also including the development of probes as reported by Miaschi et al. (2020), who showed that these features can be effectively used to profile the knowledge encoded in the language representations of a pretrained NLM.",
            "score": 0.4353621653098878,
            "section_title": "Linguistic Features",
            "char_start_offset": 3751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1287
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 97,
                    "matchedPaperCorpusId": "218974324"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "222134013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "277313547",
            "title": "Linguistic Blind Spots of Large Language Models",
            "text": "number of clauses per T-unit; the amount of coordination, e.g. number of coordinate phrases per clause or T-unit; and the range of surface and particular syntactic and morphological structures, e.g. frequency and variety of tensed forms (Wolfe-Quintero et al., 1998;Ortega, 2003). Blevins et al. (2023) designed structured prompting to assess the linguistic capabilities of LLMs. They provided each LLM with fully labeled demonstrations, and a query sentence and its partially tagged version. Each predicted label was appended to the partially tagged query along with the next word to iteratively tag the full query. They found that GPT-3.5 is robust to arbitrary label selections and ignores labels conflicting with its prior knowledge, indicating that the models can learn general linguistic knowledge during pre-training, rather than simply memorizing the data. Alajrami and Aletras (2022) empirically compared linguistically-motivated (e.g. masked language modeling (Devlin et al., 2019)) and non-linguistically motivated (e.g. masked first character prediction (Yamaguchi et al., 2021)) pretraining objectives for BERT on linguistic probing tasks (Linzen et al., 2016;Warstadt et al., 2020). They found the two objectives achieve similar performance. Clark et al. (2019) showed that attention heads in transformers attend to boundary tokens, positional offsets, and whole sentence; while Voita et al. (2019) showed that attention heads mainly handle positions, syntax, and rare words. Dur-  2020) compared linguistic knowledge learned by LMs at neuron level. They narrowed down neurons to a specific subset, located in lower hidden layers for lexical knowledge and in higher layers for semantic knowledge. Finally, Sharma et al. (2023) found that learning non-linguistic knowledge (e.g. numerical skills) sacrifices the linguistic knowledge of LLMs, and Ettinger (2020) found that BERT underperforms on commonsense, pragmatic inference, and negation tasks.",
            "score": 0.43518625664811816,
            "section_title": "Background",
            "char_start_offset": 4987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 279,
                    "matchedPaperCorpusId": "145719629"
                },
                {
                    "start": 281,
                    "end": 302,
                    "matchedPaperCorpusId": "253523544"
                },
                {
                    "start": 865,
                    "end": 892,
                    "matchedPaperCorpusId": "247594916"
                },
                {
                    "start": 970,
                    "end": 991,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1066,
                    "end": 1090,
                    "matchedPaperCorpusId": "237421110"
                },
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 1256,
                    "end": 1275,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1393,
                    "end": 1412,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 1720,
                    "end": 1740,
                    "matchedPaperCorpusId": "258686278"
                },
                {
                    "start": 1859,
                    "end": 1874,
                    "matchedPaperCorpusId": "199001173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48828125
        },
        {
            "corpus_id": "233289478",
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "text": "Learning dynamics Early work (McClelland and Rumelhart, 1986) observed the dynamics in a feedforward neural network to assess the cognitive plausibility of a connectionist model. They found staged learning in past tense acquisition, similar to humans. More recently, Saphra and Lopez (2019) studied linguistic and topic learning dynamics in hidden states of an LSTM language model. They found that syntactic information is encoded at an early training stage, which is consistent with our finding despite the difference in training objective and network architecture. Our work, instead, uses a rich set of probes to examine more diverse aspects of language and analyzes training iterations.\n\nConcurrent work (Chiang et al., 2020) is the closest work to ours and uses probes to investigate the learning dynamics as well. However, they find that linguistic knowledge and factual knowledge do not generally improve as pretraining proceeds, we find that factual and commonsense knowledge do ( \u00a73); we attribute such difference to our more systematic choice of probes -not only adding two more categories (i.e. commonsense and reasoning) but also more tasks in the linguistic and factual knowledge categories. For example, we found that the factual knowledge probed using SQuAD and ConceptNet data still increases as the pretraining progresses. However, Chiang et al. (2020) only used a subset of T-REx, which plateaus quickly according to our experiments. Sharing their concern of how data affects pretraining, we empirically investigate how the domain of pretraining corpus affects the dynamics of different types of knowledge.\n\nZhang et al. (2020) investigate masked language models trained on corpora of varying sizes in a domain. They experiment with linguistic probes and show that 90% of the improvement in syntactic probing performance can be achieved with a pretraining corpus of only about 10M words. In contrast, the probing performance in commonsense knowledge suffers from small training data. Different from the final models obtained in their work, we consider the entire pretraining trajectories. Nevertheless, our findings by varying corpus size ( \u00a74) are consistent with their conclusion and additionally we find that adding more data in diverse domains can improve both types of knowledge. Their results on the relation between the corpus size and probing performance also support our finding that linguistic knowledge is generally easy to learn",
            "score": 0.4351759082087632,
            "section_title": "Related Work and Further Discussion",
            "char_start_offset": 21352,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "256627634",
            "title": "CALaMo: a Constructionist Assessment of Language Models",
            "text": "In our proposed methodology, CALaMo (Constructionist Assessment of Language Models), we incorporate the UB perspective across all three aspects: input, stability and systematicity. \n\nAs far as input is concerned, CALaMo differs from standard approaches by considering input data an important factor in determining the shape of the learner's grammatical knowledge. In traditional scenarios, the input only serves as a triggering factor and its features play little to no role in the analysis. From a UB perspective, instead, the relation between the abstract grammatical structure of the input and the acquired grammar, which then constrains the production of the learner, is strict. \n\nRegarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training. \n\nThe compositionality vs. productivity perspectives, finally, entail a different organization of linguistic knowledge: the mainstream compositionality perspective tends to set meaning aside, and treat the lexicon as an organized repository of meanings (it makes sense therefore to test NLM's capabilities on semantically nonsensical sentences or to extend the known rules to completely unknown lexical items). In the productivity perspective, instead, meaning is intrinsically part of the process and is treated as a systematic aspect of grammar, too.",
            "score": 0.4351148373740425,
            "section_title": "CALaMo",
            "char_start_offset": 12727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 183,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1685
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "258833033",
            "title": "Prompting is not a substitute for probability measurements in large language models",
            "text": "Few technologies have been as exciting -and divisive -for language science as large language models (LLMs). LLMs are capable of incredibly sophisticated linguistic behaviors, which emerge through statistical learning with massive amounts of text data and highly expressive, domain-agnostic learning architectures. On the one hand, the success of these models has sparked a growing movement to treat them as candidate models of human language acquisition and processing (e.g., Baroni, 2022;Warstadt and Bowman, 2022;Wilcox et al., 2022;Contreras Kallens et al., 2023) -indeed, Piantadosi (2023) even claims that they \"refute\" Chomsky's approach to language. On the other hand, linguists have highlighted shortcomings of current models that make them unsuitable as cognitive theories (e.g., Dupre, 2021;Lan et al., 2022;Katzir, 2023;Milway, 2023;Murphy, 2023). \n\nNo matter their theoretical position, researchers need a way to assess the capabilities of LLMs in order to substantiate such claims. The fundamental unit of LLM computation is P (token|context), which, in principle, can be directly read out from a model by accessing its output layer of vocabulary logits. The distribution that this implies over word strings reflects the model's linguistic generalizations: that is, a generative model of the language seen during training, which can be used to evaluate the likelihood of previously unseen strings. Direct measurements of model-derived string probabilities have revealed capabilities such as syntactic generalizations (e.g., Linzen et al., 2016;Futrell et al., 2019;Hu et al., 2020;Warstadt et al., 2020), semantic plausibility judgments (Kauf et al., 2022), and certain coherence inferences (Beyer et al., 2021). \n\nRecently, there has been a growing trend to use prompting to evaluate LLMs' capabilities. Prompting (popularized by Brown et al., 2020) enables end-to-end interaction with models through natural language, and can be done entirely through inference (i.e., without gradient updates).",
            "score": 0.4348769473135242,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 489,
                    "matchedPaperCorpusId": "235446467"
                },
                {
                    "start": 489,
                    "end": 515,
                    "matchedPaperCorpusId": "251623202"
                },
                {
                    "start": 535,
                    "end": 565,
                    "matchedPaperCorpusId": "257205301"
                },
                {
                    "start": 1537,
                    "end": 1557,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 1557,
                    "end": 1578,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 1578,
                    "end": 1594,
                    "matchedPaperCorpusId": "218571018"
                },
                {
                    "start": 1704,
                    "end": 1724,
                    "matchedPaperCorpusId": "234338939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "15563116",
            "title": "Sources of variation in developmental language disorders: evidence from eye-tracking studies of sentence production",
            "text": "Speaking in sentences is a momentous developmental milestone that marks the beginning of a child's ability to communicate an infinite array of ideas, feelings and experiences, past and present, real or imagined, to other people. Intuitively, this seems an effortless development for most children, yet producing a fluent and meaningful utterance is a complex process. Models of skilled, adult sentence production outline at least three key processes: conceptualization, formulation and articulation [1]. Conceptualization is the stage at which speakers decide on the message to be conveyed. Formulation requires that speakers map that intended message onto word forms (lexicalization) and particular word orders (syntactic planning). Finally, the speaker must plan and make the motor movements necessary to articulate the message. The vast majority of children will begin to master this complex process in the preschool years. For those with neurodevelopmental disorders, however, language production may present lifelong challenges. Until recently though, we have known little about where in the processing chain language breaks down and how identifying points of difficulty may inform potential intervention strategies.\n\nTraditionally, psychologists and linguistics have had to rely on the product of this processing chain to infer the prerequisites needed at each stage for successful production [2]. What skilled speakers say, and importantly the timing of production elements and the errors that speakers make, have provided insights into processing mechanisms and informed computational models of how meaning is translated into the syntax of a particular language [3]. Such models have also made inroads into our understanding of how children become competent syntactic speakers. These models highlight the importance of experience-dependent and error-based learning mechanisms that are shaped by the innate (neural) architecture of the developing system [4]. For most young children, experience is socially mediated; children learn from their interactions with caregivers and very quickly can adapt their language production to their listening audience. intended outputs against expectations derived from linguistic input [4] and the social conventions shared by interlocutors [5].\n\nWhat speakers say, however, will only take us so far. Analysing speech output tells us relatively little about the process of conceptualization, for instance, how do people decide what to talk about and how do children learn to create utterances that are relevant to context and to listener need? How is the form of the",
            "score": 0.4340109540457094,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "21975212"
                },
                {
                    "start": 1670,
                    "end": 1673,
                    "matchedPaperCorpusId": "1237448"
                },
                {
                    "start": 1961,
                    "end": 1964,
                    "matchedPaperCorpusId": "8713012"
                },
                {
                    "start": 2229,
                    "end": 2232,
                    "matchedPaperCorpusId": "8713012"
                },
                {
                    "start": 2284,
                    "end": 2287,
                    "matchedPaperCorpusId": "144643962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.268798828125
        },
        {
            "corpus_id": "14098234",
            "title": "A multimodal corpus for the evaluation of computational models for (grounded) language acquisition",
            "text": "Children acquire linguistic structures through exposure to (spoken) language in a rich context and environment. The semantics of language may be learned by establishing connections between linguistic structures and corresponding structures in the environment, i.e. in different domains such as the visual one (Harnad, 1990). Both with respect to modeling language acquisition in children and with respect to enabling corresponding language acquisition capabilities in robots, which may ideally be also grounded in their environment, it is hence of great interest to explore i) how linguistic structures of different levels of complexity, e.g. words or grammatical phrases, can be derived from speech input, ii) how structured representations for entities observed in the environment can be derived, e.g. how concepts and structured representations of actions can be formed, and iii) how connections can be established between structured representations derived from different domains. In order to gain insights concerning the mechanisms at play during language acquisition (LA), which enable children to solve these learning tasks, models are needed which ideally cover several learning tasks. For instance, they may cover the acquisition of both words and grammatical rules as well as the acquisition of their grounded meanings. Complementarily, data resources are needed which enable the design and evaluation of these models by providing suitable parallel data. Aiming to provide a basis for the development and evaluation of LA models addressing the acquisition of rather complex and grounded linguistic structures, i.e. syntactic patterns, from subsymbolic input, we designed a German multimodal input corpus. The corpus consists of data of multiple speakers/actors who performed actions in front of a robot and described these actions while executing them. Subjects were recorded, i.e. parallel data of speech, stereo vision (including the view-perspective of the \"infant\"/robot) and body postures were gathered. The resulting data hence allow grounding of linguistic structures in both vision and body postures. Among others, learning processes that may be evaluated using the corpus include: acquisition of several linguistic structures, acquisition of visual structures, concept formation, acquisition of generalized patterns which abstract over different speakers and actors, establishment of correspondences between structures from different domains, acquisition of manipulation skills, and development of appropriate models for the representations of actions. This paper is organized as follows. Next, we will provide background information concerning computational models of LA.",
            "score": 0.4332559473153225,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2571
                },
                {
                    "start": 2572,
                    "end": 2607
                },
                {
                    "start": 2608,
                    "end": 2691
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 323,
                    "matchedPaperCorpusId": "3204300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "266348523",
            "title": "Review of Unsupervised POS Tagging and Its Implications on Language Acquisition",
            "text": "Similarly, from the engineering side, we saw how grounding each category to a lexical item could improve upon the standard HMM performance (Stratos et al., 2016). In this anchored HMM (Stratos et al., 2016), the model does not add information about lexical meaning, but the lexical items form the basis for the syntactic categories. In particular, the HMM is constrained such that each category must have an anchor word that only this category emits. Additionally, from the engineering side, we saw that extending unsupervised models with a prototype model produced competitive performance to more recent modeling efforts (Christodoulopoulos et al., 2010). Again, this engineering work does not group based on meaning, but it is identifying lexical items most representative of a certain syntactic category. \n\nIn summary, this last piece of additional information identifies lexical items that are representative for a syntactic category, but the engineering approaches (prototype and anchoring) differs from the acquisition modeling approach by how these lexical items are selected. The engineering side attempts to identify these anchors through distributional information while the acquisition modeling side attempts to leverage some other child knowledge assumed in the literature (namely, a limited lexicon of word meanings) to identify words that serve as the basis for different categories. Depending on the stage of development and the granularity of the syntactic category the child is learning, a combination of lexical meaning and distributional information may be relevant for identifying prototypes. Future modeling work can extend the engineering approach to corpora of child directed speech and investigate the extent to which semantic information assists identifying prototypes (extend beyond only noun and verb categories to see the granularity of categories that semantic information affords). \n\nAs a whole, these proposals of additional sources of information leveraged during categorization presents a range of details that must be accounted for when building models of acquiring syntactic categories. The ultimate goal of acquisition modeling work is to approximate as closely as possible the computations that children are using with all the possible information children leverage from their input. To this end, these additional sources of information proposals outline different possibilities along the dimensions of computations and information leveraged from the input.",
            "score": 0.4332336705305973,
            "section_title": "beyond the HMM",
            "char_start_offset": 51251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2320
                },
                {
                    "start": 2321,
                    "end": 2494
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 161,
                    "matchedPaperCorpusId": "13298680"
                },
                {
                    "start": 184,
                    "end": 206,
                    "matchedPaperCorpusId": "13298680"
                },
                {
                    "start": 622,
                    "end": 655,
                    "matchedPaperCorpusId": "7784215"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "273374794",
            "title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models",
            "text": "We conduct a comprehensive analysis of the linguistic and visual capabilities of PIXEL that can be used to aid further development of pixel-based language models. Concretely: RQ1: How much linguistic knowledge is encoded in PIXEL? RQ2: How much visual capability does PIXEL have? 2  We find that PIXEL learns surface-level linguistic information in the lower layers, resulting in higher-level syntactic and semantic abstractions appearing in higher layers than BERT ( \u00a75.1). When comparing to VIT-MAE, PIXEL underperforms on image tasks, with visual probing accuracy decreasing in the higher layers ( \u00a75.2). Thus, the surface-level information is diluted as it acquires linguistic knowledge in the higher layers. Lotz et al. (2023) trained newer pixel-based language models that add some orthographic constraints to the input that can potentially augment linguistic learning in the lower layers. In this context, we ask the following question: RQ3: Does adding orthographic constraints to the input enhance the linguistic capabilities in PIXEL? \n\nWe find that a rendering strategy that makes word boundaries more explicit in the input enables PIXEL to learn surface-level linguistic features earlier in the model, thereby aiding semantic understanding ( \u00a75.3). \n\nOverall, we take inspiration from BERTology, the study of the linguistic capabilities in BERT (Rogers et al., 2020), and aim for this work to foster future explorations and advancements for PIXEL.",
            "score": 0.4328194939813699,
            "section_title": "Introduction",
            "char_start_offset": 1863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1459
                }
            ],
            "ref_mentions": [
                {
                    "start": 713,
                    "end": 731,
                    "matchedPaperCorpusId": "264832733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "50773083",
            "title": "Chromatic transitions in the emergence of syntax networks",
            "text": "The origins of human language have been a matter of intense debate. Language is a milestone in our evolution as a dominant species and is likely to pervade the emergence of cooperation and symbolic reasoning [1][2][3][4]. Maybe the most defining and defeating trait is its virtually infinite generative potential: words and sentences can be constructed in recursive ways to generate nested structures of arbitrary length [3,5]. Such structures are the product of a set of rules defining syntax, which are extracted by human brains through language acquisition during childhood after a small sample of the whole combinatorial universe of sentences has been learned. And yet, in spite of its complexity, syntax is accurately acquired by children, who master their mother tongue in a few years of learning. Indeed, around the age of two, linguistic structures produced by children display a qualitative shift on their complexity, indicating a deep change on the rules underlying them [6,7]. This sudden increase of grammar complexity is known as the syntactic spurt, and defines the edge between the two words stage, where only isolated words or combinations of two words occur, to a stage where the grammar rules governing this syntax are close to the one we can find in adult speech -although the cognitive maturation of kids makes the semantic content or the pronunciation different from the adult one. How can we explain or interpret such nonlinear pattern? Statistical physicists have approached the problem of language evolution showing for example that nontrivial patterns are shared between language inventories (collections of words) and some genetic and ecological neutral models [8] -see [9] and references therein. However, most of these models do not make any assumption about the role played by actual interactions among words, or, more generally, linguistic units, which largely define the nature of linguistic structures. In this context, a promising approach to its structure and evolution involves considering language in terms of networks of interconnected units instead of unstructured collections of elements (e.g., words or syllabes) [10]. In this context, syntactic networks, in which nodes are words and links the projection of actual syntactic relations, have been shown to be an interesting abstraction to grasp general patterns of language production [10][11][12].",
            "score": 0.4322786852706629,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2237548828125
        },
        {
            "corpus_id": "226283990",
            "title": "On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers",
            "text": "The focus of our work is on studying how finetuning affects the representations learned by a pretrained model. We assess this change through sentence-level probing tasks. We focus on sentencelevel probing tasks since during fine-tuning we explicitly train a model to represent sentence-level context in the CLS token.\n\nThe fine-tuning and probing tasks we study concern different linguistic levels, requiring a model to focus more on syntactic, semantic or discourse information. The extent to which knowledge of a particular linguistic level is needed to perform well differs from task to task. For instance, to judge if the syntactic structure of a sentence is intact, no deep discourse understanding is needed. Our hypothesis is that if a pre-trained model encodes certain linguistic knowledge, this acquired knowledge should lead to a good performance on a probing task testing for the same linguistic phenomenon. Extending this hypothesis to fine-tuning, one might argue that if fine-tuning introduces new or removes existing linguistic knowledge into/from a model, this should be reflected by an increase or decrease in probing performance. 1 However, we argue that encoding or forgetting linguistic knowledge is not necessarily the only explanation for observed changes in probing accuracy. Hence, the goal of our work is to test the abovestated hypotheses assessing the interaction between fine-tuning and probing tasks across three different encoder models.",
            "score": 0.4321601249381285,
            "section_title": "Methodology and Setup",
            "char_start_offset": 6032,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "271923865",
            "title": "A Language-agnostic Model of Child Language Acquisition",
            "text": "Our approach differs theoretically from other recent approaches to language acquisition. Ambridge (2020) argues that language acquisition can be understood purely based on the recall of all past occasions on which an utterance was used. This is claimed to adequately account for a range of linguistic phenomena without recourse to syntactic or semantic abstractions. Our model, on the other hand, uses abstractions in the form of lexical entries for words, and combinatory CCG rules. Chater and Christiansen (2018) treat language acquisition as the learning of a perceptuo-motor skill. One point of emphasis for Chater and Christiansen (2018) is the fact that much relevant information to language learning is forgotten quickly, necessitating that learning occurs rapidly and in real-time (in this sense, the polar opposite of Ambridge ( 2020)). Another point of emphasis is the social context in which the child hears the utterance. We account for the first point by training on each example only once, one at a time, in the order they appear to the child. Pragmatic context is not currently represented in our input to the learner. We also differ from these works in that ours is not purely theoretical but is based on a working model. An earlier work, Regier (2005) proposes a programmable model whose framework is similar to the theoretical account of Ambridge (2020). The data consists of utterances paired with manually created binary strings, where each bit indicates the presence or absence of a syntactic feature. Yang et al. (2002) proposes a probabilistic language acquisition model that assumes that the child begins with access to all possible grammars, which can be specified by a finite set of parameters, i.e. the principles and parameters framework (Chomsky, 1981;Hyams, 1986). It then learns to place more weight on those grammars that successfully parse observed sentences. This differs from our model, which acquires a statisical model of language-specific syntax, lexicon and logical form simultaneously by semantic bootstrapping from utterance meaning representations. \n\nIn the realm of word learning from speech, R\u00e4s\u00e4nen and Khorrami (2019) presents a model for early word learning from real multimodal data, testing on English only.",
            "score": 0.4313696277174782,
            "section_title": "Discussion",
            "char_start_offset": 31936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2090
                },
                {
                    "start": 2093,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 1255,
                    "end": 1268,
                    "matchedPaperCorpusId": "1344370"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "64435976",
            "title": "Explicit Causal Connections between the Acquisition of Linguistic Tiers: Evidence from Dynamical Systems Modeling",
            "text": "A crucial question in language acquisition concerns how (or, according to some, whether) children learn the grammars of their native languages. Some researchers, mainly coming from the generative tradition, argue that, although the grammatical rules are possibly 'innate' (e.g., Pinker, 1994), children still need to learn how to map the different semantic/grammatical roles onto the different options offered by Universal Grammar (e.g., 'parameter-setting'). The evidence, however, does not seem to support this hypothesis. For instance, Bowerman (1990) notes that the type of semantic aspects learned by the child do not match well into the prototypical roles that would be required to map into hard linguistic rules (e.g., learning an AGENT category to map onto the SUBJECT syntactic role). Other researchers (e.g., Goldberg, 2003;Tomasello, 1992;Tomasello, 2005) propose that there is a gradual increase in the generality of the structures learned by the child, which are slowly acquired through distributional learning. Such a picture is strongly supported by the remarkably little creativity exhibited by children, most of whose utterances are often literal repetitions of those that they have previously heard (Lieven et al., 1997;Pine and Lieven, 1993), with little or no generalization in the early stages. It appears as though children progressively and conservatively increase the level at which they generalize linguistic constructions, building from the words upwards, in what some have termed 'lexically-based positional analysis' (Lieven et al., 1997). \n\nThe Theory of Dynamical Systems offers powerful tools for modeling human development (e.g, Smith and Thelen, 2003;van Geert, 1991). It provides a mathematical framework for implementing the principle that development involves the mutual and continuous interaction of multiple levels of the developing system, which simultaneously unfold over many time-scales. Typically, a dynamical system is described by a system of coupled differential equations governing the temporal evolution of multiple parts of the system and their interrelations.",
            "score": 0.4310128784892655,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 292,
                    "matchedPaperCorpusId": "143780834"
                },
                {
                    "start": 539,
                    "end": 554,
                    "matchedPaperCorpusId": "145008666"
                },
                {
                    "start": 819,
                    "end": 834,
                    "matchedPaperCorpusId": "12393863"
                },
                {
                    "start": 850,
                    "end": 866,
                    "matchedPaperCorpusId": "15476748"
                },
                {
                    "start": 1217,
                    "end": 1238,
                    "matchedPaperCorpusId": "28497319"
                },
                {
                    "start": 1238,
                    "end": 1260,
                    "matchedPaperCorpusId": "21050989"
                },
                {
                    "start": 1545,
                    "end": 1566,
                    "matchedPaperCorpusId": "28497319"
                },
                {
                    "start": 1661,
                    "end": 1684,
                    "matchedPaperCorpusId": "5712760"
                },
                {
                    "start": 1684,
                    "end": 1700,
                    "matchedPaperCorpusId": "201309932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2666015625
        },
        {
            "corpus_id": "252815591",
            "title": "Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training",
            "text": "Pre-trained Language Models (PLMs) such as BERT (Devlin et al., 2019) are pre-trained by selfsupervised learning on large-scale text corpora to capture the rich semantic knowledge of words (Li et al., 2021;Gong et al., 2022), improving various downstream NLP tasks significantly (He et al., 2020;Xu et al., 2021;Chang et al., 2021). Although these PLMs have stored much internal knowledge (Petroni et al., 2019(Petroni et al., , 2020)), they can hardly understand external background knowledge from the world such as factual and linguistic knowledge (Colon-Hernandez et al., 2021;Cui et al., 2021;Lai et al., 2021). \n\nIn the literature, most approaches of knowledge injection can be divided into two categories, including relational knowledge and linguistic knowledge. \n\n(1) Relational knowledge-based approaches inject entity and relation representations in Knowledge Graphs (KGs) trained by knowledge embedding algorithms (Zhang et al., 2019;Peters et al., 2019) or convert triples into sentences for joint pre-training (Liu et al., 2020;Sun et al., 2020). ( 2) Linguistic knowledge-based approaches extract semantic units from pre-training sentences such as part-ofspeech tags, constituent and dependency syntactic parsing, and feed all linguistic information into various transformer-based architectures (Zhou et al., 2020;Lai et al., 2021). We observe that there can be three potential drawbacks. (1) These approaches generally utilize a single source of knowledge (i.e., inherent linguistic knowledge), which ignore important knowledge from other sources (Su et al., 2021) (i.e., relational knowledge from KGs). \n\n(2) Training large-scale KEPLMs from scratch requires high-memory computing devices and is timeconsuming, which brings significant computational burdens for users (Zhang et al., 2021(Zhang et al., , 2022)). (3) Most of these models are pre-trained in English only.",
            "score": 0.43065602174300843,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 69,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 189,
                    "end": 206,
                    "matchedPaperCorpusId": "234742522"
                },
                {
                    "start": 206,
                    "end": 224,
                    "matchedPaperCorpusId": "248780227"
                },
                {
                    "start": 279,
                    "end": 296,
                    "matchedPaperCorpusId": "222208701"
                },
                {
                    "start": 296,
                    "end": 312,
                    "matchedPaperCorpusId": "229679923"
                },
                {
                    "start": 312,
                    "end": 331,
                    "matchedPaperCorpusId": "235390794"
                },
                {
                    "start": 389,
                    "end": 410,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 410,
                    "end": 435,
                    "matchedPaperCorpusId": "212411919"
                },
                {
                    "start": 580,
                    "end": 597,
                    "matchedPaperCorpusId": "235436397"
                },
                {
                    "start": 597,
                    "end": 614,
                    "matchedPaperCorpusId": "233240836"
                },
                {
                    "start": 924,
                    "end": 944,
                    "matchedPaperCorpusId": "158046772"
                },
                {
                    "start": 944,
                    "end": 964,
                    "matchedPaperCorpusId": "202542757"
                },
                {
                    "start": 1022,
                    "end": 1040,
                    "matchedPaperCorpusId": "202583325"
                },
                {
                    "start": 1040,
                    "end": 1057,
                    "matchedPaperCorpusId": "222090412"
                },
                {
                    "start": 1308,
                    "end": 1327,
                    "matchedPaperCorpusId": "265038860"
                },
                {
                    "start": 1327,
                    "end": 1344,
                    "matchedPaperCorpusId": "233240836"
                },
                {
                    "start": 1802,
                    "end": 1825,
                    "matchedPaperCorpusId": "244799139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "1065242",
            "title": "Sentence Comprehension as Mental Simulation: An Information-Theoretic Perspective",
            "text": "As discussed in the Introduction, the language and the world form two different sources of statistical patterns with respect to which information measures can be defined. We will call these syntactic information and semantic information, respectively. It is important to keep in mind that we use these terms in a very specific sense, which may be slightly different from their use in (psycho)linguistics. By syntax we refer to the probabilities of word strings, where grammatical sentences are precisely those strings with strictly positive probability. That is, we do not mean to evoke any particular grammar from which sentence probabilities (or even just grammaticality judgments) arise. Our use of semantics is restricted to the relation between a sentence and the state-of-affairs in the world to which it refers. That is, we do not attempt to relate our model to the representations and analyses that are common in the field of formal semantics. Also, we only make a strict analytical distinction between syntactic and semantic information here, in order to clarify the current modeling work. We do not wish to claim that these two information source are also cognitively distinct. As we said before, language is part of our world so there is no a priori reason to assume that linguistic knowledge and world knowledge are acquired using fundamentally different learning processes. \n\nAn orthogonal distinction can be made between two different definitions of the amount of information conveyed by each word of a sentence, which have both been proposed in the computational psycholinguistics literature: surprisal [20,23] and entropy reduction [21,22,39]. The first is a formal measure of the extent to which a word came unexpected, whereas the latter quantifies the extent to which a word reduces uncertainty about the upcoming material. \n\nThis section explains how these four information measures (syntactic surprisal, semantic surprisal, syntactic entropy reduction, and semantic entropy reduction) can be defined in general and how their specific values follow from the microworld and microlanguage discussed in the previous section.",
            "score": 0.43040155281671866,
            "section_title": "Quantifying Word Information",
            "char_start_offset": 23233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "5490143"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "6211281"
                },
                {
                    "start": 1652,
                    "end": 1655,
                    "matchedPaperCorpusId": "9487181"
                },
                {
                    "start": 1655,
                    "end": 1658,
                    "matchedPaperCorpusId": "15922633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "267406858",
            "title": "Linguistic Productivity: the Case of Determiners in English",
            "text": "Many computational models have framed the general problem of inducing abstract categories from unannotated text as clustering words into lexical categories based on the distributional properties of their context (e.g., Redington et al., 1998;Clark, 2000;Mintz, 2003;Parisien et al., 2008;Chrupa\u0142a, 2011), showing the possibility of learning categories that resemble parts of speech from raw text. Alishahi and Chrupa\u0142a (2012) and Abend et al. (2017) model concurrent acquisition of word meanings and syntactic categories and focus on the impact of integrating knowledge of syntax (and particularly the syntactic category of a word) into the word learning process. However, none of these models focus on the nature and developmental trajectory of the induced categories, nor do they compare their linguistic productivity to humans. An exception is Parisien et al. (2008) who present an incremental Bayesian model for learning syntactic categories from linguistic context, and test it on child-directed data from the Manchester corpus. Their analysis of the emerging categories shows that the categories follow the same trend as children's categories in that nouns are learned before verbs, followed by adjectives (Kemp et al., 2005). However they do not analyze the emergence of each category and its use in child-produced speech. \n\nWith the increased dominance of deep neural models of language, much effort is put into analyzing the learned representations in inner layers of these models and to search for encoding syntactic information (see Belinkov and Glass, 2019, for an overview). Various analyses have shown that deep language models encode information about syntactic categories and syntactic dependencies in their learned representations without explicit training (e.g., Adi et al., 2017;Hewitt and Manning, 2019;Chrupa\u0142a and Alishahi, 2019;Tayyar Madabushi et al., 2022). However, the focus of this body of work is mainly on large language models that are trained on massive datasets (with some exceptions; see Grimm et al., 2015), and comparison with human language learning is not common.",
            "score": 0.4300515059904914,
            "section_title": "Related Work",
            "char_start_offset": 4662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 242,
                    "matchedPaperCorpusId": "2596605"
                },
                {
                    "start": 242,
                    "end": 254,
                    "matchedPaperCorpusId": "1747915"
                },
                {
                    "start": 266,
                    "end": 288,
                    "matchedPaperCorpusId": "3062627"
                },
                {
                    "start": 288,
                    "end": 303,
                    "matchedPaperCorpusId": "1557484"
                },
                {
                    "start": 397,
                    "end": 425,
                    "matchedPaperCorpusId": "8264070"
                },
                {
                    "start": 430,
                    "end": 449,
                    "matchedPaperCorpusId": "206866667"
                },
                {
                    "start": 847,
                    "end": 869,
                    "matchedPaperCorpusId": "3062627"
                },
                {
                    "start": 1212,
                    "end": 1231,
                    "matchedPaperCorpusId": "19376934"
                },
                {
                    "start": 1544,
                    "end": 1568,
                    "matchedPaperCorpusId": "56657817"
                },
                {
                    "start": 1781,
                    "end": 1798,
                    "matchedPaperCorpusId": "6771196"
                },
                {
                    "start": 1798,
                    "end": 1823,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1823,
                    "end": 1851,
                    "matchedPaperCorpusId": "155099789"
                },
                {
                    "start": 1851,
                    "end": 1881,
                    "matchedPaperCorpusId": "249538362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "270562766",
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "text": "Given our primary goal: to demonstrate how the interplay between syntactic and semantic acquisition can follow from joint learning, using simulated children's book data presented a sufficient environment to examine these dynamics.\n\nJoint learning works because it helps mutually constrain related hypothesis spaces, here grammar and semantic representations.These types of constraints are likely necessary for human learners who-unlike large language models-are limited in terms of memory and processing capacity as well as amount of input evidence.Even with these limitations though, we learn language and better yet, we learn representations which allow to generalize and use language in completely novel contexts.The reason for our learning efficiency and generalization abilities may lie in our effective learning strategies, which we argue are built on joint learning.\n\nEmpirical evidence suggests that semantic and syntactic processing during language comprehension or production are not separable into distinct areas of the brain, but instead represent distributed processes which overlap across a wide region referred to as the language network (Fedorenko, Blank, Siegelman, & Mineroff, 2020;Hu et al., 2022;Shain et al., 2023;Fedorenko, Ivanova, & Regev, 2024;Shain et al., 2024).Furthermore, children's lexicon and their syntactic production abilities grow side by side during language development (Bates et al., 1994;Brinchmann, Braeken, & Lyster, 2019;Frank, Braginsky, Marchman, & Yurovsky, 2021).These results all support our proposal: that language learning is joint learning across many levels of linguistic representation.The acquisition of morphemes, words, syntax, semantics, pragmatics have for the most part been considered in isolation.However, if language learning is indeed a joint inference problem across many levels of linguistic structure, then future research in the field should try to understand how learning biases or constraints within these different levels arise as a function of joint learning.For example, how does the acquisition of semantic knowledge affect the acquisition of syntax?or how does learning morpheme boundaries interplay with the acquisition of semantic knowledge?Understanding how these constraints arise and interact, we suggest should be the next key direction in language learning debates.\n\nComputational modeling is not new to the fields of language development and cognitive science.",
            "score": 0.42987219962851014,
            "section_title": "Discussion",
            "char_start_offset": 49997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 232,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 873
                },
                {
                    "start": 875,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2123
                },
                {
                    "start": 2123,
                    "end": 2217
                },
                {
                    "start": 2217,
                    "end": 2346
                },
                {
                    "start": 2348,
                    "end": 2442
                }
            ],
            "ref_mentions": [
                {
                    "start": 1153,
                    "end": 1200,
                    "matchedPaperCorpusId": "213787994"
                },
                {
                    "start": 1200,
                    "end": 1216,
                    "matchedPaperCorpusId": "251323162"
                },
                {
                    "start": 1216,
                    "end": 1235,
                    "matchedPaperCorpusId": "259328196"
                },
                {
                    "start": 1235,
                    "end": 1269,
                    "matchedPaperCorpusId": "269112224"
                },
                {
                    "start": 1269,
                    "end": 1288,
                    "matchedPaperCorpusId": "269474005"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "19579561"
                },
                {
                    "start": 1428,
                    "end": 1464,
                    "matchedPaperCorpusId": "205063826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "276574996",
            "title": "UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings",
            "text": "The UrduLLaMA 1.0 model is trained on the Causal Language Modeling (CLM) task, enabling it to predict and generate the next word in a sequence. This stage plays a crucial role in refining LLaMA's proficiency in Urdu by allowing the model to grasp the language's intricate syntactic structures, semantic nuances, and unique linguistic traits. Leveraging its autoregressive nature, CLM mirrors the human process of language comprehension and generation, which is inherently contextdependent. Consequently, by the end of this initial training phase, LLaMA acquires the ability to generate and interpret Urdu text with contextual relevance and linguistic accuracy.",
            "score": 0.4292436072474447,
            "section_title": "Continual Pretraining",
            "char_start_offset": 16130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 660
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "277740884",
            "title": "Linguistic Interpretability of Transformer-based Language Models: a systematic review",
            "text": "Aside from syntactic knowledge, semantic knowledge seems to be overall well represented in state-of-the-art pre-trained PLMs [20,80,161], with BERT representations \"satisfy [ing] two desiderata for psychologically valid semantic representations: i) they have a stable semantic core which allows people to interpret words in isolation and prevents words to be used arbitrarily and ii) they interact with sentence context in systematic ways, with representations shifting as a function of their semantic core and the context\" [20]. \n\nBERT is also found to \"split core semantic roles into many fine-grained categories, and seem[s] to encode broad notions of syntactic and semantic structure\" [105]. \n\nDuring the pre-training of the models, syntactic capabilities are rapidly acquired while semantic knowledge is learned in later stages of the model's training, in a progressive manner [116]. In the case of RoBERTa, it has been shown that the model is slower at learning facts and commonsense knowledge, depending as well on the domain [86]. \n\nRegarding more specific semantic phenomena, we can also provide the following conclusions: \n\n\u2022 BERT encodes polysemy well [46], across several languages -English, French, Spanish and Greek. However, polysemous representations are found to be better in monolingual BERT models compared to mBERT -a fact that is partly blamed on the use of an English-oriented tokenizer for the latter, which skews multilingual representations to that language [46]. \n\n\u2022 BERT seems to possess knowledge of dates, scalar and measurable values, and is capable of distinguishing between small and large numbers [105]. \n\n\u2022 The modeling of verb-argument structure -relying on the Dowty theory of thematic proto-roles [37]-is well encoded in BERT [136]. \n\n\u2022 In multilingual models, the encoding of 'subjecthood' -i.e. the notion of subject in a sentence-is not only based on syntactical factors but also dependent on semantic ones [127]. Further, multilingual models seem to capture lexical features well -although not so on nominal and verbal features [188]. 31",
            "score": 0.4291131991289159,
            "section_title": "Is semantic information well represented in PLMs?",
            "char_start_offset": 52043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "250279831"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "232092403"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "233476317"
                },
                {
                    "start": 1483,
                    "end": 1487,
                    "matchedPaperCorpusId": "233476317"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "6330195"
                },
                {
                    "start": 1763,
                    "end": 1768,
                    "matchedPaperCorpusId": "252819504"
                },
                {
                    "start": 1947,
                    "end": 1952,
                    "matchedPaperCorpusId": "231719085"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "53324448",
            "title": "Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model",
            "text": "Language modelling involves learning to predict the next word in a sequence of words, using large text corpora as the training input. Language models must therefore learn to represent information from the preceding context which is relevant for future word prediction, and, intuitively, this should include information about the syntactic structure of the context and the meanings of constituent words. Today's state-of-the-art language models make use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory cells (LSTMs) (Hochreiter and Schmidhuber, 1997) which can handle time series information by remembering salient information over latent variables (Mikolov et al., 2010). Because of their wide applicability, there has been much interest in developing a better understanding of the inner workings of RNN models, and, in particular, researchers have investigated how syntactic knowledge is encoded and processed by such networks (Dyer et al., 2016;Linzen et al., 2016;Jozefowicz et al., 2016;Mc-Coy et al., 2018;Gulordava et al., 2018). Karpathy et al. (2015) performed an in-depth analysis of the types of errors RNN's make, in order to understand how recurrent mechanisms can encode long-term dependency information. Linzen et al. (2016) present a more direct analysis by examining LSTM language models' ability to understand difficult long-range dependencies such as the form of a verb linked to a noun subject. Recently, researchers have started to study the semantic embeddings generated by these networks (Chrupa\u0142a et al., 2015), especially for those focused on encoding visual grounding (Kiela et al., 2017;Yoo et al., 2017). However, compared to syntax, there has been relatively less work on how LSTM networks represent lexical semantic knowledge. \n\nIn this work, we evaluate latent semantic knowledge present in the LSTM activation patterns produced before and after the word of interest. We evaluate whether these activations predict human similarity ratings, human-derived property knowledge, and brain imaging data.",
            "score": 0.42837110955638397,
            "section_title": "Introduction & Related Work",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 531,
                    "end": 565,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 664,
                    "end": 686,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 1027,
                    "end": 1050,
                    "matchedPaperCorpusId": "4460159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "259089351",
            "title": "Language acquisition: do children and language models follow similar learning stages?",
            "text": "For both of these approaches, however, the measures from children at such early ages can be noisy and fragmented. \n\nInterestingly, these issues do not apply to modern language models. Deep learning architectures trained to predict words from their proximal contexts have proved immensely effective at learning to process natural language (Radford et al., 2019;Devlin et al., 2019). Unlike humans, these algorithms can be easily probed during training, at any time point and rate, and with unlimited number of test stimuli, without interfering with their language acquisition (Jawahar et al., 2019;Manning et al., 2020;Bowman and Dahl, 2021). Furthermore, high-performing deep nets have been shown to implicitly (Lakretz et al., 2019;Gulordava et al., 2018) or explicitly learn to represent and use syntactic structures (Manning et al., 2020), as well as to use features such as concreteness and lexical class to learn language (Chang and Bergen, 2022). Finally, and importantly, these deep neural networks have recently been shown to represent lexical, syntactic and compositional representations similarly to the adult brain (Jain and Huth, 2018;Toneva and Wehbe, 2019;Caucheteux and King, 2022;Pasquiou et al., 2022Pasquiou et al., , 2023;;Caucheteux et al., 2023). Evidencing similar learning trajectories in children and language models could thus provide an invaluable framework to better understand the computational principles underlying language acquisition. \n\nHere, we compare the trajectory of language acquisition between human children and modern language models. We focus on three main questions. First, do these models learn linguistic skills in a systematic order? Second, is this trajectory sequential or parallel? Third, is this trajectory similar to that of children? These hypotheses are illustrated in Figure 1. \n\nSpecifically, we train 48 GPT-2 architectures (Radford et al., 2019) from scratch, using a standard next-word prediction objective.",
            "score": 0.4278303444784689,
            "section_title": "Introduction",
            "char_start_offset": 1909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 575,
                    "end": 597,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 597,
                    "end": 618,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 618,
                    "end": 640,
                    "matchedPaperCorpusId": "233033916"
                },
                {
                    "start": 733,
                    "end": 756,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 819,
                    "end": 841,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 927,
                    "end": 951,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1126,
                    "end": 1147,
                    "matchedPaperCorpusId": "53983498"
                },
                {
                    "start": 1147,
                    "end": 1170,
                    "matchedPaperCorpusId": "167217728"
                },
                {
                    "start": 1170,
                    "end": 1196,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 1196,
                    "end": 1217,
                    "matchedPaperCorpusId": "250334501"
                },
                {
                    "start": 1217,
                    "end": 1242,
                    "matchedPaperCorpusId": "257232607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "277740884",
            "title": "Linguistic Interpretability of Transformer-based Language Models: a systematic review",
            "text": "Many works across our survey positively report on syntactic information being encoded in PLMs [27,161] -although some works also note 'unstable' probing results across different monolingual BERT models [71]. Syntactic information is said to be more strongly encoded in these models compared to semantics [58,140], while it is also reported to have been acquired already during the pre-training of the models [32,116] -and potentially being learned by the models first, during the earlier stages of their training, preceding that of semantic knowledge [116]. For the linguistic phenomenon of pronominal anaphora, this seems to be well encoded in a specifically studied model: Transformer-XL [156]. For multilingual models there is evidence of syntactic knowledge being transferred in mBERT across languages [48], although sensitivity to syntactic knowledge in mBERT and mBART is also said to be different depending on either the language they are trained on or their used pre-training objectives [103,158]. \n\nThere also some works reporting negative or inconclusive results of the internal encoding in PLMs of syntactic information. For instance, some works claim that word order is partially responsible for the apparent encoding of syntax-like structures in Transformer-based models [129,147]. In the same line, it has been shown that it is possible to pre-train PLMs on a corpora of texts with shuffled word order and still obtain good end results with the trained model [153], thus concluding that the statistical co-occurrence of words might be more important for the final model's performance. Conversely, other works find that Transformer-based models do not rely on positional information to derive syntactic trees [103], and that these models process positional information in lower layers but later change to a more hierarchical-based encoding in later layers [83]. \n\nOther linguistic phenomena were studied for which the authors did not find conclusive evidence of their encoding in the model, for instance: subject-verb agreement for less frequently-seen verb forms [172], reflexive anaphora [83], or implicit causality (IC) verbs [34].",
            "score": 0.42756953724147717,
            "section_title": "Is syntactic information well represented in PLMs?",
            "char_start_offset": 37438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "248164185"
                },
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "252819062"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "233864607"
                },
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "203690445"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "251439336"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "232092403"
                },
                {
                    "start": 551,
                    "end": 556,
                    "matchedPaperCorpusId": "232092403"
                },
                {
                    "start": 690,
                    "end": 695,
                    "matchedPaperCorpusId": "220047937"
                },
                {
                    "start": 806,
                    "end": 810,
                    "matchedPaperCorpusId": "237690935"
                },
                {
                    "start": 995,
                    "end": 1000,
                    "matchedPaperCorpusId": "229292797"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "238215568"
                },
                {
                    "start": 1284,
                    "end": 1289,
                    "matchedPaperCorpusId": "247447149"
                },
                {
                    "start": 1289,
                    "end": 1293,
                    "matchedPaperCorpusId": "248780571"
                },
                {
                    "start": 1473,
                    "end": 1478,
                    "matchedPaperCorpusId": "233231592"
                },
                {
                    "start": 1722,
                    "end": 1727,
                    "matchedPaperCorpusId": "229292797"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "174799346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "272398333",
            "title": "A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Hum4n L4ngu4ge and the W0rld behind W0rds?",
            "text": "Yet the full conceptual meaning of words themselves remain opaque; we suspect because LLMs do not 'map' to some independent conceptual system with its own rules and representations. As our experiment with leet code revealed, when pushed beyond the training data through being tested against unusual linguistic examples, LLMs provide outputs that suggest that when the lexical envelope into which concepts are embedded is altered, the models cannot perform like humans, neither quantitatively nor qualitatively. Put another way, humans know how these words are used and thus they can successfully re-anchor concepts to their altered lexical envelopes. Models rely on form, hence disturbances in the latter are harder to compensate for. \n\nIt remains to be seen whether this turns out to just be a problem of scale and fine-tuning, or if LLMs can even in principle be augmented sufficiently to approximate human-like linguistic competence. We suspect that any researchers aiming optimistically in this direction will have to consider appropriate inductive biases (e.g., syntactic priors). Even when language models succeed in learning from small datasets, it is purely because they have \"non-trivial structural priors facilitating language acquisition and processing\" (Baroni 2022: 7). For example, some researchers have developed transformer models implementing recursive syntactic composition of phrase representations through attention (Sartran et al. 2022), showing improved performance when internalizing structural properties of language. \n\nMoving forward, we recommend that a closer examination be given to how LLM outputs align with subtle syntactic, acceptability judgments in humans. This is because many models may succeed in putative syntactic tasks but for reasons to do with the lexico-semantic-related statistics of the data, given how closely these often align (Matchin 2023). Indeed, mixed results have already been reported with respect to correlating model performance with human acceptability judgments (Lau et al. 2017); but for a more optimistic assessment, see Pavlick (2023) and Lake & Baroni (2023) (although even Lake & Baroni 2023 stress that systematicity still remains a serious challenge for models, including GPT-4).",
            "score": 0.42741350452900706,
            "section_title": "Discussion",
            "char_start_offset": 34671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 1436,
                    "end": 1457,
                    "matchedPaperCorpusId": "247187809"
                },
                {
                    "start": 1874,
                    "end": 1888,
                    "matchedPaperCorpusId": "260790761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5986328125
        },
        {
            "corpus_id": "264555617",
            "title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of Abstract Meaning Representation",
            "text": "A large body of work has examined various aspects of syntactic and semantic capabilities in language models (c.f. Mahowald et al., 2023), indicating that LLMs show strong knowledge of syntactic structure, while semantic capabilities are more mixed. Nonetheless, LLMs have also been used for fewshot semantic parsing with some success. In particular, Shin et al. (2021) and Shin and Van Durme (2022) find that few-shot learning in GPT-3 and Codex produces semantic parses that outperform baselines with comparable training sizes. These semantic parsing datasets, which focus on producing database queries in particular domains, are less complex and domain-general than AMR, but the results suggest that LLMs should contain aspects of the knowledge needed to analyze semantic structure. As for AMR, pre-trained transformer models have helped to advance the state of the art in AMR parsing, with recent AMR parsers building on the foundation of models like BART (Bevilacqua et al., 2021;Bai et al., 2022;Lee et al., 2022;Zhou et al., 2021). This indicates that pre-trained models may also pick up on representational capabilities relevant for supporting AMR. \n\nThough these prior works are suggestive that LLMs and pre-trained transformers capture certain aspects of linguistic structure, it is not clear from existing results how detailed or reliable LLMs' ability to analyze meaning structure may beformalisms used for prior few-shot semantic parsing are simpler and more domain-specific than AMR, and the supervised fine-tuning of BART for AMR parsing obscures the details of what original knowledge may have been contained in the pre-trained model. we directly examine pre-trained models' ability to produce AMR information, and we do so across a number of potentially productive zero-and fewshot settings for maximum insight about model capabilities. We also prioritize fine-grained, manual analysis of models' accuracies at multiple levels of AMR information, in order to provide more detailed insights into model capabilities.",
            "score": 0.42710818035688325,
            "section_title": "Related Work",
            "char_start_offset": 3296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "233297024"
                },
                {
                    "start": 959,
                    "end": 984,
                    "matchedPaperCorpusId": "235349016"
                },
                {
                    "start": 984,
                    "end": 1001,
                    "matchedPaperCorpusId": "247450724"
                },
                {
                    "start": 1001,
                    "end": 1018,
                    "matchedPaperCorpusId": "245144297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "273661991",
            "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
            "text": "Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP). However, there are still gaps in our understanding of these models, leading to challenges in controlling their behavior. As a pertinent example, the training of these models appears to demonstrate sudden improvements in metrics correlated with various capabilities [8], prompting questions about whether learning of a given capability can be predicted by tracking predefined progress measures and why such sudden changes occur. If undesirable capabilities can suddenly 'emerge' (despite any explicit supervision for them) [16], such sudden changes can be a challenge for AI regulation [21]. \n\nTo better understand such sudden changes during model training, this work investigates training BERT [12] on the classical mathematical task of low-rank matrix completion (LRMC) [6]. Making an analogy with masked language modeling (MLM), where sudden learning of syntactical structures was recently demonstrated [8], we argue matrix completion captures the core aspect of this learning problem (Fig. 1): given some relevant context (observed tokens), fill the missing elements (masked tokens). Specifically, we assume access to a matrix with some fraction of its entries missing, and would like to complete the missing entries of this matrix assuming the ground truth matrix is low-rank. We find that despite being a simplified abstraction of MLM, this setting already demonstrates a sharp decrease in loss as the model undergoes training (Fig. 1 (B)), preceded by a loss plateau for a significant number of training steps (akin to Chen et al. [8]). The simplicity of our setting further affords us interpretability, as we find that the point of sudden drop coincides with a precise change in how the model solves the task-we call this change an algorithmic transition. Specifically, we show that the pre-transition model simply copies the input (predicting 0 at masked positions), while the post-transition model accurately predicts missing values at masked positions. To perform the latter, distinctive changes occur in the model's attention heads during the period of sudden drop, wherein the model learns to identify relevant positional information to combine various elements in the input matrix and compute missing entries for matrix completion.",
            "score": 0.4268948297964224,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "8061516"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 1634,
                    "end": 1637,
                    "matchedPaperCorpusId": "261822542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "237491997",
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "text": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "276902722",
            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
            "text": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, from multilingual translation to advanced semantic understanding (Bubeck et al., 2023). As these models become increasingly complex and widespread, the need to understand their internal mechanisms has grown significantly. This has fueled a surge of research aimed at interpreting their mechanisms and decision-making processes, leading to intriguing insights into their behavior (Casper et al., 2023;Bereska and Gavves, 2024). \n\nHowever, fundamental questions regarding how LLMs acquire and develop these capabilities remain poorly understood. For instance, do LLMs learn language-specific concepts independently, or do they simultaneously acquire cross-lingual concepts that generalize across languages? Similarly, is there a prioritization in learning low-level, tokenspecific features versus high-level, abstract concepts? \n\nIn this work, we address this gap by analyzing how the information encoded in the internal representations of LLMs evolves over time. Specifically, we employ sparse autoencoders (SAEs) (Bricken et al., 2023;Huben et al., 2024) to analyze the hidden representations from multiple checkpoints of a large language model. By examining the distribution of SAE features at each checkpoint, we identify the types of information the model encodes at different training stages of its development (see Figure 1). \n\nOur experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities.",
            "score": 0.4267210424970997,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 545,
                    "matchedPaperCorpusId": "269293418"
                },
                {
                    "start": 1155,
                    "end": 1174,
                    "matchedPaperCorpusId": "261934663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "198192195",
            "title": "A Single Paradigm for Implicit and Statistical Learning",
            "text": "Experiment 1 demonstrated that classic implicit learning and statistical learning approaches can be combined in a single methodology. We showed that the effectiveness of cross-situational learning-a task that has been used to test statistical learning in acquisition of language-can be modulated by explicit knowledge of the language structure to be acquired, consistent with many other studies of the benefit of syntactic information for language learning (Goo et al., 2015;Spada & Tomita, 2010). This alignment of approaches enables links between theories of statistical learning with theories of effects of explicit structural knowledge. For instance, the advantage of explicit knowledge about syntactic structure appeared only to able to promote learning once a proportion of the words had been acquired in the language, in accord with Gleitman's (1990) view of syntactic knowledge applying to learning only once sufficient vocabulary had been acquired in the language. However, tracking the trajectory of learning through training of the participants in the incidental group that independently learned about the structure of the syntax at the end of training indicates a slightly different picture. As shown in Fig. 3, participants who later became aware of the syntactic structure were outperforming those that remained unaware in learning the cross-situational statistics early in training. Interestingly, emerging awareness did not distinguish performance at the end of the task (as indicated in the final block of training and the vocabulary testing), but it did show a distinction in performance during the acquisition of the cross-situational statistics task. Awareness of the acquired knowledge was only measured at the end of training (by means of retrospective verbal reports). Hence, we have no direct evidence of the point at which explicit knowledge of the language structure was first derived. The steep increase in vocabulary acquisition, as shown in Fig. 3 for the incidental group that gained explicit knowledge, may have precipitated understanding of the syntactic structure, or alternatively emerging explicit knowledge may have resulted in the increase. We do not yet know the causative direction of this effect. Experiment 2 addresses this issue.",
            "score": 0.42662412427006247,
            "section_title": "Discussion",
            "char_start_offset": 22825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 496,
                    "matchedPaperCorpusId": "143075308"
                },
                {
                    "start": 840,
                    "end": 857,
                    "matchedPaperCorpusId": "144713838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "267887766",
            "title": "A deep hierarchy of predictions enables assignment of semantic roles in online speech comprehension",
            "text": "The idea that our brains adaptively entertain internal models and that this facilitates language 380 comprehension underlies much current research in speech (language) perception. Nevertheless, 381 how internal knowledge is deployed in time, in relation to the timing of continuous speech 382 unfolding, is an open question, and may be key to achieve the form-meaning distinction in neural-383 network language models (23, 24). Here, we attempt to establish a foundational framework that 384 dynamically exploits general knowledge in speech comprehension to bridge this gap. We implement 385 Language comprehension as semantic role assignment 399 Although we emphasize that speech (language) comprehension is driven by high-level behavioral 400 goals, to achieve comprehension the appropriate assignment of semantic roles is crucial for 401 (re)constructing the message conveyed in the utterance, e.g. the \"mental image\" in Fig 1A. Semantic 402 roles can be viewed as an interface between linguistic and nonlinguistic representations, the latter 403 being a fundamental, domain-general format of our internal abstraction of the world (24, 25) that is 404 shown to both behaviorally and neurophysiologically influence language comprehension (47, 48). 405\n\nThe process of semantic role assignment is central in psycholinguistic process theories (46,(49)(50)(51), 406 yet seldom reflected explicitly in existing computational models of language. A major challenge for 407 modeling semantic role assignment during language processing is in combining meaning extraction 408 with compositionality: words that carry semantic contents are presented in an order dictated by 409 compositional rules, thus the extraction of persisting meanings must take place dynamically 410 alongside the decomposition. These two aspects have only been addressed separately in some 411 existing models, e.g. topic models (9, 52) fulfill (lexical) semantic processing but ignore the word 412 order. On the other hand, the Discovery of Relation by Analogy model (11, 53)  interactions within and between meaningful computational hierarchies, and second, they can 438 account for dynamics of neuronal activities such as local field potentials (39, 55). We therefore 439 believe our model is better suited to our goal of explaining language processing within a potentially 440 unifying account of neuronal message passing, rather than in terms of neural-like network 441 activations",
            "score": 0.4263314148947924,
            "section_title": "379",
            "char_start_offset": 28819,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "102350997",
            "title": "Unsupervised Recurrent Neural Network Grammars",
            "text": "It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provide useful inductive biases and improves performance. \n\nFinally, in modeling child language acquisition, the complex interaction of the parser and the grammatical knowledge being acquired is the object of much investigation (Trueswell and Gleitman, 2007); our work shows that apparently grammatical constraints can emerge from the interaction of a constrained parser and a more general grammar learner, which is an intriguing but underexplored hypothesis for explaining human linguistic biases.",
            "score": 0.4259931435731236,
            "section_title": "Conclusion",
            "char_start_offset": 30227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 968
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 122,
                    "matchedPaperCorpusId": "5068376"
                },
                {
                    "start": 235,
                    "end": 252,
                    "matchedPaperCorpusId": "52127792"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "271088482",
            "title": "The Computational Learning of Construction Grammars: State of the Art and Prospective Roadmap",
            "text": "The ultimate model captures all linguistic knowledge that a language user needs to comprehend and formulate utterances in the form of acquired form-meaning mappings (constructions).This means that no grammar rules, system of categories, or other linguistic structures are predefined.The model can only rely on general strategies to construct form-meaning mappings, combine them, and generalise over them.The grammar should be able to capture form-meaning mappings of varying degrees of abstraction, so that meaning representations that do not mirror the compositionality of the morpho-syntactic structures that express them can also be modelled.The processing engine that performs construction-based language comprehension and production should be able to combine the information captured in large numbers of constructions, so that morpho-syntactically and/or semantically complex utterances can be handled.Indeed, such utterances typically instantiate a wide variety of constructions.Finally, constructions should be able to incorporate sequential word order patterns, agreement patterns or a combination of both.It is therefore important to use a framework that strongly adheres to these basic principles of construction grammar, such as Fluid Construction Grammar (Steels, 2004;van Trijp et al., 2022;Beuls andVan Eecke, 2023, 2025).\n\nLearning constructions If the input to the language learning process consists of situationally grounded, unsegmented, multi-modal observations of utterances, the first constructions that are learnt can only be holistic pairings between observed utterances and hypotheses about their intended meaning (Tomasello, 2003).More general constructions can later be distilled as generalisations over both the form and meaning sides of previously acquired constructions with respect to novel observations.This generalisation process requires access to general syntactico-semantic generalisation algorithms.Initial prototypes of such algorithms have been presented by Van Eecke (2018), Nevens et al. (2022) and Doumen et al. (2023).Apart from improving these algorithms to learn more modular grammars that can, for example, elegantly handle recursive patterns, a crucial target in this direction concerns the design of algorithms that can learn agreement relations on an abstract level, i.e. by expressing congruence without referring to specific categories.",
            "score": 0.42597708281712526,
            "section_title": "Representing constructions",
            "char_start_offset": 43838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1336
                },
                {
                    "start": 1338,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1935
                },
                {
                    "start": 1935,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 1267,
                    "end": 1281,
                    "matchedPaperCorpusId": "612963"
                },
                {
                    "start": 1281,
                    "end": 1304,
                    "matchedPaperCorpusId": "249521235"
                },
                {
                    "start": 2014,
                    "end": 2034,
                    "matchedPaperCorpusId": "252819054"
                },
                {
                    "start": 2039,
                    "end": 2059,
                    "matchedPaperCorpusId": "258378235"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "215790482",
            "title": "A Computational Theory for the Emergence of Grammatical Categories in Cortical Dynamics",
            "text": "Given the complexity of human language, it is difficult to understand how children can exploit its internal structure in order to convey meaningful communicative behavior. Nevertheless, most of them achieve such behavior successfully within the first few years of life (Saffran et al., 2001). Some lines of research highlight the importance of the statistical structure underlying language in general (Romberg and Saffran, 2010;Lopopolo et al., 2017), while others show that 11-20 month-olds are able to acquire different aspects of abstract grammatical rules (Cyr and Shi, 2013;van Heugten and Christophe, 2015). \n\nMany psycho-linguistic models propose that in on-line sentence processing, different types of constraints are integrated very quickly in a coherent manner determining how words are systemically combined in grammatical sentences (Gibson and Pearlmutter, 1998). It is proposed that qualitatively distinct constraints such as semantic/conceptual, phonological, and syntactic structures operate alongside on a referential binding into a discourse model (Rego and Bryant, 1993;Lopopolo et al., 2017). In some models, unification operations during sentence comprehension take place in a parallel fashion at the semantic, syntactic, and phonological levels of processing (Hagoort, 2005). During on-line comprehension, lexical items are processed sequentially as the time course of the input elapses. The structural frames associated with each word are combined by means of an incremental unification mechanism, in the order that the input imposes. \n\nIn the present work, we introduce a bio-inspired neurocomputational model in which each word from the mental lexicon is associated with a structural frame. Each structural frame consists of the combination of Distributional Semantic (DS) (Harris, 1954) and coarse-grained syntactical word category information-specifically function word category, content word category, and from the last one we segregate verb word category. The coarse-grained word category information used in this work has been shown to emerge from phonological constraints in early language acquisition (Shi et al., 2006;Lohmann, 2017). A structural frame used in this approach constitutes the environment for a particular lexical item.",
            "score": 0.425671169148454,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 291,
                    "matchedPaperCorpusId": "41867354"
                },
                {
                    "start": 401,
                    "end": 428,
                    "matchedPaperCorpusId": "11807455"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "11664902"
                },
                {
                    "start": 560,
                    "end": 579,
                    "matchedPaperCorpusId": "12722145"
                },
                {
                    "start": 579,
                    "end": 612,
                    "matchedPaperCorpusId": "14483980"
                },
                {
                    "start": 844,
                    "end": 874,
                    "matchedPaperCorpusId": "7896553"
                },
                {
                    "start": 1065,
                    "end": 1088,
                    "matchedPaperCorpusId": "144259211"
                },
                {
                    "start": 1088,
                    "end": 1110,
                    "matchedPaperCorpusId": "11664902"
                },
                {
                    "start": 1280,
                    "end": 1295,
                    "matchedPaperCorpusId": "2826729"
                },
                {
                    "start": 2132,
                    "end": 2150,
                    "matchedPaperCorpusId": "43237897"
                },
                {
                    "start": 2150,
                    "end": 2164,
                    "matchedPaperCorpusId": "126371154"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219970703125
        },
        {
            "corpus_id": "274965758",
            "title": "Learning from Impairment: Leveraging Insights from Clinical Linguistics in Language Modelling Research",
            "text": "Curriculum Learning with Human-Inspired Complexity: Ranking training data based on complexity hierarchies derived from neurolinguistics and clinical treatment could be a viable strategy to optimize the curriculum for LMs, mimicking how humans learn progressively from simpler to more complex structures. Such hierarchies can also serve as benchmarks for evaluating the effectiveness of other training data optimization methods, including those inspired by language acquisition scenarios. While studies so far have reported mixed or negative findings on developmental-inspired learning strategies for LM training, the prevailing assumption is that child-directed speech provides an optimal foundation to foster linguistic generalization capabilities in smaller-scale training regimes (Huebner et al., 2021;Martinez et al., 2023;Eldan and Li, 2023). However, a SLP-informed curriculum presents an alternative, data-efficient pretraining technique by systematically organizing training data around explicit syntactic complexity metrics. Unlike the variability inherent in childdirected speech, this approach creates a structured pathway through progressively complex syntactic forms, which could accelerate generalization and adaptability to novel linguistic constructions. Linguistic Learning Objectives: Aphasia treatment protocols can inform the creation of new pre-training objectives for LMs. These objectives, recast based on linguistic tasks proposed by clinicians, can supplement traditional language modelling approaches by injecting a structured linguistic bias at a foundational level. Notably, while linguistically-enhanced pre-training tasks like semantic role labeling have shown promise (Cui et al., 2022;Zhou et al., 2020), their effectiveness also combined with training on progressively more complex sentences in terms of diverse argument structures, as in the MT protocol, remains unexplored. Targeted Prompts for Improved Generalization: \n\nInsights from aphasia research can guide the creation of targeted prompts for use in knowledge distillation scenarios. These prompts, designed to address specific linguistic constructs, improve the model's capability to generalize and master complex structures. Similar to aphasia therapy, where patients progress through increasingly complex linguistic tasks to (re-)build language abilities, this approach can be applied in LLMs. Structuring prompts to begin with simpler forms and gradually introduce more complex sentence constructions may enable the student model to effectively learn a wide range of linguistic phenomena.",
            "score": 0.4256453637651302,
            "section_title": "Learning Strategies",
            "char_start_offset": 15334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2388
                },
                {
                    "start": 2389,
                    "end": 2584
                }
            ],
            "ref_mentions": [
                {
                    "start": 783,
                    "end": 805,
                    "matchedPaperCorpusId": "241583340"
                },
                {
                    "start": 805,
                    "end": 827,
                    "matchedPaperCorpusId": "265213396"
                },
                {
                    "start": 1717,
                    "end": 1735,
                    "matchedPaperCorpusId": "265038860"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "235658129",
            "title": "A Theory of Language Learning",
            "text": "A theory of language learning is described, which uses Bayesian induction of feature structures (scripts) and script functions. Each word sense in a language is mentally represented by an m-script, a script function which embodies all the syntax and semantics of the word. M-scripts form a fully-lexicalised unification grammar, which can support adult language. Each word m-script can be learnt robustly from about six learning examples. The theory has been implemented as a computer model, which can bootstrap-learn a language from zero vocabulary. The Bayesian learning mechanism is (1) Capable: to learn arbitrarily complex meanings and syntactic structures; (2) Fast: learning these structures from a few examples each; (3) Robust: learning in the presence of much irrelevant noise, and (4) Self-repairing: able to acquire implicit negative evidence, using it to learn exceptions. Children learning language are clearly all of (1) - (4), whereas connectionist theories fail on (1) and (2), and symbolic theories fail on (3) and (4). The theory is in good agreement with many key facts of language acquisition, including facts which are problematic for other theories. It is compared with over 100 key cross-linguistic findings about acquisition of the lexicon, phrase structure, morphology, complementation and control, auxiliaries, verb argument structures, gaps and movement - in nearly all cases giving unforced agreement without extra assumptions.",
            "score": 0.4253835704584024,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "267999756",
            "title": "How semantics works in Chinese relative clause processing: insights from eye tracking",
            "text": "Duncan et al. (1997) detected a strong attention-withdrawing effect in their experiment and concluded that syntax processing is autonomous from systems that use the same stimulus modality. In Hahne and Friederici's (2002) research on German sentence comprehension, syntactic processing normally influenced semantic processes, but initial syntactic structure building occurred independent of semantic information. Pulverm\u00fcller et al. (2007) found that the syntactic process appeared in the early stage of sentence processing, was highly independent, and was not interfered with by other information. Bemis and Pylkk\u00e4nen (2011) investigated the neural circuits underlying simple linguistic compositions and found the temporal ordering of the effects is consistent with many processing models that posit syntactic composition before semantic composition during the construction of linguistic, representations. However, the sequential syntax-first model has recently been challenged by plenty of evidence from eye-tracking and ERP studies. \n\nAs an alternative view, the interactive model, together with the constraint-based lexicalist model (MacDonald et al., 1994), the concurrent model (Boland, 1997), the non-syntactocentric dynamic model (Kuperberg, 2007), rejects the assumption of syntactic primacy. It divides language comprehension into three phases. During the first phase, the initial syntactic structure is constructed based on word category. During the second phase, lexical-semantic and morphosyntactic processing takes place together for the goal of thematic role assignment. During the third phase, varieties of information integrate with each other. Overall, semantic information can guide and contribute to the syntactic processing in language comprehension, which is manifested by such questions as how much information of word orders can be derived from semantics. \n\nThe interactive model is supported by a variety of evidence. Green and Mitchell (2006) found the reading of syntactically ambiguous sentences is affected by lexical semantics as well as global semantic information. Event-related potential studies have also suggested semantic processing can operate independently of morphosyntactic control (Kuperberg, 2007). Ye et al. (2006) concluded semantic analysis depended largely on local syntactic building in German, while syntactic and semantic processing proceeded independently and parallelly in the early stage of Chinese analysis.",
            "score": 0.42509699288669034,
            "section_title": "Interactions of syntax and semantics",
            "char_start_offset": 2839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2460
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 221,
                    "matchedPaperCorpusId": "14649252"
                },
                {
                    "start": 413,
                    "end": 439,
                    "matchedPaperCorpusId": "13870754"
                },
                {
                    "start": 599,
                    "end": 625,
                    "matchedPaperCorpusId": "12479545"
                },
                {
                    "start": 1137,
                    "end": 1161,
                    "matchedPaperCorpusId": "15560738"
                },
                {
                    "start": 1184,
                    "end": 1198,
                    "matchedPaperCorpusId": "62150473"
                },
                {
                    "start": 1238,
                    "end": 1255,
                    "matchedPaperCorpusId": "17364244"
                },
                {
                    "start": 1943,
                    "end": 1968,
                    "matchedPaperCorpusId": "144375369"
                },
                {
                    "start": 2222,
                    "end": 2239,
                    "matchedPaperCorpusId": "17364244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "258155200",
            "title": "Infant Communication: A New Understanding",
            "text": "The speech stages are usually divided into vocabulary, sentences, and grammar. As for vocabulary, the most important thing is acquisition and use, the progress in these two aspects is generally reflected in the continuous acquisition of vocabulary, the acquisition of the meaning of some words in different situations, and the acquisition of some words with general and referential functions, etc. [10]. \n\nThe second is the occurrence and development of sentences, which can be divided into two stages: the word-sentences stage from one to one and a half years of age and the multi-word sentences stage after one and a half years of age [6,8]. This includes the transition between words and sentences, and the phenomenon of substituting words for sentences appears, and the infant thus begins the wordsentence stage, which is called the negative vocabulary stage by some scholars. 10-19 months is the stage when the infant's speed of mastering new words is significantly improved. At the same time, the infant's word sentences are also constantly developing, from strong situational to the use of double word sentences to communicate later, but there are still obvious defects, such as the use of sounds instead of words, multiple meanings in one word, narrowing of the lexical meaning and specialization of word meaning, and the use of overlapping sounds are common. After the transition from 20-24 months, the infant speech stage enters the multi-word stage, also known as the active speech stage. Infants begin to speak simple sentences of 3-5 words, and sentences of no more than 5-6 words at most, and their vocabulary increased greatly, and began to ask questions [8,10]. \n\nThe critical period for infants to master grammar mainly focuses on 20-30 months, and the grammar rule system is mastered at 36 months [10]. Scholars have debated how infants master a system of grammatical rules during these ten months. The more well-known ones are the semantic guidance advocated by Bowerman and Pinker, which shows that infants use knowledge of semantic relations to learn syntactic relations; Other common ones are \"general grammar, UG\", which Chomsky extended from the hypothesis of the language acquisition device-LAD [5], and some scholars believe that grammar formation is promoted by the continuous improvement of infants' cognition.",
            "score": 0.4249901887365334,
            "section_title": "Speech Stage",
            "char_start_offset": 10352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160400390625
        },
        {
            "corpus_id": "274965758",
            "title": "Learning from Impairment: Leveraging Insights from Clinical Linguistics in Language Modelling Research",
            "text": "The way human language develops and breaks down offers a privileged lens to investigate central aspects of human language competence, including the nuanced sensitivity to sentence acceptability and complexity (Caramazza and Zurif, 1978;Guijarro-Fuentes, 2020;Crain, 1991;Goodman, 1997). This exploration is not only crucial to test linguistic theories on empirical evidence but also has significant implications for crafting language resources informed by cognitive principles. In the current NLP era dominated by Large Language Models (LLMs), such resources are gaining increasing significance (Opitz et al., 2024), serving as essential benchmarks to unravel the linguistic competence implicitly encoded in neural network representations and possibly shedding light on similarities and differences between how humans and LLMs acquire, represent, and process this knowledge (Warstadt and Bowman, 2022;Belinkov and Glass, 2019;Baroni, 2022). To date, much research in this direction has been focused on creating targeted diagnostics to assess LLMs' syntactic and semantic abilities inspired by formal linguistics and psycholinguistics (Warstadt et al., 2020(Warstadt et al., , 2019;;Ettinger, 2020;Li et al., 2021;Someya et al., 2024). Alternatively, researchers have drawn inspiration from language acquisition research to explore the use of developmental complexity principles in building more efficient language models (Warstadt et al., 2023), or to compare how humans and algorithms learn language, aiming to identify shared patterns (Evanson et al., 2023;Yedetore et al., 2023;Yedetore and Kim, 2024). \n\nBuilding upon this research paradigm that seeks to investigate how human-like language abilities naturally emerge in neural language modelsstarting with the creation of well-motivated and rigorous test sets and training methods-this position paper proposes a novel complementary approach. Specifically, we argue for leveraging the realm of speech and language pathology (SLP) and rehabilitation research as rich data sources to gain insights not only for isolating a set of complexity phenomena but also for devising metrics that facilitate the identification of hierarchies of complexity within the same phenomenon, with a particular emphasis on syntactic ones. These nuanced complexity metrics may serve dual purposes.",
            "score": 0.42472269319441136,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2328
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 259,
                    "matchedPaperCorpusId": "149297185"
                },
                {
                    "start": 259,
                    "end": 271,
                    "matchedPaperCorpusId": "146789070"
                },
                {
                    "start": 271,
                    "end": 285,
                    "matchedPaperCorpusId": "67201080"
                },
                {
                    "start": 926,
                    "end": 939,
                    "matchedPaperCorpusId": "235446467"
                },
                {
                    "start": 1156,
                    "end": 1182,
                    "matchedPaperCorpusId": "44072099"
                },
                {
                    "start": 1182,
                    "end": 1197,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1197,
                    "end": 1213,
                    "matchedPaperCorpusId": "234742522"
                },
                {
                    "start": 1213,
                    "end": 1233,
                    "matchedPaperCorpusId": "25174497"
                },
                {
                    "start": 1421,
                    "end": 1444,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 1537,
                    "end": 1559,
                    "matchedPaperCorpusId": "259089351"
                },
                {
                    "start": 1559,
                    "end": 1581,
                    "matchedPaperCorpusId": "250563921"
                },
                {
                    "start": 1581,
                    "end": 1604,
                    "matchedPaperCorpusId": "273901649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48974609375
        },
        {
            "corpus_id": "218470495",
            "title": "Reinforcement learning of minimalist grammars",
            "text": "Sofar we discussed how a cognitive agent, being either human or an intelligent machine, could produce and understand utterances that are described in terms of minimalist grammar. An MG is given by a mental lexicon as in example Tab. 1, encoding a large amount of linguistic expert knowledge. Therefore, it seems unlikely that speech-controlled user interfaces could be build and sold by engineering companies for little expenses. \n\nYet, it has been shown that MG are effectively learnable in the sense of Gold's formal learning theory [19]. The studies [7,31,51] demonstrated how MG can be acquired by positive examples from linguistic dependence graphs [8,41]. The required dependency structures can be extracted from linguistic corpora by means of big data machine learning techniques, such as the expectation maximization (EM) algorithm [28]. \n\nIn our terminology, such statistical learning methods only consider correlations at the exponent level of linguistic signs. By contrast, in the present study we propose an alternative training algorithm that simultaneously analyzes similarities between exponents and semantic terms. Moreover, we exploit both positive and negative examples to obtain a better performance through reinforcement learning [47,56]. \n\nThe language learner is a cognitive agent L in a state X t , to be identified with L's mental lexicon at training time t. At time t = 0, L is initialized as a tabula rasa with empty lexicon \n\nand exposed to UMPs produced by a teacher T . Note that we assume T presenting already complete UMPs and not singular utterances to L. Thus we circumvent the symbol grounding problem of firstly assigning meanings \u03c3 to uttered exponents e [25], which will be addressed in future research. Moreover, we assume that L is instructed to reproduce T 's utterances based on its own semantic understanding. This provides a feedback loop and therefore applicability of reinforcement learning [47,56]. For our introductory example, we adopt the simple semantic model from Sect. 2. In each iteration, the teacher utters an UMP that should be learned by the learner.",
            "score": 0.4245726344235457,
            "section_title": "Reinforcement Learning",
            "char_start_offset": 25274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "12438987"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "16734764"
                },
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "2998847"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "404374"
                },
                {
                    "start": 654,
                    "end": 657,
                    "matchedPaperCorpusId": "17754102"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "59829005"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "1364249"
                },
                {
                    "start": 1691,
                    "end": 1695,
                    "matchedPaperCorpusId": "3204300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "265230959",
            "title": "How Is a \u201cKitchen Chair\u201d like a \u201cFarm Horse\u201d? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models",
            "text": "One rapidly growing strand of Natural Language Processing (NLP) research is that of determining whether neural language models encode certain linguistic properties, and, if so, understanding how these properties are represented.This goal has brought together a variety of researchers and analysis techniques from fields such as machine learning, linguistics, psychology, and neuroscience (Linzen 2019;Abnar et al. 2019;Gauthier and Levy 2019;Anderson et al. 2021).With the advent of Transformer-based language models (Vaswani et al. 2017) such as BERT (Devlin et al. 2018), there has been a surge of interpretability research on this type of architecture (in particular, the study of BERT and related models has gained much popularity in a wave of research sometimes referred to as \"BERTology\" [Rogers, Kovaleva, and Rumshisky 2021]).To date, however, most of the research into the interpretation of Transformer-based language models has focused on their syntactic knowledge, and while there have been investigations into their semantic capabilities (e.g., Ettinger 2020; Tenney, Das, and Pavlick 2019), our understanding of how Transformers process semantic information remains largely incomplete.In contrast to syntax, where explicit representations of the grammatical structures of interest are available, a challenge faced in probing Transformer-based language models for semantics is finding suitable experimental frameworks for investigating processes relating to semantic representation and semantic composition.\n\nIn this work, we examine the extent to which Transformer-based language models have implicit knowledge of the thematic relations used in noun-noun compounds and explore how this information is encoded in the intermediary vector representations of these models.To this end, we perform layer-wise representational analysis on six different types of Transformer-based language models, covering a range of training objectives, training data, and total number of parameters.",
            "score": 0.4236477382964561,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1519
                },
                {
                    "start": 1521,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 401,
                    "matchedPaperCorpusId": "52196319"
                },
                {
                    "start": 401,
                    "end": 419,
                    "matchedPaperCorpusId": "174797900"
                },
                {
                    "start": 419,
                    "end": 442,
                    "matchedPaperCorpusId": "202769809"
                },
                {
                    "start": 442,
                    "end": 463,
                    "matchedPaperCorpusId": "232326101"
                },
                {
                    "start": 517,
                    "end": 537,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 794,
                    "end": 832,
                    "matchedPaperCorpusId": "211532403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "237940293",
            "title": "Sorting through the noise: Testing robustness of information processing in pre-trained language models",
            "text": "Prior work has tested LMs as knowledge bases using cloze-style probes (Petroni et al., 2019;Jiang et al., 2020). As a starting point we rely on models' ability to display this type of knowledge, but our question differs importantly from that work: we are not asking whether models can recall facts about the real world from training-rather, we are trying to gauge the extent to which models form robust representations of new information presented in input after training. Somewhat more similar to ours is work like Elazar et al. (2021), which explores the consistency of models' generation of facts in the face of rephrasing of prompts. The basic intuition behind this work-that LMs' ability to make intelligent-looking predictions can be sensitive to the particulars of the context-is one that we also use as we ask more specific questions about models' processing of information in their input.\n\nA good deal of prior work has focused on testing for linguistic knowledge in language models (Rogers et al., 2020). Much of this work has prioritized testing syntax in pre-trained LMs via agreement tests (Linzen et al., 2016;Gulordava et al., 2018). Others expand to broader sets of syntactic phenomena (Wilcox et al., 2018;Futrell et al., 2019;Warstadt et al., 2020) and semantic/pragmatic phenomena (Ettinger, 2020). Other work has studied syntactic and semantic information in contextualized embeddings from these models (Hewitt and Manning, 2019;Tenney et al., 2018;Klafka and Ettinger, 2020). We take one step up from examination of these abstract linguistic capacities, with a focused examination of models' ability to use such linguistic scaffolding to process and retain new information described in text.\n\nOur use of attractors to test model robustness takes inspiration from use of attractors within syntactic testing contexts (Linzen et al., 2016;Gulordava et al., 2018), but we focus on semantic relationships in defining attractors, and use the attractors to investigate different aspects of models' processing. Some scattered work has explored more semantic types of attractors for testing LMs-in particular, there is",
            "score": 0.42343860277896994,
            "section_title": "Related Work",
            "char_start_offset": 2862,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 92,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "208513249"
                },
                {
                    "start": 992,
                    "end": 1013,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 1103,
                    "end": 1124,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 1124,
                    "end": 1147,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1202,
                    "end": 1223,
                    "matchedPaperCorpusId": "52156878"
                },
                {
                    "start": 1223,
                    "end": 1244,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 1244,
                    "end": 1266,
                    "matchedPaperCorpusId": "208527435"
                },
                {
                    "start": 1300,
                    "end": 1316,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1423,
                    "end": 1449,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1449,
                    "end": 1469,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1469,
                    "end": 1495,
                    "matchedPaperCorpusId": "218502143"
                },
                {
                    "start": 1836,
                    "end": 1857,
                    "matchedPaperCorpusId": "14091946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "53792119",
            "title": "Verb Argument Structure Alternations in Word and Sentence Embeddings",
            "text": "Artificial neural networks (ANNs) are powerful computational models that are able to implicitly learn syntactic and semantic features necessary for a variety of natural language tasks. These empirical results raise a deeper scientific question: to what extent do the features learned by ANNs resemble the linguistic competence of humans? \n\nStudying the linguistic competence of ANNs, in addition to its intrinsic value for model evaluation, can help resolve outstanding scientific questions in linguistics about the role of prior grammatical bias in human language acquisition. Chomsky (1965) suggests that the acquisition of rich grammatical distinctions is facilitated by an innate universal grammar (UG), which imparts specific grammatical knowledge to the learner. This proposal crucially depends on the poverty of the stimulus argument, which holds that the acquisition of certain linguistic features by purely domain-general datadriven learning should not be possible (Clark and Lappin, 2011). Studying the ability of low-bias learners like ANNs to acquire specific grammatical knowledge can provide evidence relevant to this argument. \n\nIn this work, we evaluate ANNs' treatment of verbs; verbs contribute to the overall meaning of sentences by encoding information about how entities are related to, and participate in, events. Concretely, we investigate if ANNs acquire the specific grammatical distinctions necessary for inferring the frame-selectional properties of verbs. Cross-linguistically, the lexical entry of a verb is associated with a set of syntactic contexts or syntactic frames in which it can appear. This information is lexically idiosyncratic, i.e., even verbs that are intuitively very similar in meaning may vary as to which syntactic frames they can appear in: \n\n(1) a. Sharon sprayed water on the plants. \n\nb. Sharon sprayed the plants with water. c. Carla poured lemonade into the pitcher. d. *Carla poured the pitcher with lemonade. 1 ertain verbs, e.g., spray, select multiple related frames and are therefore known as alternating verbs. In contrast, other semantically similar verbs, e.g., pour, select only a single frame and are thus not alternating.",
            "score": 0.4233787773218614,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "244129987",
            "title": "Interpreting Language Models Through Knowledge Graph Extraction",
            "text": "Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze\"fill-in-the-blank\"statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.",
            "score": 0.4229616022693595,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "273403825",
            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
            "text": "As one of the unique abilities of humans, the acquisition of language has long been a key area of research in psycholinguistics. During the process of language acquisition, humans go through multiple stages, from imitation and rule learning to complex contextual understanding (Goldberg, 2005). These stages bear some resemblance to the way current LMs are trained. For instance, LMs learn the statistical patterns and grammatical rules of language through training on large-scale data, similar to how infants develop language abilities by receiving a vast amount of input through listening and speaking. If we design theory-driven tests based on the human language acquisition process to evaluate the capabilities of LMs, it could help us better understand the nature, potential, and limitations of LMs in their development. \n\nOur work draws on classical theories of human language development to assess LMs in terms of a three-stage human language development framework (Chomsky, 2014;Loban, 1976;Pinker, 2003), as shown in Figure 1. The first stage involves developing basic language understanding, similar to early language acquisition in infants. At this stage, we evaluate the model's ability to recognize vocabulary, grasp syntax, and perform simple reasoning. In the second stage, the focus shifts to mastering complex grammar and semantics, where the model demonstrates a deeper understanding of language rules and logical relationships between sentences. The third stage assesses advanced language abilities, evaluating the model's capacity for complex reasoning and logical analysis. \n\nWe further investigate another theory: register theory in linguistics, which posits that different language use scenarios influence the form and structure of language (Halliday, 1977;Matthiessen, 1993). This theory offers insights into the extent to which models' abilities depend on the linguistic features encountered in specific situations, referred to as registers. In LMs, the training corpus will reflect some registers but not others, which can raise general questions or concerns about the generaliz-ability and biases contained in any given corpus. \n\nWe evaluated 15 LMs from 2019 to 2024, excluding instruction fine-tuned or chat versions, with varying parameter sizes (see \u00a74.1).",
            "score": 0.4227106823263234,
            "section_title": "Introduction",
            "char_start_offset": 2100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "3448451",
            "title": "Long-term associative learning predicts verbal short-term memory performance",
            "text": "We do this by computationally modeling the linguistic knowledge that a typical child can be expected to have gained. Since vSTM tasks are typically concerned with the maintenance and/or reproduction of sequences of verbal information, the key associative learning aspect of linguistic knowledge that should apply relates to just such sequential knowledge. Our model therefore focuses on associative learning operating on the sequential properties of the linguistic input. Specifically, we use a set of corpora of childdirected language as a proxy for the linguistic experience of 6year-old children and show how basic associative (sequential) learning operating on that experience predicts the pattern of performance on versions of digit span, nonword repetition, and sentence recall tasks. We begin by outlining a computational model of associative (sequential) learning as applied to the linguistic setting. Our broad theoretical orientation here is that performance in the vSTM setting is governed by domaingeneral associative (sequential) learning that enables the application of long-term linguistic knowledge and skill to the verbal material presented to the participant in that vSTM setting (D. M. Jones, Macken, & Nicholls, 2004;B. Macken, Taylor, & Jones, 2014;W. J. Macken & Jones, 2003). In broad terms, this means that the closer the vSTM material corresponds to the linguistic knowledge and experience of the rememberer, the better performance in the vSTM task will be. \n\nImportantly, although the sequences presented to participants in vSTM settings are by design novel (i.e., obviously familiar sequences such as runs or acronyms are excluded), this Bnovelty^is always a matter of degree: some novel sequences will more closely match the linguistic experience of the rememberer than others. We show that this grading of novelty is at play within our computational model of associative learning within those vSTM tasks typically used in the developmental setting. We then show in two behavioral experiments that the efficiency which the model processes the types of material presented to children in vSTM tasks predicts how children perform in those settings, and we go on to discuss theoretical and methodological implications of these findings as they relate to the investigation of short-term memory and to domain-general mechanisms on the part of the rememberer.",
            "score": 0.4226832105456889,
            "section_title": "body",
            "char_start_offset": 5302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 1205,
                    "end": 1237,
                    "matchedPaperCorpusId": "17454765"
                },
                {
                    "start": 1237,
                    "end": 1270,
                    "matchedPaperCorpusId": "5407788"
                },
                {
                    "start": 1270,
                    "end": 1297,
                    "matchedPaperCorpusId": "12742654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35595703125
        },
        {
            "corpus_id": "264406051",
            "title": "Bridging Information-Theoretic and Geometric Compression in Language Models",
            "text": "To speak a language is not to memorize all possible utterances, but to instead extract the finite ruleset and lexicon that generates them (Chomsky, 1986). That is, language, though nominally highdimensional, can be compressed to a comparatively small intrinsic dimension. \n\nThe recent success of (large) language models demonstrates that artificial neural networks, too, can acquire linguistic knowledge. Current language models (LMs) are Transformer-based architectures at-scale (Vaswani et al., 2017) that are trained on the conditional distribution of natural language (OpenAI, 2023;Zhang et al., 2022;Touvron et al., 2023;Chowdhery et al., 2022) and that are inching closer to human-like linguistic robustness (Brown et al., 2020;Liang et al., 2022;Wang et al., 2019b). For an LM to faithfully model language, it must encode linguistic training data into finitely many variables that allow generalization to infinitely many grammatical utterances. That is, it must perform a successful form of data compression. \n\nThus, following the line of research that aims to better understand LM behavior (Wei et al., 2022;Zhang et al., 2021;Rogers et al., 2020), in this work we provide initial insights on how they compress linguistic knowledge. We demonstrate an empirical link between two types of compression: geometric and information-theoretic. In particular, we ask: how, and by how much, do LMs compress linguistic data? Furthermore, what are linguistic correlates to compressibility? Is compression a good predictor of rapid adaptation? We show that (1) intrinsic dimension (ID) of linguistic data representations under an LM tracks information-theoretic coding length; (2) greater data compression predicts easeof-adaptation in causal language modeling tasks; \n\n(3) interpretable linguistic properties such as vocabulary size and syntactic structure modulate ID; and (4) different model sizes recover similar ranges of ID. Finally, as a practical contribution, (5) we explore different ways to estimate ID of linguistic data, and find only some to capture the relation between ID, coding length, and ease-of-adaptation.",
            "score": 0.42263410905570586,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 502,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 734,
                    "end": 753,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 753,
                    "end": 772,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "267627053",
            "title": "A systematic investigation of learnability from single child linguistic input",
            "text": "Our results suggest that even limited data can be indicative of differences between datasets and, potentially, that child-directed speech may better equip models with the necessary linguistic abilities for certain tests. \n\nThe second key contribution of our study is an in-depth examination of the robustness of the findings by Wang et al. (2023), which were originally based on one single-child dataset: SAYCam-S. We expanded this investigation to include 3 single-child datasets with 2 baselines and 6 model architectures, significantly broadening the scope. Additionally, we enhanced the methodology for linguistic evaluation using the Zorro test suite (Huebner et al., 2021). Wang et al. previously limited their analysis to sentence pairs from Zorro that matched SAYCam-S's vocabulary, which resulted in a reduced test scope covering only 15 out of 23 tests and fewer than 700 sentence pairs per test. This limited size potentially weakened the validity of their conclusions. In contrast, we regenerated the Zorro test suite to align with the intersected vocabulary. Our models were then tested on comprehensive new 23 tests encompassing all 13 linguistic phenomena, with 2,000 sentence pairs in each test. This approach has yielded more robust and reliable results. \n\nOur study demonstrates that models with different configurations can consistently learn to distinguish several syntactic and semantic categories and are sensitive to certain linguistic tests based solely on the linguistic input from a single child. However, we acknowledge several limitations. Firstly, while models demonstrate the ability to form syntactic and semantic clusters distinguishing lexical classes, it remains unclear how they acquire this representation and whether their understanding of these categories aligns with human cognition. Secondly, our evaluation methods, though insightful, are not exhaustive. The behavioral tests using Zorro are valuable for assessing responses to grammatical variations in sentences. However, it is important to note that Zorro has its limitations (V\u00e1zquez Mart\u00ednez, Lea Heuser, Yang, & Kodner, 2023), and we still lack more systematic semantic evaluations. Lastly, our models are exclusively trained on transcribed speech.",
            "score": 0.4223947235627788,
            "section_title": "General Discussion",
            "char_start_offset": 17810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 678,
                    "matchedPaperCorpusId": "241583340"
                },
                {
                    "start": 2070,
                    "end": 2122,
                    "matchedPaperCorpusId": "264805336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "53246965",
            "title": "Aligning Developmental and Processing Accounts of Implicit and Statistical Learning",
            "text": "suggest that priming serves as an important function in improving communication between interlocutors). Structural priming has also been influential in shaping our understanding of what children's early syntactic knowledge is like and, more recently, to study the transition between the two: how children's syntactic knowledge develops to become adult-like. To add to this, priming has also been used to explore the types of learning mechanisms that might be involved in this process (Peter, Chang, Pine, Blything, & Rowland, 2015). Thus, it is clear that not only does structural priming have much to contribute to both the adult and child language literature, but it too is important for the statistical learning and implicit learning community who share the goal of wanting to better understand how syntactic knowledge is built.\n\nRelatedly, there have been a number of attempts to model syntax acquisition. However, these models fail to include psychological and computational features that McCauley and Christiansen (2014) argue are important for a plausible model of this process. For instance, the authors propose that a model of syntax acquisition should (a) process input on-line in a word-by-word manner, as opposed to learning entire utterances; (b) learn by calculating statistics that are tied to backward transitional probabilities as opposed to using only simple distributional information; and (c) be trained using naturalistic linguistic input as opposed to input that is artificial or lacks the properties of real language. Thus, models that do not model development incrementally (e.g., Bannard, Lieven, & Tomasello, 2009), focus only on simple distributional information (e.g., Redington et al., 1998), or are not fed input with the structure of real natural languages (e.g., Howell & Becker, 2001, in which the model is trained on a 390-word language comprising twoand three-word sentences) are not fully able to capture the process by which children develop adult-like syntactic knowledge.\n\nPerhaps more important, none of them provides an explanation that scales up to adult language use, incorporating an implicit learning explanation of why we see structural priming effects in both children and adults. Conversely, a number of models have been developed to capture structural priming effects (e.g.",
            "score": 0.42200976834175463,
            "section_title": "Introduction",
            "char_start_offset": 13735,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 484,
                    "end": 531,
                    "matchedPaperCorpusId": "43394269"
                },
                {
                    "start": 994,
                    "end": 1026,
                    "matchedPaperCorpusId": "10606317"
                },
                {
                    "start": 1605,
                    "end": 1639,
                    "matchedPaperCorpusId": "6227768"
                },
                {
                    "start": 1697,
                    "end": 1720,
                    "matchedPaperCorpusId": "2596605"
                },
                {
                    "start": 1795,
                    "end": 1816,
                    "matchedPaperCorpusId": "189719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420166015625
        },
        {
            "corpus_id": "202539551",
            "title": "Language Models as Knowledge Bases?",
            "text": "Many studies have investigated pretrained word representations, sentence representations, and language models. Existing work focuses on understanding linguistic and semantic properties of word representations or how well pretrained sentence representations and language models transfer linguistic knowledge to downstream tasks. In contrast, our investigation seeks to answer to what extent pretrained language models store factual and commonsense knowledge by comparing them with symbolic knowledge bases populated by traditional relation extraction approaches. Baroni et al. (2014) present a systematic comparative analysis between neural word representation methods and more traditional count-based distributional semantic methods on lexical semantics tasks like semantic relatedness and concept categorization. They find that neural word representations outperform count-based distributional methods on the majority of the considered tasks. Hill et al. (2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs. Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset consists of sentence pairs with a grammatical and an ungrammatical sentence. While a good language model should assign higher probability to the grammatical sentence, they find that LSTMs do not learn syntax well. \n\nAnother line of work investigates the ability of pretrained sentence and language models to transfer knowledge to downstream natural language understanding tasks (Wang et al., 2018). While such an analysis sheds light on the transfer-learning abilities of pretrained models for understanding short pieces of text, it provides little insight into whether these models can compete with traditional approaches to representing knowledge like symbolic knowledge bases. \n\nMore recently, McCoy et al. (2019) found that for natural language inference, a model based on BERT learns to rely heavily on fallible syntactic heuristics instead of a deeper understanding of the natural language input. Peters et al. (2018b) found that lower layers in ELMo specialize on local syntactic relationships, while higher layers can learn to model long-range relationships. Similarly, Goldberg (2019) found that BERT captures English syntactic phenomena remarkably well. Tenney et al. (2019) investigate to what extent language models encode sentence structure for different syntactic and semantic phenomena and found that they excel for the former but only provide small improvements for tasks that fall into the latter category.",
            "score": 0.42191554289966704,
            "section_title": "Related Work",
            "char_start_offset": 6060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2601
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 582,
                    "matchedPaperCorpusId": "85205"
                },
                {
                    "start": 944,
                    "end": 962,
                    "matchedPaperCorpusId": "3226120"
                },
                {
                    "start": 1556,
                    "end": 1575,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 2081,
                    "end": 2102,
                    "matchedPaperCorpusId": "52098907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "246298665",
            "title": "Unifying Physical Interaction, Linguistic Communication, and Language Acquisition of Cognitive Agents by Minimalist Grammars",
            "text": "So far we discussed how a cognitive agent, being either human or an intelligent machine, could produce and understand utterances that are described in terms of minimalist grammar. An MG is given by a mental lexicon as in example (Table 2), encoding a large amount of linguistic expert knowledge. Therefore, it seems unlikely that speech-controlled user interfaces could be build and sold by engineering companies for little expenses. \n\nYet, it has been shown that MG are effectively learnable in the sense of Gold's formal learning theory (Gold, 1967). The studies of Bonato and Retor\u00e9 (2001), Kobele et al. (2002), andStabler et al. (2003) demonstrated how MG can be acquired by positive examples from linguistic dependence graphs (Nivre, 2003;Boston et al., 2010). The required dependency structures can be extracted from linguistic corpora by means of big data machine learning techniques, such as the expectation maximization (EM) algorithm (Klein and Manning, 2004). \n\nIn our terminology, such statistical learning methods only reveal correlations at the exponent level of linguistic signs. By contrast, in the present study we propose an alternative training algorithm that simultaneously analyzes similarities between exponents and semantic terms. Moreover, we exploit both positive and negative examples to obtain a better performance through reinforcement learning (Skinner, 2015;Sutton and Barto, 2018). \n\nThe language learner is a cognitive agent L in a state X t , to be identified with L's mental lexicon at training time t. At time t = 0, X 0 is initialized as a tabula rasa with the empty lexicon \n\nand exposed to UMPs produced by a teacher T. Note that we assume T presenting already complete UMPs and not singular utterances to L. Thus, we circumvent the symbol grounding problem of firstly assigning meanings \u03c3 to uttered exponents e (Harnad, 1990), which will be addressed in future research. Moreover, we assume that L is instructed to reproduce T's utterances based on its own semantic understanding. This provides a feedback loop and therefore applicability of reinforcement learning (Skinner, 2015;Sutton and Barto, 2018). \n\nFor our introductory example, we adopt the simple semantic model from Sect. 5.",
            "score": 0.4217870548409612,
            "section_title": "LANGUAGE ACQUISITION",
            "char_start_offset": 63442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2145
                },
                {
                    "start": 2148,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 551,
                    "matchedPaperCorpusId": "12438987"
                },
                {
                    "start": 568,
                    "end": 592,
                    "matchedPaperCorpusId": "16734764"
                },
                {
                    "start": 594,
                    "end": 619,
                    "matchedPaperCorpusId": "2998847"
                },
                {
                    "start": 732,
                    "end": 745,
                    "matchedPaperCorpusId": "59829005"
                },
                {
                    "start": 745,
                    "end": 765,
                    "matchedPaperCorpusId": "17754102"
                },
                {
                    "start": 945,
                    "end": 970,
                    "matchedPaperCorpusId": "1364249"
                },
                {
                    "start": 1852,
                    "end": 1865,
                    "matchedPaperCorpusId": "3204300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59765625
        },
        {
            "corpus_id": "2766311",
            "title": "Generating Natural Language Text in a Dialog System",
            "text": "This base includes knowledge about mor-phology= syntax and to a certain degree of semantics of the language. \n\nThe lexicon stores declarative knowledge of the language in the form of following entries: <primary form> <stem> ~type of stem> <semantic characteristics of word> Morphological rules should guarantee the morphological analysis end synthesis of the words used, i.e. a transition from the word form to its morphological representation (number I case, tense, person) in analysis and the reversed transition in generation. \n\nThe output of syntactic analysis (and input to syntactic generation) is a tree of dependencies. \n\nIn order to reduce the number of possible resulting dependency trees we may use instead of purely syntactic rules suntacticosemantic rules which combine syntactic and semantic features of a word: Linguistic knowledge base is used mainly by the Linguistic Processor. During parsing the input to the Linguistic Processor is the user's utterance in natural language, the output is the syntactic representation of the turn in the form of dependency trees. In surface generation the input to the Linguistic Processor is the dependency tree(s) and the output is an answer turn in natural language.",
            "score": 0.4216196509938252,
            "section_title": "Linguistic knowledge base",
            "char_start_offset": 11284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "237581628",
            "title": "Does Vision-and-Language Pretraining Improve Lexical Grounding?",
            "text": "Analyzing Pretrained LMs. There has been substantial prior work on analyzing pretrained LMs and the linguistic properties of their representations, looking, e.g., at syntactic parse structure (Hewitt and Manning, 2019;Linzen et al., 2016), semantic structure such as semantic roles and coreference (Tenney et al., 2019a), lexical semantics (Chronis and Erk, 2020;Vuli\u0107 et al., 2020), and lexical composition (Yu and Ettinger, 2020). Particularly relevant to our studies is prior work which has explored how well text-only models capture commonsense knowledge about the physical world via intrinsic (Ettinger, 2020;Forbes et al., 2019) and extrinsic (Zellers et al., 2018(Zellers et al., , 2019Bisk et al., 2020b) measures. Despite the interest in representations of the non-linguistic world, such analyses have not, to our knowledge, been run on multimodal LMs.\n\nVision-and-Language Pretraining. There is a long history of multimodal distributional semantics models (Howell et al., 2005;Lazaridou et al., 2015), to which pretrained transformer-based models are the latest addition (Sun et al., 2019;Li et al., 2020). Evaluations of these recent visionand-language (VL) models has tended to focus on inherently multimodal tasks , e.g., image and video captioning (Sun et al., 2019), visual question answering (Li et al., 2020), or instruction following in robotics (Majumdar et al., 2020). Cao et al. (2020) describes a series of \"probing\" analyses for multimodal language representations, but focuses on explicit grounding, e.g., to where do models attend in the image when processing \"dog\"? Little work has analyzed whether the presence of grounded training data impacts the linguistic representations in general. Work that does perform exploratory analyses of the multimodal conceptual representations (Tan and Bansal, 2020;Radford et al., 2021) does not include analysis of comparable text-only models, limiting the conclusions that",
            "score": 0.4215247794964652,
            "section_title": "Related Work",
            "char_start_offset": 2312,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "246871423",
            "title": "Models of Language and Multiword Expressions",
            "text": "Providing an account of what constructions at different levels of abstraction mean, and how that meaning can be acquired through linguistic experience, is a crucial step for making this program viable and coherent with the assumptions of usage-based approaches. \n\nFurthermore, as was mentioned above, the challenge of specifying the kind of cognitive architecture that implements these processes is still open. The limited but notorious success of eliminativist models (Baayen et al., 2013), in addition to the explanatory force of exemplars (Ambridge, 2020a), highlight the difficulty of establishing the limits of what is learned and how. Advancing on this issue will allow us to research other pressing issues on the implementation of this program, such as how much exposure is needed for an exemplar to be used in processing (e.g., Is a single exposure enough for matching?) or how abstract these representations can become (e.g., How many levels of abstraction are needed to account for language use? Is there a completely abstract sequence?). \n\nApart from the identification and matching of sequences, there are other proposals that characterize the computations behind language processing that are compatible with this proposal. One example is the recent work by Fedorenko et al. (2020) which suggests that the computations of the language network in the brain is guided by the semantic features of the words in a sequence rather than by their syntactic structure. In this proposal, \"semantic composition\" (Mollica et al., 2020), combining the meaning of the words in a sentence without strict syntactic parsing, is the core computation of the language network. \n\nAnother candidate is predictive sentence processing (Shain et al., 2020). But a problematic aspect of this proposals is that, traditionally, they posit massively parallel syntactic parsing (e.g., Van Schijndel et al., 2013) that violate the Now-or-Never bottleneck of language processing (Christiansen and Chater, 2016) and use a words-and-rules approach (Probabilistic Context-Free Grammars, Hale, 2001;Levy, 2008) to model predictions. However, other recent work suggests an alternative in item-and pattern-based prediction.",
            "score": 0.4214481078575936,
            "section_title": "CONCLUSION",
            "char_start_offset": 56960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 469,
                    "end": 490,
                    "matchedPaperCorpusId": "15697093"
                },
                {
                    "start": 542,
                    "end": 559,
                    "matchedPaperCorpusId": "203426826"
                },
                {
                    "start": 1270,
                    "end": 1293,
                    "matchedPaperCorpusId": "213787994"
                },
                {
                    "start": 1513,
                    "end": 1535,
                    "matchedPaperCorpusId": "210181450"
                },
                {
                    "start": 1723,
                    "end": 1743,
                    "matchedPaperCorpusId": "203900749"
                },
                {
                    "start": 1867,
                    "end": 1894,
                    "matchedPaperCorpusId": "16164968"
                },
                {
                    "start": 1959,
                    "end": 1990,
                    "matchedPaperCorpusId": "49732265"
                },
                {
                    "start": 2064,
                    "end": 2075,
                    "matchedPaperCorpusId": "5490143"
                },
                {
                    "start": 2075,
                    "end": 2086,
                    "matchedPaperCorpusId": "6211281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3974609375
        },
        {
            "corpus_id": "214714929",
            "title": "Relating Lexical and Syntactic Knowledge to Academic English Listening: The Importance of Construct Representation",
            "text": "To establish a uniform framework for comparing the construct representations of lexical and syntactic knowledge, a brief account of psycholinguistic theories of language comprehension is inevitable. Fortunately, descriptions of the key stages of comprehension are more or less the same across the rich variations of models, such that a \"basic\" model can be conceptualized, comprising word-, sentence-, and discourse-level processes (Fern\u00e1ndez and Cairns, 2018). A variation of this basic model often cited in applied linguistics literature is the three-stage cognitive model of Anderson (2015), consisting of perception, parsing, and utilization. The division into three stages is supported by neurological evidence, such that psychologists are able to identify the different combinations of brain regions involved in the three stages (Anderson, 2015). \n\nIn the L2 listening literature, the three stages are sometimes rephrased as decoding, parsing, and meaning construction (Field, 2011). In brief, the listener converts the acoustic-phonetic signal into words, relates the words syntactically for a combined meaning, and enriches the meaning by integrating it with meaning derived from earlier text, context, and background. While the three-stage model deals with the cognitive processes of listening comprehension, these processes depend upon a multitude of sources, linguistic, contextual, and schematic, among which linguistic sources can be further classified into phonetic, phonological, prosodic, lexical, syntactic, semantic, and pragmatic processes (Lynch, 2010). \n\nThe interplay between lexical and syntactic processes is an essential part of the cognitive processes in L2 listening. For one thing, word-level processes, such as the identification of a single word, depend on both lexical-semantic and syntactic cues in the context (Buck, 1991(Buck, , 1994;;Anderson, 2015). Neurologically, the speech signal of a word needs to be combined with information about its acoustic-phonological, syntactic, and conceptual semantic properties before it is recognized (Hagoort, 2013). Similarly, parsing also draws on both syntactic and lexical-semantic cues (Anderson, 2015).",
            "score": 0.421062116214432,
            "section_title": "Lexical and Syntactic Processes in Listening",
            "char_start_offset": 2241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 975,
                    "end": 988,
                    "matchedPaperCorpusId": "143729042"
                },
                {
                    "start": 1559,
                    "end": 1572,
                    "matchedPaperCorpusId": "149206454"
                },
                {
                    "start": 1843,
                    "end": 1854,
                    "matchedPaperCorpusId": "146340607"
                },
                {
                    "start": 1854,
                    "end": 1869,
                    "matchedPaperCorpusId": "144994213"
                },
                {
                    "start": 2071,
                    "end": 2086,
                    "matchedPaperCorpusId": "88609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3408203125
        },
        {
            "corpus_id": "271430405",
            "title": "Modelling constructivist language acquisition through syntactico-semantic pattern finding",
            "text": "In particular, we provide a fully operational model of how a communicatively adequate linguistic system can be captured in the form of a collection of learned form-meaning mappings.These mappings can cover syntactico-semantic patterns of variable extent and degree of abstraction.The emerged constructions provide a unique insight into the compositional and non-compositional aspects of the learned language, as a consequence of the pattern-finding processes implemented by the repair strategies.Through these pattern finding processes, non-compositional pairings between aspects of observed form and meaning are included in constructions, while compositional aspects are generalized over through the use of variable slots.Like constructions, grammatical categories also emerge during the language acquisition process.In the spirit of radical construction grammar [4], categories are construction-specific and functionally motivated.They are conceived as fine-grained abstractions over observed syntacticosemantic usage patterns, and are captured in the form of a dynamic and adaptive categorial network.\n\nFinally, the methodology and experiments presented in this paper pave the way for learning computationally tractable, large-scale, usage-based grammars that facilitate both language comprehension and production.The proposed learning algorithm supports online, interactive, incremental, transparent and data-efficient learning.The learner builds up its human-interpretable inventory of constructions and categories through the application of transparent syntacticosemantic generalization processes.Already after a single observation, the fragment of linguistic knowledge acquired by the learner can be successfully used for language comprehension and production.As more and more utterance-meaning pairs are observed, the linguistic knowledge of the learner quickly expands and becomes better fit for achieving their communication goals.As a result of the dynamics of rewarding successful construction applications and punishing competing ones, the grammar of the learner remains ever-adaptive to any changes in the task or environment.Owing to their online, interactive, incremental, transparent and data-efficient nature, the proposed mechanisms for learning computational construction grammars that facilitate both language comprehension and production can serve as an excellent basis for implementing the language acquisition ability of autonomous agents.",
            "score": 0.4210332631940338,
            "section_title": "Discussion and conclusion",
            "char_start_offset": 68262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1104
                },
                {
                    "start": 1106,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1432
                },
                {
                    "start": 1432,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1767
                },
                {
                    "start": 1767,
                    "end": 1941
                },
                {
                    "start": 1941,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "219315567",
            "title": "Emergent linguistic structure in artificial neural networks trained by self-supervision",
            "text": "can develop interesting knowledge of the linguistic structure of whatever human language the system is trained on. Indeed, this has been the dominant perspective in linguistics, where language models have long been seen as inadequate and having no scientific interest, even when their usefulness in practical engineering applications is grudgingly accepted (15,16).\n\nStarting in 2018, researchers in natural language processing (NLP) built a new generation of much larger artificial Fig. 1. A hearer must reconstruct that the store is in a relative clause modifying the chef to know that it is the chef who is out of food rather than the linearly closer store. neural network models, which construct rich, word-token specific deep contextual representations of human language as numeric vectors (17,18). In this paper, we examine how, at this larger scale, there is a dramatic increase in what is modeled by such networks. The simple task of word prediction is a highly effective self-supervision signal: Neural networks can and do improve on this task by inducing their own representations of sentence structure which capture many of the notions of linguistics, including word classes (parts of speech), syntactic structure (grammatical relations or dependencies), and coreference (which mentions of an entity refer to the same entity, such as, e.g., when \"she\" refers back to \"Rachel\"). We examine learned attention structure in models and develop simple probes to show that these models know about each of these types of linguistic information. Indeed, the learned encoding of a sentence to a large extent includes the information found in the parse tree structures of sentences that have been proposed by linguists.\n\nThis is a startling and intriguing result. Traditionally much of the emphasis in NLP has been on using labels for part of speech, syntax, etc., as an aid in other downstream tasks. This result suggests that large-scale hand construction of syntactically labeled training data may no longer be necessary for many tasks. Despite its simple nature, the generality of word prediction, as a task that benefits from syntactic, semantic, and discourse information, leads to it being a very powerful multidimensional supervision signal.\n\nWhile the work presented here is interesting food for thought about the starting point and process of human language acquisition, we make no attempt to model human learning. These models are",
            "score": 0.4209569096514194,
            "section_title": "body",
            "char_start_offset": 4385,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "2799578"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "26717686"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "275212689",
            "title": "Setting Standards in Turkish NLP: TR-MMLU for Large Language Model Evaluation",
            "text": "Advancements in artificial intelligence (AI) and natural language processing (NLP) have revolutionized the field of computational linguistics, particularly with the advent of large language models (LLMs) such as GPT-4, BERT, and Llama. These models, trained on massive datasets, have demonstrated remarkable capabilities in a diverse range of applications, including machine translation, question answering, content generation, and even code synthesis. Their integration into real-world systems has transformed industries, driving innovation and efficiency. However, evaluating the true capabilities of these models remains a persistent challenge, particularly for resource-limited languages like Turkish, where performance often falls short of human-level comprehension [1]. \n\nExisting benchmarks for assessing LLMs predominantly focus on widely spoken languages like English. While these benchmarks, such as MMLU and SuperGLUE, have provided comprehensive evaluation frameworks for high-resource languages, they often overlook the unique linguistic complexities of morphologically rich and agglutinative languages like Turkish. Turkish, with its intricate morphology and syntactic structure, poses distinct challenges for NLP. For instance, a single Turkish verb root can generate numerous word forms through suffixation, encoding tense, person, and mood in a compact yet complex manner. This variability complicates tokenization and semantic parsing, requiring models to grasp both surface patterns and deep linguistic structures. \n\nTo effectively evaluate Turkish NLP models, it is essential to account for these unique features. Questions derived from Turkey's education system offer an ideal basis for such assessments. The education system's curriculum, with its wide-ranging topics and rich linguistic content, provides a robust framework for evaluating the depth of knowledge and linguistic capabilities of LLMs. These questions test not only factual recall but also conceptual understanding, logical reasoning, and cultural context, making them invaluable for benchmarking Turkish language models. Two core dimensions underpin the evaluation of LLMs: instruction following and knowledge assessment. Instruction following assesses a model's ability to execute specific commands, focusing on adherence to predefined instructions and task completion. In contrast, knowledge assessment examines a model's comprehension and understanding, evaluating the breadth and depth of its knowledge base. While instruction-following tasks are important for practical applications, knowledge assessment provides a more comprehensive evaluation of a model's linguistic and conceptual capabilities.",
            "score": 0.42048880762309293,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2357
                },
                {
                    "start": 2358,
                    "end": 2499
                },
                {
                    "start": 2500,
                    "end": 2690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "267627053",
            "title": "A systematic investigation of learnability from single child linguistic input",
            "text": "Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.",
            "score": 0.4203048304480879,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "272661905",
            "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?",
            "text": "For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable. They also demonstrated a more direct causal link-targeted interventions designed to prevent the model from learning these high-level representations obstruct its acquisition of the corresponding grammatical capabilities. Other evidence suggests that higher level descriptions also play a causal role once a model is trained. Manipulating the nodes and weights that correspond to higher level interpretations elicits the expected changes in the models' behavior. For example, editing the weights that a model has used to produce \"The Eiffel Tower is in Paris\" to instead produce \"The Eiffel Tower is in Rome\" has related downstream effects, such as causing the edited model to give directions that refer to the \"nearby\" St. Peter's Basilica (Meng et al., 2022).",
            "score": 0.42029512189168633,
            "section_title": "Challenging the Black-Box Assumption",
            "char_start_offset": 6550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset). Although the functional complexity of the model declines for the rest of training, it increases between these breaks. \u2022 We introduce a regularizer to examine the causal role of SAS (defined in Section 2.2) and use it to show that SAS is necessary for handling complex linguistic phenomena (Section 4.2) and that SAS competes with an alternative strategy that exhibits its own break in the loss curve, which we call the alternative strategy onset. \u2022 Section 4.3 shows that briefly suppressing SAS improves model quality and accelerates convergence. \n\nSuppressing past the alternative strategy onset damages performance and blocks SAS long-term, suggesting this phase transition terminates a critical learning period. BERT first learns to focus on syntactic neighbors with specialized attention heads, and then exhibits grammatical capabilities in its MLM objective. The former (internal) and the latter (external) model behaviors both emerge abruptly, at moments we respectively call the structure onset (\u25b2) and capabilities onset ( ) (quantified as described in Section 2.3). We separately visualize three runs with different seeds, noting that these seeds differ in the stability of Unlabeled Attachment Score (UAS; see Section 2.1) after the structure onset, but uniformly show that SAS emerges almost entirely in a brief window of time. We show (a) MLM loss, with 95% confidence intervals across samples bynonparametric bootstrapping; (b) internal grammar structure, measured by UAS on the parse induced by the attention distributions; and (c) external grammar capabilities, measured by average BLiMP accuracy with 95% confidence intervals across tasks by nonparametric bootstrapping.",
            "score": 0.4199385203993197,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "184486818",
            "title": "Reinforcement Learning of Minimalist Numeral Grammars",
            "text": "Speech-controlled user interfaces facilitate the operation of devices and household functions to laymen. State-of-the-art language technology scans the acoustically analyzed speech signal for relevant keywords that are subsequently inserted into semantic slots to interpret the user\u2019s intent. In order to develop proper cognitive information and communication technologies, simple slot-filling should be replaced by utterance meaning transducers (UMT) that are based on semantic parsers and a mental lexicon, comprising syntactic, phonetic and semantic features of the language under consideration. This lexicon must be acquired by a cognitive agent during interaction with its users. We outline a reinforcement learning algorithm for the acquisition of the syntactic morphology and arithmetic semantics of English numerals, based on minimalist grammar (MG), a recent computational implementation of generative linguistics. Number words are presented to the agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as arithmetic terms. Since MG encodes universal linguistic competence through inference rules, thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach unifies generative grammar and reinforcement learning, hence potentially resolving the still pending Chomsky-Skinner controversy.",
            "score": 0.41964233091829917,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "214727705",
            "title": "Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent Neural Networks",
            "text": "\u2022 RQ2: Do m-NNs induce a common representation space with shared syntactic categories? \n\nOur research questions are reminiscent of well-known questions in the fields of psycholinguistic and second language acquisition, where a considerable body of work has shown that both lexical and syntactic representations are shared in the mind of bilingual individuals (Hartsuiker et al., 2004a;Vasilyeva et al., 2010). Taking inspiration from this body of work, we investigate what factors are needed for m-NNs to successfully transfer linguistic knowledge, including vocabulary overlap, language relatedness, number of training languages, training regime (joint vs sequential) and training objective (next word prediction vs translation to a third language). \n\nIn contrast to the current mainstream focus on BERT-like models, we evaluate more classical LSTMbased models trained for next word prediction or translation over a moderate number of languages (2 or 9). We choose this setup because (i) it allows for more controlled and easy-to-replicate experiments in terms of both training data and model configuration and (ii) LSTMs trained on a standard sequence prediction objective are more cognitively plausible and directly applicable to our main probing task, namely agreement prediction. \n\nIn this setup, we find limited and rather inconsistent evidence for the transfer of implicit grammatical knowledge when semantic cues are removed (Gulordava et al., 2018). While moderate PoS category transfer occurs, truly language-agnostic syntactic categories (such as noun or subject) do not seem to emerge in our m-NN representations. Finally, we find that optimal conditions for lexical-semantic transfer may not be optimal for syntactic transfer.",
            "score": 0.4191758215672581,
            "section_title": "Introduction",
            "char_start_offset": 2068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1739
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 385,
                    "matchedPaperCorpusId": "42454629"
                },
                {
                    "start": 385,
                    "end": 408,
                    "matchedPaperCorpusId": "21813085"
                },
                {
                    "start": 1433,
                    "end": 1457,
                    "matchedPaperCorpusId": "4460159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "261049443",
            "title": "On the Unexpected Abilities of Large Language Models",
            "text": "Large Language Models (LLMs) are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. One of the most striking skills of LLMs is formal linguistic competence (Mahowald et al., 2023), i.e., the capacity to produce and comprehend language. These systems can produce text that is hard to distinguish from human output and are capable of correctly discriminating grammatical vs ungrammatical sentences by passing challenging tests designed by the natural language research community (Warstadt et al., 2020;Warstadt & Bowman, 2022). The acquisition of this ability is perhaps not too surprising considering that these systems are trained by using a massive collection of human-written text. On the other hand, the quality of the competence acquired largely surpasses what linguists could imagine only 5 years ago and falsifies past claims stating that statistical approaches would never be able to capture the complex syntactic and semantic features of language (Pinker & Prince, 1988;Petroni et al., 2019;Everaert et al., 2015). \n\nLLMs trained on large text corpora also acquire large amounts of factual knowledge (Petroni et al., 2019;Robert et al., 2020;Elazar et al., 2021) which enable them to achieve state-of-the-art results on open-domain question answering benchmark without accessing external knowledge (Robert et al., 2020). Also this remarkable ability is not particularly surprising since the unstructured text used to train them contains a wealth of non-linguistic information, such as \"the capital of Italy is Rome\" and \"two plus three is five\". \n\nIn addition to formal linguistic and factual knowledge competence, LLMs display a large set of additional competences that have surprised everyone, including the developers of these systems. Below are some of the most remarkable examples. \n\nLLMs can perform dynamical semantic operations, i.e., understanding how the meaning of a sentence alters the context described in the preceding sentences (Li et al, 2021). For example, consider the sentence \"You see an open chest. The only thing in the chest is an old key.",
            "score": 0.41913874312578836,
            "section_title": "Unexpected abilities",
            "char_start_offset": 3896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 621,
                    "matchedPaperCorpusId": "208527435"
                },
                {
                    "start": 1271,
                    "end": 1291,
                    "matchedPaperCorpusId": "231740560"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "237433614",
            "title": "How much pretraining data do language models need to learn syntax?",
            "text": "The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016;Lau et al., 2017;Gulordava et al., 2018;Marvin and Linzen, 2018;Futrell et al., 2019). Regarding the evaluation of modern language models, Warstadt et al. (2020a) present a challenge set that isolates specific phenomena in syntax, morphology, and semantics, finding that state-of-the-art models struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.  test 20 model type combinations and data sizes on 34 English syntactic test suites, finding substantial differences in syntactic generalization performance by model architecture.\n\nSupervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018;Liu et al., 2019a;Tenney et al., 2019;Voita and Titov, 2020;Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT's vector geometry (Hewitt and Manning, 2019b;Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020;Pimentel et al., 2020;Elazar et al., 2020).",
            "score": 0.4187185980507677,
            "section_title": "Syntactic assessment of language models",
            "char_start_offset": 3959,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 288,
                    "end": 311,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 311,
                    "end": 335,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 335,
                    "end": 356,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 410,
                    "end": 433,
                    "matchedPaperCorpusId": "208527435"
                },
                {
                    "start": 974,
                    "end": 996,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 996,
                    "end": 1014,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 1014,
                    "end": 1034,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1034,
                    "end": 1056,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1175,
                    "end": 1202,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1202,
                    "end": 1219,
                    "matchedPaperCorpusId": "218580945"
                },
                {
                    "start": 1361,
                    "end": 1385,
                    "matchedPaperCorpusId": "202538609"
                },
                {
                    "start": 1461,
                    "end": 1484,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1484,
                    "end": 1506,
                    "matchedPaperCorpusId": "215238965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "235294065",
            "title": "John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs",
            "text": "The study of the linguistic capacities of neural language models (LMs) has become especially relevant in current NLP research, where representations from PLMs feed into systems for various complex tasks, typically improving performance. Many of the testing paradigms used in psycholinguistics lend themselves well to LM analysis as they rely on a textual stimulus and a lexical response. Linzen et al. (2016) were first to borrow from the psycholinguistic testing paradigm, in a study of the capabilities of LSTM-based models to resolve subject-verb number agreement. Goldberg (2019) adopted the psycholinguistic approach in an assessment of BERT (Devlin et al., 2019) on a number of syntactic tasks and found it to perform remarkably well on all. Hawkins et al. (2020) studied the ability of different LMs to capture human preferences as to the argument structure of English verbs.\n\nThe analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018;Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali\u016bnait\u0117 and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020;Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics encoded by these models. Peters et al. (2018) and  observed that PLMs do encode some higher-order syntactic abstractions in the higher layers (whereas lower-order syntactic information is encoded in the lower layers). However, in a comparison of contextualized and static word embeddings,  concluded that PLMs do not generally offer the same improvement with respect to semantics as they do for syntax.\n\nAt the crossroad of semantic analysis and psycholinguistic approaches, Ettinger (2020) introduced a suite of six psycholinguistic diagnostics for the analysis of semantic awareness in LMs. The tasks were selected based on a specific pattern observed in the response of human participants in psycholinguistic studies: an initial expectation (marked by an N400 electrophysiological response) that diverges from",
            "score": 0.4184144650163777,
            "section_title": "Related Work",
            "char_start_offset": 3238,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 408,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 647,
                    "end": 667,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 748,
                    "end": 769,
                    "matchedPaperCorpusId": "222140794"
                },
                {
                    "start": 979,
                    "end": 1004,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1004,
                    "end": 1029,
                    "matchedPaperCorpusId": "52144417"
                },
                {
                    "start": 1142,
                    "end": 1175,
                    "matchedPaperCorpusId": "221761454"
                },
                {
                    "start": 1227,
                    "end": 1247,
                    "matchedPaperCorpusId": "222177135"
                },
                {
                    "start": 1247,
                    "end": 1265,
                    "matchedPaperCorpusId": "219708687"
                },
                {
                    "start": 1390,
                    "end": 1410,
                    "matchedPaperCorpusId": "52098907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "235097491",
            "title": "Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing",
            "text": "Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model\u2019s output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model\u2019s linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?",
            "score": 0.41815342660021726,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "254853878",
            "title": "Language model acceptability judgements are not always robust to context",
            "text": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models\u2019 syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of syntactic phenomena it contains, and whether or not there are grammatical violations. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure. Among all tested models (GPT-2 and five variants of OPT), we find that model performance is affected when we provided contexts with matching syntactic structure: performance significantly improves when contexts are acceptable, and it significantly declines when they are unacceptable. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by acceptability-preserving syntactic perturbations. This sensitivity to highly specific syntactic features of the context can only be explained by the models\u2019 implicit in-context learning abilities.",
            "score": 0.41798395084099804,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "266180578",
            "title": "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings",
            "text": "Overall, there are fewer instances where the models are able to exceed the CL baseline on SuperGLUE tasks. However, the hardest tasks, whereby models achieved the lowest scores are mostly BLiMP tasks. Namely, NPI (lowest = 18.91), FILLER GAP, and QUANTIFIERS (lowest = 33.08), as can be seen in Table 2). \n\nAs noted by Warstadt et al. (2020a), tasks such as NPI licensing and Quantifiers require in-depth semantic knowledge. LMs seem to lack such knowledge, as they tend to make errors that produce contradictory language and show a lack of understanding of assumptions and ideas (Marvin and Linzen, 2018). Interestingly, upon inspecting the predictions made by the models, it appears that there is a strong preference for constructions that contain the adverb \"ever\". In fact, all the predictions made by the models incorporated this adverb. The predictions for the Quantifier task also exhibit consistent patterns of ungrammatically. For instance, they do not seem to know superlative quantifiers cannot be embedded under negation. \n\nTable 5 provides examples that illustrate these judgements. Taken together, this effectively shows the models have not been able to generalise conditions for NPI licensing, which is, that NPIs prefer not to occur in positive sentences and are restricted to specific contexts, primarily negative environments. In addition, the models seem to have also not learned that NPI licensing environments ex-ist and can take the form of negation and negative quantifiers. Similarly, the model has not learned the required knowledge for resolving the right quantifier constructions. \n\nIn this light, solely relying on CL with varying kinds of lexical complexity for forming curricula may not be sufficient. Additional efforts are required to explicitly introduce language models with the knowledge necessary for completing both semantic and syntax tasks successfully. This draws questions to LMs' abilities to generalise syntactical patterns in language. Whilst this 10M-word corpus might be sufficient for humans acquiring language, LMs perhaps require more targeted training and additional data.",
            "score": 0.4175409829195278,
            "section_title": "Difficulties",
            "char_start_offset": 17758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 605,
                    "matchedPaperCorpusId": "52113185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27001953125
        },
        {
            "corpus_id": "18249471",
            "title": "Toward a New Scientific Visualization for the Language Sciences",
            "text": "The review above highlights how a strong interconnectivity exists between information sources in language at all putative levels, both in adult language processing and in language acquisition. In principle, a box-and-arrow model augmented with additional processing arrows could still be invoked as consistent with the above data. Under this -interim\u2016 paradigm, it may still be assumed that there are identifiably separate levels of representation in which information flows both -bottom-up\u2016 and -top-down\u2016. For example, a modular account of single word reading, the dual-route model (e.g., [62]), was updated to share some assumptions about cascaded processing and bi-directional information flow that are embodied in more neurally-inspired connectionist models (e.g., [63]). And the MERGE model of speech perception [64,65] accounts for higher-order word-level influence on phonemic decisions by asserting that while speech perception is feed-forward, phonemic decisions are made by a (later) decision-making mechanism that is sensitive to multiple levels of representation (but cf. [66]). Similarly, the variable-choice reanalysis model of syntactic processing accommodates immediate integration of multiple sources of contextual information, while still attempting to preserve a discrete stage-based selection among syntactic alternatives [67,68]; but cf. [69]. \n\nWhile interactive processing remains an area of intense debate, we identify here some important limitations that derive from thinking about language in terms of separate levels of representation and processing. We focus on three arguments from the following research areas: (1) language development; \n\n(2) continuous linguistic processing in adults; and (3) the neurocognitive architecture of the brain. \n\nLet us consider language acquisition by children first. Thinking in terms of self-contained modular blocks of knowledge operated by stage-based processes implies that the child must first possess mature linguistic competence in at least one putative linguistic level, which would bootstrap the rest-such as lexical segmentation depending on fully-learned phonology. This assumption has led researchers into severe \u2017chicken-and-egg' conundrums.",
            "score": 0.41753200162108883,
            "section_title": "Limits of the Interim Model",
            "char_start_offset": 23750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "24273935"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "557347"
                },
                {
                    "start": 818,
                    "end": 822,
                    "matchedPaperCorpusId": "32291239"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "39296119"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "11521363"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "14572070"
                },
                {
                    "start": 1347,
                    "end": 1350,
                    "matchedPaperCorpusId": "17078993"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "20292199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26806640625
        },
        {
            "corpus_id": "236478283",
            "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
            "text": "Recent breakthroughs of Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and GPT3 (Brown et al., 2020) have demonstrated the effectiveness of self-supervised learning for a range of downstream tasks. Without being guided by structural information in training, PLMs show the potential for learning implicit syntactic structures and language semantic, which can be transferred to other tasks. To better understand and leverage what PLMs have learned, several work has emerged to probe or induce syntactic structures from PLMs. According to prior studies (Rogers et al., 2020), most existing work focuses on syntactic structures such as part of speech (Liu et al.,Figure 1: Typical scenarios for grounding, here the linguistic tokens \"george washington\" can be grounded into different real-world concepts. 2019), constituency tree (Wu et al., 2020) and dependency tree (Hewitt and Manning, 2019;Jawahar et al., 2019), paying much less attention on language semantics (Tenney et al., 2019). However, as well known, semantic information is essential for high-level tasks like machine reading comprehension (Wang and Jiang, 2019).\n\nRegarding to language semantics, an important branch is grounding, which is overlooked by most previous work. Broadly speaking, grounding means \"connecting linguistic symbols to real-world perception or actions\" (Roy, 2005). It is generally thought to be important for a variety of tasks, such as video descriptions (Zhou et al., 2019), visual question answering (Zhu et al., 2016) and semantic parsing (Guo et al., 2019). In this paper, we focus on single-modal scenarios, where grounding refers more specifically to mapping linguistic tokens into a real-world concept described in natural language. As shown in Figure 1, \"george washington\" can be grounded into either a cell value in a structured table, or an entity in knowledge bases.\n\nIn single-modal scenarios, grounding is especially important for semantic parsing, the task of translating a natural language sentence into its corresponding executable logic form. For earlier work, grounding is essential since earlier work almost conceptualized semantic parsing as grounding an utterance to a task-specific meaning representation (Zelle and Moon",
            "score": 0.41734701040291455,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 593,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 848,
                    "end": 865,
                    "matchedPaperCorpusId": "216914626"
                },
                {
                    "start": 984,
                    "end": 1005,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1121,
                    "end": 1143,
                    "matchedPaperCorpusId": "155091891"
                },
                {
                    "start": 1358,
                    "end": 1369,
                    "matchedPaperCorpusId": "112685"
                },
                {
                    "start": 1462,
                    "end": 1481,
                    "matchedPaperCorpusId": "56340721"
                },
                {
                    "start": 1509,
                    "end": 1527,
                    "matchedPaperCorpusId": "5714907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "232092403",
            "title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics",
            "text": "into the pretrained model's behavior.\n\n\u2022 We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training.\n\nSection 3 proposes an information-theoretic formulation of the rediscovery hypothesis and a measure of the dependence between the text and linguistic annotations. In particular, Theorem 1 states that neural language models 28 do need to discover linguistic information to perform at their best: when the language models are denied access to linguistic information, 27. Pruning was done through iterative magnitude pruning (aka Lottery Ticket Hypothesis) or 1-shot magnitude pruning 28. This is, language models which rely on intermediate representations of tokens. their performance is not optimal. The metric \u03c1, which naturally appears in the development of our information-theoretic framework proposes a measure of the relationship between a given linguistic property and the underlying text. We show that it is also a reliable predictor of how suboptimal a language model will be if it is denied access to a given linguistic property. Our proposed methods of estimating \u03c1 can be used in further studies to quantify the dependence between text and its per-token annotation.\n\nA number of previous studies (Tenney et al., 2019b;Conneau et al., 2018;Hewitt & Manning, 2019;Zhang et al., 2020) have shown some evidence in favour of rediscovery hypothesis, while many others (Voita & Titov, 2020;Pimentel et al., 2020b;Hewitt & Liang, 2019;Elazar et al., 2021) questioned the methodology of those studies. However, none of the above-mentioned works questioned the rediscovery hypothesis per se nor demonstrated any evidence for rejecting it. We believe that our work brings an additional understanding of this phenomenon. In addition, it is the first attempt to formalize the rediscovery hypothesis and provide a theoretical frame for it.",
            "score": 0.4173279510664004,
            "section_title": "Conclusion",
            "char_start_offset": 52507,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1519,
                    "end": 1541,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1541,
                    "end": 1562,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 1562,
                    "end": 1585,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1685,
                    "end": 1706,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1706,
                    "end": 1729,
                    "matchedPaperCorpusId": "215238965"
                },
                {
                    "start": 1729,
                    "end": 1750,
                    "matchedPaperCorpusId": "202538609"
                },
                {
                    "start": 1750,
                    "end": 1770,
                    "matchedPaperCorpusId": "227408471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "3937209",
            "title": "Utility-based evaluation metrics for models of language acquisition: A look at speech segmentation",
            "text": "1 The problem with model evaluation Over the past decades, computational modeling has become an increasingly useful tool for studying the ways children acquire their native language. Modeling allows researchers to explicitly evaluate learning strategies by whether these strategies would enable acquisition success. But how do researchers determine if a particular learning strategy is successful? Traditionally, models have been evaluated against adult linguistic knowledge, typically captured in an explicit \"gold standard\". If the modeled learner succeeds at acquiring this adult linguistic knowledge, then it is said to have succeeded and the learning strategy is held up as a viable option for how the acquisition process might work. \n\nGold standard evaluation has two key benefits. First, it provides a uniform measure of evaluation, especially when gold standards are relatively similar across corpora (e.g. orthographic segmentation for speech). Second, this kind of evaluation is typically straightforward to implement for labeled corpora, and so is easy to use for model comparison. \n\nStill, there are several potential disadvantages to gold standard evaluation. First, the choice of an appropriate gold standard is non-trivial for many linguistic tasks since there is disagreement about what the adult knowledge actually is (e.g., speech segmentation, grammatical categorization, syntactic parsing). Second, implementation may require a large amount of time-consuming manual annotation (e.g. visual scene labeling for word-object mapping). Third, and perhaps most importantly, it is unclear that adult knowledge is the appropriate output for some modeled learning strategies, particularly those that are meant to occur early in acquisition. \n\nFor example, consider the early stages of speech segmentation that rely only on probabilistic cues. The earliest evidence of speech segmentation comes at six months (Bortfeld, Morgan, Golinkoff, & Rathbun, 2005) and it appears that probabilistic cues to segmentation, which are language-independent because their implementation does not depend on the specific language being acquired, give way to language-dependent cues between eight and nine months (Johnson & Jusczyk, 2001;Thiessen & Saffran, 2003). So, accurate models of this early stage of speech segmentation should output the knowledge that a nine-month-old has, and this may differ quite significantly from the knowledge an adult has about how to segment speech.",
            "score": 0.41675699889751994,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2475
                }
            ],
            "ref_mentions": [
                {
                    "start": 1919,
                    "end": 1964,
                    "matchedPaperCorpusId": "220645357"
                },
                {
                    "start": 2205,
                    "end": 2230,
                    "matchedPaperCorpusId": "16048559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "254167225",
            "title": "Linguistic signs in action: The neuropragmatics of speech acts",
            "text": "A long-standing debate between linguists and cognitive scientists in experimental pragmatics is how early brain indexes of linguisticpragmatic information about communicative functions occur. Upon perceiving a word like \"cookies\" in a request to obtain them, when would the speaker's communicative intentions be processed? Very quickly, immediately after word onset, or only later, once phonological, semantic and/or morphosyntactic information has been processed? \n\nIntuitively, one would assume that comprehension mechanisms during the perception of an utterance proceed in discrete steps, where phonetic/phonological information has to be processed before accessing higher-level semantic, syntactic and pragmatic information, which may also be retrieved in sequential steps. This view is consistent with most current psycholinguistic models of language comprehension, which advocate the serial processing of different linguistic representations in a cascade fashion. Upon hearing a linguistic utterance, the cascade comprehension timeline would start with processing phonological information followed by lexico-syntactic access and several stages of lexical and semantic analysis, and only at the end would pragmatic comprehension (i.e., interpretation) come into play (Fig. 2, boxes on left). Crucially, the delays between the different representations are in the range of 100 ms, suggesting that interpretation of the literal semantic meaning of an utterance does not occur until 400 ms after onset and that the processing of pragmatic information will not occur before 1000 ms (Friederici, 2002;Friederici, 2011). Other cascade models (Pickering & Garrod, 2004, 2013) advocate more flexible processing of the different linguistic levels, but the processing of pragmatic information (i.e., interpretation/situational model) at the final stage is common to these cascade models. In contrast, the so-called instant/parallel models advocate early and parallel processing of the different linguistic representations, where access to all representations occurs in parallel or nearly simultaneously (within 200 ms) during the perception and recognition processes (Fig. 2, boxes on right, Marslen-Wilson, 1987;Marslen-Wilson and Tyler, 1975;Pulverm\u00fcller et al., 2009;Shtyrov, 2010;Strijkers et al., 2017).",
            "score": 0.416698446404922,
            "section_title": "Brain dynamics of speech act processing",
            "char_start_offset": 16382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 1601,
                    "end": 1618,
                    "matchedPaperCorpusId": "3224368"
                },
                {
                    "start": 1641,
                    "end": 1666,
                    "matchedPaperCorpusId": "42596294"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "222140842",
            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
            "text": "The world of NLP has gone through some tremendous revolution since the proposal of contextualized word embeddings. Some big names are ELMo (Peters et al., 2018), GPT (Radford et al.), and BERT (Devlin et al., 2019), along with its variants (Sanh et al., 2019;Liu et al., 2019b;Lan et al., 2019). Performance boosts on miscellaneous downstream tasks have been reported by finetuning these totipotent pretrained language models. With a view to better grasping what has been learned by these contextualized word embedding models, probing is generally applied to the pretrained models and the models finetuned from them. Probing targets can range from linguistic knowledge, including semantic roles and syntactic structures (Liu et al., 2019a;Tenney et al., 2019Tenney et al., , 2018;;Hewitt and Manning, 2019), to world knowledge (Petroni et al., 2019). \n\nWhile the previous work focuses on what knowledge has been learned after pretraining of transformer-based language models, few delve into their dynamics during pretraining. What happened during the training process of a deep neural network model has been widely studied, including Gur-Ari et al. (2018), Frankle et al. (2019), Raghu et al. (2017), Morcos et al. (2018). Some previous works also study the dynamics of the training process of an LSTM language model (Saphra andLopez, 2018, 2019), but the training dynamics of a large scale pretrained language models are not well-studied. In this work, we probe ALBERT (Lan et al., 2019) during its pretraining phase every N parameter update steps and study what it has learned and what it can achieve so far. We perform a series of experiments, detailed in the following sections, to investigate the development of predicting and reconstructing tokens (Section 3), how linguistic and world knowledge evolve through time (Section 4, Section 6), and whether amassing those information serves as an assurance of good downstream task performances (Section 5). \n\nWe have the following findings based on AL-BERT:",
            "score": 0.4161185089539753,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 160,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 193,
                    "end": 214,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 277,
                    "end": 294,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 781,
                    "end": 806,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 827,
                    "end": 849,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1157,
                    "end": 1178,
                    "matchedPaperCorpusId": "211259030"
                },
                {
                    "start": 1180,
                    "end": 1199,
                    "matchedPaperCorpusId": "23890457"
                },
                {
                    "start": 1201,
                    "end": 1221,
                    "matchedPaperCorpusId": "49271358"
                },
                {
                    "start": 1317,
                    "end": 1328,
                    "matchedPaperCorpusId": "53633715"
                },
                {
                    "start": 1470,
                    "end": 1488,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "256868755",
            "title": "Word class representations spontaneously emerge in a deep neural network trained on next word prediction",
            "text": "According to Diessel, grammar is a \"dynamic system of emergent structures\" and it needs to be explained \"how linguistic structures evolve\" during language acquisition [18]. \n\nPredictive coding and processing are thought to be canonical computations of the human brain [19][20][21][22], in particular during speech and language processing which involves the prediction of which words come next [23]. In previous studies, we already demonstrated that efficient successor representations to form cognitive maps of space and language can be learned by artificial neural networks [24,25]. In particular, we demonstrated how a neural network model can infer the underlying word classes of a simplified artificial language just by observing sequences of words, i.e. sentences, and without any prior knowledge about actual word classes or grammar. The emerging representations share important properties with network-like cognitive maps, enabling e.g. navigation in arbitrary abstract and conceptual spaces, and thereby broadly supporting domain-general cognition, as proposed by Bellmund et al. [26]. \n\nIn this follow-up study, we further address the question if abstract linguistic categories and structures can be learned from experienced language alone in a more complex and naturalistic linguistic task, i.e. word prediction in a natural language scenario. In particular, we trained an artificial deep neural network to predict the next word (successor) in a novel given the nine consecutive predecessor words as input. Subsequently, we analyzed the emerging activation patterns in the hidden layers of the neural network. Strikingly, we find that the internal representations of nine-word input sequences cluster according to the word class of the tenth word to be predicted as output, even though the neural network did not receive any explicit information about syntactic rules or word classes during training. This surprising result suggests, that also in the human brain, abstract representational categories such as word classes may naturally emerge as a consequence of predictive coding and processing of language input. Based on these findings we hypothesize that during language acquisition -which at least partly corresponds to learn to predict which word or utterance comes next -, word classes spontaneously emerge as clusters of successor representations of perceived utterances. We conclude that word classes need not to be innate to enable efficient language acquisition as suggested by universal grammar.",
            "score": 0.41601181639481544,
            "section_title": "Introduction",
            "char_start_offset": 1935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2389
                },
                {
                    "start": 2390,
                    "end": 2517
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "59557878"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "10497123"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "2024772"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "53105231"
                },
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "247749433"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "250281640"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "53237667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "227229069",
            "title": "Intrinsic Knowledge Evaluation on Chinese Language Models",
            "text": "Recent years witnessed much success achieved by pre-trained LMs in the field of Natural Language Processing (Peters et al., 2018;Devlin et al., 2019). The performance of these models is often evaluated on downstream tasks like reading comprehension (RC), natural language inference (NLI), and sentiment analysis (SA). However, improvements on downstream hardly explain the reasons behind models' excellence, as well as what they learn during pre-training. Therefore, an emerging body of work starts to investigate the knowledge encoded in their contextual representations. \n\nLinguistic probing methods are designed to uncover the intriguing properties stored in the contextual representations. Among the linguistic knowledge, syntax is broadly explored across sensitive structures (Goldberg, 2019), grammatical correctness (Marvin and Linzen, 2018), and parsing dependencies (Hewitt and Manning, 2019). However, existing language probes face three challenges: \n\n(1) A skewing on syntax, for few semantic tasks ever study the contextual representations; (2) Most probes are built as classifiers that require extra training. It raises the question 'Do the representations encode linguistic structure, or just that the probe has learned the linguistic task' (Hewitt and Liang, 2019)? and (3) Existing probing tasks scope to only an English language setting. \n\nIn addition to the linguistics, tasks on common sense and facts are also introduced to test models on memorizing real-world knowledge during pre-training (Bisk et al., 2019;Zhou et al., 2019;Petroni et al., 2019). Nonetheless, the knowledge encoding ability of BERT is controversial (Poerner et al., 2019), and the template-based cloze questions are often too short to be leveraged by models for informative contextualizations. \n\nInspired by the above works, this paper proposes the first intrinsic knowledge evaluation benchmark of Chinese pre-trained LMs. Linguistically, it covers both the syntactic and semantic knowledge. One task aims at the language-specific syntactic features of Chinese, and another on languageindependent semantic features. Meanwhile, we inspect world knowledge from two tasks on common sense and facts, further enable questions with natural contexts.",
            "score": 0.4156314926567396,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 149,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 875,
                    "end": 901,
                    "matchedPaperCorpusId": "106402715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6689453125
        },
        {
            "corpus_id": "6669837",
            "title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
            "text": "This paper addresses the challenge of situated language comprehension for intelligent agents that continually learn from their experience in complex domains . A primary purpose of a linguistic faculty in an intelligent agent is the exchange of information with collaborators, which influences and guides reasoning, behavior, and learning. This motivates the study of language as a communication system that functions with, and is informed by, other cognitive capabilities. In this light, we propose five properties that a situated comprehension model should incorporate:\n\nD1 Referential. It must implement a theory of translating amodal linguistic symbols to modal representations of beliefs, knowledge, and experience that are external to the linguistic system. The Indexical Model addresses this by formulating the problem of language comprehension as search over perceptions, short-term memory, and long-term knowledge.\n\nD2 Integrative. Human language is highly contextual and relies on several non-linguistic sources to convey meaning. To successful comprehend language, a model must exploit multiple information sources, including perceptions, domain knowledge, common-sensical knowledge, and short-and long-term experiences. It should also readily incorporate information from nonverbal communication such as gestures and eye gazes. In the Indexical Model, the background knowledge constrains and guides search. We have shown how diverse kinds of background knowledge can be exploited to generate and evaluate interpretations to handle ambiguities and missing information.\n\nD3 Active. The model should actively use all its knowledge and reasoning capabilities to generate and reject candidate hypotheses. Such active processing not only informs further communication with the collaborator, by the way of requesting for clarification and repetition, but may also inform knowledge acquisition. The Indexical Model retains information that helps to determine the cause of failure and to generate and ask questions in case of ambiguity or missing information.\n\nD4 Adaptive and expandable. As Rosie gathers more experience and knowledge of the environment, its comprehension capabilities must scale elegantly and robustly. This desideratum was not explicitly pursued in this paper but our previous work (Mohan et al., 2012) shows that indexical comprehension scales with acquisition of perceptual, spatial, and action knowledge.\n\nD5 Incremental. A model must build up the meaning representation as each word is processed. Likely continuations may be inferred from these partial structures informing linguistic perception and syntactic ambiguity resolution.",
            "score": 0.4156280091943232,
            "section_title": "Related Work and Qualitative Analysis",
            "char_start_offset": 43130,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "244896159",
            "title": "Probing Linguistic Information For Logical Inference In Pre-trained Language Models",
            "text": "Pre-trained language models have replaced traditional symbolic-based natural language processing systems on a variety of language understanding tasks, mainly because symbolic-based NLP systems often rely on linguistic properties as features. Those features are hard to acquire. Many types of linguistic information are either hand-written rules or background knowledge extracted from traditional knowledge base, which make symbolic-based systems hard to scale up on large benchmarks such as GLUE (Wang et al. 2018). On the other hand, many recent probing studies have revealed that sentence representations of pre-trained language models encode a large amount of linguistic information and background knowledge (Tenney et al. 2019;Petroni et al. 2019;Bouraoui, Camacho-Collados, and Schockaert 2020). However, it remains unknown if these representations also encode implicit linguistic information for inference crucial to symbolic inference systems.\n\nCopyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Given pre-trained language models, the probing classifier extracts linguistic infromation for a given probing task. The amount of intimation is measured by the probing accuracy and the information gain, compared with baselines.\n\nIn this paper, we propose an inference information probing framework (Figure 1). We define a set of probing tasks that focus on different types of linguistic information required by symbolic systems. In particular, we cover linguistic information for simple and complex semantic phenomena. Simple semantic phenomena often relies on partial or no context and does not require advanced linguistic skills like contextual understanding and reasoning. Our simple phenomena include word-to-word semantic relations, lexical semantics, and contradiction signatures. Complex phenomena depends on multiple types of reasoning skills like reasoning on event context, monotonicity, coreference, and commonsense knowledge. For complex phenomena, we probe sentiment, relational knowledge, anaphora resolution, and monotonicity reasoning. We are interested in answering two questions: (1) Do pre-trained language models encode linguistic information essential to symbolic inference systems? (2) Do pre-trained language models acquire new linguistic information for inference during the fine-tuning process for the NLI task? For each task, we conducted probing experiments on multiple contextualized language models and compared results to several strong baselines.\n\nOur analysis",
            "score": 0.41537939363892207,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 711,
                    "end": 731,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 731,
                    "end": 751,
                    "matchedPaperCorpusId": "202539551"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "13635218",
            "title": "Syntactic learning by mere exposure - An ERP study in adult learners",
            "text": "Language is a highly structured temporally sequenced stream of vocal sounds. One of the first steps in learning a language is to discover and to analyze its sequential structure which is by no means a trivial task. Leaving aside the acquisition of phonological and semantic aspects language acquisition can be seen, at least in part, as a special form of auditory sequence learning. This idea is supported, for example, by similar patterns of impairment in sequence learning and language tasks in aphasia, computer simulation studies, and artificial grammar learning experiments with healthy participants which suggest at least partially shared cognitive mechanisms in both auditory sequence and language learning [1][2][3]. Due to this close relationship sequence learning studies with artificial languages have been taken to model word or rule acquisition processes that operate in natural language learning. In these studies it has been shown that humans possess the ability to extract words as well as simple syntactic rules from an auditory input stream consisting of syllables or words [4,5]. It is important to mention that no explicit instruction of the underlying rule seems to be necessary in order to achieve this and that the learning process can be initiated by mere exposure to speech.\n\nA relatively simple but important model for syntax acquisition in language has been non-adjacent dependency learning [6][7][8][9][10][11]. Natural language syntax is characterized not only by local phrase structure but also by dependency relations between more distant elements (e.g. subject-verb agreement: The baby (singular) who is in the bed is (singular) laughing, or auxiliary-main verb inflection agreement: The baby is laughing.). Such dependencies finally enable the construction of hierarchical sentence structures which are considered defining features of human language [12]. Being able to learn and process distant relations between linguistic elements is thus a basic prerequisite for acquiring syntax. There are a number of behavioural studies which have demonstrated the human ability to learn non-adjacent dependencies by mere exposure to artificial or natural speech [6][7][8][9]13,14]. In the literature artificial grammar learning studies can be found in different research areas. Depending on the learning process which is proposed by the researchers they are referred to as statistical learning, rule learning or incidental learning studies although they often investigate the learning of similar structures.",
            "score": 0.415188620497176,
            "section_title": "Background",
            "char_start_offset": 13,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "6757424"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "1947994"
                },
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 1095,
                    "end": 1097,
                    "matchedPaperCorpusId": "9844515"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "9058661"
                },
                {
                    "start": 1421,
                    "end": 1424,
                    "matchedPaperCorpusId": "14696791"
                },
                {
                    "start": 1424,
                    "end": 1427,
                    "matchedPaperCorpusId": "3835729"
                },
                {
                    "start": 1427,
                    "end": 1430,
                    "matchedPaperCorpusId": "17504525"
                },
                {
                    "start": 1430,
                    "end": 1434,
                    "matchedPaperCorpusId": "1768437"
                },
                {
                    "start": 1434,
                    "end": 1438,
                    "matchedPaperCorpusId": "1686540"
                },
                {
                    "start": 2186,
                    "end": 2189,
                    "matchedPaperCorpusId": "9058661"
                },
                {
                    "start": 2189,
                    "end": 2192,
                    "matchedPaperCorpusId": "14696791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "261101220",
            "title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?",
            "text": "This suggests that the models comprehend the semantics of relationship words using the current syntax but struggle to accurately discern the initiators and recipients of these relationships. \n\nWe observe that the performance of VLP models in Tab. IV is consistent with their performance on the Semantic Structure (Tab. II). When models (BLIP) are sensitive to semantic and syntactic relationships of content words (with the highest probability choosing \"shuffle non-content words\" and the secondary probability choosing \"Correct\" in Semantic Structure), they become easier to differentiate relationships between nouns (easier to choose Cor). When models (CLIP) have high sensitivity to syntactic relationships across all words (with the highest probability choosing \"Correct\" in Semantic Structure), they can pose challenges in distinguishing relationships between nouns and verbs and tend to more equitably probable choices of Cor and Exc. When models (Flava) only depend on discrete words semantic without syntax (selecting all classes with similar probabilities in Semantic Structure), they tend to opt for the None that excludes intricate relationships. This indicates that syntax is indispensable for the VLP model's acquisition of linguistic knowledge. However, utilizing function words (non-content words) to construct intricate syntactic structures may not effectively enhance the understanding of relationships between words. C. Semantic Level Probing Results 1) VLP models cannot distinguish negation logic in multimodal alignment. In the Negation Logic task in Tab. III, only the BLIP model achieves an accuracy (79.0%) above the random level. The performance of CLIP (47.3%) and X-VLM (48.1%) is close to the random level. Flava, on the other hand, tends to select sentences with negation words. This indicates that it is challenging for the pretraining process to transfer the understanding of negation words from the linguistic knowledge in datasets and language models to VLP models. \n\n2) VLP models exhibit poor perception of spatial relationships, making it difficult for them to correctly identify simple spatial relationships, especially \"left\" and \"right\". In Tab.",
            "score": 0.41501034634484957,
            "section_title": "B. Syntactic Level Probing Results",
            "char_start_offset": 25101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "18095882",
            "title": "Sensorimotor input as a language generalisation tool: a neurorobotics model for generation and generalisation of noun-verb combinations with sensorimotor inputs",
            "text": "Important recent developments in social robotics, such as robots performing human-like emotion expression [76] and social attention for autonomous movement [45], have been accompanied by language understanding approaches focusing on the grounding of natural language into the agent's sensorimotor experience and its situated interaction [5,60]. \n\nFor instance, in [66,39], syntactic parsing techniques are used to ground the language into primitive motor actions (e.g., pickup, move, place), which can be inferred within graph models. Similarly, Misra et al. [43] developed a system for mobile robots which is able to learn to ground the language instructions from a corpus of pairs of natural language including both verbs and spatial information. [74] proposed that in order to understand the object affordance which can be described by adjectives, the most crucial property is the shape-related one. \n\nBesides the direct modelling methods for robot language learning, an alternative approach to build a learning model for language is based on developmental robotics [71,1,10]. Taking inspiration from developmental psychology and developmental neuroscience studies, this approach emphasises the role of the environment and of the interactions that occur during learning, over a progression of learning stages. In the context of language understanding, the core of developmental robotics approaches to language learning is following a similar developmental pathway of infants acquiring grounded representations of natural language and forming a symbol system through embodied interaction with the physical environment [6]. Furthermore, via language learning an agent should also be able to generalise by inferring un-trained combinations of words within the lexical constructions acquired. \n\nVarious developmental robotics models have been developed that incrementally model the various stages of language acquisition in infants, from phoneme acquisition, to object and action names, to word combinations. For example, the cognitive model presented in [21] outlines the cortical interactions in the syllable generation process which result in different developmental phenomena. This mimics the first stage of language development. The Elija model [28] is a vocal apparatus which strictly follows detailed developmental stages. Working as an articulatory synthesizer, it firstly learns the production of sounds on its own.",
            "score": 0.4148135604182934,
            "section_title": "Language Understanding for Robot Systems",
            "char_start_offset": 956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2232
                },
                {
                    "start": 2233,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2423
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "16331555"
                },
                {
                    "start": 337,
                    "end": 340,
                    "matchedPaperCorpusId": "10024679"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "220828823"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "1658890"
                },
                {
                    "start": 559,
                    "end": 563,
                    "matchedPaperCorpusId": "8297420"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "25822719"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "54131797"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "171437"
                },
                {
                    "start": 2054,
                    "end": 2058,
                    "matchedPaperCorpusId": "5921759"
                },
                {
                    "start": 2249,
                    "end": 2253,
                    "matchedPaperCorpusId": "14322012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459228515625
        },
        {
            "corpus_id": "256627634",
            "title": "CALaMo: a Constructionist Assessment of Language Models",
            "text": "When talking about NLMs and their linguistic capabilities, the issue of language acquisition (A) is often formalized as how much language \u039b can be learned by the (artificial) speaker, given a certain level of computational complexity C by being exposed to a certain type of data I: \n\nAll the components of the equation have been central to the linguistic debate. However, starting from this basic formalization, we identify two major focus points that we specifically address in our framework. Firstly, the above formula describes acquisition as instantaneous, but it is actually better described as a process \n\nFrom a cognitive perspective the process is fully continuous, while in the artificial scenario, input data is often fed in 'batches'. We can however imagine that, if we had the ability to increase the number of steps at will (i.e., make N larger while keeping constant the amount of data), we could formalize steps small enough to make the two processes comparable. Secondly, language is often seen as something that the learner has acquired and gained knowledge of. We want to bring back in the framework the role of the linguist-observer, that builds an abstraction over the linguistic behavior of the speaker ( \u00a73.3). As the actual knowledge acquired by the speaker is undetectable and only explainable metalinguistically, in a way that is not viable with neural networks (i.e., we cannot ask NLMs what they know about linguistic regularities), we must take into account the fact that we are always analyzing both the linguistic input received by the speaker and the output produced as an effect of the acquisition process through analytical categories that are created and used by the linguist-observer. In other words, \u039b is not a property of the speaker, but rather a function operated by the linguist-observer. It does not evolve per se during the acquisition process, but rather it helps us detect and characterize the evolution of the speaker's abilities.",
            "score": 0.41450130713776434,
            "section_title": "Acquiring language",
            "char_start_offset": 14435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1975
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "275788769",
            "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities",
            "text": "Despite recent successes of large language models (henceforth, LLMs), matching or exceeding human performance in wide array of tasks, it is still not clear to what extent they can be considered as models of human language (Milli\u00e8re, 2024) or human cognition (Mahowald et al., 2024). Ever since the emergence of large foundation language models, there has been a great interest in gaining a deeper understanding of their general linguistic and cognitive capabilities, ranging from studies into syntactic or semantic processing in LLMs (Gulordava et al., 2018;Blevins et al., 2018;Goldberg, 2019;Arefyev et al., 2020) to analyses of their arithmetic or logical reasoning abilities (Huang and Chang, 2022;Webb et al., 2023;Wu et al., 2023;Yuan et al., 2023). Since LLMs are trained on massive amounts of training data and learn huge amounts of parameters in their internal layers, it is often difficult to establish whether an LLM has \"really\" generalized a certain rule or piece of knowledge, or whether it memorized superficial token-level patterns (Weissweiler et al., 2023). This challenge exists in particular in domains where linguistic knowledge is closely intertwined with nonlinguistic aspects of cognition (Mahowald et al., 2024) such as, e.g., pragmatics and commonsense reasoning (Chang and Bergen, 2024). \n\nTo assess LLMs linguistic capabilities research has drawn from the methods of psychology, applying established experimental paradigms to language models. For example, Ettinger (2020) created a diagnostic suite for language models inspired by psycholinguistic tests, ranging over different linguistic disciplines. A study by Hawkins et al. (2020) compared human and model acceptability judgements for double object sentences, capturing the differences in their biases. Outside the realm of psycholinguistics, Binz and Schulz (2023) used vignette studies from cognitive psychology to gauge the cognitive abilities of GPT-3 (Brown et al., 2020).",
            "score": 0.4139963384587513,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 281,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 594,
                    "end": 615,
                    "matchedPaperCorpusId": "227231059"
                },
                {
                    "start": 702,
                    "end": 720,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1213,
                    "end": 1236,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1289,
                    "end": 1313,
                    "matchedPaperCorpusId": "257636789"
                },
                {
                    "start": 1484,
                    "end": 1499,
                    "matchedPaperCorpusId": "199001173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "264590507",
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "text": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
            "score": 0.4136987656015606,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "257985059",
            "title": "Conceptual structure coheres in human cognition but not in large language models",
            "text": "Since Elman's pioneering work (Elman, 1990) showcasing the ability of neural networks to capture many aspects of human language processing (Rumelhart et al., 1986), such models have provided a useful tool, and sometimes a gadfly, for developing hypotheses about the cognitive and neural mechanisms that support language. When trained on a task that seems almost absurdly simplistic-continuous, sequential prediction of upcoming words in sentences-early models exhibited properties that upended received wisdom about what language is and how it works. They acquired internal representations that blended syntactic and semantic information, rather than keeping these separate as classic psycho-linguistics required. They handled grammatical dependencies, not by constructing syntactic structure trees, but by learning and exploiting temporal patterns in language. Perhaps most surprisingly, they illustrated that statistical structure latent in natural language could go a long way toward explaining how we acquire knowledge of semantic similarity relations among words. Because words with similar meanings tend to be encountered in similar linguistic contexts (Firth, 1957;Osgood, 1952;Harris, 1954), models that exploit contextual similarity when representing words come to express semantic relations between them. This enterprise of learning by predicting has persisted into the modern era of Transformer-based language models (Devlin et al., 2018;Brown et al., 2020a). \n\nThough early work was limited in the nature and complexity of the language corpora used to train models (Elman, 1991;McClelland et al., 1990), these ideas spurred a variety of computational approaches that could be applied to large corpora of written text. Approaches such as latent semantic analysis (Deerwester et al., 1990) and skipgram models (Mikolov et al., 2013a;Bojanowski et al., 2017), for instance, learn vector-space representations of words from overlap in their linguistic contexts, which turn out to capture a variety of semantic relationships amongst words, including some that are highly abstract (Grand et al., 2022;Elman, 2004;Lupyan and Lewis, 2019).",
            "score": 0.4134128328998642,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 43,
                    "matchedPaperCorpusId": "2763403"
                },
                {
                    "start": 1159,
                    "end": 1172,
                    "matchedPaperCorpusId": "208093066"
                },
                {
                    "start": 1172,
                    "end": 1185,
                    "matchedPaperCorpusId": "17077596"
                },
                {
                    "start": 1449,
                    "end": 1469,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1577,
                    "end": 1590,
                    "matchedPaperCorpusId": "7069311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "233137618",
            "title": "When Language Evolution Meets Multimodality: Current Status and Challenges Toward Multimodal Computational Models",
            "text": "First of all, to set up the multimodal agent-based conceptual framework depicted in Fig. 4, the first challenge that needs to be addressed is the representation of the meanings of the various objects that constitute the world in which the agents act. This representation implies a formalisation of the semantics at the base of a successful (i.e. unambiguous and consistent) communication between agents. A semantic formalisation that allows assessing semantic relations between concepts is fundamental for modeling generalisation capabilities in a population of agents [70]. \n\nAs well explained by Steels [71], the steps of the emergence of semantic meanings in a population of artificial agents, which are equipped as described in the three reference models depicted in Fig. 3, can be summarised in the following four: (i) inferred: sender and receiver agents try to come to a common understanding of an object without explicitly expressing all meanings but inferring most of them from the contextual information; (ii) lexical: a meaning fragment becomes associated with an object without any concern for grammar; (iii) syntactic: meaning is expressed using hierarchical patterns of symbols that can be combined using syntactic rules; (iv) morphological: meaning is expressed using complex symbols, hierarchical structure and semantic relations between symbols organised through grammatical agreement. \n\nMost of the models of language evolution developed in the literature focused predominantly on the lexical and syntactic level, by shedding light on the emergence of phonological systems [17]- [19], lexical development [14], [36], the emergence of syntax [42], [43], and, more specifically, compositionality [62], [64], [72], [73]. The major problem of these models stays in the fact that the emergent phonological/ lexical/syntactic (compositional) systems arise only because the symbols are explicitly coupled with pre-existing, structured semantic meanings; how these meanings are originated and why they are associated with the symbols remains an open problem in these models. \n\nTo answer these questions, several approaches to conceptualisation and semantic formalisation of concepts have been put forward in the literature on natural language [68], [69], [75].",
            "score": 0.41305760046157464,
            "section_title": "A. A SEMANTIC FRAMEWORK FOR CONCEPTUALISATION",
            "char_start_offset": 25565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 2084
                },
                {
                    "start": 2087,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "49297559"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "4722048"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "1671696"
                },
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "170757641"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "15281767"
                },
                {
                    "start": 1659,
                    "end": 1663,
                    "matchedPaperCorpusId": "18149863"
                },
                {
                    "start": 1665,
                    "end": 1669,
                    "matchedPaperCorpusId": "13922777"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "24100971"
                },
                {
                    "start": 1724,
                    "end": 1728,
                    "matchedPaperCorpusId": "3142767"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "13548281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359130859375
        },
        {
            "corpus_id": "271039363",
            "title": "Testing learning hypotheses using neural networks by manipulating learning data",
            "text": "Neural network language models are systems that learn probability distributions over sequences of words given a text corpus.While they are not tuned or designed to predict linguistic acceptability, they can be used to model acceptability judgments through targeted syntactic evaluations (Lau et al., 2017;Linzen et al., 2016;Marvin and Linzen, 2018;Warstadt et al., 2020).Given a minimal pair such as (1), a neural network language model can be used to assign each sequence of words a probability score.A model that assigns a higher probability to the grammatical sentence (1a) than than the ungrammatical (1b) shows sensitivity to the underlying syntactic differences between the two sentences.Targeted syntactic evaluations have shown that neural network language models are sensitive to a variety of syntactic and semantic constraints (Linzen et al., 2016;Warstadt et al., 2020).\n\nBy using neural network language models as theories of acquisition, we can address an existing methodological limitation in acquisition studies with humans, as these models allowing for complete control of the input provided to the learner: specifically, We can train multiple models on corpora which differ in controlled and targeted ways, and compare how learners with the same initial state and learning goals, but different input, diverge in their behavior at the end of learning.\n\nWe present a case study using this method.We train neural network language models on approximately the same amount of linguistic data that humans are exposed to, and use these models to answer two questions: firstly, is a human-scale amount of linguistic input sufficient for a model to learn to make judgments that are similar to human judgments on passive exceptions?Secondly, what kinds of information might a learner be using from the linguistic input to learn to make those judgments?\n\nIn Experiment 1, we answer the first question in the affirmative.We show that neural network language models can learn to make acceptability judgments on exceptions to passivization that are similar (though not identical) to those of English speakers.Such behavior suggests that language models can make use of indirect evidence in the linguistic input to learn exceptions.\n\nTo answer the second question, we take inspiration from work that uses systematic interventions on a model's training corpus to test causal links between a model's input and its eventual inferences (e.g.",
            "score": 0.4117620279291482,
            "section_title": "The role of neural networks",
            "char_start_offset": 4602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 882
                },
                {
                    "start": 884,
                    "end": 1368
                },
                {
                    "start": 1370,
                    "end": 1412
                },
                {
                    "start": 1412,
                    "end": 1739
                },
                {
                    "start": 1739,
                    "end": 1859
                },
                {
                    "start": 1861,
                    "end": 1926
                },
                {
                    "start": 1926,
                    "end": 2112
                },
                {
                    "start": 2112,
                    "end": 2234
                },
                {
                    "start": 2236,
                    "end": 2439
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 305,
                    "matchedPaperCorpusId": "1056628"
                },
                {
                    "start": 305,
                    "end": 325,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 325,
                    "end": 349,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "208527435"
                },
                {
                    "start": 838,
                    "end": 859,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 859,
                    "end": 881,
                    "matchedPaperCorpusId": "208527435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "27228816",
            "title": "Grounded Language Learning in a Simulated 3D World",
            "text": "Before it can exhibit any lexical knowledge, the agent must learn various skills and capacities that are independent of the specifics of any particular language instruction. These include an awareness of objects as distinct from floors or walls; some capacity to sense ways in which those objects differ; and the ability to both look and move in the same direction. In addition, the agent must infer that solving the solution to tasks is always contingent on both visual and linguistic input, without any prior programming or explicit teaching of the importance of inter-modal interaction. Given the complexity of this learning challenge, it is perhaps unsurprising that the agent requires thousands of training episodes before evidence of word learning emerges. \n\nTo establish the importance of this 'pre-linguistic' learning, we compared the speed of vocabulary acquisition in agents with different degrees of prior knowledge. The training set consisted of instructions (and corresponding environments) from the twenty shape terms banana, cherries, cow, flower, fork, fridge, hammer, jug, knife, pig, pincer, plant, saxophone, shoe, spoon, tennis-racket, tomato, tree, wine-glass and zebra. The agent with most prior knowledge was trained in advance (in a single room setting with two objects) on the remaining twenty shapes from the full environment. The agent with minimal prior knowledge was trained only on the two terms ball and tv. Both regimes of advanced training were stopped once the agent reached an average reward of 9.5/10 across 1,000 episodes. The agent with no prior knowledge began learning directly on the training set. \n\nThe comparison presented in Figure 4 demonstrates that much of the initial learning in an agent trained from scratch involves acquiring visual and motor, rather than expressly linguistic, capabilities. An agent already knowing two words (and therefore exhibiting rudimentary motor and visual skills) learned new words at a notably faster rate than an agent trained from scratch. Moreover, the speed of word learning appeared to accelerate as more words were learned. This shows that the acquisition of new words is supported not only by general-purpose motor-skills and perception, but also existing lexical or semantic knowledge. In other words, the agent is able to bootstrap its existing semantic knowledge to enable the acquisition of new semantic knowledge.",
            "score": 0.41067219907120567,
            "section_title": "Word learning speed experiment",
            "char_start_offset": 18200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2272
                },
                {
                    "start": 2273,
                    "end": 2404
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "278000565",
            "title": "Do Large Language Models know who did what to whom?",
            "text": "Large Language Models (LLMs) have achieved unprecedented success at natural language processing. Their success demonstrates the power of statistical learning over strings of linguistic forms (Contreras Kallens et al., 2023;Piantadosi, 2023): by merely learning to predict the next word (or a missing word) in a text, LLMs develop the ability to produce texts that conform to the syntactic rules of a language (e.g., McCoy et al., 2023). Indeed, the internal representations and next-word predictions of LLMs suggest that these systems have acquired many complex grammatical generalizations (for reviews, see Chang and Bergen, 2024;Futrell and Mahowald, 2025;Linzen and Baroni, 2021). As a result, LLMs have been proposed as models of human language processing (Blank, 2023) and are being used to predict both behavioral and neuroimaging data (Antonello et al., 2021;Caucheteux and King, 2022;Evanson et al., 2023;Hosseini et al., 2022;Merkx and Frank, 2020;Oh and Schuler, 2022;Schrimpf et al., 2021;Tang et al., 2024;Tuckute et al., 2024;Wilcox et al., 2020). Nonetheless, humans do not use grammar as an end in itself, but rather as an intermediate step in mapping linguistic input forms (e.g., phonological or orthographic information) onto meaning (semantics; Jackendoff and Wittenberg, 2017). Therefore, a key question is whether LLMs can extract meaning from their textual input. More specifically: what kinds of meaning can be acquired merely from learning to predict the next (or a missing) word? \n\nThe general question of semantics in LLMs is hotly debated. Whereas the behavior and internal activity of LLMs exhibit some signatures of semantic representations (for a review, see (Chang and Bergen, 2024;Pavlick, 2022), these models are often criticized for not truly \"understanding\" language (e.g., Bender & Koller, 2020). The critiques use various definitions of \"understanding\".",
            "score": 0.41045944639441756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 223,
                    "matchedPaperCorpusId": "257205301"
                },
                {
                    "start": 416,
                    "end": 435,
                    "matchedPaperCorpusId": "244345615"
                },
                {
                    "start": 608,
                    "end": 631,
                    "matchedPaperCorpusId": "257636789"
                },
                {
                    "start": 658,
                    "end": 682,
                    "matchedPaperCorpusId": "216080851"
                },
                {
                    "start": 760,
                    "end": 773,
                    "matchedPaperCorpusId": "261367991"
                },
                {
                    "start": 842,
                    "end": 866,
                    "matchedPaperCorpusId": "235390553"
                },
                {
                    "start": 866,
                    "end": 892,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 935,
                    "end": 957,
                    "matchedPaperCorpusId": "233346957"
                },
                {
                    "start": 978,
                    "end": 1000,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 1000,
                    "end": 1018,
                    "matchedPaperCorpusId": "258833330"
                },
                {
                    "start": 1018,
                    "end": 1039,
                    "matchedPaperCorpusId": "258216421"
                },
                {
                    "start": 1264,
                    "end": 1296,
                    "matchedPaperCorpusId": "12326340"
                },
                {
                    "start": 1689,
                    "end": 1713,
                    "matchedPaperCorpusId": "257636789"
                },
                {
                    "start": 1713,
                    "end": 1727,
                    "matchedPaperCorpusId": "244549257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "273025776",
            "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
            "text": "Figure 4a shows the performance of OLMo 1B and 7B models10 from various stages of pretraining as an initial state. We observe that models in the final stages of pretraining struggle more with acquiring new knowledge A(\u03b8) and exhibit greater forgetting F(\u03b8). As shown in Figure 4b, continually training the models at the mid-point of the pretraining as the initial checkpoint tends to yield the best performance in knowledge probing and downstream tasks compared to both models from the initial and final stages of pretraining. While early-stage models demonstrate high knowledge acquisition with minimal forgetting, their overall performance is limited by weaker language modeling capabilities. Conversely, later-stage models, despite being trained on larger datasets, exhibit lower knowledge acquisition and higher rates of forgetting, resulting in lower overall performance compared to the mid-stage models. This aligns with previous research suggesting that a model in the final stage of pretraining tends to struggle when learning new knowledge, showing a trade-off between plasticity and stability (Dohare et al. ). Therefore, we suggest that using a mid-stage checkpoint strikes a good balance, making it a practical choice as an initial starting point for further training to inject new knowledge. \n\nWe consistently observe this pattern of later-stage models underperforming compared to earlier-stage models across various hyperparameter settings, including batch size, learning rate, training corpus, and the number of epochs. A detailed analysis of these results is provided in Appendix B.4.",
            "score": 0.41045944639441756,
            "section_title": "KNOWLEDGE ACQUISITION AND RETENTION DECREASES ACROSS PRETRAINING STAGE",
            "char_start_offset": 19577,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1600
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "While often, the performance of language models scales predictably (Kaplan et al., 2020;Srivastava et al., 2022), some tasks instead show breakthrough behavior where a single point along a scaling curve (i.e. compute/number of tokens seen/number of parameters versus test loss) shows a sudden improvement in performance (Srivastava et al., 2022;Wei et al., 2022;Caballero et al., 2023). One computational structure, the induction head, emerges in autoregressive language models at a discrete phase change (Olsson et al., 2022) and is associated with handling longer context sizes and in-context learning. In machine translation, Dankers et al. (2022) find a learning progression in which a Transformer first overgeneralizes the literal interpretations of idioms and then memorizes idiomatic behavior. When the training set makes grammatical rules ambiguous, Murty et al. (2023) show that language modelling eventually leads to phase transitions towards the hierarchical version of a rule over an alternative based on linear sequential order. Outside of NLP, phase changes are observed in the acquisition of concepts in strategy games (Lovering et al., 2022;McGrath et al., 2022) and arithmetic (Liu et al., 2022;Nanda et al., 2023). Our work also identifies a specific phase in MLM training, the SAS phase, and analyzes its role in performance and generalization behavior. \n\nWe also observe an alignment between phase changes in representational complexity and in generalization performance (Section 4.1), which parallels the observation of Finally, although we find the same phase transition occurs across multiple training runs, other work indicates that generalization capabilities are sensitive to random seed (Sellam et al., 2022;Juneja et al., 2023;Jordan, 2023). Furthermore, phase transitions may be primarily an artifact of poor hyperparameter settings (Liu et al., 2023), which lead to unstable optimization (Hu et al., 2023). Therefore, it is possible that these abrupt breakthroughs would vanish under the correct architecture and optimizer settings.",
            "score": 0.41045944639441756,
            "section_title": "B SYNTACTIC REGULARIZER",
            "char_start_offset": 33116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 385,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 629,
                    "end": 650,
                    "matchedPaperCorpusId": "236987160"
                },
                {
                    "start": 1134,
                    "end": 1157,
                    "matchedPaperCorpusId": "253534142"
                },
                {
                    "start": 1157,
                    "end": 1178,
                    "matchedPaperCorpusId": "244270420"
                },
                {
                    "start": 1194,
                    "end": 1212,
                    "matchedPaperCorpusId": "248965387"
                },
                {
                    "start": 1714,
                    "end": 1735,
                    "matchedPaperCorpusId": "235683534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "226283774",
            "title": "Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models",
            "text": "Neural Language Modelling has recently gained popularity in NLP. A Neural Network Language Model (NNLM) is tasked with learning a conditional probability distribution over the occurrences of words in text (Mikolov et al., 2011). This language modelling objective requires a neural network with sufficient capacity to learn meaningful linguistic information such as semantic knowledge and syntactic structure. Due to their ability to learn these important linguistic phenomena, NNLMs have been successfully employed as an effective method for generative pretraining (Dai and Le, 2015) and transfer learning to other natural language tasks (Peters et al., 2018a;Howard and Ruder, 2018;Radford et al., 2018). As previously suggested by Bengio et al. (2003), Mnih and Hinton (2007) and Mnih and Teh (2012), the weights of the final fully-connected output layer, or output embeddings, which compute the conditional probability distribution over the lexicon, also constitute a legitimate set of embedding vectors representing word meaning, as is the case for the input embeddings. This commonality between the input and output layers of the NNLM has motivated researchers to tie these representations together during training, improving performance on language modelling tasks (Inan et al., 2016;Press and Wolf, 2017). Furthermore, such a procedure is intuitive, since both the input and output embeddings of the network would appear to be performing a similar task of encoding information about lexical content. As described by Inan et al. (2016), they clearly live in an identical semantic space in language models, unlike other machine learning models were the input and output embeddings have no direct link.\n\nOn the other hand, it would also be reasonable to assume that the output representations require highly task-specific features (Peters et al., 2018a,b;Devlin et al., 2019). Despite their utility in language modelling, in-depth analysis of these input and output vector representations remains limited. The goal of this work is to gain a deeper understanding of the aspects of language captured in these contrasting representations. Our two main contributions 1 are as follows:\n\n1. We perform an investigation to uncover both the broad types of semantic knowledge and fine-grained linguistic phenomena encoded within each set of word representations.",
            "score": 0.4101728018448685,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 227,
                    "matchedPaperCorpusId": "14850173"
                },
                {
                    "start": 565,
                    "end": 583,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 638,
                    "end": 660,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 660,
                    "end": 683,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 733,
                    "end": 753,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 755,
                    "end": 777,
                    "matchedPaperCorpusId": "577005"
                },
                {
                    "start": 782,
                    "end": 801,
                    "matchedPaperCorpusId": "6633369"
                },
                {
                    "start": 1290,
                    "end": 1311,
                    "matchedPaperCorpusId": "836219"
                },
                {
                    "start": 1859,
                    "end": 1879,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "276647333",
            "title": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models",
            "text": "Hierarchical Linguistic Structure. To systematically interpret the language capabilities of large models, we adopt a six level structure based on theoretical linguistics (Fromkin et al., 2017): phonetics, phonology, morphology, syntax, semantics, and pragmatics. The structure follows a logical progression from the external, physical realization of sound to the internal, contextual understanding of meaning. Each linguistic capability contains several concrete linguistic features, e.g., semantics level includes metaphor, simile, etc. We provide the exact definition for each linguistic capability in Appendix A.1 Dataset Construction. The sparse feature activation distribution of SAE is closely related to the conditions under which their corresponding linguistic features hold in linguistic knowledge. To find the linguistic features and evaluate its dominance, we propose a method to construct the dataset and analyze feature activation frequencies. \n\nFor each linguistic feature, we first construct a set of sentences that significantly align with the desired feature. The feature activation representing this linguistic feature in SAE's hidden space will be significantly activated on these sentences. However, this is not enough to accurately identify them, as there are some background noise vectors that are activated on all sentences in the dataset and interfere with our judgment. We need to include a control group without the feature in the constructed sentences. \n\nWe introduce two types of control groups: minimal pairs and counterfactual sentences. Minimal pairs are constructed by changing only the part of a sentence that corresponds to a particular linguistic feature, while keeping all other parts unchanged. However, this approach often results in syntactically incorrect sentences. \n\nTo overcome this limitation, we also construct fully grammatically correct control groups, called counterfactual sentences, which differ from the original sentence only in terms of its linguistic features. Detailed dataset construction procedures are provided in Appendix A.2.",
            "score": 0.4095971933420457,
            "section_title": "Linguistic Structure",
            "char_start_offset": 7721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2085
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "209316287",
            "title": "Differential Access: Asymmetries in Accessing Features and Building Representations in Heritage Language Grammars",
            "text": "converted to intake and hence consolidated linguistic knowledge. In their view, the commonly attested acquisitional path among heritage speakers that goes from reduced levels of activation for production to reduced levels of activation for comprehension purposes may result in varying degrees of integration of phonological, syntactic, and semantic knowledge. A crucial point worth noting here is that this model should not be misunderstood to advocate an exclusively usagebased model of the acquisition of language. (Polinsky 2018, p. 31) summarizes this position best in stating that \"not all input turns into intake\". Therefore, although frequency undoubtedly plays an essential role in language acquisition and maintenance, it should be understood that (S\u00e1nchez 2013) make reference to input that has contributed to intake. Putnam and S\u00e1nchez (2013) conceptualize that the core base of atomic linguistic knowledge is represented as features that function as the essential building blocks of larger representations. They propose the following four-stage model to account for various stages of observed performance in heritage language production and comprehension:\n\n\u2022 Stage 1: Transfer or re-assemble of some FFs (formal features) from the L2 grammar to the L1 phonological features (PF) and semantic features which may coincide with the activation of L2 lexical items on a more frequent basis from the standpoint of linguistic production; \u2022 Stage 2: Transfer or re-assemble of massive sets of FFs from the L2 to the L1 PF and semantic features, while concurrently showing significantly higher rates of activation of L2 lexical items than L1 lexical items for production purposes (i.e., they might code-switch more than bilinguals in the previous situation); \u2022 Stage 3: Exhibit difficulties in activating PF and semantic features (as well as other FFs) in the L1 for production purposes but are able to do so for comprehension of some high frequency lexical items; and, \u2022 Stage 4: Have difficulties activating PF features and semantic features (as well as other FFs) in the L1 for both production and comprehension purposes.\" (Putnam and S\u00e1nchez 2013, pp. 489-90) The extent to which these stages are clearly distinct and form part of a continuum in development is an empirical matter that requires further research. In this paper we propose that asymmetries in the quality of linguistic representations observed in production and comprehension",
            "score": 0.4093825487919994,
            "section_title": "Modeling Heritage Speakers' Grammars",
            "char_start_offset": 8156,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230712890625
        },
        {
            "corpus_id": "18249471",
            "title": "Toward a New Scientific Visualization for the Language Sciences",
            "text": "This assumption has led researchers into severe \u2017chicken-and-egg' conundrums. For instance, the very existence of lexical categories (e.g., dog and dinner are nouns, while eat and run are verbs) presupposes knowledge of syntactic rules that apply to them, such as how to generate noun and verb phrases. However, to learn those syntactic rules, one must already have those lexical categories in place. In this and other types of chicken-and-egg paradoxes, the question that has been (ill-)posed is -what knowledge comes absolutely first in language development?\u2016 In fact, both perceptual [70] and statistical learning abilities [71] of infants and toddlers develop gradually and simultaneously over time. Likewise, syntactic knowledge in toddlers such as verb argument structure [72] and pragmatic knowledge [73] seem far from adult competence. Several corpus and behavioral studies now suggest that children's early language tends to contain mostly phrases and utterance fragments that have been heard and used before. Very little substitution of lexical items or application of abstract syntactic patterns occurs in early language productions [74]. \n\nSince no level of representation can be independently mature at any stage to single-handedly kick start the learning process of the others, a more viable solution is that different representations co-develop gradually, mutually assisting each other via multi-directional correlations. This characterization of language development is inconsistent with the modular view of language knowledge as encapsulated into independent boxes. \n\nAnother related problem with the single bootstrapping \u2017level' is that invariably no single cueing approach provides a perfect correlation with a given linguistic structure. For instance, distributional learning can fall prey to spurious correlations such as John eats meat, John eats slowly, The meat is good, which would erroneously lead the child to infer that The slowly is good should be grammatical [50,75]. Likewise, prosodic cueing is only partially successful because phonological phrase boundaries do not map perfectly into syntactic phrase boundaries [76][77][78]. Similarly, Pinker [79] discussed the limitations of syntactic information for discovering detailed semantic aspects of verbs (the syntactic bootstrapping argument).",
            "score": 0.40920341064242594,
            "section_title": "Limits of the Interim Model",
            "char_start_offset": 25890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "14518662"
                },
                {
                    "start": 1993,
                    "end": 1996,
                    "matchedPaperCorpusId": "44148298"
                },
                {
                    "start": 2146,
                    "end": 2150,
                    "matchedPaperCorpusId": "59747691"
                },
                {
                    "start": 2150,
                    "end": 2154,
                    "matchedPaperCorpusId": "36969856"
                },
                {
                    "start": 2154,
                    "end": 2158,
                    "matchedPaperCorpusId": "202288441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2939453125
        },
        {
            "corpus_id": "269214372",
            "title": "Exploring the landscape of large language models: Foundations, techniques, and challenges",
            "text": "Pre-training large language models involves training a neural network on a massive corpus of text data, allowing the model to learn intricate patterns, contextual relationships, and language structures (Guu et al., 2020;Z. Lin et al., 2023).During pre-training, the model does not have specific knowledge about downstream tasks it will later perform.Instead, it learns a generalized understanding of language.The language understanding capabilities of the LLM are stored within the model's parameters, which in fact act as the model's memory.In other words, the more the number of model's parameters, the more the model's memory and ability to perform sophisticated tasks (Gholami & Omar, 2023; W. X.Zhao et al., 2023).During pre-training, these parameters are adjusted with respect to a pre-training objective in order to minimize the training loss and maximize the model accuracy.Once pre-trained, the LLM can be fine-tuned on smaller, taskspecific datasets for various applications, leveraging the comprehensive linguistic knowledge acquired during pre-training.",
            "score": 0.40911319812568203,
            "section_title": "Pre-training LLMs",
            "char_start_offset": 5665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 700
                },
                {
                    "start": 700,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 220,
                    "end": 240,
                    "matchedPaperCorpusId": "257019559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "268689979",
            "title": "Computational Modeling of the Segmentation of Sentence Stimuli From an Infant Word-Finding Study",
            "text": "Denying that explicit computation of syllabic transitional probability statistics is necessary for statistical learning leaves open several fundamental questions about the nature of the learning. We consider it likely that parsed tokens become chunks that are mentally represented as such, and that may become protolexical units available for entry into syntactic and semantic linguistic networks if they continue to be supported in further language experience (e.g., Swingley, 2007). But how and when this happens is a matter of debate, and may involve multiple neurally distinct processes (e.g., Henin et al., 2021;Su\u010devi\u0107 & Schapiro, 2023;Wang et al., 2019). The modeling presented here is neutral in this regard. Any computational model meant to mimic the developmental progress of the infant's mental representations for language should, ideally, acquire the same knowledge from experimental stimuli that actual infants apparently do, at least as inferred from the logic of preferential listening experiments. Testing models on experimental stimuli is different from the more typical evaluation in which a model's outputs are scored according to gold standards given by assumptions about the language-its set of consonants and vowels, the canonical representations of its words, and so on. One risk of the latter sort of evaluation is that it might project onto infants representations they do not actually possess at the developmental stage being modeled. Evaluations based on outputs from exposure to experimental materials bring the test closer to what can be claimed empirically about infants' knowledge state. \n\nThe set of infant word segmentation experiments is small (on the order of several dozen), and the set of experiments for which auditory materials and exact transcripts are available is smaller still. The tests presented here required, as a preliminary step, the segmentation of the stimulus corpora into a gold-standard set of phonological and lexical transcriptions. This is a nontrivial process. Still, we argue that in the long run, evaluating computational models against a battery of speech materials from infant studies would be a productive testing strategy, and recommend that future studies of infants' treatment of speech materials provide those materials freely for use in the benchmarking of computational models, as we have begun to do here.",
            "score": 0.40904497799420686,
            "section_title": "Conclusions",
            "char_start_offset": 42375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 617,
                    "matchedPaperCorpusId": "218644144"
                },
                {
                    "start": 642,
                    "end": 660,
                    "matchedPaperCorpusId": "201702897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "3264603",
            "title": "Data-Oriented Language Processing. An Overview",
            "text": "We have seen that there is now a large and rapidly growing class of computational models that process new language input not by invoking the rules of a competence grammar, but by directly constructing analogies with concrete past language experiences. Independent experiments with different versions of this approach indicate that it disambiguates more accurately than other probabilistic methods. \n\nMany of these experimental results have been achieved, however, with impoverished annotation formalisms (labelled phrase-structure trees), with very small corpora (ATIS), or with a sub-optimal version of the data-oriented approach (subtrees of depth one). Much work is still needed to be able to annotate large corpora with rich syntactic/semantic structures, and to develop parsing/disambiguation algorithms that can use such corpora in an efficient way. \n\nTo validate the psycholinguistic relevance of these models, they should be refined and extended in several ways. For instance, the parsing/disambiguation algorithms should be made to operate incrementally; recency and discourse structure should be brought into the picture; and generation algorithms should be developed. \n\nThe most challenging question about the data-oriented perspective on human language processing concerns the issue of language acquisition. If we want to avoid stipulating an \"innate grammar\", we must show how a corpus with syntactic structures may gradually come into being, starting with an initial state in which a child only has a corpus with non-linguistic experiences. We must show, therefore, how syntactic structures get bootstrapped by a process which (1) projects semantic/pragmatic structures onto the word sequences that accompany them, and (2) invents syntactic categories by noting semantic/pragmatic and distributional correspondences between syntactic constituents. Describing this process in any degree of detail will obviously be very difficult. Nevertheless, it constitutes a more promising research agenda",
            "score": 0.4089594321096338,
            "section_title": "Conclusion",
            "char_start_offset": 60227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "264591406",
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "text": "Inspired by cognitive science research, this paper investigates whether specific functional regions exist within LLMs. We identify a core region specifically responsible for language processing within LLMs. This region occupies only about 1% of the model's parameters but plays a crucial role in maintaining the overall linguistic competence of the model. Invalid changes in the parameters of this region can severely impair the model's linguistic competence. We also observe a pronounced dimension dependence in the core region of linguistic competence. Surprisingly, in a large model like LLaMA-13B, which boasts 13 billion parameters, altering just one parameter could potentially inflict significant damage to its linguistic competence. This study further elucidates the relationship between linguistic competence and knowledge in large language models. We find that an improvement in linguistic competence does not necessarily imply an enhancement in knowledge level. This suggests the presence of a knowledge storage region in LLMs that operates independently of language processing. In summary, the findings of this paper shed new light on how the capabilities and knowledge are structured in large language models and help explain why the pre-training and fine-tuning processes of these large models differ significantly from their smaller predecessors.",
            "score": 0.4084351913154989,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 25015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1361
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "258841873",
            "title": "GATology for Linguistics: What Syntactic Dependencies It Knows",
            "text": "Linguistic knowledge can often be modeled and represented on graphs in natural language processing tasks, e.g., semantic and syntactic information. \n\nGAT is a graph neural network that uses an attention mechanism to create a graph across a spatial domain. This mechanism aggregates data from surrounding nodes and determines the relative importance of neighbors to provide new features for each node. It has attracted much interest since it can be used with inductive and transductive learning (Salehi and Davulcu, 2019;Busbridge et al., 2019). So far, most work has focused only on applying syntactic knowledge by GAT in downstream tasks. \n\nIt is unclear how it represents syntactic knowledge and how model structures, e.g., model layers and attention heads, contribute to syntactic knowledge learning. Also, given that GAT can represent explicit linguistic knowledge in different downstream tasks, its integration with the pre-trained model BERT has attracted the most research focus. (Huang et al., 2020) inject syntactic cognitive knowledge into the model using GAT representation of syntactic knowledge and BERT pre-trained knowledge, which results in better interaction between context and aspectual words. While employing BERT to obtain representations of emotions and contexts, (Li et al., 2021) use GAT to gather structural data about contexts in the span-level emotion cause analysis task. (Ma et al., 2020) use graph features and word embeddings to model and represent linguistic knowledge to classify the comparative preference between two given entities. (Brody et al., 2021) proposes new dynamic attention in GAT but lacks tests of linguistic knowledge. How GAT and BERT interact regarding syntactic knowledge is still being determined, although combining them into downstream tasks can improve performance. Most of the studies have concentrated on discussing and exploring linguistic knowledge in BERT (Clark et al., 2019;Papadimitriou et al., 2021a), while the representation of such knowledge in GAT remains unclear. Although some works try to use syntactic knowledge for MT tasks (Peng et al., 2021;Mc-Donald and Chiang, 2021b), they do not discuss the possibilities of GAT.",
            "score": 0.40840756139661294,
            "section_title": "Related Work",
            "char_start_offset": 5033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 987,
                    "end": 1007,
                    "matchedPaperCorpusId": "227231731"
                },
                {
                    "start": 1286,
                    "end": 1303,
                    "matchedPaperCorpusId": "240230780"
                },
                {
                    "start": 1400,
                    "end": 1417,
                    "matchedPaperCorpusId": "220047268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "236478283",
            "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
            "text": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.",
            "score": 0.40839932213322183,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "277740729",
            "title": "Playpen: An Environment for Exploring Learning Through Conversational Interaction",
            "text": "It is hard to deny that modern Large Language Models (LLMs) pick up some form of language competence from their extensive training on what mostly is observational data: found text, in which the actual next token is to be predicted. It is also hard to shake the feeling that there is something unsatisfactory about it: The amount of observations needed appears to be orders of magnitude larger than what human language learners require (Warstadt & Bowman, 2022); and the mode of acquisition seems to be very different as well, with human language learners undergoing a developmental stage first where they use language, long before they observe it in large quantities (through reading) (Rvachew & Savage, 2006). Increasingly, the training of LLMs now does indeed contain interactive elements as well (Kumar et al., 2025), where some additional component provides feedback on larger productions (e.g., attempts at instruction following; steps in a reasoning chain). The current methods, however, either require human preference annotation (rankings of prompt or response pairs) or only work in formal domains (reasoning verifiers). \n\nHumans acquire language primarily through social interaction where input data is multimodal (auditory, visual, tactile), contextualized, and can be accompanied by non-verbal cues (facial expressions, gestures), which has often been stressed in human developmental literature (Clark, 2016). Often, this interaction specifically is playful interaction, as remarked by Bruner & Watson (1983): \"like the word games made famous by Ludwig Wittgenstein in his Philosophical Investigations, each of the games that are played by children and their parents is a self-contained 'form of life' as well\" ... \"a game is virtually syntactic: its object is to be well formed. It is, moreover, completely conventional and 'non-natural'.\" Current LLMs achieve remarkable language-related capabilities but require exponentially more linguistic input compared to humans, highlighting inefficiencies in their learning process. In this work, we explore methods that enable learning through interaction, where the learner actively influences the linguistic material it learns from, rather than relying solely on passive observation.",
            "score": 0.40784630172319725,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 460,
                    "matchedPaperCorpusId": "251623202"
                },
                {
                    "start": 685,
                    "end": 709,
                    "matchedPaperCorpusId": "29048900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64697265625
        },
        {
            "corpus_id": "17560213",
            "title": "Reusing a Statistical Language Model for Generation",
            "text": "The first stage translates logical form relations and features to corresponding syntactic terms, referring to lexical entries where necessary. For example, the semantic representation: run (+past) actor: John manner: quickly is translated unit-by-unit to a verb phrase, a subject noun phrase, and an adverbial phrase modifier. The syntactic units are linked in a graph structure, but with no ordering constraints between them. \n\nIn the current prototype system, this stage in fact enforces a one-to-one mapping of semantic to syntactic features, though there is clear scope for extending the use of the statistical language model to direct the translation here, and to allow for a one-to-many mapping to be ranked subsequently. The analysis grammar builds logical forms compositionally, with individual grammar rules fully specifying their semantic contributions, but at present these specifications are not automatically extracted for the simplified generation grammar, and the language model is not yet fully sensitive to the semantic features in the rules. The extent to which this mapping stage can be automated is currently being investigated.",
            "score": 0.4075197591693226,
            "section_title": "Semantic to syntactic mapping",
            "char_start_offset": 7942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "257057439",
            "title": "The role of Function Words to build syntactic knowledge in French-speaking children",
            "text": "The question of how children learn Function Words (FWs) to build syntactic knowledge is a central issue in developmental psycholinguistics and in the applied fields of education, speech-language pathology and early intervention. Since seminal work of Bloom 1 , Brown 2 , Braine 3 , and Maratsos 4 , this issue was modelled in several theories (see Ambridge and Lieven 5 and Ambridge 6 for a review). Theories of language acquisition disagree mostly on whether infants come to this process equipped with some language-specific innate predisposition [7][8][9] or whether general learning abilities can account for it [10][11][12][13][14] . According to the general learning theories, infants are thought to grasp the statistical regularities and transitional probabilities present in the input they receive, with the help of a critical social learning ability; the outcome of this process of abstraction is semantically specified schemas, related to each kind of meaningful situation. The nativist approach, on the contrary, insists on the innovative linguistic forms and utterances produced by infants, their rule-like patterns of errors and the lack of negative feedback in the input they get. \n\nLeaving aside the nativist vs. emergentist debate, the analyses presented, in this study, are in line with the so-called \"syntactic connectivity\" account according to which children would learn grammatical words as an interconnected system based on early rather than late structure building (e.g., Ninio 15 ). \n\nHerein, we challenge the lexicalist approach or usage-based account, which assumes that children begin to produce multiword utterances before they have any grammatical knowledge (late structure building hypothesis). We postulate that acquisition of FWs and morphemes bootstraps syntactic development (for more detail on the early structure building hypothesis, see Le Normand, Ninio, Szagun [16][17][18] ). According to this latter view, FWs are considered as linguistic cues to the grammatical category of other words. Exploring whether FWs may be grammatically functional early in the acquisition of language is also in line with the bottom-up driven hypothesis.",
            "score": 0.4074544826436208,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 279,
                    "matchedPaperCorpusId": "144261489"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "171054440"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "205117494"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "27013606"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "141526380"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "150078077"
                },
                {
                    "start": 1903,
                    "end": 1907,
                    "matchedPaperCorpusId": "148989881"
                },
                {
                    "start": 1907,
                    "end": 1911,
                    "matchedPaperCorpusId": "149912553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2193603515625
        },
        {
            "corpus_id": "7588509",
            "title": "A Probabilistic Generative Grammar for Semantic Parsing",
            "text": "There is significant room for future work and exploration in the subject presented in this manuscript. In this section, we discuss shortcomings of various aspects of our approach, and give suggestions for how to overcome them. \n\nThe performance of our parser and generator depend heavily on the production rules of the grammar. Although the preterminal production rules are induced during training, we had to specify the other production rules by hand. While this does give us a great deal of control over the grammar, and enables us to incorporate prior knowledge about the English language into the grammar, it is very time-consuming. It would be valuable to look into ways in which these production rules can be induced from data. Recall that every production rule in our grammar is annotated with semantic transformation functions. These functions are intimately tied with the semantic formalism and effectively implement a theory of formal semantics. It would also be valuable to explore whether these transformation functions can be learned as well. One promising direction would be to decompose the semantic transformation functions into a sequence of elementary \"instructions.\" Each semantic transformation function could then be equivalently written as short programs in a simple programming language. We could then induce the semantic transformation functions by searching over the space of these short programs, perhaps by attempting to add or remove instructions, etc. However, it is not clear how much grammar induction would improve our current grammar for English. But such an approach would certainly help to learn grammar for other languages, about which we have much less knowledge. The statistical efficiency of our approach could greatly aid in natural language processing for low-resource languages, for which training data is very scarce. \n\nDuring parsing, our method uses an upper bound on the objective function (as defined in equations 43, 44, and 45) that takes into account syntactic information. While this works well enough for our purposes, it may be possible to further improve the performance of the parser by defining tighter upper bound, possibly by taking into account semantic information. \n\nOur semantic parsing model assumes that the sentences are noise-less: there are no spelling or grammatical errors in the utterances. This assumption helps to simplify the problem and to focus the scope of the thesis more onto language understanding and reasoning. But real-world language is noisy, and thus further work to extend the semantic parsing model to noisy settings is warranted.",
            "score": 0.40739857768679577,
            "section_title": "Future work",
            "char_start_offset": 65692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2225
                },
                {
                    "start": 2228,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2491
                },
                {
                    "start": 2492,
                    "end": 2616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "258999187",
            "title": "Birth of a Transformer: A Memory Viewpoint",
            "text": "As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors. \n\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [5]. These \"in-context\" learning capabilities are often attributed to the transformer architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require \"global\" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model. \n\nIn order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head mechanism [14,36], namely a \"circuit\" of two attention heads that allows the transformer to predict b from a context [\u2022 \u2022 \u2022 , a, b, \u2022 \u2022 \u2022 , a], and which appears to be ubiquitous in transformer language models [36,48]. \n\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13,29,40].",
            "score": 0.40720614748052253,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1777,
                    "end": 1780,
                    "matchedPaperCorpusId": "253244237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "261530292",
            "title": "Explainability for Large Language Models: A Survey",
            "text": "The basic idea behind classifier-based probing is to train a shallow classifier on top of the pre-trained or fine-tuned language models such as BERT (Devlin et al., 2019b), T5 (Raffel et al., 2020). To perform probing, the parameters of the pre-trained models are first frozen, and the model generates representations for input words, phrases, or sentences and learns parameters like attention weights. These representations and model parameters are fed into a probe classifier, whose task is to identify certain linguistic properties or reasoning abilities acquired by the model. Once the probe is trained, it will be evaluated on a holdout dataset. The labeled data comes from available taggers or gold-annotated datasets. \n\nAlthough each probe classifier is often tailored for a certain task, the scheme for training classifiers to probe different knowledge is consistent. Related studies will be presented according to probed model components, i.e. vector representations and model parameters. \n\nWe first examine works that study vector representations to measure embedded knowledge. In this category, knowledge means either syntax knowledge at a low level or semantic knowledge at a high level. Studies demonstrate that lower layers are more predictive of word-level syntax, whereas higher layers are more capable of capturing sentence-level syntax and semantic knowledge (Belinkov et al., 2017;Peters et al., 2018;Blevins et al., 2018;Jawahar et al., 2019). \n\nSyntactic labels can be further categorized into word-or sentence-level categories. The word-level syntactic labels provide information about each individual word, such as part-of-speech tags, morphological tags, smallest phrase constituent tags, etc. The sentence-level syntactic labels describe attributes of the whole sentence, such as voice (active or passive), tense (past, present, future), and top-level syntactic sequence. \n\nFor word-level syntax probing, parse trees are often introduced by dependency parser (Dozat & Manning, 2017) to help extract dependency relations (Tenney et al., 2019b).",
            "score": 0.40715097587596805,
            "section_title": "Classifier-Based Probing",
            "char_start_offset": 27014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 171,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 176,
                    "end": 197,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1377,
                    "end": 1400,
                    "matchedPaperCorpusId": "24544277"
                },
                {
                    "start": 1400,
                    "end": 1420,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 1420,
                    "end": 1441,
                    "matchedPaperCorpusId": "21663989"
                },
                {
                    "start": 1441,
                    "end": 1462,
                    "matchedPaperCorpusId": "195477534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "238222411",
            "title": "Processing Verb Meanings and the Declarative/Procedural Model: A Developmental Study",
            "text": "Indeed, researchers have shown that contextual information facilitates the semantic processing of incoming words (Kuperberg and Jaeger, 2016). This facilitation is explained by a predictive preactivation of upcoming information thanks to contextual information. Thus, there is a need to clarify the roles of the declarative and procedural memory systems during language comprehension. A solution may lie in the interaction of the two systems, as Ullman (2020) outlined. This idea is consistent with the dual-path model (Chang et al., 2006), which was proposed to describe sentence production but can be extended to sentence comprehension. This model considers that prediction occurs during language processing in that upcoming words are predicted, using the meaning of the immediately preceding word as input. This conceptualization is similar to the D/P Model since two pathways are described: a meaning system and a sequencing system. In other words, the functions attributed to these two systems are similar to the roles attributed to declarative and procedural memory, respectively. Interestingly, the dual-path model adds to the D/P Model since it considers how the systems work together. The sequencing system implicitly learns the syntactic structure of language but, in so doing, it also acquires semantic information. Specifically, it learns the thematic roles of verbs and semantic information concerning their agents and patients and can predict what words are allowed in the position N + 1. For instance, the sequencing system learns what kinds of entities can be arrested or accused and predicts that the verbs arrested or accused should be followed by a word referring to a human being rather than an inanimate object. Similar frameworks have been proposed in the field of language acquisition (Alishahi and Stevenson, 2010). Thus, the dual-path model could explain how semantic knowledge can contribute to processing, which is assumed to depend on procedural memory. Our view of the role of procedural memory agrees with an idea of Chang et al. (2006) that procedural memory determines which kinds of patients are allowed after a given verb and which are not. \n\nThe purpose of this study is to show that semantic knowledge contributes, in both children and adults, to the kind of processing that is supposed to rely on procedural memory. More specifically, we studied the patient typicality effect from a developmental perspective.",
            "score": 0.40700251661835074,
            "section_title": "INTRODUCTION",
            "char_start_offset": 10397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2174
                },
                {
                    "start": 2177,
                    "end": 2352
                },
                {
                    "start": 2353,
                    "end": 2446
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 141,
                    "matchedPaperCorpusId": "451348"
                },
                {
                    "start": 446,
                    "end": 459,
                    "matchedPaperCorpusId": "18897427"
                },
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "1237448"
                },
                {
                    "start": 1808,
                    "end": 1838,
                    "matchedPaperCorpusId": "12738653"
                },
                {
                    "start": 2047,
                    "end": 2066,
                    "matchedPaperCorpusId": "1237448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "268537136",
            "title": "Computational Models to Study Language Processing in the Human Brain: A Survey",
            "text": "Besides semantics, large language models capture syntactic and morphological aspects, with ongoing studies aiming to distinguish these features in brain processing [84,10,85].Caucheteux et al. (2021) [86] developed an embedding taxonomy for large language models to analyze brain activities in narrative listening.They found compositional representations engaged a broader cortical network, including the bilateral temporal, parietal, and prefrontal cortices, more extensively than lexical representations.Zhang et al. (2022) investigated the brain's syntactic processing using modified embeddings, revealing its distributed nature across brain networks [12].\n\nAlthough large language model embeddings excel in aligning with brain activities, the underlying reasons and mechanisms of this synchrony are not fully understood [87,88].Research aims to clarify how these models mirror human linguistic comprehension and brain structures [89].Sun et al. (2020) found that impairing semantic processing in large language models diminishes their brain activity alignment [13].Conversely, Merlin and Toneva (2022) enhanced this alignment by focusing on nextword prediction and word-level semantics [82].Aw and Toneva (2023) showed that training models on narrative summarization enhances brain activity synchronization [90].Caucheteux et al. (2023) found that integrating multi-timescale predictions into large language models improves brain mapping.Hierarchically, frontoparietal cortices predict higher-level, longer-range representations compared to temporal cortices, emphasizing the role of hierarchical predictive coding in language processing [91].Further, task-specific tuning of large language models in NLP tasks has been shown to influence their brain pattern alignment [92,93,94], indicating that their semantic feature capture is a key factor in this alignment.",
            "score": 0.4068650848297814,
            "section_title": "Modelling linguistic Representations",
            "char_start_offset": 21828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "235390553"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "238232275"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "269447418"
                },
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "235436365"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "235436365"
                },
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "247863040"
                },
                {
                    "start": 824,
                    "end": 828,
                    "matchedPaperCorpusId": "247693544"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "222421597"
                },
                {
                    "start": 1195,
                    "end": 1215,
                    "matchedPaperCorpusId": "257255248"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "257255248"
                },
                {
                    "start": 1316,
                    "end": 1340,
                    "matchedPaperCorpusId": "257309545"
                },
                {
                    "start": 1642,
                    "end": 1646,
                    "matchedPaperCorpusId": "257309545"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "260861189"
                },
                {
                    "start": 1777,
                    "end": 1780,
                    "matchedPaperCorpusId": "263828913"
                },
                {
                    "start": 1780,
                    "end": 1783,
                    "matchedPaperCorpusId": "248505919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "268513036",
            "title": "Rules still work for Open Information Extraction",
            "text": "Linguistic grammar-based models emphasize the pivotal role of linguistic grammar in model design.During the early stages of OIE modeling, the limited availability of annotated data and robust machine-learning techniques led to the emergence of linguistic grammar-oriented approaches as a prevalent option.There are various external forms of this model, including rule-based models, which directly design extraction rules depending on the linguistic grammar, and clause-based models, which usually reshape the input sentence structure according to the linguistic grammar and make it easier to conduct the extraction procedures.\n\nRule-based models operate based on predefined extraction rules.These rules primarily rely on manual design and require substantial linguistic knowledge.Several kinds of linguistic information can be used to construct extraction rules.Initial models utilized more basic syntactic cues like POS tags or noun phrase fragments [14,15,16].\n\nSubsequent models employed deeper syntactic information, such as constituency information [17] and dependency structures [18,19,20,21].In addition to syntactic information, some models also leverage semantic information to define extraction rules [22,23,24].To improve the performance, clause-based models [25,26] incorporate a sentence transforming processing into the whole modes.The additional processing changes the complex input sentences into simple ones, thereby improving the accuracy of the models.\n\nThe linguistic grammar-oriented models are easy to implement and have low resource demands.Usually, they are characterized by their straightforwardness in understanding what these models can extract, what they cannot, and why erroneous extractions might occur.Nevertheless, there are some drawbacks to this kind of model.\n\nThey are very time-consuming, which makes the models unable to be fully designed to deal with the vast array of linguistic nuances and variations.Consequently, the performance of the models might decline when dealing with unconventional language patterns or evolving language use.Additionally, an inherent challenge lies in the propagation of errors from natural language processing systems, such as POS taggers and dependency parsing tools, which can negatively impact the accuracy of the models.",
            "score": 0.4065827657344078,
            "section_title": "Linguistic grammar-oriented models",
            "char_start_offset": 3939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 626
                },
                {
                    "start": 628,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 962
                },
                {
                    "start": 964,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1346
                },
                {
                    "start": 1346,
                    "end": 1471
                },
                {
                    "start": 1473,
                    "end": 1564
                },
                {
                    "start": 1564,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1794
                },
                {
                    "start": 1796,
                    "end": 1942
                },
                {
                    "start": 1942,
                    "end": 2076
                },
                {
                    "start": 2076,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "10318045"
                },
                {
                    "start": 955,
                    "end": 958,
                    "matchedPaperCorpusId": "2986730"
                },
                {
                    "start": 958,
                    "end": 961,
                    "matchedPaperCorpusId": "215989853"
                },
                {
                    "start": 1054,
                    "end": 1058,
                    "matchedPaperCorpusId": "2077889"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "4699390"
                },
                {
                    "start": 1089,
                    "end": 1092,
                    "matchedPaperCorpusId": "742960"
                },
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "3409061"
                },
                {
                    "start": 1095,
                    "end": 1098,
                    "matchedPaperCorpusId": "226262315"
                },
                {
                    "start": 1211,
                    "end": 1215,
                    "matchedPaperCorpusId": "15188996"
                },
                {
                    "start": 1215,
                    "end": 1218,
                    "matchedPaperCorpusId": "2556949"
                },
                {
                    "start": 1218,
                    "end": 1221,
                    "matchedPaperCorpusId": "51875609"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "6015236"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "246871423",
            "title": "Models of Language and Multiword Expressions",
            "text": "However, other recent work suggests an alternative in item-and pattern-based prediction. For example, Schrimpf et al. (2021) found that language models trained to predict the next lexical item in a sequence have an almost perfect fit to neural activity during sentence processing. Crucially, the best performance is achieved by transformer models such as GPT-2 (Radford et al., 2019) which does not use rules and words, but instead relies exclusively on the transitional probabilities between lexical items and can be characterized as storing exemplar information of the trained sequences and performing shallow abstractions over them to extract patterns (Ambridge, 2020b;McClelland, 2020) that are then used in predictions. Intriguingly, whereas the state-of-the-art transformer model, GPT-3 (Brown et al., 2020) can be fooled into producing sentences that are factually incorrect or semantically odd, it almost exclusively produces grammatically correct sentences. More work needs to be done on conjoining these two possible mechanisms, and other candidates, with the evidence for the ubiquity of familiarity with multiword expressions to go beyond mere compatibility and develop a more fully rounded proposal of processing and acquisition that eschews the assumption stemming from theories based on words and rules. \n\nNevertheless, even with all these difficulties, the perspective on language that emerges inspired by the prevalence of multiword expressions is a promising avenue of research that eschews the core assumptions of the traditional bipartite viewpoint of the words-and-rules approach. Instead, it offers a view of language that is rooted in our general cognitive capacities and a developmentally plausible account of how linguistic knowledge can be acquired and perfected. And, even more importantly, it highlights the actual patterns of language use instead of an imagined, but never realized, idealized capacity for language.",
            "score": 0.4065064406629768,
            "section_title": "CONCLUSION",
            "char_start_offset": 59069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1944
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 124,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 361,
                    "end": 383,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 655,
                    "end": 672,
                    "matchedPaperCorpusId": "222111194"
                },
                {
                    "start": 672,
                    "end": 689,
                    "matchedPaperCorpusId": "213377364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "269791149",
            "title": "DocuMint: Docstring Generation for Python using Small Language Models",
            "text": "In this section, we introduce the various training phases of a language model and the tasks that it is trained to perform.",
            "score": 0.4064586197702927,
            "section_title": "Preliminaries",
            "char_start_offset": 6607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "266348523",
            "title": "Review of Unsupervised POS Tagging and Its Implications on Language Acquisition",
            "text": "A foundational goal for the work of language acquisition research is identifying the learning goals children must achieve and uncovering the computations that contribute to their success while learning language. The learning goal investigated in this work is syntactic category induction. \n\nSyntactic categories, groupings of words like nouns or verbs, are essential in understanding the syntactic structure of a language, and learning these groupings allows a child to generalize their knowledge of language to novel instances. For example, understanding that the words \"cat\" and \"penguin\" belong to the same category allows a learner to generalize the word \"penguin\" to novel contexts where \"cat\" appears: if one can say \"the cat eats\" then \"the penguin eats\" may also be a part of the language. Fundamentally, the linguistic principle of productivity is at stake in syntactic category learning. Productivity is the principle of creativity and generalizability in human language where speakers can employ their linguistic knowledge to produce and understand both words in novel contexts and sentences that have never been uttered. Illustrating the power of productivity in language, Wilhelm von Humboldt famously remarked that language makes \"infinite use of finite means\" (Chomsky 1965quoting von Humboldt 1836). Acquiring syntactic categories is one of the first steps in leveraging the incredible expressiveness in the structure of human language as these groupings are necessary for building novel structures that follow the rules of the language. \n\nIn what follows, we will begin by more precisely defining syntactic categories which will help motivate why children might care about them and how we know that they do care. We will then investigate how children could learn these categories by reviewing computational approaches which implement potential mental computations -ultimately, leading to the popular acquisition model of Frequent Frames (Mintz, 2003). To better understand the breadth of possible computations that could be applied to this learning problem and in particular, the ones children might apply, we will then review some engineering approaches to unsupervised part of speech tagging. This engineering task is similar to the problem children are solving during syntactic category induction because the engineering models are trying to infer structural categories text alone. But, we will also accompany this comparison with a discussion of how the research goals of acquisition models for syntactic category induction differs from the goals of part of speech tagging.",
            "score": 0.4064115539666351,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2211
                },
                {
                    "start": 2212,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2594
                }
            ],
            "ref_mentions": [
                {
                    "start": 1954,
                    "end": 1967,
                    "matchedPaperCorpusId": "6345319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "260438429",
            "title": "Curricular Transfer Learning for Sentence Encoded Tasks",
            "text": "As posed by (Bengio et al., 2009), a curriculum approach has a beautiful interpretation and practical application for both perspectives. In our approach, we teach a model in the same way humans learn by gradually increasing the task's difficulty, so it first acquires the basic skills to thrive in more complex scenarios. Also, relying on this curriculum approach, we might minimize a less noisy version of the original problem to arrive at the global optima, a continuation method. \n\nFor human language acquisition, the generative grammar theory assumes that a learner has an innate universal grammar that restricts what kind of grammar a learner could acquire (White, 2003). The process starts by recognizing simple structures in a grammar; instead of memorizing, the learner identifies syntactic structures they encounter and evaluates the feedback in an environment to determine precisely the grammar being used (Guasti, 2004). \n\nWhereas most of a child's primary language is obtained culturally, without directed control, for second language acquisition the process is almost entirely performed in a controlled environment. In the initial stages, the learner recognizes simple grammatical structures; which is crucial for complete grammar acquisition, as many possible candidate grammars are filtered in this search step (Komarova et al., 2001). By exploiting this universal grammar, the learner could quickly acquire the grammar of the language it is inserted. \n\nSeveral approaches in Artificial Intelligence (AI) explore the advantages of posing easier instances of the problem or addressing sequentially surrogate objectives to achieve more complex goals. In RL, robotic hand manipulation is an example. Instead of directly training the robot to put a red box over a blue box, it is easier first to teach the arm to recognize which color to pick, then how to hold the box correctly, and later how to place it over the blue box (Manela and Biess, 2022). This decomposition of complexity allows agents to learn the final task faster than directly attempting to perform the final task. \n\nIn the context of sequence-to-sequence modeling, we can view the problem as a grammar acquisition process, starting from basic construction to then acquiring more complex ones. In our developed solution, we first present the learner with basic grammar with a more simple composition, to later extend for more complex elements (Davies, 1980;Guasti, 2004).",
            "score": 0.4062409167293407,
            "section_title": "Grammar Acquisition with Curricular Transfer Learning",
            "char_start_offset": 13198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2090
                },
                {
                    "start": 2093,
                    "end": 2269
                },
                {
                    "start": 2270,
                    "end": 2447
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 33,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 916,
                    "end": 930,
                    "matchedPaperCorpusId": "215745290"
                },
                {
                    "start": 1326,
                    "end": 1349,
                    "matchedPaperCorpusId": "15685577"
                },
                {
                    "start": 1935,
                    "end": 1959,
                    "matchedPaperCorpusId": "221246278"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "263780995",
            "title": "From task structures to world models: what do LLMs know?",
            "text": "We can understand the instrumental knowledge of a system in terms of its ability to perform tasks posed for it across relevant domains. Indeed, a motivating perspective on large language models is the idea of unsupervised multitask learning 19 . Internet-scale natural language data can be seen as a large dataset of a multitude of tasks, posed in varying ways and forms, consistent with the messiness of how language is used naturally. For instance, the abbreviation \"TL;DR\" or a paragraph that starts with the phrase \"In summary, \u2026\" might signal a summarization-like task; nearby or paired sentences or phrases spanning multiple languages might suggest the task of translation between those languages. Radford et al. (2019)  19 speculated that for a model to accurately predict the next word in a sequence, it may be critical for the model to spontaneously infer the task structure from the preceding context, and condition the next word predictions on that task structure. After the training of an LLM is over, inferring such task structure from natural language, and conditioning the activations within the model according to this structure, is a possible source of instrumental knowledge (Fig. 1A, B). \n\nCould instrumental knowledge occur without (or with very little) worldly knowledge? A plausible example of such a scenario is machine language translation. Instead of focusing on building systems that translate through semantic analyzers or any other formal notion of meaning, most progress in machine translation relies on increasingly sophisticated statistical approaches. 20,21  is plausible that LLMs represent a new frontier in this progression of models, one in which the models infer the task structure of language translation, in terms of how words, phrases, and even paragraphs are emitted within and across pairs of languages, and use this structure to translate -without necessarily projecting a common knowledge of the physical world across different languages. Such a possibility is further suggested by \"relational\" theories of word meaning (e.g., such as a conceptual role semantics, where the meaning of a word or phrase is defined directly by its relation to other words or concepts, and only indirectly through reference and causal connections to the nonlinguistic world, with limited transmission of worldly content) 22 . \n\nRelatedly, some have asked whether LLM \"knowledge\" is merely an ability to follow linguistic rules and language patterns.",
            "score": 0.4053910251731076,
            "section_title": "Instrumental knowledge",
            "char_start_offset": 2474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2349
                },
                {
                    "start": 2352,
                    "end": 2473
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 243,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 727,
                    "end": 729,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "6244213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "263310649",
            "title": "Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback",
            "text": "Recently, LLMs are emerged and boost many natural language processing tasks. The architecture of LLMs, particularly the Transformer [55], leads to significant improvements in capturing textual semantics. This advancement empowers many influential models such as BERT [17] and GPT [43]. These models pave the way for subsequent advancements like GPT-2 [44] and GPT-3 [7], with increasing model sizes and capabilities. The training pipeline of LLMs also earned significant attention in recent years due to its pivotal role in enabling models like GPT to exhibit remarkable language understanding and generation capabilities. Pre-training is a cornerstone of training LLMs and involves training the model on a massive corpus to learn linguistic patterns and structures, leveraging the tasks such as masked language modeling [17], next token prediction [43] and etc. By utilizing large-scale pre-training, LLMs acquire a general understanding of language, making them available for various downstream tasks. Supervised Fine-Tuning (SFT) involves training LLMs on task-specific datasets with labeled examples. This stage adapts the generic linguistic knowledge acquired during pre-training to specific tasks, such as sentiment analysis [20], text classification [19,21], and dialogues [41]. Alignment technique facilitates LLMs in learning from the generated responses and environmental feedback, thereby aligning the capability with the desired attribute. The environment feedback could be from human [41] or other models [2]. This approach has shown promise in improving the helpfulness and harmlessness of LLMs.",
            "score": 0.4053861015047501,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "233236941"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "235792270"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "277065732",
            "title": "A Survey on Federated Fine-tuning of Large Language Models",
            "text": "The training of LLMs typically involves two distinct stages (Xin et al., 2024): pre-training and fine-tuning, as shown in Figure 4. 1) Pre-training involves training the model on extensive, publicly available corpora, such as text from books and web pages. During this phase, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts. \n\n2) Fine-tuning, the second stage, tailors the pre-trained model to specific tasks or domains (Ding et al., 2023). In this phase, the model is further trained on task-specific datasets, typically using supervised learning techniques, to optimize its performance for particular applications such as text generation, sentiment analysis, or question answering. Fine-tuning refines the model's capabilities, enabling it to specialize its general language understanding to meet the requirements of the target task. However, traditional methods typically involve centralizing data from multiple sources on a single server for fine-tuning (Huang et al., 2025), which raises significant privacy concerns. This challenge highlights the need for innovative fine-tuning techniques that can effectively preserve data privacy.",
            "score": 0.40527966033469653,
            "section_title": "Training of LLMs",
            "char_start_offset": 9866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1780
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 623,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1061,
                    "end": 1080,
                    "matchedPaperCorpusId": "257316425"
                },
                {
                    "start": 1599,
                    "end": 1619,
                    "matchedPaperCorpusId": "265067168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "11413146",
            "title": "Interconnected growing self-organizing maps for auditory and semantic acquisition modeling",
            "text": "During language acquisition, children receive various kinds of information through their interactions with communication partners and the surrounding environment. In this process, children are presented with information from different channels simultaneously, including auditory information (which can be acquired through auditory feedback), somatosensory information (which can be acquired through tactile feedback from their articulators) and semantic information (which can be abstracted from visual feedback, tactile feedback, olfactory feedback, etc.). Children face the task of acquiring information and organizing it into the appropriate linguistic categories. However, children do not receive explicit language instruction, nor are they able to make inquiries about the structures that they are learning (Gauthier et al., 2007a). Instead, they must discover the linguistic categories of their native language through their interactions with communication partners. This task is further complicated by the fact that they do not know how many categories there are to discover along any particular input dimension (Gauthier et al., 2007a). Language acquisition, for children, is a combination of statistical learning and (social) communicative learning in which they combine pattern detection and computational abilities with special social skills (Kuhl, 2004(Kuhl, , 2010)). In the present paper, we propose a feasible approach that is able to explain the acquisition of auditory categories, semantic categories and the associations between auditory and semantic information. \n\nFrom the perspective of modeling, language acquisition can be abstracted as a knowledge acquisition process. Among various kinds of knowledge acquisition algorithms, Kohonen (1982Kohonen ( , 1990Kohonen ( , 2001Kohonen ( , 2013) ) introduced the idea of self-organizing neural networks, known as the Self-Organizing Map (SOM), which has the ability to project high-dimensional data onto a low-dimensional feature map. Its highly visual nature (especially in the case of twodimensional feature maps) enables analysts to gain an overview of the underlying category structures of a data set. Ritter and Kohonen (1989) were among those who first applied the SOM algorithm to linguistic tasks. Their research on semantic modeling revealed that SOM has the ability to detect the \"logical similarity\" between words and group similar words into clusters.",
            "score": 0.4050699664392641,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2272
                },
                {
                    "start": 2273,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 812,
                    "end": 836,
                    "matchedPaperCorpusId": "853803"
                },
                {
                    "start": 1119,
                    "end": 1143,
                    "matchedPaperCorpusId": "853803"
                },
                {
                    "start": 1353,
                    "end": 1364,
                    "matchedPaperCorpusId": "205500033"
                },
                {
                    "start": 1364,
                    "end": 1379,
                    "matchedPaperCorpusId": "207215937"
                },
                {
                    "start": 1750,
                    "end": 1763,
                    "matchedPaperCorpusId": "206775459"
                },
                {
                    "start": 1763,
                    "end": 1779,
                    "matchedPaperCorpusId": "9348814"
                },
                {
                    "start": 1795,
                    "end": 1814,
                    "matchedPaperCorpusId": "17289060"
                },
                {
                    "start": 2173,
                    "end": 2198,
                    "matchedPaperCorpusId": "10189623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "248693088",
            "title": "Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling",
            "text": "We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed improves perplexity and other metrics. Moreover, this architecture provides a new way to compare different frameworks of linguistic representation. In our oracle graph setup, training and evaluating on English WSJ, semantic constituency structures prove most useful to language modeling performance\u2014outpacing syntactic constituency structures as well as syntactic and semantic dependency structures.",
            "score": 0.4050555201671794,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "256627634",
            "title": "CALaMo: a Constructionist Assessment of Language Models",
            "text": "The framework can be used to observe how the acquisition process unfolds over time. We can in fact set a number of steps n and observe: (i) how the shape of grammar changes over the course of learning, comparing the various steps, as in: \n\nii) how the grammar of the input can be compared to that acquired by the speaker, as in: \u039b(I) \u223c \u039b(o n ). Given a subset K \u2286 \u039b(I) 1 of interesting constructions, we can observe their behaviour over the learning process. \n\nA popular constructionist hypothesis (Goldberg, 2006), for example, states that the meaning of a construction (e.g., the ditransitive pattern Subj V Obj Obj2), and therefore its productivity, emerges from the association with specific lexical items in the input received by the learner (e.g., give in the case of the ditransitive): part of the lexical meaning remains attached to the meaning of the syntactic pattern, and therefore its distributional properties with it. Let's assume that the speaker has acquired construction \u03ba (e.g., the ditransitive construction). Once they're able to use it in a productive and creative way (i.e., in a more varied contexts than the give contexts the construction is strongly associated with in the input), we can use the proposed framework to check whether the distributional meaning of two constructions \u03ba i , \u03ba j \u2208 \u039b(I) with c(\u03ba i , \u03ba j ) = 1 (i.e., with \u03ba i being a less abstract instance of \u03ba j ) influences the learnability of \u03ba j as an independent construction. \n\nThe notion of abstraction chain introduced before helps us testing this hypothesis as we can check the behaviour of the chain (\u03ba i , \u03ba j ) at each timestep. We can denote \u03ba \u03bb k i the construction \u03ba i \u2208 \u03bb k and similarly \u03ba \u03bb k j the construction \u03ba j \u2208 \u03bb k , through distributional analysis we can capture how the contexts in which \u03ba i and \u03ba j vary, and whether this variation is associated with grammatical generalization. We expect, in fact, d(\u03ba i , \u03ba j ) to increase during acquisition: \n\nIf \u03ba j is produced in contexts that do not perfectly",
            "score": 0.40490498054881785,
            "section_title": "Individual acquisition over time",
            "char_start_offset": 22485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2012
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "25422730",
            "title": "Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task",
            "text": "This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009;Bender et al., 2011;Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K\u00e1d\u00e1r et al., 2016;Li et al., 2015) and what linguistic information is encoded in models' produced representations (Adi et al., 2016;Ettinger et al., 2016). Our aim in organizing this work-1 During a panel at the 1st Workshop on Representation Learning for NLP (ACL 2016; https://sites. google.com/site/repl4nlp2016/) some panelists acknowledged the fact that they could probably break any NLP system with very little effort-meaning it shouldn't be hard to invent reasonable examples that would confuse the systems. \n\nshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems' linguistic capacities, and welcoming further related research contributions to stimulate additional discussion. \n\n3 Shared Task: Build It Break It, The Language Edition \n\nTo address the issues identified above, we developed a shared task inspired by the Build It Break It Fix It Contest2 and adapted for application to NLP. The shared task proceeded in three phases: a building phase, a breaking phase, and a scoring phase: \n\n1. In the first phase, \"builders\" take a designated NLP task and develop techniques to solve it. \n\n2. In the second phase, \"breakers\", having seen the output of the builders' systems on some development data, are tasked with constructing minimal-pair test cases intended to identify the boundaries of the systems' capabilities. \n\n3. In the third phase, builders run their systems on the newly created minimal pair test set and provide their predictions for scoring. \n\nBuilders are scored based how well their systems can withstand the attacks of breakers, and breakers are scored based on how well they can identify system boundaries.",
            "score": 0.4048078683320799,
            "section_title": "Motivation: Robust NLP Systems",
            "char_start_offset": 4185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 356,
                    "matchedPaperCorpusId": "9357023"
                },
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "17352617"
                },
                {
                    "start": 376,
                    "end": 397,
                    "matchedPaperCorpusId": "762228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "271916426",
            "title": "Clinical insights: A comprehensive review of language models in medicine",
            "text": "During pre-training, a model learns general language representations from large text corpora in an unsupervised manner. For example, BERT [25] predicts masked tokens, while GPT-2 [19] predicts the next token in a sequence. The fine-tuning phase then adapts these pre-trained models to specific tasks (e.g., classification, summarization) using labeled data. Although effective, this approach often demands substantial task-specific datasets, which can limit its scalability for diverse downstream applications. \n\nIn contrast, zero-shot learning enables models to generalize to unseen tasks without requiring task-specific fine-tuning. Instead, the model leverages its pre-trained knowledge and interprets carefully designed prompts to perform tasks directly. For example, a model in a zero-shot setting can respond to a prompt such as \"Summarize the key findings of this medical report\" without being explicitly trained on clinical summarization datasets. \n\nIn-context learning enhances the capabilities of these models by enabling them to solve tasks using information provided within the input prompt. Unlike fine-tuning, ICL does not require updating the model's weights. Instead, the model temporarily \"learns\" from examples in the prompt. For instance, the model can be provided with a series of medical cases that include patient symptoms, relevant background knowledge, and corresponding diagnoses. When presented with a new case, such as a patient with specific symptoms (e.g., coughing up phlegm and blood), the model can infer the most likely diagnosis based on the examples and background knowledge, without explicit task-specific fine-tuning [26]. \n\nNevertheless, challenges remain, as highlighted by Mahowald et al. [27], who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines.",
            "score": 0.4048053171953685,
            "section_title": "Language models",
            "char_start_offset": 4847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1654,
                    "end": 1658,
                    "matchedPaperCorpusId": "258741378"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "268551442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "247656607",
            "title": "Emergent Structures and Training Dynamics in Large Language Models",
            "text": "Recent research has complicated the picture of grammar learning presented in Sections 2, 2.2, and 2.3. Specifically, there have been two separate but related types of critique leveled at probing and grammar learning. First, specific to probing, researchers question whether probes really identify linguistic representations at all. Secondly, and more fundamentally, it is unclear to what degree language models even learn grammar.\n\nHall Maudslay and Cotterell (2021) suggest that semantic \"cues\" may contaminate syntax probes, making it difficult to evaluate their scores. By employing \"Jabberwocky probing\", where pseudowords with no lexical meaning replace the original components of the sentence in a way that preserves grammar, the authors discovered that performance of syntactic probes considerably dropped for large language models, calling into question whether syntactic probes actually isolate syntactic knowledge withing language models.\n\nA more fundamental issue for syntax learning in language models has been their performance when trained on perturbed or permuted data. Sinha et al. (2021) use a variety of word order permutations that preserve distributional information to isolate whether what language models learn is actually syntax. Word order has been assumed to be important not only for natural language understanding by humans but also by language models, particularly for learning syntax. Surprisingly then, word order appears to have less influence than one would expect on the downstream performance of language models and their performance on probing tasks. In part, the authors note that some syntax information can be acquired during fine-tuning to sufficiently answer tasks that require it. Moreover, in the context of syntax probes, the authors note that \"while natural word order is useful for at least some probing tasks, the distributional prior of randomized models alone is enough to achieve a reasonably high accuracy on syntax sensitive probing\". Furthermore, the results distinguish between parametric and non-parametric probes, where performance on the latter using randomization models degrades significantly. This degradation provides evidence that non-parametric probes are able to test for syntax learning in ways that parametric probes cannot. Similarly, O'Connor and Andreas (2021) use syntax-level perturbations and ablations to conclude that the information in context windows most useful to language models are local ordering statistics and content words, e.g. nouns, verbs",
            "score": 0.40474016251908973,
            "section_title": "Critique of Testing Methods",
            "char_start_offset": 18825,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1085,
                    "end": 1104,
                    "matchedPaperCorpusId": "233231592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "248791053",
            "title": "Toward understanding the communication in sperm whales",
            "text": "All human infants undergo similar stages during acquisition of language in their first years of life, regardless of the language in their environment. For example, the babbling period during which language-acquiring infants produce and repeat basic syllables (such as [da] or [ba]) or reduced handshapes and movements in sign languages (Petitto and Marentette, 1991) is a well-documented developmental stage during the first 6-13 months (Fagan, 2009). Another well-documented concept in language acquisition is the critical period: if children are deprived of primary linguistic inputs in their first years, acquisition is not complete, often resulting in severe linguistic impairments (Friedmann and Rusou, 2015). The study of the developmental stages in language acquisition has yielded insights into how humans learn to discretize the acoustic speech stream into mental units, analyze meaning, and in turn produce language. In human language, for example, syllables that are produced first during language acquisition (e.g. [ma] or [ba]) are also most common in the world's languages, most stable, and easiest to produce. Similarly, morphological and syntactic constructions that are acquired first are the most basic (Crain Thornton, 2012). \n\nThere are currently several known parallels in the developmental stages between human language and animal communication. Acquisition of birdsong in some species, for example, involves the presence of babbling as well as the critical period (Doupe and Kuhl, 1999). These parallels likely stem from common neural and genetic mechanisms behind human speech and animal vocalizations (Bolhuis et al., 2010;Musser et al., 2014). However, in cetacean research, existing data on the vocalizations of non-adult whales in their natural setting are limited. Continuous and longitudinal data acquisition capabilities are required to record vocalizations of calf-mother pairs and collect behavioral data on their interactions as calves mature. Such data will provide insights into the order of acquisition of coda types, leading to insights into the articulatory effort of the vocalization as well as identification of the most basic structural building blocks and their functions.",
            "score": 0.4045122476238309,
            "section_title": "Language acquisition",
            "char_start_offset": 39169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 366,
                    "matchedPaperCorpusId": "9812277"
                },
                {
                    "start": 437,
                    "end": 450,
                    "matchedPaperCorpusId": "41964944"
                },
                {
                    "start": 686,
                    "end": 713,
                    "matchedPaperCorpusId": "44264290"
                },
                {
                    "start": 1221,
                    "end": 1243,
                    "matchedPaperCorpusId": "34191173"
                },
                {
                    "start": 1487,
                    "end": 1509,
                    "matchedPaperCorpusId": "1505871"
                },
                {
                    "start": 1626,
                    "end": 1648,
                    "matchedPaperCorpusId": "924972"
                },
                {
                    "start": 1648,
                    "end": 1668,
                    "matchedPaperCorpusId": "24680314"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1650390625
        },
        {
            "corpus_id": "277634005",
            "title": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms",
            "text": "This process allows the model to capture statistical relationships between tokens, including real-world factual information present in the corpus. For example, when trained on a sentence like \"The president of the United States in 2024 is Biden\", the model learns to predict \"Biden\" based on the preceding context, increasing the probability (Biden | The president...is). Radford et al. (2019) noted that this training paradigm facilitates transfer to downstream tasks because tasks can be framed within the same probabilistic framework, e.g., ( | ). For instance, a factual question-answering task can be expressed as ( | ), leveraging the patterns learned during pre-training. \n\nLearning language modeling on fact-related corpora is essentially the process by which the model acquires knowledge, this aligns with the definition of knowledge in LLMs. Knowledge is an awareness of facts, and a fact can be expressed in various forms, such as question-answer pairs or sentences. For LLMs, knowledge is most commonly defined as the model possessing knowledge k if it can correctly answer the corresponding question (Wang et al., 2024a). In simpler terms, an LLM's knowledge is its capability to produce factual outputs for inputs related to the real world. This is a performancebased definition, focusing on the observable behavior of the model: as long as it demonstrates factual input-output mappings or ( | ), it is considered to possess knowledge. And this is exactly what language modeling accomplishes. \n\nWithin this framework, knowledge in LLMs is represented in a highly implicit manner. The model's knowledge capabilities are encoded and manifested entirely through language modeling. Abstracting away from architectural specifics, all knowledge is captured and leveraged as statistical associations between tokens, which are learned and stored within black-box parameters. We first informally summarize the current paradigm of how LLMs model knowledge as follows: \n\nCurrent LLM Knowledge Paradigm (informal) \n\nLLMs encode factual information into model weights through pre-training based on probabilistic language modeling, forming input-output mappings that statistically capture real-world relationships in the training data. Such paradigm can be described as a \"Probabilistic Language Modeling-based Paradigm.\"",
            "score": 0.4043402555170619,
            "section_title": "How do LLMs Model Knowledge: Background and Preliminaries",
            "char_start_offset": 3983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1971
                },
                {
                    "start": 1974,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 393,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1113,
                    "end": 1133,
                    "matchedPaperCorpusId": "271328900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "174798289",
            "title": "Like a Baby: Visually Situated Neural Language Acquisition",
            "text": "The theory of situated cognition postulates that a person's knowledge is inseparable from the physical or social context in which it is learned and used (Greeno and Moore, 1993). Similarly, Perceptual Symbol Systems theory holds that all of cognition, thought, language, reasoning, and memory, is grounded in perceptual features (Barsalou, 1999). Knowledge of language cannot be separated from its physical context, which allows words and sentences to be learned by grounding them in reference to objects or natural concepts on hand (see Roy and Reiter, 2005, for a review). Nor can knowledge of language be separated from its social context, where language is learned interactively through communicating with others to facilitate problem-solving. Simply put, language does not occur in a vacuum. \n\nYet, statistical language models, typically connectionist systems, are often trained in such a vacuum. Sequences of symbols, such as sentences or phrases composed of words in any language, such as English or German, are often fed into the model independently of any real-world context they might describe. In the classical language modeling framework, a model learns to predict a word based on a history of words it has seen so far. While these models learn a great deal of linguistic structure from these symbol sequences alone, acquiring the essence of basic syntax, it is highly unlikely that this approach can create models that acquire much in terms of semantics or pragmatics, which are integral to the human experience of language. How might one build neural language models that \"understand\" the semantic content held within the symbol sequences, of any language, presented to it? \n\nIn this paper, we take a small step towards a model that understands language as a human does by training a neural model jointly on corresponding linguistic and visual data. From an imagecaptioning dataset, we create a multi-lingual corpus where sentences are mapped to the real-world images they describe. We ask how adding such real-world context at training can improve language model performance.",
            "score": 0.40384942293234705,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 177,
                    "matchedPaperCorpusId": "2258750"
                },
                {
                    "start": 538,
                    "end": 558,
                    "matchedPaperCorpusId": "8106325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "256863867",
            "title": "Moving away from lexicalism in psycho- and neuro-linguistics",
            "text": "A non-lexicalist model of language production. \n\na series of temporally ordered articulatory gestures in order to be uttered as spoken or signed language. \n\nThe mechanisms in this model can be divided into two groups, as shown in Figure 6. The first is responsible for generating relational representations-conceptual representations, syntactic representations, and phonological (or other form) representationsand translating between them, then maintaining them in working memory, predominantly through circuits in the left temporal lobe. The second set of mechanisms, localized in the left frontal lobe, exert influence on the translations between representations and work to organize those representations into a linear (temporal) order. One additional feature of note in this model is that prosodic computations are split into separate pre-syntactic and post-syntactic stages. Prosody is determined by a combination of linguistic and non-linguistic factors; for example, contrastive focus is in part determined based on the speaker's knowledge of the common ground and theory of mind for other discourse participants; heavy NP shift and stress clash in double object constructions is created by the stress that different phrases may carry, which is lexically specified; the choice between rising question intonation and lowering declarative intonation is determined by the speaker's goals in the discourse. A natural solution to the diversity of features that enter into the prosodic calculation is to posit that it is accomplished in two stages, one calculated pre-syntactically, before any syntactic information is available, and another which must be calculated post-syntactically, perhaps after specific phonological information has become available. \n\nHere we briefly summarize each component of the model to illustrate how the production process can work in the absence of traditional lexical items: \n\n1. Message generation: Many different sources of information are consolidated into a message, including conceptual representations for the entities involved in the sentence, event structure, thematic roles, and information structure. This is message is \"languageconstrained\" in that much of the information determining the message is not uniquely linguistic, but it also cannot be purely conceptual, because it must be partially determined by how the message is mapped to syntactic structure and which features in that language are grammatically encoded.",
            "score": 0.40384270715023307,
            "section_title": "FIGURE",
            "char_start_offset": 56410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2467041015625
        },
        {
            "corpus_id": "231939087",
            "title": "The Same Ultra-Rapid Parallel Brain Dynamics Underpin the Production and Perception of Speech",
            "text": "Language behavior concerns a complex, multi-component process where different linguistic representations (semantic, lexical, syntactic, and phonological knowledge) need to be retrieved and merged together in order to produce and perceive communicative signals. Insights into \"when\" and \"how\" different linguistic knowledge becomes activated is therefore key to understanding the cortical mechanics that can sustain this unique human behavior (Pulverm\u00fcller 1999(Pulverm\u00fcller , 2018Friederici 2002Friederici , 2011Indefrey and Levelt 2004;Hauk 2016). Despite the time course of language processing being a longstanding question in the field, debate remains between proponents of more sequential dynamics versus those arguing for parallel retrieval of linguistic knowledge.\n\nAt least two main reasons exist for this debate: first, in terms of data, empirical evidence has been found for both sequential and parallel brain dynamics; second, it is often difficult to disentangle whether neurophysiological responses to linguistic knowledge support a sequential or parallel time course. This is in part due to the fact that language processing recruits a vast number of regions in the brain, making it hard to know whether temporal differences reflect functionally distinct mental operations or merely physical distance in the brain (and different axonal conductance velocities) but functionally parallel activations. Interestingly though, the debate concerning the temporal dynamics in brain language models exists both for speech production and speech perception, even though traditionally these modalities have been studied separately (Price 2012). This shared debate (but separated approach) is interesting because directly contrasting the time course of language processing between the production and perception of words could circumvent the abovementioned conceptual problems and provide novel and more explicit insights into how our brain computes language in time. This is because a sequential model predicts the reverse temporal dynamics between the production and perception of words, while a parallel model predicts the same, simultaneous onset of word components across the language modalities. Therefore, in the current study, we systematically contrasted the event-related brain potentials (ERPs) elicited at the single-trial level by lexicosemantic and phonological word knowledge in both production and perception.\n\nIn sequential models of word production (Levelt et al. 1999;Indefrey and Levelt 2004;Indefrey 2011;Hi",
            "score": 0.40373916139117466,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 442,
                    "end": 460,
                    "matchedPaperCorpusId": "23318336"
                },
                {
                    "start": 460,
                    "end": 480,
                    "matchedPaperCorpusId": "31638635"
                },
                {
                    "start": 480,
                    "end": 495,
                    "matchedPaperCorpusId": "16634731"
                },
                {
                    "start": 495,
                    "end": 512,
                    "matchedPaperCorpusId": "3224368"
                },
                {
                    "start": 512,
                    "end": 537,
                    "matchedPaperCorpusId": "12662702"
                },
                {
                    "start": 537,
                    "end": 547,
                    "matchedPaperCorpusId": "433527"
                },
                {
                    "start": 1632,
                    "end": 1643,
                    "matchedPaperCorpusId": "2916525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "225218972",
            "title": "The Bilingual Mental Lexicon and Lemmatic Transfer in Second Language Learning",
            "text": "The MHM recognizes conceptual transfer by differentiating between semantic and conceptual levels of representation. Accordingly, it makes two claims: Semantic transfer occurs \"at the point of mapping words to concepts and doe not involve the structure of conceptual categories\" and conceptual transfer is \"the use of L2 words in accordance with L1 linguistic categories\" or \"the use of L1 words in accordance with L2 linguistic categories\" (Pavlenko, 2009, pp. 148-149). In other words, semantic transfer is caused by inappropriate mapping, and conceptual transfer is caused by inadequate knowledge of the structure of a target lexical category (Jarvis & Pavlenko, 2008). The MHM views L2 learning as a process of conceptual restructuring for the development of target-like linguistic categories. This view offers important implications for the Published by SCHOLINK INC. \n\nstudy of nature and activity of the bilingual mental lexicon in L2 learning and IL development. \n\nAccording to Jarvis, \"a person's knowledge of the form-related properties of a word\" and \"the person's knowledge of the word's syntactic constraints and semantic associations\" are stored separately in the mental lexicon, and the latter \"in turn is stored separately from the person's conceptual knowledge\" (2009, p. 99). Jarvis uses the term \"lemma; in its original sense as defined by Levelt (1989) and Levelt, Roelofs, and Meyer (1999) to refer to both the syntactic and semantic properties of a word. He uses the term \"lemmatic transfer\" to describe lexical transfer in general and transfer of semantic and syntactic properties of words in particular. He holds that the lexical knowledge and use acquired through L1 can affect L2 learners\" mental lexicon and assumes that lexical transfer involves two mental processes on the bilingual mental lexicon: Lemmas from two or more languages are mentally represented, and lemmas underlying certain words of one language are activated by the speaker using another language. \n\nThe notion of lemma transfer becomes crucial to the BLAM. \n\nIn addition to the notion of lemma transfer as assumed by Jarvis ( 2009), Grosjean's notion of \"language mode\" also becomes crucial to the BLAM.",
            "score": 0.4036617759444305,
            "section_title": "Bilingual Lexical Processing and Conceptual Representation",
            "char_start_offset": 9642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 1376,
                    "end": 1409,
                    "matchedPaperCorpusId": "2363595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "1758925",
            "title": "Arbitrary Symbolism in Natural Language Revisited: When Word Forms Carry Meaning",
            "text": "Much of our understanding about word recognition has been informed by research focusing on aspects of bottom-up processing, although a considerable body of recent emerging work has addressed top-down, expectancy-based contributions. Such topdown expectancy effects are apparent in several domains related to language learning, including learning grammatical distinctions from phonological cues (i.e., syntactic bootstrapping), using transitional phonotactic probabilities to parse word boundaries, and using prosodic and phonetic variation to highlight semantic distinctions during early childhood development [36,[37][38][39]. These cognitive processes-although unique in their own rightare subsumed under the more general rubric of statistical learning, a process wherein listeners exploit regularities at one level of linguistic processing to facilitate higher level linguistic distinctions [40]. \n\nPerhaps the most widely accepted form of statistical learning is known as syntactic bootstrapping [36,38,39]. This cognitive phenomenon operates upon distributional properties of nouns versus verbs across many natural languages. In English, verbs are typically longer and more derivationally complex than nouns; they are less likely to end in a final consonant that is voiced [41]; they are more likely to contain front vowels [42]; and they tend to carry primary stress on non-initial syllables [41,43,44]. Syntactic bootstrapping occurs when listeners use the combined strength of many such formal cues to assign a rapid, tentative grammatical parse to the elements of an incoming utterance, a process referred to by Kelly [41] as using sound to solve syntactic problems [42,45,46] Monaghan and colleagues [42] showed that computers were able to categorize words and humans were able to categorize nonwords as either verbs or nouns with better than chance accuracy using only phonological (and string frequency) cues. \n\nAnother prominent example of statistical learning involves exploiting expectancies about phonotactic probabilities to parse probable word boundaries. Speech perception offers a unique problem for language learning in that speech is typically a continuous signal. Since there are frequently no acoustic ''breaks'' between spoken words in running speech, one great mystery involves how children learn to efficiently chunk words.",
            "score": 0.4034889696965271,
            "section_title": "Introduction",
            "char_start_offset": 3716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2350
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "18806029"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "18806029"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "6084725"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "10232677"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "6084725"
                },
                {
                    "start": 1405,
                    "end": 1408,
                    "matchedPaperCorpusId": "46394618"
                },
                {
                    "start": 1627,
                    "end": 1631,
                    "matchedPaperCorpusId": "6084725"
                },
                {
                    "start": 1675,
                    "end": 1679,
                    "matchedPaperCorpusId": "10232677"
                },
                {
                    "start": 1679,
                    "end": 1682,
                    "matchedPaperCorpusId": "26034489"
                },
                {
                    "start": 1682,
                    "end": 1685,
                    "matchedPaperCorpusId": "6610733"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "10232677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45849609375
        },
        {
            "corpus_id": "268691480",
            "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs",
            "text": "Probing LM Tenney et al. (2019) explored how much sentence structure is captured by contextually-embedded word representations.They used a series of diagnostic tasks to test whether these models can capture various grammatical and semantic features of sentences.What should be mentioned is while they did examine the outputs of LMs, their approach differed from a true per-layer probing in our study as they combined information from multiple layers so that it does not allow for a direct examination of specific linguistic information present within each layer.\n\nSimilarly, Hewitt and Manning (2019) delved into the intricacies of neural representations by introducing a \"Structural Probe\".This tool specifically quantifies the degree to which word representations in a sentence capture syntactic tree distances.Hewitt and Manning's approach highlights the potential of these embeddings to encapsulate the underlying syntactic structures of sentences, suggesting that neural network architectures inherently learn syntactic relationships as a consequence of their training.While these studies have made significant contributions to understanding the linguistic capabilities of intermediate embeddings, they often rely on broader linguistic tests or syntactic trees, which might not offer the granularity needed to dissect the specific linguistic phenomena being captured.The use of minimal pairs provides a finer-grained lens.It allows us to pinpoint with greater precision the exact linguistic distinctions these embeddings can discern, thereby offering a more detailed understanding of the nuances in linguistic representation across different layers.Brain-inspired LM probing Concurrently, cognitive neuroscientists also provide insights from neuroscience into the internal neural representation of LMs.For instance, Caucheteux et al. (2021) delved into the GPT-2 embeddings and successfully isolated syntactic and semantic representations.Their findings further showed that these disentangled linguistic embeddings have cognitive neuroscience support, reinforcing the connection between neural LMs and language processing in the brain.While by using electrophysiology recording as a form of 'human measurement filter', Chen et al. (2023) suggest that intermediate layers of the deep speech model and language model share high-level contextual information.",
            "score": 0.4034075067697984,
            "section_title": "Related Work",
            "char_start_offset": 24586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 562
                },
                {
                    "start": 564,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1654
                },
                {
                    "start": 1654,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1944
                },
                {
                    "start": 1944,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 600,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1821,
                    "end": 1845,
                    "matchedPaperCorpusId": "235436365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "264451507",
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
            "text": "Contemporary advances in NLP are built on the representational power of latent embedding spaces learned by self-supervised language models (LMs). At their core, these approaches are built on the distributional hypothesis (Harris, 1954;Firth, 1957), for which the effects of scale have been implicitly and explicitly studied via the community's use of increasingly large models and datasets (Teehan et al., 2022;Wei et al., 2022). The learning dynamics by which these capabilities emerge during LM pre-training have, however, remained largely understudied. Understanding how and when the  LM training objective begins to encode information that is relevant to downstream tasks is crucial, as this informs the limits of what can be learned using current objectives.",
            "score": 0.4032821451586132,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 763
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 247,
                    "matchedPaperCorpusId": "208093066"
                },
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "247656607"
                },
                {
                    "start": 411,
                    "end": 428,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "273403825",
            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
            "text": "LMs are computational systems designed to understand and generate text in human language. Over time, advancements in LMs, particularly in pretrained models like GPT (Radford et al., 2019) and BERT (Devlin et al., 2019), have significantly improved performance across various NLP tasks. Large LMs, which leverage vast amounts of data and computational power, can capture more intricate nuances in language (Bommasani et al., 2022;Wei et al., 2022a), improving its generative capabilities involving masked token or next-token predictions. \n\nThese models are typically fine-tuned for specific tasks after pre-training, further enhancing their adaptability and versatility in practical applications (Gururangan et al., 2020). As noted previously, systematically evaluating the performance of LMs is critical as they grow in their complexity and diversity (Srivastava et al., 2023). Benchmarking not only provides a standardized way to compare different models; it also highlights areas where improvements are needed, guiding future advancements in the field. \n\nThere are many benchmarks that evaluate LMs' abilities. Some focus on specific aspects, whereas others cover a broad range of tasks. For instance, the SST2 dataset (Socher et al., 2013) measures text classification and the TriviaQA dataset (Joshi et al., 2017) focuses on question answering. Additionally, comprehensive benchmark suites like GLUE (Wang et al., 2019), SuperGLUE (Wang et al., 2020), and MMLU (Hendrycks et al., 2021) assess multitask language understanding across a wide range of topics and tasks. However, these benchmarks do not provide insights about a model's capabilities in terms of human language acquisition, such as in the three-stage framework we provided. Insights from previous studies on the stages of human language development may offer valuable reference points for evaluating models' performance. \n\nPrevious studies have demonstrated that models can learn hierarchical syntactic structures and exhibit sensitivity to various linguistic phenomena, even when trained with the amount of data that humans typically encounter (Milli\u00e8re, 2024;Wilcox et al., 2024). Assessing these models through the lens of human language development can provide further insights and deepen our understanding of LMs' capabilities.",
            "score": 0.4031324929528479,
            "section_title": "Related Works",
            "char_start_offset": 4989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 187,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 695,
                    "end": 720,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1221,
                    "end": 1241,
                    "matchedPaperCorpusId": "990233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "34591980",
            "title": "Looking for structure: Is the two-word stage of language development in apes and human children the same or different?",
            "text": "Indeed, the above researchers draw several comparisons between their chimpanzee subjects and the children in Brown's study. This position clearly runs counter to the biolinguistic approach which, while it fully endorses testing theories of language origin by identifying possible biological homologies and analogies of human linguistic mechanisms in nonhuman species (Fitch, 2010), nevertheless maintains both that the early acquisition by children of abstract syntactic knowledge can only result from a \"genetic endowment of human beings that is responsible for the emergence of this remarkable linguistic capacity\" (Narita & Fujita, 2010, p. 356), and furthermore that, as Chomsky says, \"there comes a time when the (language acquisition) system just isn't working anymore\" (as quoted in Bley-Vroman, 2009, p. 180). In short, such claims regarding the syntactic capacities of chimpanzees, if validated, would clearly pose a serious challenge to the biolinguistic claims concerning the species-specificity of language and its corollary notion of a critical period. \n\nThis paper, then, examines previously published corpora of two-word utterances produced by three chimpanzees (Greenfield & Savage-Rumbaugh, 1991;Lyn, Greenfield, & Savage-Rumbaugh, 2011) and compares them to the earliest available two-word productions of three human children, using the classic Brown (1973) data for Adam, Eve, and Sarah, from the CHILDES database (MacWhinney, 2000). \n\nAs is well known, Brown (1973) characterized Stage I child language primarily in semantic terms, and found that a set of eight \"prevalent\" two-term semantic relations (agent + action; action + object; agent + object; action + locative; entity + locative; possessor + possession; entity + attribute; demonstrative + entity) could account for approximately 70% of Stage I production (p. 178). In further characterizing Stage I language, Brown also listed three \"operations of reference\" (nomination, recurrence, and denial), noted that the modalities of interrogation, negation, and the imperative have their beginnings in Stage I (p.",
            "score": 0.40309417419180327,
            "section_title": "body",
            "char_start_offset": 3302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "238408423",
            "title": "Word Acquisition in Neural Language Models",
            "text": "In this work, we isolated ways that language models differ from children in how they acquire words, emphasizing the importance of sensorimotor experience and cognitive development for human-like language acquisition. Future work could investigate the acquisition of syntactic structures or semantic information in language models. Non-distributional learning We showed that distributional language models acquire words in very different ways from children. Notably, children's linguistic experience is grounded in sensorimotor and cognitive experience. Children as young as ten months old learn word-object pairings, mapping novel words onto perceptually salient objects (Pruden et al., 2006). By the age of two, they are able to integrate social cues such as eye gaze, pointing, and joint attention (\u00c7etin\u00e7elik et al., 2021). Neural network models of oneword child utterances exhibit vocabulary acquisition trajectories similar to children when only using features from conceptual categories and relations (Nyamapfene and Ahmad, 2007). Our work shows that these grounded and interactive features impact child word acquisition in ways that cannot be explained solely by intra-linguistic signals. \n\nThat said, there is a growing body of work grounding language models using multimodal information and world knowledge. Language models trained on visual and linguistic inputs have achieved state-of-the-art performance on visual question answering tasks (Antol et al., 2015;Lu et al., 2019;Zellers et al., 2021b), and models equipped with physical dynamics modules are more accurate than standard language models at modeling world dynamics (Zellers et al., 2021a). There has also been work building models directly for non-distributional tasks; reinforcement learning can be used for navigation and multi-agent communication tasks involving language (Chevalier-Boisvert et al., 2019;Lazaridou et al., 2017;Zhu et al., 2020). These models highlight the grounded, interactive, and communicative nature of language. Indeed, these nondistributional properties may be essential to more human-like natural language understanding (Bender and Koller, 2020;Emerson, 2020). Based on our results for word acquisition in language models, it is possible that these multimodal and non-distributional models could also exhibit more human-like language acquisition.",
            "score": 0.4029900456530099,
            "section_title": "Discussion",
            "char_start_offset": 31667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "702364"
                },
                {
                    "start": 1007,
                    "end": 1035,
                    "matchedPaperCorpusId": "9747388"
                },
                {
                    "start": 1451,
                    "end": 1471,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 1637,
                    "end": 1660,
                    "matchedPaperCorpusId": "235266260"
                },
                {
                    "start": 1903,
                    "end": 1920,
                    "matchedPaperCorpusId": "218581602"
                },
                {
                    "start": 2120,
                    "end": 2145,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 2145,
                    "end": 2159,
                    "matchedPaperCorpusId": "218517013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "13635218",
            "title": "Syntactic learning by mere exposure - An ERP study in adult learners",
            "text": "extraction mechanism as an initial acquisition stage. If syntactic rules were acquired in an all-or-nothing rule extraction process, a P600 should have been present for the learners too. Instead, it seems to be the case that the learners use perceptual features of an item's form (N400) to guide acquisition and general cognitive attentional processes (P3a) in order to solve the grammaticality judgment task. This is consistent with a view of language learning in which the distributions of perceptual features are computed initially while generalized, abstracted knowledge is established in a later stage. A crucial issue for future research will be to describe the transition from the distributional to the rule-based stage during artificial and second language learning and also possibly in first language acquisition.",
            "score": 0.40277547831697513,
            "section_title": "ERP results",
            "char_start_offset": 23504,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "259076231",
            "title": "Second Language Acquisition of Neural Language Models",
            "text": "The trajectory of the OVERALL scores in Figure 3 suggest that linguistic ability generally improves along with the number of epochs. There was a tendency for large improvements to emerge after dozens of epochs; in other words, the models began to acquire L2 knowledge after seeing the same examples many times, e.g., 50-100 times. Note that humans are argued to acquire a vocabulary after encountering the same word about 12 times (Nation, 2014), and of course, the lexical and syntactic acquisition is not comparable, but the observation that the L2 knowledge improves after 50-100 rounds of the corpus may be in the direction that LMs are inefficient at acquiring a new language. \n\nDifferences in grammar items: Focusing on the general trajectory shapes for each grammatical item, we observed at least four patterns: (i) spike-at-the-end (D-N AGR., IRREGULAR, S-V AGR.), (ii) flat (ARG.STR., CTRL.RAIS., ISLAND), (iii) bumpy (ANA.AGR., ELLIPSIS, NPI, QUANTI-FIERS), and (iv) mixed (FILLER-GAP, BINDING). In addition, these groups roughly mirror the linguistic categories of the grammar items (morphology, syntax, semantics, and syntax&semantics); for example, all the items in the spike-at-the-end group are morphological phenomena, while all the semantic categories (NPI, QUANTIFIERS) yielded the bumpy patterns. Note that existing studies reported that low-level (e.g., morphological) linguistic skills could be acquired earlier and vice versa (Liu et al., 2021;Blevins et al., 2022); but at least in our cognitively-inspired bilingual training scenario, we did not observe such an explicit tendency.",
            "score": 0.40251032825928523,
            "section_title": "General improvement after dozens of epochs:",
            "char_start_offset": 15261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 1448,
                    "end": 1466,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1466,
                    "end": 1487,
                    "matchedPaperCorpusId": "249017999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "260775882",
            "title": "Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis",
            "text": "During the training phase of a Transformer-based model, we harness the power of self-attention mechanisms and large-scale pre-training to develop a robust and versatile language model. Transformer models, such as XLNET (EXtreme Learning Network), DistilBERT (Distilled Bidirectional Encoder Representations from Transformers), BERT (Bidirectional Encoder Representations from Transformers), and RoBERTa (Robustly Optimized Bidirectional Encoder Representations from Transformers Approach), have revolutionized natural language processing tasks. The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns. After pre-training, the model undergoes fine-tuning on specific downstream tasks. This involves training the model on labeled task-specific data. The labeled data could include sentiment analysis, named entity recognition, text classification, or question answering, among others. During fine-tuning, the model adapts its pre-trained knowledge to the task at hand, refining its internal parameters and learning task-specific patterns. The training data is typically split into training and validation sets. The model is trained on the training set, and its performance is evaluated on the validation set to monitor progress and prevent overfitting. Various optimization techniques, such as stochastic gradient descent (SGD) and Adam optimization, are employed to update the model's parameters iteratively and minimize the loss function. Throughout the training process, hyperparameter tuning is often performed to find the optimal settings for the model architecture, learning rate, batch size, and regularization. This fine-tuning allows the model to generalize well, improve its accuracy, and perform effectively on unseen data.",
            "score": 0.40248703568666333,
            "section_title": "Transformer based model training",
            "char_start_offset": 25115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "18903447",
            "title": "Hidden Understanding Models of Natural Language",
            "text": "Tree structured meaning expressions can range in complexity from simple special purpose sublanguage representations to the structural equivalent of detailed syntactic parse trees. The possibilities are limited only by two fundamental requirements: (I) semantic concepts must be hierarchically nested within a tree structure, and (2) the sets of terminal and nonterminal nodes must remain disjoint. Both of these requirements can be satisfied by trees possessing most of the structural characteristics of conventional syntactic parse trees. Since our objective is to model meaning, the nodes must still be labeled to reflect semantic categories. However, additional and augmented labels may be introduced to reflect syntactic categories as well. \n\nRepresentations of this form contain significantly more internal structure than specialized sublanguage models. This can be seen in the example in figure 4. The specialized sublanguage representation requires only seven nodes, while a full syntactically motivated analysis requires fifteen. The additional nodes are used to distinguish what is being shown to whom, to reflect the fact that the stopover phrase is part of a relative clause, and to determine the internal structure of the relative clause. \n\nOne interesting characteristic of these more elaborate trees is their similarity to those produced by classical, linguistically motivated, natural language systems. Thus, a hidden understanding model can serve to replace the part-ofspeech tagger, parser, and semantic interpreter of a classical system. Instead of writing grammar and semantic interpretation rules by hand, the training program automatically constructs a statistical model from examples of meaning trees. \n\nRegardless of the details of the tree structure and labels, the components comprising a hidden understanding system remain unchanged. The only difference is in how the system is trained.",
            "score": 0.40232749659644834,
            "section_title": "Alternative Tree Representations",
            "char_start_offset": 5922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "1300761",
            "title": "Representing and Integrating Linguistic Knowledge",
            "text": "A natural way to model knowledge of language is as a large collect.ion of facts or regularities about the language. In this theory I use a single representational device, the grammatical construction, to represent these regularities. Our entire model of linguistic knowledge consists of a database of these constructions, uniforrnly representing lexieal knowledge, syntactic knowledge, and semantic knowledge. \n\nAlthough the notion of grammatical construction I use is based on that of Fillmore e! al. (1988) it differs in a few respects. Filhnore et al. define a construetion as a structure which represents a \"pairi~9 of a syntaclic pallcrn with, a meanin 9 sl'ruclure\". Such a construction is a sig., in the sense of Saussure, or a rule-pairing in the sense of Montague. 1 use a somewhat extended notion: a construction is a relation between one information structure and one or more others. These information structures can be semantic, syntactic or both. Thus, whereas the \"sign\" expressed a relation between a set of ordered phonemes ~md a meaning structure, the construction abstracts over this by replacing 'ordered sets of phonemes' with *I want to express my tha.nks to Peter Norvig, Nigel Ward, and l{obert Wilensky for many helpfld comments and discussions on these ideas. This research was ~ponsored by the Defense Advanced Research Projects Agency (DoD), monitored by the Space and Naval War-Ikre Systems Command under N00039-88-C-0292, and the Office of Naval Research, under contract N00014-89.-,1-3205. abstractions over them. These abstractions can be syntactic or semantic ways of expressing more abstract categories to which these phoneme sequences belong. The construction is then a part-whole structuring relating these categories.",
            "score": 0.40229653394986664,
            "section_title": "Representation",
            "char_start_offset": 850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1753
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2216796875
        },
        {
            "corpus_id": "3115453",
            "title": "Flexible Composition: MEG Evidence for the Deployment of Basic Combinatorial Linguistic Mechanisms in Response to Task Demands",
            "text": "Few previous studies have directly investigated the extent to which combinatorial linguistic mechanisms can be flexibly deployed to novel contexts, though much work touches upon this question indirectly.Clearly, at some level, combinatorial linguistic processes can be applied to novel contexts, as people are capable of learning to extract meaning from written words and foreign languages, neither of which evoke successful combinatorial linguistic processing without instruction.Neurolinguistic investigations into both types of processing, while not definitive, indicate a large overlap in the neural signatures associated with combinatorial speech comprehension and those evoked both by reading [2], [6], [7] and foreign language processing [8], [9], [10].More relevantly, a large body of evidence suggests that linguistic mechanisms, broadly construed, can be extended to the processing of artificial 'languages' as well.In the canonical artificial grammar learning (AGL) paradigm [11], sets of arbitrary symbols, ranging from foreign words [12], to letter strings [13], to visual objects [14], are generated using a finite automaton such that they obey various syntactic constraints.Subjects are then shown exemplars from these sets during a training period and subsequently asked to judge the 'grammaticality' of a test set of strings, some of which are generated by the automaton and some of which are not.There are countless variations on the paradigm, in terms of syntactic constraints, 'language' symbols, learning method, and more (see [15] for a review), however, a consistent finding from both behavioral [16], [17] and neural studies [18], [12], [13] is that the processing of such artificial grammars can appear similar to that of natural language in many respects.Thus, in broad strokes, AGL studies provide evidence that linguistic mechanisms can be deployed to novel contexts given appropriate task demands.\n\nNevertheless, several factors prevent these results from directly addressing the concern of the present study, which is to determine the extent to which basic linguistic combinatorial mechanisms can be flexibly and rapidly deployed to novel contexts.",
            "score": 0.40222465985933054,
            "section_title": "Previous Studies on the Bounds of Combinatory Language Processing",
            "char_start_offset": 2269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1926
                },
                {
                    "start": 1928,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 699,
                    "end": 702,
                    "matchedPaperCorpusId": "16527906"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "20124799"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "13570728"
                },
                {
                    "start": 745,
                    "end": 748,
                    "matchedPaperCorpusId": "63662"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "8557363"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "29196950"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "145282756"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "7955526"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "18099287"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "16577309"
                },
                {
                    "start": 1548,
                    "end": 1552,
                    "matchedPaperCorpusId": "1533832"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "12975519"
                },
                {
                    "start": 1625,
                    "end": 1629,
                    "matchedPaperCorpusId": "15141302"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "35582087"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "7955526"
                },
                {
                    "start": 1661,
                    "end": 1665,
                    "matchedPaperCorpusId": "18099287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "258787743",
            "title": "The odyssey to next-generation computers: cognitive computers (\u03baC) inspired by the brain and powered by intelligent mathematics",
            "text": "The universal language comprehension (ULC) model. \n\nfor specific purposes. This observation reveals a fundamental difference between \u03baC and classical data-regressive AI technologies. \n\nThe cognitive informatics model of language comprehension is a cognitive process according to the LRMB and OAR models of internal knowledge representation (Wang, 2007;Wang et al., 2011Wang et al., , 2016a)). A rigorous description of the cognitive process of comprehension has been formally described in Wang (2009bWang ( , 2021b)). The cognitive model between a language expression and its syntaxes and semantics may be explained as shown in Figure 10 known as the universal language comprehension (ULC) model (Wang and Berwick, 2012). Figure 10 explains that linguistic analyses are a deductive process that maps the 1-D language expression L into a 5-D semantic space C via 2-D syntactical analyses S, where C is determined by the dimensions of the subject (J), the object (O), the action (A), the space (S), and the time point (T). \n\nThe ULC model indicates that syntactic analysis is a coherent part of semantic comprehension in formal language processing and machine knowledge learning by \u03baC. It is found that the semantics of a sentence in a natural language expression may be comprehended by \u03baC iff : (a) The parts of speech in the given sentence are analytically identified; (b) All parts of speech of the sentence are reduced to the terminals W 0 , which are a predefined real-world entity, a primitive abstract concept, or a derived behavior; and (c) The individual semantics of each part of speech and their logical relations are synthesized into a coherent structure. \n\nAccording to the ULC theory, the methodology for knowledge learning by \u03baC is a semantic cognition process enabled by a language comprehension engine (LCE) (Wang, 2016c). LCE is implemented in a closed loop of syntactic analysis, semantic deduction, and semantic synthesis for enabling the entire process of semantic comprehension by \u03baC. LCE is supported by a cognitive knowledge base (CKB) (Wang, 2014a).",
            "score": 0.40222008874808857,
            "section_title": "FIGURE",
            "char_start_offset": 45379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 52,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 352,
                    "matchedPaperCorpusId": "10863940"
                },
                {
                    "start": 352,
                    "end": 369,
                    "matchedPaperCorpusId": "14532650"
                },
                {
                    "start": 369,
                    "end": 391,
                    "matchedPaperCorpusId": "26281250"
                },
                {
                    "start": 489,
                    "end": 500,
                    "matchedPaperCorpusId": "3748575"
                },
                {
                    "start": 696,
                    "end": 720,
                    "matchedPaperCorpusId": "259278349"
                },
                {
                    "start": 1823,
                    "end": 1836,
                    "matchedPaperCorpusId": "2749883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "239054621",
            "title": "Priming of Frames and Slots in Bilingual Children\u2019s Code-Mixing: A Usage-Based Approach",
            "text": "Language use is one of the central criteria that define humans and therefore, it is not surprising that many researchers have tried to uncover the mechanisms underlying acquisitional processes (e.g., Ambridge and Lieven, 2011). The predominant goal of language acquisition studies is to understand how children learn language. For a long time, research in child language has been dominated by nativist assumptions, arguing that language input is so impoverished that children's rich knowledge of grammar could not possibly be extracted from it (Chomsky, 1965). To avoid the problem of impoverished input, proponents of the nativist perspective suggested instead that children are equipped with an innate universal grammar and all languages share one set of underlying syntactic principles of grammar. \n\nOver the years, an increasing number of researchers have challenged the concept of the \"poverty of stimulus\" and claimed that children are able to extract linguistic knowledge from the input through learning mechanisms which are responsible for most other types of learning (see Ambridge andLieven, 2011 for an overview, Tomasello, 2003). The UB approach relies on the assumption that humans build up knowledge (or \"mental representation, \" or \"competence\") on the basis of their experience. As such, UB approaches counter the generative position of a universal grammar and view linguistic knowledge as emerging from item-specific learning. Linguistic constructions (form-function pairings) are the basic building blocks of grammar. What children acquire is a growing inventory of constructions that move along a continuum of varying complexity and different levels of abstractness, ranging from lexically specific units, or \"chunks\" (What's up), via partially schematic or frame-and-slot patterns (I want X), to fully schematic schemas (Pronoun VP NP). Frame-and-slot patterns are a vital link on a continuum of schematicity. Children learn multi-word units which subsequently become analyzed as frameand-slot patterns, creating open slots. By definition, this means that the construction becomes productive; the growing number of its different instantiations may finally give rise to the ultimate schematization: the abstract grammatical pattern. \n\nFrom a methodological point of view, the question arises how to account for patterns in children's speech.",
            "score": 0.401801235647676,
            "section_title": "LANGUAGE ACQUISITION FROM A USAGE-BASED PERSPECTIVE",
            "char_start_offset": 3682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2251
                },
                {
                    "start": 2254,
                    "end": 2360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "12081738",
            "title": "Learning to Distinguish PP Arguments from Adjuncts",
            "text": "In trying to solve the question of how to get a machine to automatically learn language from data, we can look at the way people do it. When we acquire our mother language we are exposed to an environment that includes noisy and ungrammatical sentences, the potential influence of other languages, and many other linguistic phenomena. In spite of that, most children are successful in the acquisition of a grammar in a relatively short time, acquiring a sophisticated mechanism for expressing their ideas, based on data that is said to be too impoverished to generate such a complex capacity. One approach to explain the acquisition of languages proposes that children must have some innate knowledge about language, a Universal Grammar (UG), to help them overcome the problem of the poverty of the stimulus and acquire a grammar on the basis of positive evidence only (Chomsky 1965). According to Chomsky's Principles and Parameters Theory (Chomsky 1981), the UG is composed of principles and parameters, and the process of learning a language is regarded as the set-ting of values of a number of parameters, given exposure to this particular language. Another likely source of information that is available to children when learning a language is the semantic interpretation or related conceptual representation. Indeed, as Steedman (1994) puts it: \n\n\"Since the main thing that syntax is for is passing concepts around, the belief that syntactic structure keeps as close as possible to semantics, and that in both evolutionary and child language acquisition terms, the early development of syntax amounts to little more than hanging words onto the preexisting armatures of conceptual structure is so simple and probable as to amount to the null hypothesis\". \n\nA third source of information can be found in the statistical properties of the input data to which children seem to be sensitive, as observed in recent work in psycholinguistics.",
            "score": 0.40164276495679446,
            "section_title": "Language Acquisition",
            "char_start_offset": 3527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1941
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37890625
        },
        {
            "corpus_id": "72940921",
            "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state",
            "text": "We begin by studying subordinate clauses, a key example of a construction requiring stack-like representation of syntactic state. In such constructions, as shown in Example (1), a subordinator such as \"as\" or \"when\" serves as a cue that the following clause is a subordinate clause, meaning that it must be followed by some main (matrix) clause. In an incremental language model, this knowledge must be maintained and carried forward while processing the words inside subordinate clause. A grammar-based symbolic language model (e.g., Stolcke, 1995;Manning and Carpenter, 2000) would maintain this knowledge by keeping track of syntactic rules representing the incom- plete subordinate clause and the upcoming main clause in a stack data structure. Psycholinguistic research has clearly demonstrated that humans maintain representations of this kind in syntactic processing (Staub and Clifton, 2006;Lau et al., 2006;Levy et al., 2012). Here we ask whether the string completion probabilities produced by neural language models show evidence of the same knowledge. \n\nWe can detect the knowledge of syntactic state in this case by examining whether the network licenses and requires a matrix clause following the subordinate clause. These expectations can be detected by examining surprisal differences between sentences of the form in Example (2): \n\n(2) a. As the doctor studied the textbook, the nurse walked into the office. [SUBordinator, MATRIX] b. *As the doctor studied the textbook. \n\n[SUB, NO-MATRIX] c. ?The doctor studied the textbook, the nurse walked into the office. \n\n[NO-SUBordinator, MATRIX] d. The doctor studied the textbook. \n\n[NO-SUB, NO-MATRIX] \n\nIf the network licenses a matrix clause following the subordinate clause-and maintained knowledge of that licensing relationship throughout the clause, from the subordinator to the comma-then this should be manifested as lower surprisal at the matrix clause in (2-a) as compared to (2-c). We call this the matrix licensing effect: the surprisal of the condition [SUB, MA-TRIX] minus [NOSUB, MATRIX], which will be negative if there is a licensing effect.",
            "score": 0.40159753396276593,
            "section_title": "Subordinate clauses",
            "char_start_offset": 8543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 549,
                    "matchedPaperCorpusId": "11595344"
                },
                {
                    "start": 549,
                    "end": 577,
                    "matchedPaperCorpusId": "6847055"
                },
                {
                    "start": 874,
                    "end": 899,
                    "matchedPaperCorpusId": "14788444"
                },
                {
                    "start": 899,
                    "end": 916,
                    "matchedPaperCorpusId": "5993065"
                },
                {
                    "start": 916,
                    "end": 934,
                    "matchedPaperCorpusId": "2274165"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "226289695",
            "title": "When Do You Need Billions of Words of Pretraining Data?",
            "text": "the edge probing suite of NLP tasks (Tenney et al., 2019b) to measure the quality of the syntactic and semantic features that can be extracted by a downstream classifier with each level of pretraining. Second, we apply minimum description length (MDL) probing (Voita and Titov, 2020) to the edge probing suite, with the goal of quantifying the accessibility of these features. Third, we test the models' knowledge of various syntactic phenomena using unsupervised acceptability judgments on the BLiMP suite (Warstadt et al., 2020a). Fourth, we probe the models' world knowledge and commonsense knowledge using unsupervised language model knowledge probing with the LAMA suite (Petroni et al., 2019). Finally, we fine-tune the models on five tasks from SuperGLUE  to measure their ability to solve conventional NLU tasks.\n\nFor each evaluation method, we fit an exponential learning curve to the results as a function of the amount of pretraining data, shown in Figure 1. We have two main findings: First, the results of classifier probing, MDL probing, and unsupervised relative acceptability judgement (BLiMP) show that the linguistic knowledge of models pretrained on 100M words and 30B words is similar, as is the description length of linguistic features. Second, RoBERTa requires billions of words of pretraining data to effectively acquire factual knowledge and to make substantial improvements in performance on dowstream NLU tasks. From these results, we conclude that there are skills critical to solving downstream NLU tasks that LMs can only acquire with billions of words of pretraining data. Future work will likely need to look beyond core linguistic knowledge if we are to better understand and advance the abilities of large language models.",
            "score": 0.401519469357862,
            "section_title": "Introduction",
            "char_start_offset": 1862,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 58,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 260,
                    "end": 283,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 507,
                    "end": 531,
                    "matchedPaperCorpusId": "208527435"
                },
                {
                    "start": 676,
                    "end": 698,
                    "matchedPaperCorpusId": "15641339"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "248005680",
            "title": "A parallel architecture perspective on pre-activation and prediction in language processing",
            "text": "Before discussing our approach to processing, we briefly set out the relevant tenets of the Parallel Architecture and its progeny, Simpler Syntax (Culicover & Jackendoff, 2005) and Relational Morphology (Jackendoff & Audring, 2020).\n\nAt its foundation, the PA couches the fundamental questions of linguistic theory in psycholinguistic terms:\n\n\u2022 What linguistic entities are stored in long-term memory, and crucially, in what form are they stored? (a.k.a. \"knowledge of language,\" \"theory of \"competence,\" theory of representations) \u2022 How are these entities put to use in language comprehension and production? (theory of \"performance,\" theory of processing) \u2022 How are these entities acquired? (theory of acquisition) \u2022 What is the innate foundation that supports language acquisition? (\"Universal Grammar\") \u2022 How does language fit into the rest of the mind?\n\nThis section addresses the first two and the last of these.\n\nThe feature that gives the framework its name is its treatment of phonology, syntax, and semantics as independent combinatorial systems, operating in parallel. Each of these levels of representation has its own generative capacity, built out of its own characteristic units. Fig. 1 sketches the overall architecture. The double-headed arrows represent interface links: correspondences between levels rather than derivations from one level to another. 4 A well-formed sentence has well-formed structures at each level, plus well-formed links among the structures.\n\nFor the simplest sort of example, the word cat consists of a piece of semantic structure (the meaning of the word), a piece of phonological structure (/kaet/), and the syntactic category Nounplus interface links",
            "score": 0.40132167773829974,
            "section_title": "The Parallel Architecture",
            "char_start_offset": 8446,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "235657379",
            "title": "Analyzing Commonsense Emergence in Few-shot Knowledge Models",
            "text": "Driven by advances in pretrained language models [Liu et al., 2019, Raffel et al., 2019], recent NLP systems have demonstrated considerable improvement [Clark et al., 2020, Lourie et al., 2021] on benchmarks that test the commonsense representation and reasoning abilities of NLP systems [Talmor et al., 2019, Sap et al., 2019b, Bhagavatula et al., 2020, Sakaguchi et al., 2020]. Despite this success, it remains unclear how much commonsense knowledge is actually directly encoded by these models. When prompted out-of-the-box to complete commonsense declarative relationships (as depicted in Figure 1), they exhibit limited ability to map their language modeling abilities to this task [Petroni et al., 2019, Zhou et al., 2020]. \n\nHowever, the zero-shot expression of declarative commonsense knowledge is but one use of language that these models are pretrained to manifest. Infilling evaluations that entangle this ability with general language expression may be narrow tests of true commonsense representation ability. Indeed, commonsense knowledge models [Bosselut et al., 2019] which finetune pretrained LMs on examples from commonsense knowledge graphs [Speer et al., 2017, Sap et al., 2019a, Jiang et al., 2021] -learn to express declarative commonsense relationships much more effectively. This learning procedure allows these models to recalibrate to the actual task of hypothesizing declarative knowledge. Simultaneously, these systems are Figure 1: Commonsense knowledge models can be trained to effectively hypothesize commonsense knowledge when trained in few-shots, implying that finetuning serves to learn and interface to knowledge encoded during pretraining. evaluated in challenging train/test settings (where head entities tested on cannot be seen during training), indicating they may learn to transfer implicit representations of declarative knowledge that are learned during pretraining [Hwang et al., 2021]. \n\nHowever, current commonsense knowledge models are trained on a large number of examples from knowledge graphs.",
            "score": 0.4010303858777811,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 171,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 171,
                    "end": 193,
                    "matchedPaperCorpusId": "232335877"
                },
                {
                    "start": 288,
                    "end": 308,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 308,
                    "end": 327,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 1178,
                    "end": 1197,
                    "matchedPaperCorpusId": "53170360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "258987811",
            "title": "How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases",
            "text": "Our findings about the amount of pre-training data required for the acquisition of syntactic biases also have implications for cognitive modeling research. Humans learn language from far fewer words than contemporary LMs, and at the same time generalize their linguistic knowledge to new settings more robustly; conversely, standard NLP evaluations, which do not take the pre-training corpus into consideration, implicitly reward architectures that learn well from vast amounts of data, raising the concern that those architectures are suboptimal for cognitive modeling (Linzen, 2020). Our evaluation setup and empirical results go some way towards addressing these concerns: we show that pre-training on a developmentally plausible amount of data can induce human-like inductive biases that improve outof-distribution generalization. This suggests that Transformers, when trained in cognitively relevant regimes, may serve as fruitful models of human language acquisition and processing (see also Hosseini et al. 2022).",
            "score": 0.4009620849557898,
            "section_title": "Discussion",
            "char_start_offset": 30009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 998,
                    "end": 1019,
                    "matchedPaperCorpusId": "252764051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "234475291",
            "title": "Learning Actions From Natural Language Instructions Using an ON-World Embodied Cognitive Architecture",
            "text": "ving being the desired outcome. Vocabulary, grammar, and syntactic and semantic soundness are yielded as a natural consequence of the human-guided interaction between language and action. This research presents the first robotic instantiation of a complex language architecture based on the Baddeley's Working Memory (WM) model (Baddeley, 2012). We extend the procedural learning mechanisms of the model, to grant functionalities of information elaboration that integrates language with representation forms of perception and action. The goal is to learn how to map semantic forms to the perceived meanings from vision and motors, starting from primitive concepts (direct link) to more abstract concepts. The latter can be indirectly mapped to the physical world by transferring the links for primitive concepts-internal state of the robot-through verbal explanations on how these links are combined to construct an abstract concept-human tutor. 2. To achieve our aim on the acquisition of semantic meanings from the environment, continuously and on a scale-up corpus not anticipated in the model pre-training, we implemented new mechanisms of knowledge representation in and retrieval from the long-term memory. This enables the architecture to learn continuously, to gradually expand its internal knowledge with information and meanings from novel workspaces and to apply this newly-acquired knowledge directly on-demand. Language constructions, here referring to the linguistic definition as the group of words forming a phrase (grammar, syntax), are therefore acquired as the robot operates in its environment and converts natural language commands into action and/or partial language production. Finally, we implemented a learning feedback in the model, which enables the robotic system to infer the lack of or inconsistency of information during task-solving, to initiate a request for auxiliary input from the interlocutor and, to actively reuse the provided information to continue working on the task.\n\nThe remainder of the paper is organised as follows. In section Related Works, we review the body of literature on grounding language into perception and action. In section Materials and Methods, we describe the bio-inspired framework used to draw our method and our final robotic system. The contributed methodology is extensively explained in section The Proposed Learning Methodology. The robot learning experiments are described in section The Robot Learning Experiments. Section Handling Sensor-based Uncertainty discusses sensory-based error handling. The obtained results are presented in section Results and outcomes are further discussed",
            "score": 0.4006737717629546,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6411,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 344,
                    "matchedPaperCorpusId": "265952091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "Here, we illustrate the training processes of LLMs, which consist of at least three main steps: pretraining, which is done on the task of next token prediction, instruction fine-tuning, and preference alignment.\n\nPre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.\n\nInstruction Fine-Tuning.Instruction fine-tuning is the part of the training process of large language models aimed at enhancing their ability to follow specific directives and perform specialized tasks (Taori et al., 2023).This phase involves further training of the base model on curated datasets that include explicit instructions paired with corresponding responses.The objective is to teach the model how to interpret and execute various types of commands or queries effectively.During instruction fine-tuning, the model is exposed to a wide range of examples that illustrate how to respond to different types of prompts, from answering questions to generating summaries and more complex task-oriented interactions.The primary purpose of instruction fine-tuning is to improve the model's usability and performance in real-world applications.By aligning the model's behavior with the specific needs and expectations of users, instruction finetuning ensures that the model can handle diverse and nuanced tasks more effectively, enhancing its overall functionality and practical utility.This pro-cess might involve using an instruction template, where specific patterns and structures for providing instructions and expected responses are standardized to facilitate consistent and effective learning.It is worth noticing that this is still achieved with traditional supervised learning, unlike the next phase we are considering.",
            "score": 0.4006667585124933,
            "section_title": "LLM Training",
            "char_start_offset": 6387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 213,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1002
                },
                {
                    "start": 1004,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2092
                },
                {
                    "start": 2092,
                    "end": 2305
                },
                {
                    "start": 2305,
                    "end": 2433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "218470495",
            "title": "Reinforcement learning of minimalist grammars",
            "text": "Speech-controlled user interfaces facilitate the operation of devices and household functions to laymen. State-of-the-art language technology scans the acoustically analyzed speech signal for relevant keywords that are subsequently inserted into semantic slots to interpret the user's intent. In order to develop proper cognitive information and communication technologies, simple slot-filling should be replaced by utterance meaning transducers (UMT) that are based on semantic parsers and a mental lexicon, comprising syntactic, phonetic and semantic features of the language under consideration. This lexicon must be acquired by a cognitive agent during interaction with its users. We outline a reinforcement learning algorithm for the acquisition of syntax and semantics of English utterances, based on minimalist grammar (MG), a recent computational implementation of generative linguistics. English declarative sentences are presented to the agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as formulas of predicate logic. Since MG codifies universal linguistic competence through inference rules, thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach unifies generative grammar and reinforcement learning, hence potentially resolving the still pending Chomsky-Skinner controversy.",
            "score": 0.40042462974155546,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "272969654",
            "title": "The Effect of Learning Environment on Language Acquisition:A Survey of Chinese and Pakistani Siblings",
            "text": "The poverty of stimulus (1980) is a hypothesis in language learning which states that natural language grammar is not acquirable to the full given the relatively limited data available to children and adults learning a language, and therefore this knowledge is supplemented with some sort of innate linguistic capacity. Humans are born with a specific representational adaptation for language that both supports and constrains their competence to acquire linguistic representations in the course of their linguistic maturation. But the language a child is exposed to in the environment is full of confusing information (for example false starts. incomplete sentences or sips of tongue) and does not provide all the information which the child needs but still he ends up knowing far more about language than is exemplified in the language we hear around us. It is well known that children make mistakes during the course of their language journey but they are neither corrected nor do they pay much attention to these corrections. If this is the case how do they recover from their mistakes? According to Cook (1991) there are four steps to the poverty of stimulus argument which are shown in the figure below. Similarly, the principles of Universal Grammar underline the specific grammar of all languages that need to be acquired. This can only be acquired instinctively without coaching or tutoring. The left hemisphere of the brain is considered to be involved in the acquisition of human language and this part of the brain is specially equipped for language. This theory arose as a challenge to children as well as for their syntax development. \n\nAlthough syntactic knowledge does not provide adequate information about the language, it shares their brains to construct similar structure utterances and this provides a healthy environment to acquire the necessary knowledge.",
            "score": 0.4003504999144292,
            "section_title": "The Poverty of Stimulus:",
            "char_start_offset": 27649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09368896484375
        },
        {
            "corpus_id": "202542763",
            "title": "Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study",
            "text": "The experiments presented here extend and refine a line of research investigating what linguistic knowledge is acquired by neural language models. Previous studies have demonstrated that sequential models trained on a simple regime of optimizing the next word can learn long-distance syntactic dependencies in impressive detail. Our results provide complimentary insights, demonstrating that a range of model architectures trained on a variety of datasets can learn fine-grained information about the interaction of CoordNPs and local syntactic context, but their behavior remains unhumanlike in many key ways. Furthermore, to our best knowledge, this work presents the first psycholinguistic analysis of neural language models trained on French, a high-resource language that has so far been under-investigated in this line of research. \n\nIn the simple coordination experiment, we demonstrated that models were able to capture some of the agreement behaviors of humans, although their performance deviated in crucial aspects. Whereas human behavior is best modeled as a 'percolation' process, the neural models appear to be using a linear combination of NP constituent number to drive CoordNP/verb number agreement, with the second noun weighted more heavily than the first. In these experiments, supervision afforded by the RNNG and ActionLSTM models did not translate into more robust or humanlike learning outcomes. The complex coordination experiments provided evidence that the neural models tested were not using a simple 'bag of features' strategy, but were sensitive to syntactic context. All models tested were able to interpret material that had similar surface form in ways that corresponded to two different tree-structural descriptions, based on local context. The inverted coordination experiment provided a contrasting example, in which models were unable to modulate expectations based on subtleties in the syntactic environment. Across all our experiments, the French models performed consistently better on subject/verb number agreement than on subject/predicate gender agreement. Although there are likely more examples of subject/verb number agreement in the French training data, gender agreement is syntactically mandated and widespread in French.",
            "score": 0.40024567053559346,
            "section_title": "Discussion",
            "char_start_offset": 25363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "1008913",
            "title": "Order matters: Distributional properties of speech to young children bootstraps learning of semantic representations",
            "text": "The aim of this paper was twofold. First, we wanted to show that the CHILDES corpus \"starts small\". We showed this is the case for utterance length and variance and that these measures gradually increase over the course of the corpus. We also showed that the early portion of CHILDES contains fewer n-grams, indicating reduced structural complexity, and the same words are repeated more often compared to later portions of the corpus. \n\nOur second goal was to assess whether the simplification of linguistic input provided by CDS can produce an observable benefit for computational models of language and semantic development. Rohde & Plaut (1999) found that \"starting small\" is most effective when important linguistic dependencies span uninformative clauses in an artificial language corpus, arguing therefore that \"starting small\" should not benefit learning of naturalistic language. Indeed, we found no evidence that training over age-ordered CDS gradually increasing in complexity improves learning of sequential structure. However, we did find a beneficial effect on a semantic classification task. This effect was only observed when semantic representations were accessed without context. We also showed that removal of punctuation eliminates this effect. Training with utterance boundary cues likely helps the model to constrain what contextual information is most useful for learning. \n\nThree important questions remain. First, why does the order of linguistic experiences matter for learning of semantics but not syntax? It is possible that our measure of syntactic learning, based on the cross-entropy error, is not psychologically appropriate given that it measures the model's fit to the data rather than some deeper structural properties of language. Furthermore, since cross-entropy is an average measure over all words in the corpus, it simply might not be sensitive enough. \n\nSecond, why did we observe an advantage for the acquisition of abstract, (non-contextualized) and not contextualized representations? One possible answer is that abstraction requires learning of higher order features of the input, and therefore \"starting small\" might direct the model's representational trajectory towards learning those higher order features earlier (Clark, 1994). \n\nThird, what are the specific reasons that training on the chronologically ordered corpus yielded better results? We have argued that the effects are due to \"starting small\", and specifically due to the organizational role of early boundary cues.",
            "score": 0.40016409461211794,
            "section_title": "Discussion",
            "char_start_offset": 22297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2276
                },
                {
                    "start": 2279,
                    "end": 2391
                },
                {
                    "start": 2392,
                    "end": 2524
                }
            ],
            "ref_mentions": [
                {
                    "start": 627,
                    "end": 647,
                    "matchedPaperCorpusId": "961169"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "247768613",
            "title": "Can Recurrent Neural Networks Validate Usage-Based Theories of Grammar Acquisition?",
            "text": "Usage-based theories take a graded view on acquisition of linguistic structures, acknowledging that partial competence can be observed, blurring the distinction between semantic and syntactic knowledge, and ultimately, allowing for a range of varied grammatical intuitions across speakers. Existing studies on the grammatical abilities of RNNs report results which tend to confirm this view, but they are interpreted in different ways, as we will presently see. Wilcox et al. (2018) address the phenomenon of filler-gap dependencies (e.g., the dependency existing between what and its gap in I know what/ \u22c6 that the lion devoured -at sunrise), evaluating the surprisal values assigned by the pre-trained language models of Gulordava et al. (2018) and Chelba et al. (2013). Their results show that neural language models show high peaks of surprisal in the post-gap position, irrespective of the syntactic position where the gap happens (either subject, object or prepositional phrase). When considering the whole clause, however, predictions related to the subject position are much stronger than for the other two positions, correlating with human online processing results. Overall, their results indicate that filler-gap dependencies, and the constraints on them, are acquired by language models, albeit in a graded manner, and in many cases correlate with human judgements. Similar results are reported by Chowdhury and Zamparelli (2018), but the authors commit to a stronger binary distinction between competence and performance, ultimately stating that their model \"is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality.\" \n\nA call for full abstraction, as opposed to a graded view of syntactic abilities, is also expressed in Marvin and Linzen (2018): English artificial sentence pairs (i.e., a grammatical sentence with its ungrammatical counterpart) are automatically built using a non recursive context free grammar, with the intent of minimizing \"the semantic or collocational cues that can be used to identify the grammatical sentence.\"",
            "score": 0.4001070733821217,
            "section_title": "GRADED VS. DISCRETE NOTION OF GRAMMAR",
            "char_start_offset": 14918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 462,
                    "end": 482,
                    "matchedPaperCorpusId": "52156878"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1410,
                    "end": 1441,
                    "matchedPaperCorpusId": "52010508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "53034786",
            "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks",
            "text": "Natural language has a sequential overt form as spoken and written, but the underlying structure of language is not strictly sequential. This structure is usually tree-like. Linguists agree on a set of rules, or syntax, that governs this structure (Sandra & Taft, 2014), and the structure also dictates how the words compose to form components of sentences. Despite being discovered by linguistics, human brain can also implicitly acquire the latent structure of language. In human language acquisition, children are not given annotated parse trees. This observation brings more interest in studying the latent structure induction with artificial neural network approaches, which are inspired by information processing and communication patterns in biological nervous systems. \n\nFrom a practical point of view, integrating this tree structure into a language model is also important for different reasons: \n\n1. to obtain a hierarchical representation with increasing levels of abstraction, a key feature of deep neural networks (Bengio et al., 2009;LeCun et al., 2015;Schmidhuber, 2015); \n\n2. to capture complex linguistic phenomena, like the long-term dependency problem (Tai et al., 2015) and the compositional effects (Socher et al., 2013); \n\n3. to provide shortcut for gradient back-propagation (Chung et al., 2016). \n\nDeveloping deep neural network techniques that can leverage syntactic knowledge, or at least some tree structure (Williams et al., 2018;Shi et al., 2018) , to form better semantic representations have received a great deal of attention in recent years (Shen et al., 2017;Jacob et al., 2018;Bowman et al., 2016;Choi et al., 2018;Yogatama et al., 2016). \n\nOne straightforward way of obtaining the tree structure is through a supervised syntactic parser. Trees produced by these parsers have been used to guide the composition of word semantics into sentence semantics (Socher et al., 2013;Bowman et al., 2015), or even to help next word predicition given previous words (Wu et al., 2017).",
            "score": 0.4000726642459815,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1049,
                    "matchedPaperCorpusId": "207178999"
                },
                {
                    "start": 1049,
                    "end": 1068,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 1068,
                    "end": 1086,
                    "matchedPaperCorpusId": "11715509"
                },
                {
                    "start": 1221,
                    "end": 1242,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1436,
                    "end": 1459,
                    "matchedPaperCorpusId": "3527842"
                },
                {
                    "start": 1594,
                    "end": 1613,
                    "matchedPaperCorpusId": "51881558"
                },
                {
                    "start": 1633,
                    "end": 1651,
                    "matchedPaperCorpusId": "24462966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "264590507",
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "text": "Recent advancements in natural language processing (NLP) can be attributed to the development and pretraining of large language models (LLMs) such as BERT, GPT-3, and many others (Devlin et al., 2019;Brown et al., 2020;Touvron et al., 2023). For their intended use of providing general-purpose language representations suitable for many NLP tasks, these models must efficiently capture a wide range of linguistic features within their finite capacity. Despite their success, little is known about the way in which different types of linguistic information are organized in these models. Systematically understanding how these models represent linguistic phenomena and their interaction is crucial for the development of more effective NLP methods. \n\n\u22c6 Corresponding author: giulio.starace@gmail.com. \n\nExisting research probed LLMs for their encoding of various linguistic properties such as agreement (Jawahar et al., 2019), word order and sentence structure (Tenney et al., 2018;Hewitt and Manning, 2019), co-reference (Tenney et al., 2019), semantics (Ettinger, 2020) and multilinguality (Ravishankar et al., 2019;Libovick\u00fd et al., 2020). Taking a step further, Tenney et al. (2019) and Clark et al. (2019) studied where linguistic information is encoded in LLMs by probing different layers. Their results demonstrated that a linguistic hierarchy emerges in BERT representations, with lower layers capturing local syntax and higher layers being employed in higher-level semantic and discourse tasks. However, we do not yet understand how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. \n\nThere are many dependencies between processing different linguistic phenomena: for instance, information about a word's part of speech is likely to be employed when disambiguating its word sense. Alternatively, lower-level syntax is an important first step for semantic composition and natural language understanding tasks. In this work, we investigate how the (hierarchical) dependencies between different linguistic categories are encoded in LLMs, focusing on syntax. We ask a set of novel questions: \n\n(1) how related syntactic categories (e.g.",
            "score": 0.3997664351960497,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2233
                },
                {
                    "start": 2236,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 200,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 902,
                    "end": 924,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 960,
                    "end": 981,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 981,
                    "end": 1006,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1021,
                    "end": 1042,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1054,
                    "end": 1070,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1091,
                    "end": 1117,
                    "matchedPaperCorpusId": "203690445"
                },
                {
                    "start": 1117,
                    "end": 1140,
                    "matchedPaperCorpusId": "215745575"
                },
                {
                    "start": 1165,
                    "end": 1185,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1190,
                    "end": 1209,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "14435666",
            "title": "Self-organizing map models of language acquisition",
            "text": "For simultaneous learning, the two lexicons were presented to the network and trained in parallel (see Li and Farka\u0161, 2002 for a previous example in this training regime). For early L2 learning, the onset time of L2 input to the model was slightly delayed relative to that of L1 input, that is, training on L2 vocabulary occurred at a point after 1/5 of the entire L1 vocabulary had been presented to the network. For late L2 learning, the onset time of L2 input was significantly delayed relative to that of L1, that is, training on L2 vocabulary occurred at a point after 4/5 of the entire L1 vocabulary had been presented to the network. Specifically, the simultaneous learning situation is analogous to a situation in which children are raised in a bilingual family and receive linguistic inputs from the two languages simultaneously (e.g., Li and Farka\u0161, 2002 used input based on the two parents' different language input). The early learning situation could be compared to the situation in which bilinguals acquired their L2 early in life (e.g., in early childhood) while the late learning situation to that of a bilingual's learning of L2 later in life (e.g., after puberty). \n\nOne key pattern from our simulations is illustrated in Figure 4, which shows how lexical representations from the two languages are distributed differently in the three different learning conditions. Here, black regions indicate those nodes that represent the L2 (English) words whereas white regions the L1 (Chinese) words learned by the model. Specifically, if a unit's weight vector is the closest to the input vector of an English word, the unit is marked in black. If a unit's weight vector is most similar to the input pattern of a Chinese word, the unit is marked in white. \n\nIt is clear from Figure 4 that the relative onset time of L2 vs. L1 plays an important role in modulating the overall representational structure of the L24 . For the simultaneous acquisition situation (Figures 4A,B), DevLex-II showed clear distinct lexical representations of the two languages at both the phonological and semantic levels and within each language.",
            "score": 0.39954976552913324,
            "section_title": "Modeling age-of-acquisition effects",
            "char_start_offset": 46991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 122,
                    "matchedPaperCorpusId": "34932504"
                },
                {
                    "start": 845,
                    "end": 864,
                    "matchedPaperCorpusId": "34932504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "243538114",
            "title": "Kindergarteners Use Cross-Situational Statistics to Infer the Meaning of Grammatical Elements",
            "text": "Many studies demonstrate that detecting statistical regularities in linguistic input plays a key role in language acquisition. Yet, it is unclear to what extent statistical learning is involved in more naturalistic settings, when young children have to acquire meaningful grammatical elements. In the present study, we address these points, by investigating whether statistical learning is involved in acquiring a morpho-syntactic structure from input that resembles natural languages more closely. We exposed 50 kindergarteners (M\u2009=\u20095 years, 5 months) to a miniature language in which they had to learn a grammatical marker that expressed number, and which could only be acquired on the basis of the distributional properties in the input. Half of the children performed an attention check during the experiment. Results show that young children are able to learn this meaning. We found no clear evidence that facilitating attention to the input increases learning performance.",
            "score": 0.3994487026629566,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "271088482",
            "title": "The Computational Learning of Construction Grammars: State of the Art and Prospective Roadmap",
            "text": "The experiments show that the model exhibits effects of syntactic and semantic bootstrapping, that it can recover from over-generalisation, and that it is robust to noise, thereby mimicking the stages of child language acquisition.Chang (2008) presents a set of learning operators that operationalise Bayesian model merging in the framework of Embodied Construction Grammar (ECG).The task is to learn a grammar that can correctly comprehend utterances from a held-out test set at certain intervals during training.The dataset that is used consists in a schemaannotated subset of the CHILDES corpus, containing parent-child interactions from a child from 15 until 24 months of age.A lexicon and ontology are provided, corresponding to the linguistic knowledge of a child at the two-word stage.Two classes of learning operators are introduced: mapping operators and reorganisation operators.In the first class of operators, the simple mapping operator maps either all, or a part of the uncovered form to its co-occurring context information.Relational mapping creates a new construction that encodes the syntactic and semantic relations between existing constructions that could apply.The applied constructions become constituents in the newly formed item-based construction.For the input utterance \"throw ball\" for example, the lexical constructions for throw and ball may already be known.A new construction is then created that encodes that throw precedes ball, and that the ball is the throwee in the throwing event.In the class of reorganisation operators, a structural alignment step pre-computes compatible alignments for further reorganisation.Next, three operators can apply: merging, joining and splitting.The merge operator learns a generalised construction for two constructions that contain shared structure.For example, throw-ball and throw-block can be generalised to throw-toy.A second reorganisation operator joins existing constructions that have a certain overlap, for example human-throw and throw-bottle can be joined into human-throw-bottle.\n\nInversely, the splitting operator creates new constructions by taking the set difference between existing constructions, for example throw-frisbee and throw can be split into frisbee, and a new item-based construction, throw-X, which takes the newly created frisbee-construction as a constituent.",
            "score": 0.39939051403654713,
            "section_title": "Learning a grammar from utterance-meaning pairs",
            "char_start_offset": 11929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1518
                },
                {
                    "start": 1518,
                    "end": 1650
                },
                {
                    "start": 1650,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 2061
                },
                {
                    "start": 2063,
                    "end": 2359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "261696384",
            "title": "Predicting Age of Acquisition for Children's Early Vocabulary in Five Languages Using Language Model Surprisal",
            "text": "Hills et al. (2010) found that higher contextual diversity predicted earlier word acquisition, though this effect appeared to be strongest for predicates and function words when controlling for the effect of word frequency. Such a predictor can be considered a proxy for some semantic factors, approximating the overall semantic richness of children's learning environment. However, contextual 15516709, 2023, 9, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/cogs.13334 by University Of Wisconsin -Madison, Wiley Online Library on [03/11/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License E. Portelance et al. / Cognitive Science 47 (2023) 3 of 26 diversity abstracts away from the semantic information encoded in specific sequential linguistic contexts-utterances or conversations-experienced by learners (a point we discuss in Section 3.1 of this paper) and, thus, does not directly measure syntactic complexity. \n\nHere, we examine another contextual factor-the predictability of words across the linguistic contexts in which they appear. A word's predictability takes into account its linguistic context as a whole, including both syntactic and semantic information. It allows us to identify words that might be more or less difficult to learn as a function of the word sequences they tend to appear in. \n\nThere are multiple reasons for focusing on predictability. First, it is easy to compute, as we detail below. Second, it is psychologically real-we know that statistical factors such as transition probabilities play important roles in just about every aspect of early language learning (Saffran, 2020). Predictability (operationalized in terms of surprisal, see below) has also been shown to be a strong predictor of processing difficulty in adult psycholinguistic experiments (Demberg & Keller, 2008;Levy, 2008;Smith & Levy, 2013).",
            "score": 0.3989394034757589,
            "section_title": "Introduction",
            "char_start_offset": 3215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 1758,
                    "end": 1773,
                    "matchedPaperCorpusId": "213220473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1829833984375
        },
        {
            "corpus_id": "253098151",
            "title": "An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks",
            "text": "With the above questions, we empirically revisit the effectiveness of linguistic knowledge fusion in language understanding tasks. Motivated by Shi et al. (2018), we compare the performance between original dependency-parsed trees and balanced trees for syntax fusion, and compare the results between parsed semantic graphs and sequential graphs for semantic fusion. To our surprise, trivial graphs outperform syntactic trees or semantic graphs in full-supervised setting and achieve competitive results in few-shot setting. All the evidence again shows that the linguistic inductive bias might not be the major contributor of consistent improvements over baselines. Additional analysis gives some clues that the possible reasons are extra model parameters and feature interactions from fusion modules. This work encourages future research to add trivial graphs as necessary baselines when designing more advanced knowledge fusion methods for downstream tasks. Our experimental code is available at https://github.com/HKUST-KnowComp/ revisit-nlu-linguistic-knowledge.",
            "score": 0.3988464554503206,
            "section_title": "Introduction",
            "char_start_offset": 1882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1067
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 161,
                    "matchedPaperCorpusId": "52127792"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "257057439",
            "title": "The role of Function Words to build syntactic knowledge in French-speaking children",
            "text": "Exploring whether FWs may be grammatically functional early in the acquisition of language is also in line with the bottom-up driven hypothesis. Under this assumption, by attending to the recurring phonological, prosodic and distributional characteristics which FWs typically share young children could derive some useful information for (i) segmenting the continuous speech stream into a set of distinct constituents, (ii) discovering the syntactic class of words and phrases. \n\nYoung children, accordingly, could be using a comprehensive strategy in sentence processing, one which incorporates not only the meaning-carrying units (i.e., content words) but also the functional elements in language. In the learning process, children extract FWs from the input, making use of the formal distributional properties of their native language. They have a surface knowledge and use FWs to process auditory and linguistic input very early on. Doing so, they gradually construct FWs that manage the multiword sequences in an utterance. Learners do not acquire syntax in a piecemeal and isolated fashion. In such learning-based theories, the young child is sensitive to the phonological, prosodic and distributional patterns in language, and relies on general cognitive (not language-specific) mechanisms to generalize these patterns into a full grammar. \n\nThe apparent disagreement between the lexically and grammatically driven approaches (late vs early structure building hypothesis) will be tested in this corpus study using a stringent linguistic coding from the CHILDES 19 . \n\nThe main purpose of this study is to investigate how French-speaking children learn FWs to build syntactic knowledge. We hypothezize that acquisition of FWs and morphemes primarily bootstraps syntactic development in order to facilitate syntactic knowledge. A data-driven model will be used to support a syntactic connectivity approach raising the two following questions: (i) Which FW(s) is (are) the most predictive of MLU? and (ii) how FWs are hierarchically organized and reflect sentence complexity. Specifically, we will examine a subset of eighteen FWs since they have a highly predictable distribution from a large corpora of 315 monolingual French children aged 2 to 4.",
            "score": 0.39875830873168144,
            "section_title": "body",
            "char_start_offset": 2029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2253
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.182861328125
        },
        {
            "corpus_id": "258437175",
            "title": "Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks",
            "text": "The sounds of human speech are a measurable, physical property, but they also encode abstract linguistic information such as syntactic, phonological, morphological, and semantic properties. It has been shown that the model learns symbolic-like rule-like linguistic representations from raw speech (Begu\u0161, 2021a(Begu\u0161, ,b, 2020;;Begu\u0161, 2022) which illustrates that discrete mental representational units can emerge from continuous physical properties such as sound in deep neural networks. The model features several aspects of human spoken language learning that other models lack: learning by imitation, learning through the production-perception loop, and communicative intent. \n\nPrior models of human syntax are predominantly text-based and mostly lack explicit mechanisms that can be directly paralleled to language processing in the human brain. Hearing human learners, however, acquire syntax from acoustic inputs and process it with biological neural mechanisms. Few studies model syntax from speech with artificial neural networks. Previous work has shown that speech-trained models can feature syntactic representations, but the models featured specific syntactic mechanisms (Lai et al., 2023) or were pre-trained (Singla et al., 2022). These architectures do not model the emergence of syntax. \n\nHere, we propose that we can model a shift from single unit system to a concatenative and partially compositional system using ciwGAN. We model how compound signals or concatenated words can arise spontaneously in deep neural networks trained on raw speech in a fully unsupervised manner. We thus also provide evidence suggesting that domain-general learning models (such as CNNs) spontaneously concatenate signals in a learning setting that involves informative imitation: learning by imitation and with the production-perception loop from raw spoken language data (ciwGAN). The paper thus introduces a model of the first step in the development from holistic nonsyntactic signals to compositionally concatenated signals that represents the precursor to syntactic processing. We also show that the observed spontaneous concatenation is not an idiosyncratic property of a single model, but emerges in several replicated models as well as in models that features several changes in the architecture.",
            "score": 0.39874272460678656,
            "section_title": "Introduction",
            "char_start_offset": 4161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 310,
                    "matchedPaperCorpusId": "219305353"
                },
                {
                    "start": 328,
                    "end": 340,
                    "matchedPaperCorpusId": "247596646"
                },
                {
                    "start": 1223,
                    "end": 1244,
                    "matchedPaperCorpusId": "256668471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "233289478",
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "text": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
            "score": 0.39857759446045876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "278207827",
            "title": "Param$\\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost",
            "text": "We hypothesize that the language model can acquire the knowledge and capabilities characteristic of the post-training stage by incorporating the parameter differences between a post-trained checkpoint and a base checkpoint. Conventionally, the foundational knowledge of a large language model is established through initial pretraining from scratch. This is followed by a post-training process, which includes supervised finetuning (SFT) (Brown, 2020;Dubey et al., 2024;Touvron et al., 2023) and reinforcement learning including DPO (Dubey et al., 2024;Rafailov et al., 2024;Touvron et al., 2023), PPO (Ouyang et al., 2022;Schulman et al., 2017) or GRPO (Shao et al., 2024). \n\nOur proposed approach bypasses the standard post-training process and instead integrates the available post-trained model's parameter deltas into the new base model, as the parameter deltas encapsulate the embedded knowledge derived from the underlying training data and reflect the cumulative gradient changes resulting from the volume of training tasks. This can be represented as \n\nwhere \u0398 \u2032 base is an updated base model, either from newly pretrained or continually pretrained. \u2206\u0398 is calculated from \u2206\u0398 = \u0398 post \u2212\u0398 base with pre-existing model checkpoints, T post-train symbolizes the post-training process.",
            "score": 0.39857759446045876,
            "section_title": "KNOWLEDGE TRANSFER THROUGH MIXING EXISTING MODELS' PARAMETERS",
            "char_start_offset": 5270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1288
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 575,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 602,
                    "end": 623,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "276725505",
            "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs",
            "text": "Several influential works have explored the nature of latent representations in language models. Petroni et al. [2019] demonstrated that LLMs can act as implicit knowledge bases, storing factual information in their latent spaces. Similarly, Wei et al. [2022a] investigated how different model architectures and pretraining objectives impact zero-shot generalization, showing that improved latent representations correlate with enhanced generalization capabilities. A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics. \n\nKey insights include: \n\n\u2022 Specialized Directions: LLMs develop latent directions that encode specific concepts or linguistic features. \n\n\u2022 Layerwise Abstraction: Lower layers capture syntactic details while higher layers integrate complex semantic and world knowledge. \n\n\u2022 Scaling Benefits: The quality of latent representations improves with increased model scale and optimized architectures. \n\n3 Proposed Framework",
            "score": 0.39848136981393567,
            "section_title": "Latent Representations in LLMs",
            "char_start_offset": 2085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1119
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 118,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 242,
                    "end": 260,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "259236262",
            "title": "HunEmBERT: A Fine-Tuned BERT-Model for Classifying Sentiment and Emotion in Political Communication",
            "text": "In Natural Language Processing, Language Models determine the probability of word or word sequences by analyzing textual data and mainly by learning abstractions of syntactic and semantic rules. The model then applies these rules to solve linguistic-based problems (such as part of speech tagging) and to predict or generate new sentences accurately. This general language ''knowledge'' acquired on a large data set can then be used to solve downstream tasks, such as sequence labeling or named entity recognition [68].\n\nConsidering that human languages follow a sequential structure (texts are composed of sentences, their constituent clauses -syntactic constituents, which are built up by words or word combinations, and words are ultimately built of a sequence of characters in the written representation), the beginning of language modeling was marked by Recurrent Neural Network (RNN) architectures [69]. These were the first neural networks in which the states of individual neurons within a layer could interact [70].\n\nAs RNNs could suffer from ''vanishing'' or ''exploding'' gradients when handling longer sequences, an improved architecture for RNNs, the Long Short-Term Memory (LTSM) architecture [71] was developed to address this. Given that the entire history of the processed sequence was to be stored in a single state vector, this was not perfectly efficient in handling longer contexts.\n\nThe dramatic surge in available computing power soon led to solutions based on deep-learning neural networks [72].\n\nFocusing on the concept of ''attention,'' the first breakthrough transformer architecture was released in 2017 [73]. The original transformer architecture was based on an encoderdecoder architecture. The former's layers iteratively process the received sequential input (e.g., natural language text) and form encodings that contain information about which parts of the input are relevant to each other. The decoder layers work oppositely, taking all the encodings and storing the contextual information to form an output sequence. Such models (e.g., GPT-1 [74], and BERT [64]) were the first to achieve significant success in 2018 in various NLP tasks, such as language modeling, sentiment analysis and question answering.\n\nThis has led to the issue of transfer learning, where the aim is to encode knowledge accumulated while learning a particular task that is also suitable",
            "score": 0.39820383162667483,
            "section_title": "3) TRANSFORMER-BASED MODELS",
            "char_start_offset": 12977,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "233324191"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "2763403"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1514,
                    "end": 1518,
                    "matchedPaperCorpusId": "1779661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "247476208",
            "title": "Geographic Adaptation of Pretrained Language Models",
            "text": "The default tool for the majority of NLP tasks is now de facto pretrained language models (PLMs; Devlin et al., 2019;Liu et al., 2019b;Radford et al., 2019;Brown et al., 2020;Clark et al., 2020;Raffel et al., 2020;Chowdhery et al., 2022;Hoffmann et al., 2022;Touvron et al., 2023, inter alia), which are trained using language modeling objectives on large text corpora. Despite the conceptual simplicity of language modeling, pretraining induces complex forms of linguistic knowledge in PLMs, at various levels (Rogers et al., 2020;Mahowald et al., 2023): morphological (Edmiston, 2020;Hofmann et al., 2020;Weissweiler et al., 2023), lexical (Ethayarajh, 2019;Vuli\u0107 et al., 2020), syntactic (Hewitt and Manning, 2019;Jawahar et al., 2019;Wei et al., 2021;Weissweiler et al., 2022), and semantic (Wiedemann et al., 2019;Ettinger, 2020). This general linguistic knowledge is then (re-)shaped for concrete tasks via fine-tuning, i.e., supervised training on task-specific labeled data. \n\nHumans, however, additionally make use of a rich spectrum of extralinguistic features when they learn and process language, including gender (Lass et al., 1979), ethnicity (Trent, 1995), and geography (Clopper and Pisoni, 2004). Despite the growing awareness for the importance of such factors in NLP (Hovy and Yang, 2021), extralinguistic features have been typically introduced in the finetuning phase so far, i.e., when specializing PLMs for a concrete task (e.g., Rosin et al., 2022). This prevents PLMs from forming generalizable representations the way humans do, impeding the exploitation of extralinguistic knowledge for tasks other than the fine-tuning task itself.",
            "score": 0.3979512589679904,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1659
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 117,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 117,
                    "end": 135,
                    "matchedPaperCorpusId": "233296168"
                },
                {
                    "start": 156,
                    "end": 175,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 175,
                    "end": 194,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 194,
                    "end": 214,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 586,
                    "end": 607,
                    "matchedPaperCorpusId": "222206204"
                },
                {
                    "start": 607,
                    "end": 632,
                    "matchedPaperCorpusId": "264436524"
                },
                {
                    "start": 642,
                    "end": 660,
                    "matchedPaperCorpusId": "202120592"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "222290596"
                },
                {
                    "start": 691,
                    "end": 717,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 717,
                    "end": 738,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 738,
                    "end": 755,
                    "matchedPaperCorpusId": "237513713"
                },
                {
                    "start": 755,
                    "end": 780,
                    "matchedPaperCorpusId": "253097872"
                },
                {
                    "start": 819,
                    "end": 834,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1126,
                    "end": 1145,
                    "matchedPaperCorpusId": "222340392"
                },
                {
                    "start": 1157,
                    "end": 1170,
                    "matchedPaperCorpusId": "123162738"
                },
                {
                    "start": 1186,
                    "end": 1212,
                    "matchedPaperCorpusId": "19327056"
                },
                {
                    "start": 1286,
                    "end": 1307,
                    "matchedPaperCorpusId": "235097460"
                },
                {
                    "start": 1453,
                    "end": 1472,
                    "matchedPaperCorpusId": "238743808"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "254322981",
            "title": "On Robustness and Sensitivity of a Neural Language Model: A Case Study on Italian L1 Learner Errors",
            "text": "First, CItA is supplied with the manual annotation of learner errors and their corresponding correction. The variety of errors contained in the corpus, which pertain to different linguistic domains, makes it particularly suitable to deeply investigate the sensitivity of a NLMs to erroneous language uses and possibly find a hierarchy of errors according to their impact on neural representations. Second, authentic texts written by L1, rather than secondlanguage (L2) learners, are not affected by the interference of a pre-existing language and the peculiarities they exhibit can reflect both errors deriving from a still immature writing competence and less acceptable forms that are more acceptable in informal spoken language than in formal writing. \n\nContributions: In this paper, we investigated whether and to which extent the presence of non-standard linguistic forms (errors) has an impact on BERT's inner mechanisms evaluated in terms of i) semantic robustness, measured through a semantic probing task that consists in assessing the model's capability to represent the semantic similarity between sentences and evaluating whether and how it changes in presence of errors and ii) linguistic sensitivity, tested through a set of linguistic probing tasks devised to assess the model's ability to predict the distribution of multiple raw and morpho-syntactic features extracted from sentences containing errors and from their corrected counterparts. The two evaluation tasks highlighted that learner errors have an impact on BERT's representations, which is differently modulated by the typology of error itself, as well as by the different layers of representation taken into account. In particular, we showed that as the output layer is approached, the occurrence of non-standard forms makes the model, on the one hand, less robust with respect to the resolution of the semantic probing task and, on the other hand, less linguistically sensitive, thus more able to correctly predict a variety of raw and morpho-syntactic properties of a sentence despite the presence of errors. This allowed us to define two hierarchies of errors, different in terms of their impact on the semantic and linguistic abilities that the model was able to acquire during pre-training.",
            "score": 0.39792024885785904,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 5506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "238215568",
            "title": "Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations",
            "text": "An extensive body of works is devoted to analyzing syntactic knowledge of Transformer language models (LMs) (Vaswani et al., 2017;Clark et al., 2019;Goldberg, 2019;Belinkov and Glass, 2019). BERTbased LMs (Devlin et al., 2019) have demonstrated their abilities to encode various linguistic and hierarchical properties (Lin et al., 2019;Jawahar et al., 2019;Jo and Myaeng, 2020) which have a positive effect on the downstream performance (Liu et al., 2019a;Miaschi et al., 2020) and serve as an inspiration for syntax-oriented architecture improvements (Wang et al., 2019;Bai et al., 2021;Ahmad et al., 2021;Sachan et al., 2021). Besides, a variety of pre-training objectives has been introduced (Liu et al., 2020a), with some of them modeling reconstruction of the perturbed word order (Lewis et al., 2020;Tao et al., 2021;Panda et al., 2021). \n\nRecent research has adopted a new experimental direction aimed at exploring the syntactic knowledge of LMs and their sensitivity to word order employing text perturbations (Futrell et al., 2018(Futrell et al., , 2019;;Ettinger, 2020). Some studies show that shuffling word order causes significant performance drops on a wide range of QA tasks (Si et al., 2019;Sugawara et al., 2020). However, a number of works demonstrates that such permutation has little to no impact during the pre-training and finetuning stages (Pham et al., 2020;Sinha et al., 2020Sinha et al., , 2021;;O'Connor and Andreas, 2021;Hessel and Schofield, 2021;Gupta et al., 2021). The latter contradict the common understanding on how the hierarchical and structural information is encoded in LMs (Rogers et al., 2020), and even may question if the word order is modeled with the position embeddings (Wang et al., 2020;Dufter et al., 2021).",
            "score": 0.3978341074778273,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 130,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 164,
                    "end": 189,
                    "matchedPaperCorpusId": "56657817"
                },
                {
                    "start": 205,
                    "end": 226,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "174799346"
                },
                {
                    "start": 336,
                    "end": 357,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 357,
                    "end": 377,
                    "matchedPaperCorpusId": "220047270"
                },
                {
                    "start": 437,
                    "end": 456,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "222134013"
                },
                {
                    "start": 571,
                    "end": 588,
                    "matchedPaperCorpusId": "232147874"
                },
                {
                    "start": 588,
                    "end": 607,
                    "matchedPaperCorpusId": "235352715"
                },
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "221186730"
                },
                {
                    "start": 786,
                    "end": 806,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 806,
                    "end": 823,
                    "matchedPaperCorpusId": "235097612"
                },
                {
                    "start": 823,
                    "end": 842,
                    "matchedPaperCorpusId": "235097321"
                },
                {
                    "start": 1039,
                    "end": 1064,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 1064,
                    "end": 1079,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1207,
                    "end": 1229,
                    "matchedPaperCorpusId": "208201969"
                },
                {
                    "start": 1449,
                    "end": 1476,
                    "matchedPaperCorpusId": "236460117"
                },
                {
                    "start": 1476,
                    "end": 1495,
                    "matchedPaperCorpusId": "231573267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "278237653",
            "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs",
            "text": "Recent evidence shows that LLMs can align with human brain responses under biologically plausible training conditions (Hosseini et al., 2024), despite LLMs typically requiring orders of magnitude more training data than humans receive. This supports the application of ecological validity (Schmuckler, 2001) that emphasizes real-world data to mimic cognitive development. Drawing from this principle, Jagadish et al. ( 2024) selects linguistically diverse environments to reflect children's language acquisition processes, while Feng et al. ( 2024) utilizes child-directed speech to mirror naturalistic learning contexts. In parallel, developmental theories like incremental numerical understanding (Piaget, 2013) that views numerical concepts as gradually acquired through exposure offer insights into sequential data organization. Applying this framework, Sharma et al. (2024) introduces mathematically coherent numeric anchors to align the collection process. \n\nData Preprocessing Data preprocessing inspired by cognitive psychology involves refining data to enhance informational coherence prior to training. Selective attention (Treisman, 1969) -distinct from the attention mechanisms in transformersprioritizes cognitively salient information while filtering out irrelevant stimuli. Implementing this principle, Nottingham et al. (2024) develops a preprocessing model that identifies and filters irrelevant data. Meanwhile, predictive coding (Rao & Ballard, 1999) proposes anticipatory processing based on prior knowledge, providing a framework for structuring sequential information. Leveraging this insight, Araujo et al. (2021) structures sentences to enable anticipation of subsequent content, improving semantic coherence through expectation-driven processing. \n\nReinforcement Learning from Human Feedback (RLHF) A classic behavioral theory, the Operant Conditioning theory posits that behaviors are systematically strengthened or weakened by the consequences (rewards or punishments) that immediately follow them (Thorndike, 1898;Skinner, 1957). The principles of reinforcement learning align closely with this psychological framework, particularly in the post-training phase of LLM development, where RLHF explicitly operationalizes Operant Conditioning theory to align model behaviors with human values and preferences.",
            "score": 0.39766849382994485,
            "section_title": "Data Collection",
            "char_start_offset": 3990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 141,
                    "matchedPaperCorpusId": "262127677"
                },
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "17850908"
                },
                {
                    "start": 858,
                    "end": 878,
                    "matchedPaperCorpusId": "268856755"
                },
                {
                    "start": 1133,
                    "end": 1149,
                    "matchedPaperCorpusId": "9443995"
                },
                {
                    "start": 1318,
                    "end": 1342,
                    "matchedPaperCorpusId": "270514531"
                },
                {
                    "start": 1448,
                    "end": 1469,
                    "matchedPaperCorpusId": "221608503"
                },
                {
                    "start": 1616,
                    "end": 1636,
                    "matchedPaperCorpusId": "237485093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "11312452",
            "title": "A therapy programme for pre-school language impaired children.",
            "text": "Morehead and Johnson 15 have pointed out that most programmes in speech pathology use learning theory as a basis for instruction. Within this type of approach, the case is presented with a model sentence, is asked to repeat it and given immediate reward. \n\nMost speech and language therapy is structured for direct immediate experience and the unstable changes that occur with the use of this technique are well-known to therapists and generally referred to as \"carry over problems. 15 he approach utilised in this procedure hoped to avoid the problems inherent in a behaviourist model, by attempting to teach language as a system of rule-operated behaviour. Regarding language in this framework, it is hypothesised that language deviant children do not extract the rules governing the grammar of the language, as opposed to the normal developing child who somehow learns the semantic restrictions of various linguistic categories. The aim of therapy, therefore, was to enable the Ss to extract and abstract syntactic rules and generalise these rules to novel and creative situations. Thus, the child was not reinforced for a repetition of a model sentence, but rather for being able to generate novel sentences utilising rules taught in therapy. In this way, a 'cognitive' rather than 'behaviourist' approach is used. \n\nFollowing the initial diagnosis, a therapy programme was devised for each subject. Therapy was conducted over a six month period, during which time each child was seen twice weekly. An attempt was made in devising the therapy programme to establish at what point during the acquisition of syntax, the child had made an incorrect hypothesis in the learning of syntactic rules. Each programme was devised to stimulate the child at the point at which he failed to establish correct phrase structure rules and to enable him to 'releam' parts of the grammar which were dependent on the acquisition of these earlier rules. A 'slot method' was employed in an attempt to demonstrate the rule that the S had failed to acquire. It was felt that if imitation of a constant structure could be established (where the syntactic structures and semantic relations were held constant) the S might abstract the rule operating and generalise it to new creative utterances. Thus the basic structure of the sentence was held constant and the lexical item of one category varied in order to demonstrate the rule which gave rise to the underlying meaning of the sentence.",
            "score": 0.39751948024974126,
            "section_title": "THERAPY",
            "char_start_offset": 6410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 254
                },
                {
                    "start": 257,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2469
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 23,
                    "matchedPaperCorpusId": "141054509"
                },
                {
                    "start": 483,
                    "end": 485,
                    "matchedPaperCorpusId": "141054509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15234375
        },
        {
            "corpus_id": "258841771",
            "title": "Can LLMs facilitate interpretation of pre-trained language models?",
            "text": "A large body of work done on interpreting pretrained language models answers the question: What knowledge is learned within these models? Researchers have investigated the concepts encoded in pre-trained language models by probing them against various linguistic properties, such as morphological (Vylomova et al., 2017;Belinkov et al., 2017a), syntactic (Linzen et al., 2016;Conneau et al., 2018;Durrani et al., 2019), and semantic (Qian et al., 2016;Belinkov et al., 2017b) tasks, among others. Much of the methodology used in these analyses heavily rely on either having access to an annotated corpus that pertains to the linguistic concept of interest (Tenney et al., 2019;Liu et al., 2019a;Belinkov et al., 2020), or involve human-inthe-loop (Karpathy et al., 2015;K\u00e1d\u00e1r et al., 2017;Geva et al., 2021;Dalvi et al., 2022) to facilitate such an analysis. The use of pre-defined linguistic concepts restricts the scope of interpretation to only very general linguistic concepts, while human-inthe-loop methods are not scalable. We circumvent this bottleneck by using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis. \n\nGenerative Pre-trained Transformers (GPT) have been trained on an unprecedented amount of textual data, enabling them to develop a substantial understanding of natural language. As their capabilities continue to improve, researchers are finding creative ways to leverage their assistance for various applications, such as question-answering in financial and medical domains (Guo et al., 2023), simplifying medical reports (Jeblick et al., 2022), and detecting stance (Zhang et al., 2023). We carry out an investigation of whether GPT models, specifically ChatGPT, can aid in the interpretation of pre-trained language models (pLMs). \n\nA fascinating characteristic of neural language models is that words sharing any linguistic relationship cluster together in high-dimensional spaces (Mikolov et al., 2013).",
            "score": 0.39741313774222986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 376,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 376,
                    "end": 397,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 397,
                    "end": 418,
                    "matchedPaperCorpusId": "174800180"
                },
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "3104544"
                },
                {
                    "start": 656,
                    "end": 677,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 677,
                    "end": 695,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 770,
                    "end": 789,
                    "matchedPaperCorpusId": "611341"
                },
                {
                    "start": 789,
                    "end": 807,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 807,
                    "end": 826,
                    "matchedPaperCorpusId": "248810913"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "255220954",
            "title": "Rapid infant learning of syntactic\u2013semantic links",
            "text": "14 and 16). One toy grammar study, for example, demonstrated that infants can learn cross-element links, connecting phonological, distributional, and semantic cues (16). Toy grammars however face a scale-up challenge vis-\u00e0-vis natural languages. Infants' internal mental representations of toy grammar may differ from that of natural language grammar; and the cognitive mechanisms involved in learning may be different for simplified artificial environments and noisy naturalistic ones. Simulating the multifaceted complexity of syntactic-semantic links in a toy grammar model is challenging (e.g., ref. 17). Milestone and toy grammar methods only provide indirect information on the cognitive mechanisms governing learning, by predicting but not describing learning trajectories. \n\nIn this preregistered study (DOI: 10.17605/OSF.IO/ZGBP8. 10.17605/OSF.IO/X8H3A; (18)), we investigated whether infants can quickly harness novel grammatical elements in a native language environment to functionally expand their vocabulary (19). Infant acquisition of novel syntactic-semantic links in a complex ecological environment remains untested (see ref. (20) for an example with preschool children). Indirect evidence from native language milestones suggests that bootstrapping occurs months after syntactic-semantic links are formed; while evidence from simplified toy grammars suggests that bootstrapping can occur within minutes. If bootstrapping is used soon after learning a syntactic-semantic link, syntactic bootstrapping would be a more efficacious language-learning mechanism than previously thought. \n\nWe presented 20-mo-old infants with two novel subcategory-level determiners, 'ko' for animate objects and 'ka' for inanimate objects, inserted directly into their native language, French, as a replacement for existing determiners ('le', 'la', 'un', 'une': 'the' and 'a'; SI Appendix, Fig. S1). The determiners represented a novel morphological distinction in an existing syntactic structure. This method ensured that infants would interpret the novel determiners as grammatical elements and not as content words (i.e., meaning-bearing words).",
            "score": 0.3971215373423396,
            "section_title": "language acquisition | cognitive development | infant cognition | word learning | grammar learning",
            "char_start_offset": 3272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "140952515"
                },
                {
                    "start": 1144,
                    "end": 1148,
                    "matchedPaperCorpusId": "220122113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2269287109375
        },
        {
            "corpus_id": "248811189",
            "title": "Assessing the Limits of the Distributional Hypothesis in Semantic Spaces: Trait-based Relational Knowledge and the Impact of Co-occurrences",
            "text": "What models learn Evaluation of neural semantic spaces has focused on what knowledge they capture with a slew of work showing that some knowledge of analogies can be seen by applying simple transformations (Mikolov et al., 2013b;Levy and Goldberg, 2014;Arora et al., 2016;Paperno and Baroni, 2016;Gittens et al., 2017;Ethayarajh et al., 2019).Others have investigated what syntactic information neural semantic spaces seem to capture with most showing that they do capture something deeper than surface patters (Linzen et al., 2016;Gulordava et al., 2018;Giulianelli et al., 2018).However, they fail to exhaustively capture syntactic phenomena and specifically have been shown to struggle with polarity (Futrell et al., 2018;Jumelet and Hupkes, 2018) and certain filler-gap dependencies (Wilcox et al., 2018;Chowdhury and Zamparelli, 2018).Pretrained language models (PLMs) have been found to capture varying degrees of syntactic information (Peters et al., 2018;Tenney et al., 2019;Goldberg, 2019;Clark et al., 2019), however, they have also been shown to struggle to predict the grammaticality of sentences (Marvin and Linzen, 2018;Warstadt et al., 2019) and seem to depend on fragile heuristics rather than anything deeper (McCoy et al., 2019).Relational knowledge More specifically with respect to relational knowledge and semantic spaces, for some time now work has shown that semantic spaces could encode certain relational knowledge, e.g.knowledge of the relative positioning of geographical locations (Louwerse and Zwaan, 2009).Similarly, Gupta et al. (2015) found that embeddings capture something of relational knowledge associated with countries and cities, e.g.how countries related to one another with respect to GDP.Rubinstein et al. (2015) found that word embeddings captured some taxonomic relational knowledge but fared less well with respect to trait-based relational knowledge.",
            "score": 0.39685443964831457,
            "section_title": "Related work",
            "char_start_offset": 1367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 229,
                    "matchedPaperCorpusId": "7478738"
                },
                {
                    "start": 229,
                    "end": 253,
                    "matchedPaperCorpusId": "12730203"
                },
                {
                    "start": 253,
                    "end": 272,
                    "matchedPaperCorpusId": "12744871"
                },
                {
                    "start": 272,
                    "end": 297,
                    "matchedPaperCorpusId": "10169262"
                },
                {
                    "start": 297,
                    "end": 318,
                    "matchedPaperCorpusId": "39021228"
                },
                {
                    "start": 318,
                    "end": 342,
                    "matchedPaperCorpusId": "52966647"
                },
                {
                    "start": 511,
                    "end": 532,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 532,
                    "end": 555,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 555,
                    "end": 580,
                    "matchedPaperCorpusId": "52090220"
                },
                {
                    "start": 725,
                    "end": 750,
                    "matchedPaperCorpusId": "52144417"
                },
                {
                    "start": 787,
                    "end": 808,
                    "matchedPaperCorpusId": "52156878"
                },
                {
                    "start": 808,
                    "end": 839,
                    "matchedPaperCorpusId": "52010508"
                },
                {
                    "start": 942,
                    "end": 963,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 963,
                    "end": 983,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 998,
                    "end": 1017,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1109,
                    "end": 1134,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1134,
                    "end": 1156,
                    "matchedPaperCorpusId": "44072099"
                },
                {
                    "start": 1226,
                    "end": 1246,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1509,
                    "end": 1535,
                    "matchedPaperCorpusId": "10701761"
                },
                {
                    "start": 1547,
                    "end": 1566,
                    "matchedPaperCorpusId": "13992431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328857421875
        },
        {
            "corpus_id": "250244076",
            "title": "Is neural language acquisition similar to natural? A chronological probing study",
            "text": "Language learning is one of the quintessential human traits. First language acquisition(LA), unites both neurocognitive research, psycholinguistics, and computational approaches, focusing on the ability to acquire the capacity to perceive and comprehend language. \n\nStatistical language acquisition Language modeling has formed a branch in language acquisition studies named statistical language acquisition. Various aspects of language, including phonological, syntactic, lexical, morphological, and semantic features, were investigated in terms of statistical patterns children receive with the linguistic input. Recent studies postulating qualitative and quantitative measures of LA include: \n\n\u2022 Morphology and Syntax Morphology and syntax studies across language acquisition studies are definitely those explored the most. Starting with the poverty of stimulus problem and the argument between innateness and learning of grammar, it has led to typologically various sets of descriptive works and even computational models of the acquisition process. Thus, (Lewis and Elman, 2001) train a simple RNN to discriminate between grammatical strings that follow the inversion rule and those that do not (e.g., moving the first auxiliary verb such as \"Is the man that tall is nice?\"). The training data for the study is generated artificially and fails to prove that such a network generalizes on a mixture of diverse syntactic constructions. (Reali and Christiansen, 2005) use bigram models to capture the patterns of auxiliary inversion based on lifelike data from child-directed speech. The model can consistently assign higher probabilities to grammatical strings than ungrammatical strings, which was interpreted as having successfully learned the correct inversion rule. However, as (Kam et al., 2008) note, this result is because bigrams such as \"who are\" are much more frequent than the ungrammatical strings. (Prefors et al., 2006) approaches the structure dependency problem with Bayesian learning and attempts to learn a grammar that could generate additional sentences. \n\nThe model evaluates and selects between two grammars, a finite state grammar and a context-free grammar constructed by the authors based on a simplified subset of child-directed English. \n\nIt is worth noting how similar all the problem formulations are to the modern formulations of probing classification problems described below. They are also far from a complete description of the process of mastering grammar.",
            "score": 0.39685101549338536,
            "section_title": "Language acquisition and language models",
            "char_start_offset": 6679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2266
                },
                {
                    "start": 2269,
                    "end": 2411
                },
                {
                    "start": 2412,
                    "end": 2494
                }
            ],
            "ref_mentions": [
                {
                    "start": 1439,
                    "end": 1469,
                    "matchedPaperCorpusId": "7020811"
                },
                {
                    "start": 1785,
                    "end": 1803,
                    "matchedPaperCorpusId": "945275"
                },
                {
                    "start": 1914,
                    "end": 1936,
                    "matchedPaperCorpusId": "16525837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        }
    ],
    "quotes": {
        "cost": 0.176283,
        "quotes": [
            {
                "idx": 0,
                "key": "[226289695 | Zhang et al. | 2020 | Citations: 141]",
                "snippets": "First, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "First, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[232092403 | Nikoulina et al. | 2021 | Citations: 8]",
                "snippets": "We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 41,
                        "end": 411,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[233289478 | Liu et al. | 2021 | Citations: 87]",
                "snippets": "We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 478,
                        "end": 769,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[237491997 | Choshen et al. | 2021 | Citations: 29]",
                "snippets": "Results suggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 826,
                        "end": 1304,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Results suggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[238408423 | Chang et al. | 2021 | Citations: 40]",
                "snippets": "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)...Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "quote": "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[249017999 | Blevins et al. | 2022 | Citations: 31]",
                "snippets": "Our analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1674,
                        "end": 2123,
                        "sentence_offsets": [
                            {
                                "start": 1674,
                                "end": 1753
                            },
                            {
                                "start": 1754,
                                "end": 1908
                            },
                            {
                                "start": 1909,
                                "end": 2123
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[256627634 | Pannitto et al. | 2023 | Citations: 2]",
                "snippets": "Regarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "CALaMo",
                        "pdf_hash": "",
                        "start": 685,
                        "end": 1132,
                        "sentence_offsets": [
                            {
                                "start": 685,
                                "end": 857
                            },
                            {
                                "start": 858,
                                "end": 1132
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Regarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[260775882 | Krishnan | 2023 | Citations: 1]",
                "snippets": "The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Transformer based model training",
                        "pdf_hash": "",
                        "start": 545,
                        "end": 1243,
                        "sentence_offsets": [
                            {
                                "start": 545,
                                "end": 630
                            },
                            {
                                "start": 631,
                                "end": 715
                            },
                            {
                                "start": 716,
                                "end": 912
                            },
                            {
                                "start": 915,
                                "end": 1085
                            },
                            {
                                "start": 1086,
                                "end": 1243
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[261277016 | Chang et al. | 2023 | Citations: 13]",
                "snippets": "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)...N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233289478 | Liu et al. | 2021 | Citations: 87]": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
                    "[238408423 | Chang et al. | 2021 | Citations: 40]": "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.",
                    "[259089351 | Evanson et al. | 2023 | Citations: 29]": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.",
                    "[222140842 | Chiang et al. | 2020 | Citations: 42]": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 460,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 96
                            },
                            {
                                "start": 97,
                                "end": 291
                            },
                            {
                                "start": 292,
                                "end": 461
                            }
                        ],
                        "ref_mentions": [
                            "267903883",
                            "259089351",
                            "222140842",
                            "233289478"
                        ],
                        "quote": "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)"
                    },
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 358,
                        "end": 1389,
                        "sentence_offsets": [
                            {
                                "start": 358,
                                "end": 389
                            },
                            {
                                "start": 390,
                                "end": 621
                            },
                            {
                                "start": 622,
                                "end": 769
                            },
                            {
                                "start": 770,
                                "end": 962
                            },
                            {
                                "start": 963,
                                "end": 1338
                            },
                            {
                                "start": 1339,
                                "end": 1388
                            }
                        ],
                        "ref_mentions": [
                            "238408423",
                            "233289478"
                        ],
                        "quote": "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[261530292 | Zhao et al. | 2023 | Citations: 461]",
                "snippets": "A recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches...Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Explaining the Role of Fine-tuning",
                        "pdf_hash": "",
                        "start": 847,
                        "end": 1525,
                        "sentence_offsets": [
                            {
                                "start": 847,
                                "end": 988
                            },
                            {
                                "start": 989,
                                "end": 1174
                            },
                            {
                                "start": 1175,
                                "end": 1366
                            },
                            {
                                "start": 1367,
                                "end": 1526
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches"
                    },
                    {
                        "section_title": "Explaining the Role of Fine-tuning",
                        "pdf_hash": "",
                        "start": 1783,
                        "end": 2480,
                        "sentence_offsets": [
                            {
                                "start": 1770,
                                "end": 1922
                            },
                            {
                                "start": 1923,
                                "end": 2059
                            },
                            {
                                "start": 2060,
                                "end": 2148
                            },
                            {
                                "start": 2149,
                                "end": 2285
                            },
                            {
                                "start": 2286,
                                "end": 2479
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[261822542 | Chen et al. | 2023 | Citations: 74]",
                "snippets": "We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities.\n\nMost work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions.\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset.\n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities.\n\nMost work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions.\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset.\n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[264451507 | Muller-Eberstein et al. | 2023 | Citations: 10]",
                "snippets": "We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 519,
                        "end": 973,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[264590507 | Starace et al. | 2023 | Citations: 7]",
                "snippets": "Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 162,
                        "end": 399,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[267627053 | Qin et al. | 2024 | Citations: 5]",
                "snippets": "Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 621,
                        "end": 910,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[270688478 | Cuconasu et al. | 2024 | Citations: 2]",
                "snippets": "Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Training",
                        "pdf_hash": "",
                        "start": 226,
                        "end": 1002,
                        "sentence_offsets": [
                            {
                                "start": 226,
                                "end": 464
                            },
                            {
                                "start": 464,
                                "end": 667
                            },
                            {
                                "start": 667,
                                "end": 850
                            },
                            {
                                "start": 850,
                                "end": 911
                            },
                            {
                                "start": 911,
                                "end": 1002
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271916426 | Neveditsin et al. | 2024 | Citations: 5]",
                "snippets": "Nevertheless, challenges remain, as highlighted by Mahowald et al. (Mahowald et al., 2024), who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Language models",
                        "pdf_hash": "",
                        "start": 1662,
                        "end": 2249,
                        "sentence_offsets": [
                            {
                                "start": 1662,
                                "end": 1819
                            },
                            {
                                "start": 1820,
                                "end": 2032
                            },
                            {
                                "start": 2033,
                                "end": 2249
                            }
                        ],
                        "ref_mentions": [
                            "268551442"
                        ],
                        "quote": "Nevertheless, challenges remain, as highlighted by Mahowald et al. (Mahowald et al., 2024), who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[272661905 | McGrath et al. | 2024 | Citations: 9]",
                "snippets": "For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Challenging the Black-Box Assumption",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1556,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 333
                            },
                            {
                                "start": 334,
                                "end": 535
                            },
                            {
                                "start": 536,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 1104
                            },
                            {
                                "start": 1107,
                                "end": 1299
                            },
                            {
                                "start": 1300,
                                "end": 1556
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[272910877 | Liang et al. | 2024 | Citations: 0]",
                "snippets": "The pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "PRELIMINARY 2.1 Pre-training Stage in LLMs",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 741,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 324
                            },
                            {
                                "start": 325,
                                "end": 534
                            },
                            {
                                "start": 535,
                                "end": 741
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273403825 | Yang et al. | 2024 | Citations: 2]",
                "snippets": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[276725505 | Orlicki | 2025 | Citations: 0]",
                "snippets": "A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Latent Representations in LLMs",
                        "pdf_hash": "",
                        "start": 466,
                        "end": 700,
                        "sentence_offsets": [
                            {
                                "start": 466,
                                "end": 700
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[276902722 | Inaba et al. | 2025 | Citations: 1]",
                "snippets": "Our experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1453,
                        "end": 1865,
                        "sentence_offsets": [
                            {
                                "start": 1453,
                                "end": 1739
                            },
                            {
                                "start": 1740,
                                "end": 1865
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[277065732 | Wu et al. | 2025 | Citations: 5]",
                "snippets": "During pre-training, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "During pre-training, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[44093987 | Gauthier et al. | 2018 | Citations: 10]",
                "snippets": "By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 1644,
                        "end": 2008,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[53633715 | Saphra et al. | 2018 | Citations: 12]",
                "snippets": "Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 214,
                        "end": 563,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.026019,
        "cot": "The user's query asks us to analyze the sequential stages by which language models acquire linguistic capabilities during training, especially the progression from syntactic to semantic knowledge, and describe how these transitions are manifested and measured. To effectively answer this query, I will organize the quotes into logical dimensions that provide a comprehensive understanding of this developmental process.\n\n1. \"Introduction/Background\" - This section should provide background information on language model training and the concept of linguistic capability acquisition. While we may not have direct quotes for this section, it's important to set the stage for the reader.\n\n2. \"General Developmental Trajectory of Linguistic Capabilities\" - This dimension should synthesize information about the overall trajectory and stages that language models go through as they acquire linguistic knowledge. A synthesis format is appropriate since we want to provide a coherent explanation of these developmental stages.\n\n3. \"Early-Stage Acquisition: Syntax and Structural Knowledge\" - This dimension should focus on how and when syntax and structural knowledge is acquired during training. Synthesis format is appropriate to explain this phase of development.\n\n4. \"Mid-to-Late Stage Acquisition: Semantics and Complex Knowledge\" - This dimension should cover how semantic knowledge is acquired after syntactic knowledge and detail the progression toward more complex linguistic understanding. Synthesis format is best to explain this progression.\n\n5. \"Measurement and Evaluation Methods\" - This dimension should address how researchers measure and evaluate these linguistic capabilities as they emerge. A synthesis format is appropriate to explain the different methodologies used.\n\n6. \"Factors Influencing Acquisition Patterns\" - This dimension should explore what influences the pattern and speed of capability acquisition. Synthesis format is best to explain these complex relationships.\n\n7. \"Connections to Human Language Acquisition\" - This dimension should examine how the acquisition patterns in language models compare to human language acquisition. A synthesis format is appropriate to explore these connections.\n\nThese dimensions are arranged in a logical order that first introduces the concept, then follows the developmental trajectory from early to late stages, explains how these developments are measured, what factors influence them, and finally provides context by comparing to human acquisition.",
        "plan": {
            "Introduction/Background (synthesis)": [
                7,
                14,
                17,
                21
            ],
            "General Developmental Trajectory of Linguistic Capabilities (synthesis)": [
                1,
                2,
                3,
                5,
                11,
                20
            ],
            "Early-Stage Acquisition: Syntax and Structural Knowledge (synthesis)": [
                8,
                10,
                11,
                16,
                19,
                23
            ],
            "Mid-to-Late Stage Acquisition: Semantics and Complex Knowledge (synthesis)": [
                4,
                8,
                9,
                12,
                16,
                22
            ],
            "Measurement and Evaluation Methods (synthesis)": [
                0,
                2,
                6,
                18
            ],
            "Factors Influencing Acquisition Patterns (synthesis)": [
                0,
                8,
                13,
                15
            ],
            "Connections to Human Language Acquisition (synthesis)": [
                4,
                18
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Language models undergo pre-training on vast text corpora to learn fundamental linguistic capabilities through tasks like next token prediction. This self-supervised process enables models to acquire both syntactic structures and semantic relationships that form the foundation for their language understanding abilities. (4 sources)",
                "text": "\nLarge language models (LLMs) develop their linguistic capabilities through a structured training process that begins with extensive pre-training on diverse text corpora. This critical initial phase involves exposing the model to massive amounts of unlabeled text data, enabling it to learn the underlying statistical properties of natural language <Paper corpusId=\"260775882\" paperTitle=\"(Krishnan, 2023)\" isShortName></Paper> <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>. During pre-training, models typically learn through self-supervised objectives such as predicting the next token in a sequence, where the model is conditioned on previous tokens (w\u2081:\u1d62\u208b\u2081) and trained to predict the subsequent token (w\u1d62) <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>.\n\nThis unsupervised learning approach allows models to acquire fundamental linguistic patterns without task-specific guidance <Paper corpusId=\"272910877\" paperTitle=\"(Liang et al., 2024)\" isShortName></Paper>. The training techniques commonly employed include masked language modeling (MLM), where models predict missing words from context, and autoregressive language modeling, where models predict the next word in a sequence <Paper corpusId=\"260775882\" paperTitle=\"(Krishnan, 2023)\" isShortName></Paper> <Paper corpusId=\"277065732\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. Through these methods, models develop the ability to capture syntactic structures, semantic relationships, and contextual dependencies between words and phrases <Paper corpusId=\"272910877\" paperTitle=\"(Liang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277065732\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\nThe outcome of this pre-training process is what's commonly referred to as a \"base\" model, which possesses a robust, transferable understanding of linguistic structures <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>. This foundation enables the model to generalize effectively across different contexts and serves as the basis for further specialized training or fine-tuning for specific applications <Paper corpusId=\"260775882\" paperTitle=\"(Krishnan, 2023)\" isShortName></Paper> <Paper corpusId=\"272910877\" paperTitle=\"(Liang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Krishnan, 2023)",
                        "snippets": [
                            "The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns."
                        ],
                        "paper": {
                            "corpus_id": 260775882,
                            "title": "Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis",
                            "authors": [
                                {
                                    "authorId": "2217728327",
                                    "name": "Anusuya Krishnan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.89892578125
                    },
                    {
                        "id": "(Cuconasu et al., 2024)",
                        "snippets": [
                            "Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now."
                        ],
                        "paper": {
                            "corpus_id": 270688478,
                            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
                            "authors": [
                                {
                                    "authorId": "2281641042",
                                    "name": "Florin Cuconasu"
                                },
                                {
                                    "authorId": "120709579",
                                    "name": "Giovanni Trappolini"
                                },
                                {
                                    "authorId": "2281641625",
                                    "name": "Nicola Tonellotto"
                                },
                                {
                                    "authorId": "2260653973",
                                    "name": "Fabrizio Silvestri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.87890625
                    },
                    {
                        "id": "(Liang et al., 2024)",
                        "snippets": [
                            "The pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts."
                        ],
                        "paper": {
                            "corpus_id": 272910877,
                            "title": "Data Proportion Detection for Optimized Data Management for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2303856806",
                                    "name": "Hao Liang"
                                },
                                {
                                    "authorId": "2320126993",
                                    "name": "Keshi Zhao"
                                },
                                {
                                    "authorId": "2323000103",
                                    "name": "Yajie Yang"
                                },
                                {
                                    "authorId": "2260817839",
                                    "name": "Bin Cui"
                                },
                                {
                                    "authorId": "2242124536",
                                    "name": "Guosheng Dong"
                                },
                                {
                                    "authorId": "2183768304",
                                    "name": "Zenan Zhou"
                                },
                                {
                                    "authorId": "2309265357",
                                    "name": "Wentao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.95556640625
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "During pre-training, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts."
                        ],
                        "paper": {
                            "corpus_id": 277065732,
                            "title": "A Survey on Federated Fine-tuning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2297831289",
                                    "name": "Yebo Wu"
                                },
                                {
                                    "authorId": "2297767120",
                                    "name": "Chunlin Tian"
                                },
                                {
                                    "authorId": "2350520035",
                                    "name": "Jingguang Li"
                                },
                                {
                                    "authorId": "2350521999",
                                    "name": "He Sun"
                                },
                                {
                                    "authorId": "2220304337",
                                    "name": "Kahou Tam"
                                },
                                {
                                    "authorId": "2260286654",
                                    "name": "Li Li"
                                },
                                {
                                    "authorId": "2362594432",
                                    "name": "Chengzhong Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.8994140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "General Developmental Trajectory of Linguistic Capabilities",
                "tldr": "Language models follow a consistent developmental trajectory, acquiring syntactic knowledge early in training before progressing to more complex semantic and reasoning capabilities. This sequential pattern shows remarkable consistency across different models and languages, with early stages focused on structural patterns and later stages dedicated to building more abstract conceptual knowledge. (6 sources)",
                "text": "\nResearch has revealed that language models exhibit distinct developmental stages during pre-training that follow a surprisingly consistent trajectory. Multiple studies have identified a clear progression in which models first acquire syntactic capabilities before advancing to more complex semantic understanding. Nikoulina et al. found that models \"follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training\" <Paper corpusId=\"232092403\" paperTitle=\"(Nikoulina et al., 2021)\" isShortName></Paper>. This pattern appears to be universal across different model architectures and training approaches.\n\nThe acquisition of linguistic knowledge occurs rapidly in early training phases. Muller-Eberstein et al. observed that \"syntactic knowledge is acquired rapidly after 0.5% of full training\" while \"continued performance improvements primarily stem from the acquisition of open-domain knowledge\" <Paper corpusId=\"264451507\" paperTitle=\"(Muller-Eberstein et al., 2023)\" isShortName></Paper>. Similarly, Liu et al. found that \"linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired\" <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This evidence suggests a clear hierarchy in the difficulty and training time required for different types of linguistic capabilities.\n\nThe developmental trajectory appears to be highly predictable. Choshen et al. discovered that \"NLMs exhibit consistent 'developmental' stages\" and that \"the learning trajectory approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired\" <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. They also identified \"phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them\" <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. This indicates that certain linguistic phenomena are learned together, possibly because they rely on similar underlying principles.\n\nIn multilingual contexts, this developmental sequence remains consistent but with additional cross-lingual dimensions. Blevins et al. found that \"the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics\" <Paper corpusId=\"249017999\" paperTitle=\"(Blevins et al., 2022)\" isShortName></Paper>. However, they noted that \"while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process\" <Paper corpusId=\"249017999\" paperTitle=\"(Blevins et al., 2022)\" isShortName></Paper>, suggesting that cross-lingual capabilities develop more gradually throughout training.\n\nAt a more granular level, language models appear to progress from local to more abstract representations. Inaba et al. observed that models \"initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations\" and that \"LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings\" <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>. This progression from concrete to abstract understanding mirrors the development of increasingly sophisticated linguistic capabilities throughout the training process.",
                "citations": [
                    {
                        "id": "(Nikoulina et al., 2021)",
                        "snippets": [
                            "We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training."
                        ],
                        "paper": {
                            "corpus_id": 232092403,
                            "title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics",
                            "authors": [
                                {
                                    "authorId": "2841761",
                                    "name": "Vassilina Nikoulina"
                                },
                                {
                                    "authorId": "1471888023",
                                    "name": "Maxat Tezekbayev"
                                },
                                {
                                    "authorId": "2051805778",
                                    "name": "Nuradil Kozhakhmet"
                                },
                                {
                                    "authorId": "2051799158",
                                    "name": "Madina Babazhanova"
                                },
                                {
                                    "authorId": "2907260",
                                    "name": "Matthias Gall\u00e9"
                                },
                                {
                                    "authorId": "2842549",
                                    "name": "Z. Assylbekov"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 8
                        },
                        "score": 0.8671875
                    },
                    {
                        "id": "(Muller-Eberstein et al., 2023)",
                        "snippets": [
                            "We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization."
                        ],
                        "paper": {
                            "corpus_id": 264451507,
                            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
                            "authors": [
                                {
                                    "authorId": "1416353805",
                                    "name": "Max M\u00fcller-Eberstein"
                                },
                                {
                                    "authorId": "3449407",
                                    "name": "Rob van der Goot"
                                },
                                {
                                    "authorId": "2256991041",
                                    "name": "Barbara Plank"
                                },
                                {
                                    "authorId": "2261492323",
                                    "name": "Ivan Titov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 10
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92724609375
                    },
                    {
                        "id": "(Choshen et al., 2021)",
                        "snippets": [
                            "Results suggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them."
                        ],
                        "paper": {
                            "corpus_id": 237491997,
                            "title": "The Grammar-Learning Trajectories of Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                },
                                {
                                    "authorId": "2769805",
                                    "name": "Omri Abend"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 29
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Blevins et al., 2022)",
                        "snippets": [
                            "Our analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics."
                        ],
                        "paper": {
                            "corpus_id": 249017999,
                            "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
                            "authors": [
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 31
                        },
                        "score": 0.92333984375
                    },
                    {
                        "id": "(Inaba et al., 2025)",
                        "snippets": [
                            "Our experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities."
                        ],
                        "paper": {
                            "corpus_id": 276902722,
                            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
                            "authors": [
                                {
                                    "authorId": "2342412237",
                                    "name": "Tatsuro Inaba"
                                },
                                {
                                    "authorId": "2261744024",
                                    "name": "Kentaro Inui"
                                },
                                {
                                    "authorId": "2302320083",
                                    "name": "Yusuke Miyao"
                                },
                                {
                                    "authorId": "50856622",
                                    "name": "Yohei Oseki"
                                },
                                {
                                    "authorId": "2266692",
                                    "name": "Benjamin Heinzerling"
                                },
                                {
                                    "authorId": "2310230516",
                                    "name": "Yu Takagi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.98046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Early-Stage Acquisition: Syntax and Structural Knowledge",
                "tldr": "During early training, language models rapidly acquire syntactic knowledge through a distinct phase transition marked by the emergence of Syntactic Attention Structure. This early acquisition of structural patterns precedes semantic understanding, with models first learning n-gram patterns before developing more complex linguistic capabilities. (8 sources)",
                "text": "\nLanguage models demonstrate a remarkably consistent pattern in their earliest learning stages, with syntactic knowledge emerging as a foundational capability. Multiple studies have observed that syntax is acquired very early in the training process, with Muller-Eberstein et al. finding that \"syntactic knowledge is acquired rapidly after 0.5% of full training\" <Paper corpusId=\"264451507\" paperTitle=\"(Muller-Eberstein et al., 2023)\" isShortName></Paper>. This finding aligns with broader observations that language models consistently learn \"linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259089351\" paperTitle=\"(Evanson et al., 2023)\" isShortName></Paper>.\n\nThe acquisition of syntax is not a gradual process but occurs during a dramatic phase transition in training. Chen et al. identified \"a brief window in pretraining when models abruptly acquire SAS , concurrent with a steep drop in loss\" <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This \"structure onset\" represents a breakthrough moment when transformer attention heads begin to specialize in tracking specific syntactic relationships. Following this transition, researchers can even \"reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences\" <Paper corpusId=\"272661905\" paperTitle=\"(McGrath et al., 2024)\" isShortName></Paper>.\n\nThe organization of syntax in language models follows a hierarchical pattern that mirrors traditional linguistic theory. Lower layers of the model architecture tend to capture basic syntactic patterns <Paper corpusId=\"276725505\" paperTitle=\"(Orlicki, 2025)\" isShortName></Paper>, with information flowing in a way that resembles \"the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations\" <Paper corpusId=\"272661905\" paperTitle=\"(McGrath et al., 2024)\" isShortName></Paper>. This finding is consistent with earlier work by Saphra et al., who demonstrated that \"different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early\" <Paper corpusId=\"53633715\" paperTitle=\"(Saphra et al., 2018)\" isShortName></Paper>.\n\nAt the most granular level, language models initially learn through n-gram patterns before developing more sophisticated linguistic capabilities. Chang et al. describe this process as \"early n-gram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. This progression from simple token-level statistics to structured syntactic knowledge forms the foundation upon which more complex semantic understanding is later built.\n\nThe rapid acquisition of syntax appears to be a prerequisite for subsequent linguistic development. Chen et al. observed that the \"internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior\" <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This suggests that syntactic knowledge serves as a critical scaffold that enables the model to develop more advanced capabilities in later training stages, establishing a clear developmental sequence from syntax to semantics.",
                "citations": [
                    {
                        "id": "(Muller-Eberstein et al., 2023)",
                        "snippets": [
                            "We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization."
                        ],
                        "paper": {
                            "corpus_id": 264451507,
                            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
                            "authors": [
                                {
                                    "authorId": "1416353805",
                                    "name": "Max M\u00fcller-Eberstein"
                                },
                                {
                                    "authorId": "3449407",
                                    "name": "Rob van der Goot"
                                },
                                {
                                    "authorId": "2256991041",
                                    "name": "Barbara Plank"
                                },
                                {
                                    "authorId": "2261492323",
                                    "name": "Ivan Titov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 10
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)",
                            "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Evanson et al., 2023)",
                        "snippets": [
                            "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language."
                        ],
                        "paper": {
                            "corpus_id": 259089351,
                            "title": "Language acquisition: do children and language models follow similar learning stages?",
                            "authors": [
                                {
                                    "authorId": "2219624467",
                                    "name": "Linnea Evanson"
                                },
                                {
                                    "authorId": "3051598",
                                    "name": "Yair Lakretz"
                                },
                                {
                                    "authorId": "28972230",
                                    "name": "J. King"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 29
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities.\n\nMost work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions.\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset.\n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other."
                        ],
                        "paper": {
                            "corpus_id": 261822542,
                            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "authors": [
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "2240524527",
                                    "name": "Ravid Schwartz-Ziv"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "2240527814",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 74
                        },
                        "score": 0.93505859375
                    },
                    {
                        "id": "(McGrath et al., 2024)",
                        "snippets": [
                            "For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable."
                        ],
                        "paper": {
                            "corpus_id": 272661905,
                            "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?",
                            "authors": [
                                {
                                    "authorId": "2290681639",
                                    "name": "Sam Whitman McGrath"
                                },
                                {
                                    "authorId": "2321226194",
                                    "name": "Jacob Russin"
                                },
                                {
                                    "authorId": "2260118854",
                                    "name": "Ellie Pavlick"
                                },
                                {
                                    "authorId": "48522350",
                                    "name": "Roman Feiman"
                                }
                            ],
                            "year": 2024,
                            "venue": "Current Directions in Psychological Science",
                            "n_citations": 9
                        },
                        "score": 0.86181640625
                    },
                    {
                        "id": "(Orlicki, 2025)",
                        "snippets": [
                            "A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics."
                        ],
                        "paper": {
                            "corpus_id": 276725505,
                            "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs",
                            "authors": [
                                {
                                    "authorId": "20441545",
                                    "name": "J. I. Orlicki"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.890625
                    },
                    {
                        "id": "(Saphra et al., 2018)",
                        "snippets": [
                            "Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously."
                        ],
                        "paper": {
                            "corpus_id": 53633715,
                            "title": "Language Models Learn POS First",
                            "authors": [
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                },
                                {
                                    "authorId": "144871732",
                                    "name": "Adam Lopez"
                                }
                            ],
                            "year": 2018,
                            "venue": "BlackboxNLP@EMNLP",
                            "n_citations": 12
                        },
                        "score": 0.96142578125
                    },
                    {
                        "id": "(Chang et al., 2021)",
                        "snippets": [
                            "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)",
                            "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."
                        ],
                        "paper": {
                            "corpus_id": 238408423,
                            "title": "Word Acquisition in Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 40
                        },
                        "score": 0.8984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mid-to-Late Stage Acquisition: Semantics and Complex Knowledge",
                "tldr": "After establishing syntactic foundations, language models progress to acquire semantic knowledge and more complex capabilities during mid-to-late training stages, with world knowledge and reasoning abilities emerging later and less consistently. This developmental pattern shows higher model layers specializing in semantic processing, though recent research suggests this relationship becomes more complex after fine-tuning. (7 sources)",
                "text": "\nFollowing the early acquisition of syntactic knowledge, language models undergo a gradual transition to developing more advanced semantic capabilities and complex knowledge. This progression from syntax to semantics follows a consistent pattern where \"world knowledge and reasoning are learned later and less stably\" compared to syntactic rules <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. The development of these more sophisticated capabilities requires substantially more training data and time than the initial acquisition of syntactic knowledge.\n\nThe architecture of language models reflects this developmental progression through a layer-wise specialization. Research has consistently shown that \"lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing\" <Paper corpusId=\"264590507\" paperTitle=\"(Starace et al., 2023)\" isShortName></Paper>. This hierarchical organization mirrors \"the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity\" <Paper corpusId=\"272661905\" paperTitle=\"(McGrath et al., 2024)\" isShortName></Paper>. This structured progression suggests that syntactic knowledge serves as a foundation upon which semantic understanding is built.\n\nThe transition from syntax to semantics is not abrupt but follows a gradual refinement process. Chang et al. characterize this progression as \"early n-gram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. During this refinement phase, models evolve from simple statistical pattern recognition to developing more abstract conceptual representations that enable deeper semantic understanding. This progression allows models to eventually \"converge on more nuanced predictions\" beyond simple token frequencies <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nRecent research has challenged some conventional assumptions about how semantic knowledge is distributed across model layers after fine-tuning. While pre-trained models show increasing semantic capabilities in higher layers, Wu et al. found that in fine-tuned models, \"the proportion of captured semantic knowledge initially grows then drops aggressively\" <Paper corpusId=\"261530292\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. This suggests that instruction fine-tuning may reorganize how semantic knowledge is represented within the model architecture, contradicting \"conventional belief that higher layers capture more semantic knowledge\" <Paper corpusId=\"261530292\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nThe acquisition of semantic knowledge appears to benefit from establishing abstract relations between syntactic and semantic properties. Gauthier et al. suggest that \"by explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time\" <Paper corpusId=\"44093987\" paperTitle=\"(Gauthier et al., 2018)\" isShortName></Paper>. This indicates that the early acquisition of syntactic knowledge creates a framework that facilitates more efficient semantic learning in later training stages.\n\nThe importance of pre-training for semantic knowledge acquisition has been highlighted in recent studies examining the relative contributions of different training phases. Zhou et al. demonstrated that \"almost all knowledge of LLMs is acquired during pre-training,\" with subsequent fine-tuning primarily teaching models interaction styles and formats rather than introducing new knowledge <Paper corpusId=\"261530292\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. This finding underscores the critical role of the pre-training phase in establishing the semantic knowledge base upon which more specialized capabilities are later built.",
                "citations": [
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)",
                            "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92724609375
                    },
                    {
                        "id": "(Starace et al., 2023)",
                        "snippets": [
                            "Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing."
                        ],
                        "paper": {
                            "corpus_id": 264590507,
                            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
                            "authors": [
                                {
                                    "authorId": "2168285763",
                                    "name": "Giulio Starace"
                                },
                                {
                                    "authorId": "2262444926",
                                    "name": "Konstantinos Papakostas"
                                },
                                {
                                    "authorId": "2067163164",
                                    "name": "Rochelle Choenni"
                                },
                                {
                                    "authorId": "31713235",
                                    "name": "Apostolos Panagiotopoulos"
                                },
                                {
                                    "authorId": "2168109752",
                                    "name": "Matteo Rosati"
                                },
                                {
                                    "authorId": "2262444923",
                                    "name": "Alina Leidinger"
                                },
                                {
                                    "authorId": "2262445370",
                                    "name": "Ekaterina Shutova"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(McGrath et al., 2024)",
                        "snippets": [
                            "For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable."
                        ],
                        "paper": {
                            "corpus_id": 272661905,
                            "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?",
                            "authors": [
                                {
                                    "authorId": "2290681639",
                                    "name": "Sam Whitman McGrath"
                                },
                                {
                                    "authorId": "2321226194",
                                    "name": "Jacob Russin"
                                },
                                {
                                    "authorId": "2260118854",
                                    "name": "Ellie Pavlick"
                                },
                                {
                                    "authorId": "48522350",
                                    "name": "Roman Feiman"
                                }
                            ],
                            "year": 2024,
                            "venue": "Current Directions in Psychological Science",
                            "n_citations": 9
                        },
                        "score": 0.86181640625
                    },
                    {
                        "id": "(Chang et al., 2021)",
                        "snippets": [
                            "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)",
                            "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."
                        ],
                        "paper": {
                            "corpus_id": 238408423,
                            "title": "Word Acquisition in Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 40
                        },
                        "score": 0.8984375
                    },
                    {
                        "id": "(Zhao et al., 2023)",
                        "snippets": [
                            "A recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches",
                            "Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models."
                        ],
                        "paper": {
                            "corpus_id": 261530292,
                            "title": "Explainability for Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2237987232",
                                    "name": "Haiyan Zhao"
                                },
                                {
                                    "authorId": "7315244",
                                    "name": "Hanjie Chen"
                                },
                                {
                                    "authorId": "145338224",
                                    "name": "F. Yang"
                                },
                                {
                                    "authorId": "47717322",
                                    "name": "Ninghao Liu"
                                },
                                {
                                    "authorId": "13689700",
                                    "name": "Huiqi Deng"
                                },
                                {
                                    "authorId": "22561596",
                                    "name": "Hengyi Cai"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2136400100",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2237804196",
                                    "name": "Mengnan Du"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Transactions on Intelligent Systems and Technology",
                            "n_citations": 461
                        },
                        "score": 0.90478515625
                    },
                    {
                        "id": "(Gauthier et al., 2018)",
                        "snippets": [
                            "By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion."
                        ],
                        "paper": {
                            "corpus_id": 44093987,
                            "title": "Word learning and the acquisition of syntactic-semantic overhypotheses",
                            "authors": [
                                {
                                    "authorId": "24339276",
                                    "name": "Jon Gauthier"
                                },
                                {
                                    "authorId": "143643017",
                                    "name": "R. Levy"
                                },
                                {
                                    "authorId": "1763295",
                                    "name": "J. Tenenbaum"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Cognitive Science Society",
                            "n_citations": 10
                        },
                        "score": 0.85498046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Measurement and Evaluation Methods",
                "tldr": "Researchers employ a diverse range of evaluation methods to track language models' acquisition of linguistic capabilities, including probing classifiers, acceptability judgments, and fine-tuning on downstream tasks. These measurement approaches reveal that different linguistic capabilities emerge at varying rates during training, with syntactic features being reliably encoded after exposure to relatively small amounts of text. (4 sources)",
                "text": "\nMeasuring the emergence of linguistic capabilities in language models requires specialized evaluation techniques that can capture both the timing and quality of acquired knowledge. Researchers have developed a rich toolkit of assessment methods that track different aspects of language learning throughout the training process. Zhang et al. identify five primary evaluation approaches: \"classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks\" <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This multi-faceted evaluation strategy enables researchers to comprehensively track the development of different linguistic capabilities across training iterations.\n\nProbing tasks have emerged as a particularly valuable method for measuring the acquisition of specific linguistic knowledge. By plotting \"probing performance across iterations,\" researchers can create detailed learning curves that reveal when and how quickly different capabilities emerge <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. These probing experiments have demonstrated that models require surprisingly little data to acquire basic linguistic structure, with language models needing \"only about 10M to 100M words to learn to reliably encode most syntactic and semantic features\" <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. In contrast, the same studies show that \"a much larger quantity of data\" is required for models to develop commonsense knowledge and master downstream NLU tasks <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nThe assessment of grammatical competence can take either binary or gradient approaches, depending on the research question. Pannitto et al. note that researchers can either \"test whether the network is able or not to handle some linguistic phenomenon\" or adopt a gradient perspective to examine \"how and why some linguistic aspect becomes more and more salient to the network during training\" <Paper corpusId=\"256627634\" paperTitle=\"(Pannitto et al., 2023)\" isShortName></Paper>. This latter approach aligns with continuous assessment methodologies that track incremental improvements in model capabilities throughout the training process.\n\nMore recently, evaluation frameworks inspired by human language acquisition have been developed to provide structured assessment of language model capabilities. Yang et al. propose \"a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning\" <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This developmental approach draws on \"methods from linguistic research\" to evaluate model performance across a progression of increasingly complex language tasks <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Interestingly, their findings suggest that while newer models show improved overall performance, \"their developmental trajectory does not strictly follow the path of human language acquisition\" <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>, highlighting important differences between artificial and human language learning processes.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "First, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks."
                        ],
                        "paper": {
                            "corpus_id": 226289695,
                            "title": "When Do You Need Billions of Words of Pretraining Data?",
                            "authors": [
                                {
                                    "authorId": "9227100",
                                    "name": "Yian Zhang"
                                },
                                {
                                    "authorId": "46236380",
                                    "name": "Alex Warstadt"
                                },
                                {
                                    "authorId": "1993902967",
                                    "name": "Haau-Sing Li"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 141
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92724609375
                    },
                    {
                        "id": "(Pannitto et al., 2023)",
                        "snippets": [
                            "Regarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training."
                        ],
                        "paper": {
                            "corpus_id": 256627634,
                            "title": "CALaMo: a Constructionist Assessment of Language Models",
                            "authors": [
                                {
                                    "authorId": "22233530",
                                    "name": "Ludovica Pannitto"
                                },
                                {
                                    "authorId": "3352951",
                                    "name": "Aur\u00e9lie Herbelot"
                                }
                            ],
                            "year": 2023,
                            "venue": "CXGSNLP",
                            "n_citations": 2
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition."
                        ],
                        "paper": {
                            "corpus_id": 273403825,
                            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
                            "authors": [
                                {
                                    "authorId": "2326447675",
                                    "name": "Qiyuan Yang"
                                },
                                {
                                    "authorId": "2294722814",
                                    "name": "Pengda Wang"
                                },
                                {
                                    "authorId": "2326296255",
                                    "name": "Luke D. Plonsky"
                                },
                                {
                                    "authorId": "2250843586",
                                    "name": "Frederick L. Oswald"
                                },
                                {
                                    "authorId": "7315244",
                                    "name": "Hanjie Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.91357421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Factors Influencing Acquisition Patterns",
                "tldr": "Language model training dynamics are influenced by multiple factors including data volume, token frequency, and architectural choices, with most models requiring 10-100M words to acquire basic syntactic features. Environmental influences further differentiate artificial language learning from human acquisition, with LLMs heavily relying on distributional patterns while lacking the embodied experiences that shape human language development. (6 sources)",
                "text": "\nThe acquisition of linguistic capabilities in language models is shaped by several key factors that influence both the rate and pattern of learning. One of the most significant factors is the volume of training data, with research showing distinct thresholds for different types of knowledge acquisition. Zhang et al. found that language models \"require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features,\" but need \"a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks\" <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This finding indicates that basic linguistic structures can be learned with relatively modest amounts of data, while more complex knowledge requires substantially more extensive training.\n\nToken frequency and n-gram probability play crucial roles in shaping learning patterns throughout the training process. Chang et al. demonstrated that \"learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts,\" with high-probability n-grams being \"less likely to be 'forgotten'\" during training <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. This observation suggests that the statistical properties of the training corpus significantly influence which linguistic features are acquired more quickly and retained more reliably. Their analysis further indicates that language model learning might be characterized as \"early n-gram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>.\n\nThe training environment and learning context also substantially impact acquisition patterns, highlighting important differences between artificial and human language learning. While language models can develop \"sensitivity to certain linguistic phenomena\" through training, their learning process differs fundamentally from human language acquisition <Paper corpusId=\"267627053\" paperTitle=\"(Qin et al., 2024)\" isShortName></Paper>. Unlike children, who acquire language through interactive, embodied experiences, language models rely heavily on distributional patterns in text. This distinction is further emphasized in research by Mahowald et al., who highlight \"the gap between formal and functional linguistic competencies in LLMs,\" where formal competencies involve syntactic and semantic correctness, while functional competencies require applying language in practical, goal-oriented contexts <Paper corpusId=\"271916426\" paperTitle=\"(Neveditsin et al., 2024)\" isShortName></Paper>.\n\nThe architectural design of models influences their acquisition patterns, though research suggests these patterns remain relatively consistent across different model types. Studies comparing language models' learning trajectories to those of children have found that \"similarly to children, the language models tend to learn linguistic skills in a systematic order\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259089351\" paperTitle=\"(Evanson et al., 2023)\" isShortName></Paper>. However, these studies also reveal that \"some - but not all - learning stages are shared between children and these language models,\" pointing to fundamental differences in how humans and algorithms process language <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259089351\" paperTitle=\"(Evanson et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "First, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks."
                        ],
                        "paper": {
                            "corpus_id": 226289695,
                            "title": "When Do You Need Billions of Words of Pretraining Data?",
                            "authors": [
                                {
                                    "authorId": "9227100",
                                    "name": "Yian Zhang"
                                },
                                {
                                    "authorId": "46236380",
                                    "name": "Alex Warstadt"
                                },
                                {
                                    "authorId": "1993902967",
                                    "name": "Haau-Sing Li"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 141
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)",
                            "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92724609375
                    },
                    {
                        "id": "(Qin et al., 2024)",
                        "snippets": [
                            "Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset."
                        ],
                        "paper": {
                            "corpus_id": 267627053,
                            "title": "A systematic investigation of learnability from single child linguistic input",
                            "authors": [
                                {
                                    "authorId": "2283881086",
                                    "name": "Yulu Qin"
                                },
                                {
                                    "authorId": "2108329007",
                                    "name": "Wentao Wang"
                                },
                                {
                                    "authorId": "2373318",
                                    "name": "B. Lake"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.86962890625
                    },
                    {
                        "id": "(Neveditsin et al., 2024)",
                        "snippets": [
                            "Nevertheless, challenges remain, as highlighted by Mahowald et al. (Mahowald et al., 2024), who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines."
                        ],
                        "paper": {
                            "corpus_id": 271916426,
                            "title": "Clinical insights: A comprehensive review of language models in medicine",
                            "authors": [
                                {
                                    "authorId": "147692873",
                                    "name": "Nikita Neveditsin"
                                },
                                {
                                    "authorId": "2171126372",
                                    "name": "Pawan Lingras"
                                },
                                {
                                    "authorId": "2103123095",
                                    "name": "V. Mago"
                                }
                            ],
                            "year": 2024,
                            "venue": "PLOS Digital Health",
                            "n_citations": 5
                        },
                        "score": 0.8720703125
                    },
                    {
                        "id": "(Evanson et al., 2023)",
                        "snippets": [
                            "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language."
                        ],
                        "paper": {
                            "corpus_id": 259089351,
                            "title": "Language acquisition: do children and language models follow similar learning stages?",
                            "authors": [
                                {
                                    "authorId": "2219624467",
                                    "name": "Linnea Evanson"
                                },
                                {
                                    "authorId": "3051598",
                                    "name": "Yair Lakretz"
                                },
                                {
                                    "authorId": "28972230",
                                    "name": "J. King"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 29
                        },
                        "score": 0.94970703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Connections to Human Language Acquisition",
                "tldr": "Comparing language models to human acquisition reveals both similarities and differences, with models showing systematic learning progressions that partially overlap with child development. While LLMs rely primarily on distributional learning from text, they lack the embodied, interactive experiences that shape human language acquisition, resulting in different developmental trajectories despite some shared learning patterns. (2 sources)",
                "text": "\nResearch comparing language model training with human language acquisition has revealed intriguing parallels and important distinctions between these processes. Chang et al. conducted an innovative study that extracted \"learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory,\" a tool commonly used to assess vocabulary development in children <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. Their findings suggest that neural language models follow a developmental trajectory that shares some characteristics with human language acquisition, particularly in how they transition from simple to more complex prediction mechanisms.\n\nThe developmental sequence observed in language models shows a progression from relying on statistical frequencies to more sophisticated linguistic understanding. Chang et al. found that models \"predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions\" <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. This progression mirrors aspects of child language development, where early word learning is influenced by frequency and later transitions to more complex linguistic patterns. These parallels \"shed light on the role of distributional learning mechanisms in children,\" suggesting that some statistical learning principles may be shared between human and artificial language learners <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nDespite these similarities, recent research indicates significant divergences between human and artificial language acquisition trajectories. Yang et al. proposed \"a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning\" based on classical language development theories <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Their evaluation of language models using linguistic research methods revealed that \"although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition\" <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This finding highlights that while language models may acquire certain linguistic capabilities in a systematic order similar to children, the specific sequence and mechanisms differ in important ways.\n\nThese differences likely stem from the fundamentally different learning environments and mechanisms available to humans versus language models. While models rely exclusively on text-based distributional learning, humans benefit from multimodal, interactive, and socially grounded experiences that provide additional cues for language acquisition <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. The absence of embodied experience and social interaction in language model training represents a critical distinction that shapes their developmental trajectory in ways that diverge from human language acquisition patterns.",
                "citations": [
                    {
                        "id": "(Chang et al., 2021)",
                        "snippets": [
                            "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)",
                            "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."
                        ],
                        "paper": {
                            "corpus_id": 238408423,
                            "title": "Word Acquisition in Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 40
                        },
                        "score": 0.8984375
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition."
                        ],
                        "paper": {
                            "corpus_id": 273403825,
                            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
                            "authors": [
                                {
                                    "authorId": "2326447675",
                                    "name": "Qiyuan Yang"
                                },
                                {
                                    "authorId": "2294722814",
                                    "name": "Pengda Wang"
                                },
                                {
                                    "authorId": "2326296255",
                                    "name": "Luke D. Plonsky"
                                },
                                {
                                    "authorId": "2250843586",
                                    "name": "Frederick L. Oswald"
                                },
                                {
                                    "authorId": "7315244",
                                    "name": "Hanjie Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.91357421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.167871
    }
}