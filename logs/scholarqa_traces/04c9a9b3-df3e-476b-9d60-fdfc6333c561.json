{
    "query": "How do sparse attention mechanisms and memory-efficient implementations, such as FlashAttention and BigBird, compare in their ability to extend transformer context length while minimizing computational costs?",
    "user_id": "lib_user",
    "task_id": "04c9a9b3-df3e-476b-9d60-fdfc6333c561",
    "timestamp": "2025-06-23T22:27:50.937313",
    "n_retrieval": 256,
    "n_retrieved": 259,
    "n_candidates": 43,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.6239459999999999,
    "decomposed_query": {
        "rewritten_query": "Comparison of sparse attention mechanisms and memory-efficient implementations, such as FlashAttention and BigBird, in their ability to extend transformer context length while minimizing computational costs.",
        "keyword_query": "sparse attention mechanisms memory-efficient implementations FlashAttention BigBird transformer context length computational costs",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010092,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 24,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.09193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "15161448",
                    "name": "Chuhan Wu"
                },
                {
                    "authorId": "2397264",
                    "name": "Fangzhao Wu"
                },
                {
                    "authorId": "50329599",
                    "name": "Tao Qi"
                },
                {
                    "authorId": "1731776",
                    "name": "Yongfeng Huang"
                }
            ],
            "abstract": "Transformer has achieved great success in NLP. However, the quadratic complexity of the self-attention mechanism in Transformer makes it inefficient in handling long sequences. Many existing works explore to accelerate Transformers by computing sparse self-attention instead of a dense one, which usually attends to tokens at certain positions or randomly selected tokens. However, manually selected or random tokens may be uninformative for context modeling. In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention. In Smart Bird, we first compute a sketched attention matrix with a single-head low-dimensional Transformer, which aims to find potential important interactions between tokens. We then sample token pairs based on their probability scores derived from the sketched attention matrix to generate different sparse attention index matrices for different attention heads. Finally, we select token embeddings according to the index matrices to form the input of sparse attention networks. Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.",
            "corpus_id": 237260051,
            "sentences": [
                {
                    "corpus_id": "237260051",
                    "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
                    "text": "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms. In addition, on the MIND datasets, we compare two additional SOTA news recommendation methods, i.e., NRMS (Wu et al., 2019) and FIM (Wang et al., 2020a), to provide benchmarks for the comparison. The performance of different methods on different datasets are compared in Tables 2, 3 and 4. We have several observations from the results. First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens. Third, Smart Bird can achieve better performance than other compared methods on all datasets in different tasks. This is because Smart Bird incorporates learnable sparse attention to better capture token interactions that may be important for context modeling. These results demonstrate the effectiveness and generality of Smart Bird. \n\nFurthermore, we compare the theoretical computational complexity of different methods in   with other sparse attention based methods if the sequence length is not extremely long. 10 These results show that Smart Bird is also efficient.",
                    "score": 0.545263715845755,
                    "section_title": "Performance Evaluation",
                    "char_start_offset": 15625,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 142,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1883
                        },
                        {
                            "start": 1884,
                            "end": 1957
                        },
                        {
                            "start": 1960,
                            "end": 2141
                        },
                        {
                            "start": 2142,
                            "end": 2195
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 678,
                            "end": 695,
                            "matchedPaperCorpusId": "202774468"
                        },
                        {
                            "start": 704,
                            "end": 724,
                            "matchedPaperCorpusId": "220045915"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                },
                {
                    "corpus_id": "237260051",
                    "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
                    "text": "Since the vanilla Transformer is inefficient in processing long sequences, many methods explore to improve the efficiency of Transformer in different ways (Tay et al., 2020). One direction is computing a sparse attention matrix rather than a dense one by only computing attention on a sparse number of query and key vector pairs (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Zhang et al., 2021). For example, Sparse Transformer (Child et al., 2019) is a Transformer variant that combines local self-attention and stride attention. It uses half of the attention heads to attend tokens within a region and the rest attention heads to attend tokens with certain strides. Longformer (Beltagy et al., 2020) combines sliding window attention and global attention at certain positions to capture local and global contexts, respectively. BigBird (Zaheer et al., 2020) further introduces a random attention mechanism that attends several randomly selected pairs of tokens. However, the token pairs that are randomly sampled or selected by fixed rules may not be helpful for context modeling, which limits the performance of these methods. There are also many other ways to accelerate Transformers (Kitaev et al., 2020;Wang et al., 2020b). For example, Reformer (Kitaev et al., 2020) uses hashing techniques to cluster input embeddings into different buckets based on their similarities, and then chunks the buckets using a certain length. The tokens only attend to same bucket in their own chunk and previous chunk. Linformer (Wang et al., 2020b) assumes that the selfattention matrix is low-rank, and it approximates the self-attention mechanism by using low-rank attention key and value projected by separate linear transformations. Linear Transformer (Katharopoulos et al., 2020) uses kernel functions to approximate the self-attention mechanism. It derives a kernel-based formulation of self-attention based on the matrix multiplication associative property and designs a simple kernel function to approximate the computation. However, these methods do not fully consider the characteristics of natural language and may be suboptimal in text understanding.",
                    "score": 0.6491906287120446,
                    "section_title": "Related Work",
                    "char_start_offset": 4611,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1855
                        },
                        {
                            "start": 1856,
                            "end": 2036
                        },
                        {
                            "start": 2037,
                            "end": 2166
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 390,
                            "end": 409,
                            "matchedPaperCorpusId": "234339280"
                        },
                        {
                            "start": 1760,
                            "end": 1788,
                            "matchedPaperCorpusId": "220250819"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5810546875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer\n# Venue: arXiv.org\n# Authors: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang\n## Abstract\nTransformer has achieved great success in NLP. However, the quadratic complexity of the self-attention mechanism in Transformer makes it inefficient in handling long sequences. Many existing works explore to accelerate Transformers by computing sparse self-attention instead of a dense one, which usually attends to tokens at certain positions or randomly selected tokens. However, manually selected or random tokens may be uninformative for context modeling. In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention. In Smart Bird, we first compute a sketched attention matrix with a single-head low-dimensional Transformer, which aims to find potential important interactions between tokens. We then sample token pairs based on their probability scores derived from the sketched attention matrix to generate different sparse attention index matrices for different attention heads. Finally, we select token embeddings according to the index matrices to form the input of sparse attention networks. Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.\n## Related Work\nSince the vanilla Transformer is inefficient in processing long sequences, many methods explore to improve the efficiency of Transformer in different ways (Tay et al., 2020). One direction is computing a sparse attention matrix rather than a dense one by only computing attention on a sparse number of query and key vector pairs (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Zhang et al., 2021). For example, Sparse Transformer (Child et al., 2019) is a Transformer variant that combines local self-attention and stride attention. It uses half of the attention heads to attend tokens within a region and the rest attention heads to attend tokens with certain strides. Longformer (Beltagy et al., 2020) combines sliding window attention and global attention at certain positions to capture local and global contexts, respectively. BigBird (Zaheer et al., 2020) further introduces a random attention mechanism that attends several randomly selected pairs of tokens. However, the token pairs that are randomly sampled or selected by fixed rules may not be helpful for context modeling, which limits the performance of these methods. There are also many other ways to accelerate Transformers (Kitaev et al., 2020;Wang et al., 2020b). For example, Reformer (Kitaev et al., 2020) uses hashing techniques to cluster input embeddings into different buckets based on their similarities, and then chunks the buckets using a certain length. The tokens only attend to same bucket in their own chunk and previous chunk. Linformer (Wang et al., 2020b) assumes that the selfattention matrix is low-rank, and it approximates the self-attention mechanism by using low-rank attention key and value projected by separate linear transformations. Linear Transformer (Katharopoulos et al., 2020) uses kernel functions to approximate the self-attention mechanism. It derives a kernel-based formulation of self-attention based on the matrix multiplication associative property and designs a simple kernel function to approximate the computation. However, these methods do not fully consider the characteristics of natural language and may be suboptimal in text understanding.\n\n## Performance Evaluation\nWe compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms. In addition, on the MIND datasets, we compare two additional SOTA news recommendation methods, i.e., NRMS (Wu et al., 2019) and FIM (Wang et al., 2020a), to provide benchmarks for the comparison. The performance of different methods on different datasets are compared in Tables 2, 3 and 4. We have several observations from the results. First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens. Third, Smart Bird can achieve better performance than other compared methods on all datasets in different tasks. This is because Smart Bird incorporates learnable sparse attention to better capture token interactions that may be important for context modeling. These results demonstrate the effectiveness and generality of Smart Bird. \n\nFurthermore, we compare the theoretical computational complexity of different methods in   with other sparse attention based methods if the sequence length is not extremely long. 10 These results show that Smart Bird is also efficient.",
            "reference_string": "[237260051 | Wu et al. | 2021 | Citations: 3]"
        },
        {
            "title": "REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization",
            "venue": "Conference on Computational Natural Language Learning",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14418, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261279246",
                    "name": "Mohammad Reza Ghasemi Madani"
                },
                {
                    "authorId": "2294362638",
                    "name": "Pasquale Minervini"
                }
            ],
            "abstract": "Human-annotated textual explanations are becoming increasingly important in Explainable Natural Language Processing. Rationale extraction aims to provide faithful (i.e. reflective of the behavior of the model) and plausible (i.e. convincing to humans) explanations by highlighting the inputs that had the largest impact on the prediction without compromising the performance of the task model. In recent works, the focus of training rationale extractors was primarily on optimizing for plausibility using human highlights, while the task model was trained on jointly optimizing for task predictive accuracy and faithfulness. We propose REFER, a framework that employs a differentiable rationale extractor that allows to back-propagate through the rationale extraction process. We analyze the impact of using human highlights during training by jointly training the task model and the rationale extractor. In our experiments, REFER yields significantly better results in terms of faithfulness, plausibility, and downstream task accuracy on both in-distribution and out-of-distribution data. On both e-SNLI and CoS-E, our best setting produces better results in terms of composite normalized relative gain than the previous baselines by 11% and 3%, respectively.",
            "corpus_id": 264426102,
            "sentences": [
                {
                    "corpus_id": "264426102",
                    "title": "REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization",
                    "text": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, Zaheer et al. (2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware. Due to the capability to handle longer contexts, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization.",
                    "score": 0.7039680529539772,
                    "section_title": "A Model Detail",
                    "char_start_offset": 27504,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1058
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 287,
                            "end": 307,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.857421875
                }
            ],
            "relevance_judgement": 0.857421875,
            "relevance_judgment_input_expanded": "# Title: REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization\n# Venue: Conference on Computational Natural Language Learning\n# Authors: Mohammad Reza Ghasemi Madani, Pasquale Minervini\n## Abstract\nHuman-annotated textual explanations are becoming increasingly important in Explainable Natural Language Processing. Rationale extraction aims to provide faithful (i.e. reflective of the behavior of the model) and plausible (i.e. convincing to humans) explanations by highlighting the inputs that had the largest impact on the prediction without compromising the performance of the task model. In recent works, the focus of training rationale extractors was primarily on optimizing for plausibility using human highlights, while the task model was trained on jointly optimizing for task predictive accuracy and faithfulness. We propose REFER, a framework that employs a differentiable rationale extractor that allows to back-propagate through the rationale extraction process. We analyze the impact of using human highlights during training by jointly training the task model and the rationale extractor. In our experiments, REFER yields significantly better results in terms of faithfulness, plausibility, and downstream task accuracy on both in-distribution and out-of-distribution data. On both e-SNLI and CoS-E, our best setting produces better results in terms of composite normalized relative gain than the previous baselines by 11% and 3%, respectively.\n## A Model Detail\nTransformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, Zaheer et al. (2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware. Due to the capability to handle longer contexts, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization.",
            "reference_string": "[264426102 | Madani et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276535724",
                    "name": "Nathaniel Tomczak"
                },
                {
                    "authorId": "2873546",
                    "name": "S. Kuppannagari"
                }
            ],
            "abstract": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
            "corpus_id": 276106883,
            "sentences": [
                {
                    "corpus_id": "276106883",
                    "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                    "text": "Transformers have emerged as one of the most successful AI/ML techniques for modeling sequence data. They have drastically impacted a variety of scientific and engineering applications such as language modeling [1], molecular design [2], cancer pathology classification [3], genomic sequence modeling [4], and others. These models derive their power from an operation called the attention mechanism. Given a sequence of tokens of length L, the attention mechanism extracts pairwise similarities between the tokens [5]. \n\nHowever, the time and memory complexity of the attention mechanism is quadratic in L as it captures interactions between each pair of the sequence. This has severely limited the context length, i.e., the length of a sequence over which a transformer model can capture these interactions (also know as sequence length in the literature). For applications such as genomics, at least 4-5 orders of magnitude of increase in context length is needed [4]. \n\nA popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism [6]- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach. \n\nHowever, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.",
                    "score": 0.5439241977355027,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 518
                        },
                        {
                            "start": 521,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1192
                        },
                        {
                            "start": 1193,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1816
                        },
                        {
                            "start": 1819,
                            "end": 1909
                        },
                        {
                            "start": 1910,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2293
                        },
                        {
                            "start": 2294,
                            "end": 2407
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 233,
                            "end": 236,
                            "matchedPaperCorpusId": "268423149"
                        },
                        {
                            "start": 270,
                            "end": 273,
                            "matchedPaperCorpusId": "268040015"
                        },
                        {
                            "start": 301,
                            "end": 304,
                            "matchedPaperCorpusId": "259274952"
                        },
                        {
                            "start": 514,
                            "end": 517,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "259274952"
                        },
                        {
                            "start": 1086,
                            "end": 1089,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82958984375
                },
                {
                    "corpus_id": "276106883",
                    "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                    "text": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
                    "score": 0.6147172778546091,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78125
                }
            ],
            "relevance_judgement": 0.82958984375,
            "relevance_judgment_input_expanded": "# Title: Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques\n# Venue: arXiv.org\n# Authors: Nathaniel Tomczak, S. Kuppannagari\n## Abstract\nTransformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).\n## I. INTRODUCTION\nTransformers have emerged as one of the most successful AI/ML techniques for modeling sequence data. They have drastically impacted a variety of scientific and engineering applications such as language modeling [1], molecular design [2], cancer pathology classification [3], genomic sequence modeling [4], and others. These models derive their power from an operation called the attention mechanism. Given a sequence of tokens of length L, the attention mechanism extracts pairwise similarities between the tokens [5]. \n\nHowever, the time and memory complexity of the attention mechanism is quadratic in L as it captures interactions between each pair of the sequence. This has severely limited the context length, i.e., the length of a sequence over which a transformer model can capture these interactions (also know as sequence length in the literature). For applications such as genomics, at least 4-5 orders of magnitude of increase in context length is needed [4]. \n\nA popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism [6]- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach. \n\nHowever, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.",
            "reference_string": "[276106883 | Tomczak et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Big Bird: Transformers for Longer Sequences",
            "venue": "Neural Information Processing Systems",
            "year": 2020,
            "reference_count": 118,
            "citation_count": 2103,
            "influential_citation_count": 279,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.14062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "1947314",
                    "name": "Guru Guruganesh"
                },
                {
                    "authorId": "89890133",
                    "name": "Kumar Avinava Dubey"
                },
                {
                    "authorId": "1643737606",
                    "name": "J. Ainslie"
                },
                {
                    "authorId": "114577307",
                    "name": "Chris Alberti"
                },
                {
                    "authorId": "1722671",
                    "name": "Santiago Onta\u00f1\u00f3n"
                },
                {
                    "authorId": "38552691",
                    "name": "Philip Pham"
                },
                {
                    "authorId": "101210026",
                    "name": "Anirudh Ravula"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "113906155",
                    "name": "Li Yang"
                },
                {
                    "authorId": "143629707",
                    "name": "Amr Ahmed"
                }
            ],
            "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
            "corpus_id": 220831004,
            "sentences": [
                {
                    "corpus_id": "220831004",
                    "title": "Big Bird: Transformers for Longer Sequences",
                    "text": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "score": 0.5653065411400622,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82763671875
                }
            ],
            "relevance_judgement": 0.82763671875,
            "relevance_judgment_input_expanded": "# Title: Big Bird: Transformers for Longer Sequences\n# Venue: Neural Information Processing Systems\n# Authors: M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed\n## Abstract\nTransformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n",
            "reference_string": "[220831004 | Zaheer et al. | 2020 | Citations: 2103]"
        },
        {
            "title": "GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices",
            "venue": "Proceedings of the AAAI Symposium Series",
            "year": 2025,
            "reference_count": 100,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2180067662",
                    "name": "Mozhgan Navardi"
                },
                {
                    "authorId": "2346327200",
                    "name": "Romina Aalishah"
                },
                {
                    "authorId": "2335995540",
                    "name": "Yuzhe Fu"
                },
                {
                    "authorId": "2223973348",
                    "name": "Yueqian Lin"
                },
                {
                    "authorId": "2346061748",
                    "name": "Hai Li"
                },
                {
                    "authorId": "2247106559",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "2393902",
                    "name": "T. Mohsenin"
                }
            ],
            "abstract": "Generative Artificial Intelligence (GenAI) applies models and algorithms such as Large Language Model (LLM) and Foundation Model (FM) to generate new data. GenAI, as a promising approach, enables advanced capabilities in various applications, including text generation and image processing. In current practice, GenAI algorithms run mainly on the cloud server, leading to high latency and raising security concerns. Consequently, these challenges encourage the deployment of GenAI algorithms directly on edge devices. However, the large size of such models and their significant computational resource requirements pose obstacles when deploying them in resource-constrained systems. This survey provides a comprehensive overview of recent proposed techniques that optimize GenAI for efficient deployment on resource-constrained edge devices. For this aim, this work highlights three main categories for bringing GenAI to the edge: software optimization, hardware optimization, and frameworks. The main takeaways for readers of this survey will be a clear roadmap to design, implement, and refine GenAI systems for real-world implementation on edge devices.",
            "corpus_id": 276575796,
            "sentences": [
                {
                    "corpus_id": "276575796",
                    "title": "GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices",
                    "text": "Transformers have become the backbone of many GenAI models, but their multi-head self-attention mechanism can dominate runtime and memory usage. Therefore, researchers have explored a range of strategies to optimize attention on hardware and algorithmic levels. \n\nHardware-based. FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations. \n\nAlgorithmic-based. Work on sparse attention reduces the quadratic complexity of self-attention by ignoring parts of the input that do not affect the result significantly. Child et al. (Child et al. 2019) pioneered this approach by limiting attention to strided patterns using sparse factorizations of the attention matrix to reduce computation cost while maintaining performance on sequence models. Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns.",
                    "score": 0.6942539759035261,
                    "section_title": "Attention Optimization",
                    "char_start_offset": 22202,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 261
                        },
                        {
                            "start": 264,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1329
                        },
                        {
                            "start": 1332,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 2166
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82470703125
                }
            ],
            "relevance_judgement": 0.82470703125,
            "relevance_judgment_input_expanded": "# Title: GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices\n# Venue: Proceedings of the AAAI Symposium Series\n# Authors: Mozhgan Navardi, Romina Aalishah, Yuzhe Fu, Yueqian Lin, Hai Li, Yiran Chen, T. Mohsenin\n## Abstract\nGenerative Artificial Intelligence (GenAI) applies models and algorithms such as Large Language Model (LLM) and Foundation Model (FM) to generate new data. GenAI, as a promising approach, enables advanced capabilities in various applications, including text generation and image processing. In current practice, GenAI algorithms run mainly on the cloud server, leading to high latency and raising security concerns. Consequently, these challenges encourage the deployment of GenAI algorithms directly on edge devices. However, the large size of such models and their significant computational resource requirements pose obstacles when deploying them in resource-constrained systems. This survey provides a comprehensive overview of recent proposed techniques that optimize GenAI for efficient deployment on resource-constrained edge devices. For this aim, this work highlights three main categories for bringing GenAI to the edge: software optimization, hardware optimization, and frameworks. The main takeaways for readers of this survey will be a clear roadmap to design, implement, and refine GenAI systems for real-world implementation on edge devices.\n## Attention Optimization\nTransformers have become the backbone of many GenAI models, but their multi-head self-attention mechanism can dominate runtime and memory usage. Therefore, researchers have explored a range of strategies to optimize attention on hardware and algorithmic levels. \n\nHardware-based. FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations. \n\nAlgorithmic-based. Work on sparse attention reduces the quadratic complexity of self-attention by ignoring parts of the input that do not affect the result significantly. Child et al. (Child et al. 2019) pioneered this approach by limiting attention to strided patterns using sparse factorizations of the attention matrix to reduce computation cost while maintaining performance on sequence models. Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns.",
            "reference_string": "[276575796 | Navardi et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion",
            "venue": "IEEE Access",
            "year": 2022,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09984650.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2022.3229055?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2022.3229055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2195944251",
                    "name": "Jongseong Bae"
                },
                {
                    "authorId": "2196700504",
                    "name": "Byung Do Cheon"
                },
                {
                    "authorId": "2129420267",
                    "name": "Ha Young Kim"
                }
            ],
            "abstract": "The self-attention mechanism requires a huge amount of computational cost despite its successful use in the transformer. The computational cost linearly increases in proportion to the embedding dimension size, owing to the dot product operation that calculates token similarities in vector spaces. To tackle this problem, we propose a novel efficient self-attention mechanism (Pro-attention) that computes the attention scores through distribution matching in probability space. To this end, we assume that each token has its unique probability distribution and regard each component of a token vector as a sample from the probability distribution. Then, we estimate the statistics of each token-specific probability distribution from the samples, and the token similarities are obtained by using the Kullback-Leibler Divergence. According to the time complexity analysis, the computational cost is markedly saved because the time complexity is independent of the feature dimension size. Our method produces competitive performances in machine translation and language modeling benchmarks such as IWSLT\u201914 De-En, WMT\u201914 En-De, WMT\u201914 En-Fr, and WikiText-103 datasets. Moreover, our model maintains the performances with considerably reduced FLOPs of the self-attention mechanism, which is up to 87% less compared to the baseline transformer. Especially, our model improves the efficiency in a large volume of the training dataset.",
            "corpus_id": 254725259,
            "sentences": [
                {
                    "corpus_id": "254725259",
                    "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion",
                    "text": "Some models reduces the computational complexity by introducing sparse attention mechanism. In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird [18] is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length. \n\nSimilar to the above models, Routing Transformer [19] also adopts a sparse attention to achieve the complexity of 1.5 power of input sequence length. Routing Transformer selects sparsity patterns without computation of a full attention matrix. Lite Transformer [4] adopts long-short range attention. The long-short range attention separately captures sparse and diagonal patterns in the attention maps, and reduces model size and computation cost. \n\nLogSparse Transformer [20] reduces the linear complexity of each attention layer to the logarithmic complexity to relieve the quadratic total complexity. To achieve this, in the self-attention of LogSparse Transformer, each cell only attends to its previous cells with an exponential step size. LogSparse Transformer also applies convolutional self-attention to manipulate queries and keys to catch more local context and enhance the overall performance. \n\nSparse Sinkhorn transformer [17] is another sparse efficient model. Tokens in the vanilla self-attention only attend to tokens within the same block but each token attends to tokens in the sorted block in Sparse Sinkhorn attention. Then, other tokens far away in the other block can be considered. It enables the model to compute quasi-global attention and improves memory efficiency. \n\nMeanwhile, some models directly reduce the number of parameters. For example, DeLighT [3] lowers the number of parameters by using block-wise scaling. Similarly, Informer [21] adopts ProbSparse-attention to reduce the number of queries used, and enhance memory and calculation efficiency.",
                    "score": 0.6197107786670373,
                    "section_title": "B. SPARSE ATTENTION",
                    "char_start_offset": 6896,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 91
                        },
                        {
                            "start": 92,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 331
                        },
                        {
                            "start": 332,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 837
                        },
                        {
                            "start": 840,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1287
                        },
                        {
                            "start": 1290,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1584
                        },
                        {
                            "start": 1585,
                            "end": 1744
                        },
                        {
                            "start": 1747,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2044
                        },
                        {
                            "start": 2045,
                            "end": 2131
                        },
                        {
                            "start": 2134,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2284
                        },
                        {
                            "start": 2285,
                            "end": 2422
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 287,
                            "end": 291,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1312,
                            "end": 1316,
                            "matchedPaperCorpusId": "195766887"
                        },
                        {
                            "start": 1775,
                            "end": 1779,
                            "matchedPaperCorpusId": "211505992"
                        },
                        {
                            "start": 2305,
                            "end": 2309,
                            "matchedPaperCorpusId": "229156802"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81591796875
                }
            ],
            "relevance_judgement": 0.81591796875,
            "relevance_judgment_input_expanded": "# Title: Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion\n# Venue: IEEE Access\n# Authors: Jongseong Bae, Byung Do Cheon, Ha Young Kim\n## Abstract\nThe self-attention mechanism requires a huge amount of computational cost despite its successful use in the transformer. The computational cost linearly increases in proportion to the embedding dimension size, owing to the dot product operation that calculates token similarities in vector spaces. To tackle this problem, we propose a novel efficient self-attention mechanism (Pro-attention) that computes the attention scores through distribution matching in probability space. To this end, we assume that each token has its unique probability distribution and regard each component of a token vector as a sample from the probability distribution. Then, we estimate the statistics of each token-specific probability distribution from the samples, and the token similarities are obtained by using the Kullback-Leibler Divergence. According to the time complexity analysis, the computational cost is markedly saved because the time complexity is independent of the feature dimension size. Our method produces competitive performances in machine translation and language modeling benchmarks such as IWSLT\u201914 De-En, WMT\u201914 En-De, WMT\u201914 En-Fr, and WikiText-103 datasets. Moreover, our model maintains the performances with considerably reduced FLOPs of the self-attention mechanism, which is up to 87% less compared to the baseline transformer. Especially, our model improves the efficiency in a large volume of the training dataset.\n## B. SPARSE ATTENTION\nSome models reduces the computational complexity by introducing sparse attention mechanism. In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird [18] is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length. \n\nSimilar to the above models, Routing Transformer [19] also adopts a sparse attention to achieve the complexity of 1.5 power of input sequence length. Routing Transformer selects sparsity patterns without computation of a full attention matrix. Lite Transformer [4] adopts long-short range attention. The long-short range attention separately captures sparse and diagonal patterns in the attention maps, and reduces model size and computation cost. \n\nLogSparse Transformer [20] reduces the linear complexity of each attention layer to the logarithmic complexity to relieve the quadratic total complexity. To achieve this, in the self-attention of LogSparse Transformer, each cell only attends to its previous cells with an exponential step size. LogSparse Transformer also applies convolutional self-attention to manipulate queries and keys to catch more local context and enhance the overall performance. \n\nSparse Sinkhorn transformer [17] is another sparse efficient model. Tokens in the vanilla self-attention only attend to tokens within the same block but each token attends to tokens in the sorted block in Sparse Sinkhorn attention. Then, other tokens far away in the other block can be considered. It enables the model to compute quasi-global attention and improves memory efficiency. \n\nMeanwhile, some models directly reduce the number of parameters. For example, DeLighT [3] lowers the number of parameters by using block-wise scaling. Similarly, Informer [21] adopts ProbSparse-attention to reduce the number of queries used, and enhance memory and calculation efficiency.",
            "reference_string": "[254725259 | Bae et al. | 2022 | Citations: 0]"
        },
        {
            "title": "IAFormer: Interaction-Aware Transformer network for collider data analysis",
            "venue": "",
            "year": 2025,
            "reference_count": 77,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265503502",
                    "name": "W. Esmail"
                },
                {
                    "authorId": "2359256434",
                    "name": "A. Hammad"
                },
                {
                    "authorId": "2277246229",
                    "name": "M. Nojiri"
                }
            ],
            "abstract": "In this paper, we introduce IAFormer, a novel Transformer-based architecture that efficiently integrates pairwise particle interactions through a dynamic sparse attention mechanism. The IAformer has two new mechanisms within the model. First, the attention matrix depends on predefined boost invariant pairwise quantities, reducing the network parameter significantly from the original particle transformer models. Second, IAformer incorporate the sparse attention mechanism by utilizing the ``differential attention'', so that it can dynamically prioritizes relevant particle tokens while reducing computational overhead associated with less informative ones. This approach significantly lowers the model complexity without compromising performance. Despite being computationally efficient by more than an order of magnitude than the Particle Transformer network, IAFormer achieves state-of-the-art performance in classification tasks on the Top and quark-gluon datasets. Furthermore, we employ AI interpretability techniques, verifying that the model effectively captures physically meaningful information layer by layer through its sparse attention mechanism, building an efficient network output that is resistant to statistical fluctuations. IAformer highlights the need to sparse attention in any Transformer analysis to reduce the network size while improving its performance.",
            "corpus_id": 278339364,
            "sentences": [
                {
                    "corpus_id": "278339364",
                    "title": "IAFormer: Interaction-Aware Transformer network for collider data analysis",
                    "text": "Sparse attention is a technique used in Transformer models to reduce the computational complexity of the self-attention mechanism by limiting the number of key-query pairs attend to the classification. Instead of computing attention over all tokens in a sequence, sparse attention selectively attends to a subset of tokens based on predefined patterns, learned structures, or efficient approximations. This significantly lowers the memory and computational cost from quadratic to linear or sub-quadratic complexity, making it possible to process longer sequences efficiently. \n\nDifferent types of sparse attention have been introduced such as fixed pattern and learnable sparse attention. Fixed pattern sparse attention is a type of sparse attention mechanism where each token attends only to a predefined subset of tokens based on a fixed rule. [58][59][60]. On the other hand, Learnable sparse attention dynamically determines which tokens to attend to based on learned patterns. Learnable sparse attention allows the model to adaptively focus on the most relevant tokens in different contexts, leading to improved efficiency. In this paper we utilize a dynamic sparse attention mechanism incorporated in transformer called \"differential attention\" which was first introduced in [51]. \n\nwith \u03b2 is a learnable vector and W 1 , W 2 are two learnable matrices. During the training process, the value of \u03b2 vector is optimized to demote the attention score assigned to less relevant tokens, resulting in the attention score matrix to be sparse. This results in an implicit sparsity, where tokens gradually focus only on meaningful interactions while ignoring less relevant ones. The self-attention mechanism is guided by this dynamic sparsity, reducing computational overhead while preserving essential long-range dependencies. The IAFormer shows better performance compared with ParT with notably smaller number of parameters. \n\nDespite the advantages of dynamic sparse attention, a challenge remains in controlling the value of the \u03b2 parameter. In the current setup, we clip \u03b2 to lie within the range [0, 1]. Additionally, the attention score \u03b1 i,j can sometimes take negative values. In such cases, it becomes unclear how to interpret it as an attention probability.",
                    "score": 0.538612488723589,
                    "section_title": "Dynamic sparse attention via differential attention",
                    "char_start_offset": 17352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 575
                        },
                        {
                            "start": 578,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1286
                        },
                        {
                            "start": 1289,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1824
                        },
                        {
                            "start": 1825,
                            "end": 1924
                        },
                        {
                            "start": 1927,
                            "end": 2043
                        },
                        {
                            "start": 2044,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2183
                        },
                        {
                            "start": 2184,
                            "end": 2266
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 850,
                            "end": 854,
                            "matchedPaperCorpusId": "258048654"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8056640625
                }
            ],
            "relevance_judgement": 0.8056640625,
            "relevance_judgment_input_expanded": "# Title: IAFormer: Interaction-Aware Transformer network for collider data analysis\n# Venue: \n# Authors: W. Esmail, A. Hammad, M. Nojiri\n## Abstract\nIn this paper, we introduce IAFormer, a novel Transformer-based architecture that efficiently integrates pairwise particle interactions through a dynamic sparse attention mechanism. The IAformer has two new mechanisms within the model. First, the attention matrix depends on predefined boost invariant pairwise quantities, reducing the network parameter significantly from the original particle transformer models. Second, IAformer incorporate the sparse attention mechanism by utilizing the ``differential attention'', so that it can dynamically prioritizes relevant particle tokens while reducing computational overhead associated with less informative ones. This approach significantly lowers the model complexity without compromising performance. Despite being computationally efficient by more than an order of magnitude than the Particle Transformer network, IAFormer achieves state-of-the-art performance in classification tasks on the Top and quark-gluon datasets. Furthermore, we employ AI interpretability techniques, verifying that the model effectively captures physically meaningful information layer by layer through its sparse attention mechanism, building an efficient network output that is resistant to statistical fluctuations. IAformer highlights the need to sparse attention in any Transformer analysis to reduce the network size while improving its performance.\n## Dynamic sparse attention via differential attention\nSparse attention is a technique used in Transformer models to reduce the computational complexity of the self-attention mechanism by limiting the number of key-query pairs attend to the classification. Instead of computing attention over all tokens in a sequence, sparse attention selectively attends to a subset of tokens based on predefined patterns, learned structures, or efficient approximations. This significantly lowers the memory and computational cost from quadratic to linear or sub-quadratic complexity, making it possible to process longer sequences efficiently. \n\nDifferent types of sparse attention have been introduced such as fixed pattern and learnable sparse attention. Fixed pattern sparse attention is a type of sparse attention mechanism where each token attends only to a predefined subset of tokens based on a fixed rule. [58][59][60]. On the other hand, Learnable sparse attention dynamically determines which tokens to attend to based on learned patterns. Learnable sparse attention allows the model to adaptively focus on the most relevant tokens in different contexts, leading to improved efficiency. In this paper we utilize a dynamic sparse attention mechanism incorporated in transformer called \"differential attention\" which was first introduced in [51]. \n\nwith \u03b2 is a learnable vector and W 1 , W 2 are two learnable matrices. During the training process, the value of \u03b2 vector is optimized to demote the attention score assigned to less relevant tokens, resulting in the attention score matrix to be sparse. This results in an implicit sparsity, where tokens gradually focus only on meaningful interactions while ignoring less relevant ones. The self-attention mechanism is guided by this dynamic sparsity, reducing computational overhead while preserving essential long-range dependencies. The IAFormer shows better performance compared with ParT with notably smaller number of parameters. \n\nDespite the advantages of dynamic sparse attention, a challenge remains in controlling the value of the \u03b2 parameter. In the current setup, we clip \u03b2 to lie within the range [0, 1]. Additionally, the attention score \u03b1 i,j can sometimes take negative values. In such cases, it becomes unclear how to interpret it as an attention probability.",
            "reference_string": "[278339364 | Esmail et al. | 2025 | Citations: 0]"
        },
        {
            "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability",
            "venue": "UniReps",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.01989, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1505801820",
                    "name": "Yufan Zhuang"
                },
                {
                    "authorId": "2240689",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "3180064",
                    "name": "Fangbo Tao"
                },
                {
                    "authorId": "2163679367",
                    "name": "Jingbo Shang"
                }
            ],
            "abstract": "Transformer and its variants are fundamental neural architectures in deep learning. Recent works show that learning attention in the Fourier space can improve the long sequence learning capability of Transformers. We argue that wavelet transform shall be a better choice because it captures both position and frequency information with linear time complexity. Therefore, in this paper, we systematically study the synergy between wavelet transform and Transformers. We propose Wavelet Space Attention (WavSpA) that facilitates attention learning in a learnable wavelet coefficient space which replaces the attention in Transformers by (1) applying forward wavelet transform to project the input sequences to multi-resolution bases, (2) conducting attention learning in the wavelet coefficient space, and (3) reconstructing the representation in input space via backward wavelet transform. Extensive experiments on the Long Range Arena demonstrate that learning attention in the wavelet space using either fixed or adaptive wavelets can consistently improve Transformer's performance and also significantly outperform learning in Fourier space. We further show our method can enhance Transformer's reasoning extrapolation capability over distance on the LEGO chain-of-reasoning task.",
            "corpus_id": 258840932,
            "sentences": [
                {
                    "corpus_id": "258840932",
                    "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability",
                    "text": "There has been plenty of prior work to enable transformers to handle long input more effectively and efficiently. Since the inefficiency comes from the quadratic dependency on sequence length because of the dense attention operation, a large portion of research simulates the attention operation with certain approximations, for example, replacing the dense attention matrix with a sparse version, or assume that it satisfies certain low-rank structures. We briefly review some methods on this topic in this section. For a more detailed survey, we refer the readers to [36]. \n\nSparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention [27] and Multi-passage BERT [41] use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird [46] further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions. On the other front, Orthogonal Transformer [13] utilizes an iterative approach to construct an orthogonal vector basis in Euclidean space, then perform windowed attention on grouped tokens after orthogonal projection. \n\nLow-rank Approximation. The self-attention matrix, at the center of transformer, has been found to display low-rank behaviors after pre-training. Linformer [40] performed spectrum analysis on the pre-trained attention matrix, and the results indicate that the top 128 singular values composite 88%-96% of the entire 512 singular values across attention heads and layers. Based on this observation, Linformer added low-rank projection matrices in attention to approximate the original attention matrix. On a similar notion, Drone [4] extended the low-rank approximation scope to all matrices in transformer via data-driven optimal compression.",
                    "score": 0.5832486805887114,
                    "section_title": "Attention Methods",
                    "char_start_offset": 23458,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1386
                        },
                        {
                            "start": 1389,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1890
                        },
                        {
                            "start": 1891,
                            "end": 2031
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 569,
                            "end": 573,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 785,
                            "end": 789,
                            "matchedPaperCorpusId": "3353110"
                        },
                        {
                            "start": 813,
                            "end": 817,
                            "matchedPaperCorpusId": "201307832"
                        },
                        {
                            "start": 908,
                            "end": 912,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1212,
                            "end": 1216,
                            "matchedPaperCorpusId": "258509623"
                        },
                        {
                            "start": 1918,
                            "end": 1921,
                            "matchedPaperCorpusId": "245003869"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79541015625
                }
            ],
            "relevance_judgement": 0.79541015625,
            "relevance_judgment_input_expanded": "# Title: WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability\n# Venue: UniReps\n# Authors: Yufan Zhuang, Zihan Wang, Fangbo Tao, Jingbo Shang\n## Abstract\nTransformer and its variants are fundamental neural architectures in deep learning. Recent works show that learning attention in the Fourier space can improve the long sequence learning capability of Transformers. We argue that wavelet transform shall be a better choice because it captures both position and frequency information with linear time complexity. Therefore, in this paper, we systematically study the synergy between wavelet transform and Transformers. We propose Wavelet Space Attention (WavSpA) that facilitates attention learning in a learnable wavelet coefficient space which replaces the attention in Transformers by (1) applying forward wavelet transform to project the input sequences to multi-resolution bases, (2) conducting attention learning in the wavelet coefficient space, and (3) reconstructing the representation in input space via backward wavelet transform. Extensive experiments on the Long Range Arena demonstrate that learning attention in the wavelet space using either fixed or adaptive wavelets can consistently improve Transformer's performance and also significantly outperform learning in Fourier space. We further show our method can enhance Transformer's reasoning extrapolation capability over distance on the LEGO chain-of-reasoning task.\n## Attention Methods\nThere has been plenty of prior work to enable transformers to handle long input more effectively and efficiently. Since the inefficiency comes from the quadratic dependency on sequence length because of the dense attention operation, a large portion of research simulates the attention operation with certain approximations, for example, replacing the dense attention matrix with a sparse version, or assume that it satisfies certain low-rank structures. We briefly review some methods on this topic in this section. For a more detailed survey, we refer the readers to [36]. \n\nSparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention [27] and Multi-passage BERT [41] use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird [46] further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions. On the other front, Orthogonal Transformer [13] utilizes an iterative approach to construct an orthogonal vector basis in Euclidean space, then perform windowed attention on grouped tokens after orthogonal projection. \n\nLow-rank Approximation. The self-attention matrix, at the center of transformer, has been found to display low-rank behaviors after pre-training. Linformer [40] performed spectrum analysis on the pre-trained attention matrix, and the results indicate that the top 128 singular values composite 88%-96% of the entire 512 singular values across attention heads and layers. Based on this observation, Linformer added low-rank projection matrices in attention to approximate the original attention matrix. On a similar notion, Drone [4] extended the low-rank approximation scope to all matrices in transformer via data-driven optimal compression.",
            "reference_string": "[258840932 | Zhuang et al. | 2022 | Citations: 3]"
        },
        {
            "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305696370",
                    "name": "Hengyu Zhang"
                }
            ],
            "abstract": "Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.",
            "corpus_id": 270370979,
            "sentences": [
                {
                    "corpus_id": "270370979",
                    "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
                    "text": "The primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements.This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models.Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to O(n).These models achieve a balance between maintaining sufficient context for understanding while managing computational load.For achieving complexity of O(n log n), several approaches have been proposed.Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window.Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity.LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead.\n\nEquipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token.This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts.Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited.This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications.",
                    "score": 0.6746960254522978,
                    "section_title": "Long-context Transformers",
                    "char_start_offset": 7163,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 224,
                            "end": 365
                        },
                        {
                            "start": 365,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 688
                        },
                        {
                            "start": 688,
                            "end": 766
                        },
                        {
                            "start": 766,
                            "end": 905
                        },
                        {
                            "start": 905,
                            "end": 1080
                        },
                        {
                            "start": 1080,
                            "end": 1242
                        },
                        {
                            "start": 1244,
                            "end": 1431
                        },
                        {
                            "start": 1431,
                            "end": 1579
                        },
                        {
                            "start": 1579,
                            "end": 1735
                        },
                        {
                            "start": 1735,
                            "end": 1884
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 417,
                            "end": 421,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1094,
                            "end": 1097,
                            "matchedPaperCorpusId": "253157377"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7861328125
                }
            ],
            "relevance_judgement": 0.7861328125,
            "relevance_judgment_input_expanded": "# Title: SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models\n# Venue: arXiv.org\n# Authors: Hengyu Zhang\n## Abstract\nExtending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n## Long-context Transformers\nThe primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements.This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models.Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to O(n).These models achieve a balance between maintaining sufficient context for understanding while managing computational load.For achieving complexity of O(n log n), several approaches have been proposed.Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window.Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity.LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead.\n\nEquipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token.This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts.Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited.This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications.",
            "reference_string": "[270370979 | Zhang | 2024 | Citations: 2]"
        },
        {
            "title": "When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting",
            "venue": "Journal of universal computer science (Online)",
            "year": 2021,
            "reference_count": 45,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://lib.jucs.org/article/69619/download/pdf/",
                "status": "GOLD",
                "license": "CCBYND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.09691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144502599",
                    "name": "V\u00edt Novotn\u00fd"
                },
                {
                    "authorId": "31023375",
                    "name": "Michal \u0160tef\u00e1nik"
                },
                {
                    "authorId": "31538290",
                    "name": "E. F. Ayetiran"
                },
                {
                    "authorId": "1980208",
                    "name": "Petr Sojka"
                }
            ],
            "abstract": "In 2018, Mikolov et al. introduced the positional language model, which has characteristics of attention-based neural machine translation models and which achieved state-of-the-art performance on the intrinsic word analogy task. However, the positional model is not practically fast and it has never been evaluated on qualitative criteria or extrinsic tasks. We propose a constrained positional model, which adapts the sparse attention mechanism from neural machine translation to improve the speed of the positional model. We evaluate the positional and constrained positional models on three novel qualitative criteria and on language modeling. We show that the positional and constrained positional models contain interpretable information about the grammatical properties of words and outperform other shallow models on language modeling. We also show that our constrained model outperforms the positional model on language modeling and trains twice as fast.",
            "corpus_id": 233307400,
            "sentences": [
                {
                    "corpus_id": "233307400",
                    "title": "When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting",
                    "text": "Since the dense attention mechanism learns weights for all pairs of source and target words, its space complexity is O(n 2 ) in the source sequence length. Several sparse attention architectures have been proposed in literature to enable the translation of longer source sequences by making the space complexity O(n). \n\nChild et al. [Chi+19] proposed the Sparse Transformer architecture, which factorized the dense attention using p separate attention heads to learn only O(n\u2022 p \u221a n) weights. They showed that the resulting model could use larger context sizes and achieved significantly better results than Transformers on density modeling tasks. \n\nFollowing the success of Sparse Transformers, Beltagy, Peters, Cohan [BPC20] proposed the Longformer architecture, which reduced the number of attention weights to O(n) and achieved significantly better results than Transformers on multiple long document tasks including question answering, coreference resolution, and classification. \n\nFinally, Zaheer et al. [Zah+20] proposed the BigBird architecture. Like Longformers, BigBird also used O(n) weights. Unlike Longformers, BigBird has been shown to be a Turing-complete universal approximator. Although attention enabled the recollection of long-range memories, sparse attention made it computationally tractable to do so.",
                    "score": 0.7632728442014963,
                    "section_title": "Sparse attention",
                    "char_start_offset": 4772,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 647
                        },
                        {
                            "start": 650,
                            "end": 984
                        },
                        {
                            "start": 987,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1323
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77099609375
                }
            ],
            "relevance_judgement": 0.77099609375,
            "relevance_judgment_input_expanded": "# Title: When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting\n# Venue: Journal of universal computer science (Online)\n# Authors: V\u00edt Novotn\u00fd, Michal \u0160tef\u00e1nik, E. F. Ayetiran, Petr Sojka\n## Abstract\nIn 2018, Mikolov et al. introduced the positional language model, which has characteristics of attention-based neural machine translation models and which achieved state-of-the-art performance on the intrinsic word analogy task. However, the positional model is not practically fast and it has never been evaluated on qualitative criteria or extrinsic tasks. We propose a constrained positional model, which adapts the sparse attention mechanism from neural machine translation to improve the speed of the positional model. We evaluate the positional and constrained positional models on three novel qualitative criteria and on language modeling. We show that the positional and constrained positional models contain interpretable information about the grammatical properties of words and outperform other shallow models on language modeling. We also show that our constrained model outperforms the positional model on language modeling and trains twice as fast.\n## Sparse attention\nSince the dense attention mechanism learns weights for all pairs of source and target words, its space complexity is O(n 2 ) in the source sequence length. Several sparse attention architectures have been proposed in literature to enable the translation of longer source sequences by making the space complexity O(n). \n\nChild et al. [Chi+19] proposed the Sparse Transformer architecture, which factorized the dense attention using p separate attention heads to learn only O(n\u2022 p \u221a n) weights. They showed that the resulting model could use larger context sizes and achieved significantly better results than Transformers on density modeling tasks. \n\nFollowing the success of Sparse Transformers, Beltagy, Peters, Cohan [BPC20] proposed the Longformer architecture, which reduced the number of attention weights to O(n) and achieved significantly better results than Transformers on multiple long document tasks including question answering, coreference resolution, and classification. \n\nFinally, Zaheer et al. [Zah+20] proposed the BigBird architecture. Like Longformers, BigBird also used O(n) weights. Unlike Longformers, BigBird has been shown to be a Turing-complete universal approximator. Although attention enabled the recollection of long-range memories, sparse attention made it computationally tractable to do so.",
            "reference_string": "[233307400 | Novotny et al. | 2021 | Citations: 4]"
        },
        {
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2061806821",
                    "name": "Chao Lou"
                },
                {
                    "authorId": "1453587987",
                    "name": "Zixia Jia"
                },
                {
                    "authorId": "2266032392",
                    "name": "Zilong Zheng"
                },
                {
                    "authorId": "40341553",
                    "name": "Kewei Tu"
                }
            ],
            "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
            "corpus_id": 270703226,
            "sentences": [
                {
                    "corpus_id": "270703226",
                    "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
                    "text": "Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.\n\nTo tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1.Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top-k mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time.It is worth noting that our method draws inspiration from the concept of top-k attention [32,1].Unfortunately, conventional top-k attention is non-differentiable and therefore cannot be used to train the scoring network.With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows.\n\nIncremental KV Selection.\n\nThe SPARSEK operator ( \u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder.Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results.\n\nComputational and Memory Efficiency.SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32,2,47] to linear time and achieves constant memory cost in inference.This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference.Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs.This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately ( \u00a7 3.2).\n\nExtension with IO-awareness.FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness.However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application.",
                    "score": 0.5677010592890795,
                    "section_title": "Introduction",
                    "char_start_offset": 1917,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 197,
                            "end": 446
                        },
                        {
                            "start": 446,
                            "end": 749
                        },
                        {
                            "start": 749,
                            "end": 845
                        },
                        {
                            "start": 845,
                            "end": 969
                        },
                        {
                            "start": 969,
                            "end": 1109
                        },
                        {
                            "start": 1111,
                            "end": 1136
                        },
                        {
                            "start": 1138,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1385
                        },
                        {
                            "start": 1387,
                            "end": 1423
                        },
                        {
                            "start": 1423,
                            "end": 1600
                        },
                        {
                            "start": 1600,
                            "end": 1759
                        },
                        {
                            "start": 1759,
                            "end": 1881
                        },
                        {
                            "start": 1881,
                            "end": 2020
                        },
                        {
                            "start": 2022,
                            "end": 2050
                        },
                        {
                            "start": 2050,
                            "end": 2143
                        },
                        {
                            "start": 2143,
                            "end": 2267
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 838,
                            "end": 842,
                            "matchedPaperCorpusId": "235422257"
                        },
                        {
                            "start": 842,
                            "end": 844,
                            "matchedPaperCorpusId": "257622671"
                        },
                        {
                            "start": 1330,
                            "end": 1333,
                            "matchedPaperCorpusId": "257622671"
                        },
                        {
                            "start": 1529,
                            "end": 1532,
                            "matchedPaperCorpusId": "235422257"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74609375
                },
                {
                    "corpus_id": "270703226",
                    "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
                    "text": "Long-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context.Numerous efficient approaches have emerged, spanning state-space models [30,62], recurrent neural networks [45,52,49], linear attention [55,38] and low-rank approximations of self-attention [75,14,53], which replace the self-attention with novel linear blocks for long-context modeling.Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29,77].Besides, a few studies combine the Transformer with block-wise recurrence [17,35,36,12] or key-value compression [60,59,18].In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix.This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15,27,42].\n\nSparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56,51], dilated sliding windows [4,22], combination of patterns [34,13], or domain-specific patterns [31].Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81,42,27].However, these static methods often prove suboptimal in various scenarios [66,2].Alternatively, sparse patterns can be learned in a data-driven manner.For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens.Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs.Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference.",
                    "score": 0.5452731339862182,
                    "section_title": "Related Work",
                    "char_start_offset": 5218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 183,
                            "end": 469
                        },
                        {
                            "start": 469,
                            "end": 626
                        },
                        {
                            "start": 626,
                            "end": 750
                        },
                        {
                            "start": 750,
                            "end": 858
                        },
                        {
                            "start": 858,
                            "end": 1069
                        },
                        {
                            "start": 1071,
                            "end": 1308
                        },
                        {
                            "start": 1308,
                            "end": 1450
                        },
                        {
                            "start": 1450,
                            "end": 1531
                        },
                        {
                            "start": 1531,
                            "end": 1601
                        },
                        {
                            "start": 1601,
                            "end": 1834
                        },
                        {
                            "start": 1834,
                            "end": 1923
                        },
                        {
                            "start": 1923,
                            "end": 2135
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 294,
                            "end": 297,
                            "matchedPaperCorpusId": "258832459"
                        },
                        {
                            "start": 323,
                            "end": 326,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 704,
                            "end": 707,
                            "matchedPaperCorpusId": "247011581"
                        },
                        {
                            "start": 1058,
                            "end": 1062,
                            "matchedPaperCorpusId": "184486746"
                        },
                        {
                            "start": 1205,
                            "end": 1208,
                            "matchedPaperCorpusId": "3353110"
                        },
                        {
                            "start": 1303,
                            "end": 1307,
                            "matchedPaperCorpusId": "259262301"
                        },
                        {
                            "start": 1746,
                            "end": 1750,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 1869,
                            "end": 1873,
                            "matchedPaperCorpusId": "211505992"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7236328125
                },
                {
                    "corpus_id": "270703226",
                    "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
                    "text": "Transformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8,21], and more recently, constructing large language models (LLMs) [9,69].Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1,20,19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.\n\nMany recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,61] allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42,78].(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters.Ainslie et al. [1]  KV pairs are scored by u.SPARSEK computes a threshold for each query (\u03c4 (u)) such that the sum of normalized scores is k, which is 3 in this example.We select top-k KV pairs (orange cells) to perform attention.Right: the SPARSEK attention module.We fuse selection and attention in one kernel for efficiency.incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders.Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.",
                    "score": 0.6363882906227577,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 207,
                            "end": 642
                        },
                        {
                            "start": 644,
                            "end": 777
                        },
                        {
                            "start": 777,
                            "end": 1008
                        },
                        {
                            "start": 1008,
                            "end": 1095
                        },
                        {
                            "start": 1095,
                            "end": 1305
                        },
                        {
                            "start": 1305,
                            "end": 1408
                        },
                        {
                            "start": 1408,
                            "end": 1491
                        },
                        {
                            "start": 1491,
                            "end": 1536
                        },
                        {
                            "start": 1536,
                            "end": 1660
                        },
                        {
                            "start": 1660,
                            "end": 1721
                        },
                        {
                            "start": 1721,
                            "end": 1757
                        },
                        {
                            "start": 1757,
                            "end": 1818
                        },
                        {
                            "start": 1818,
                            "end": 1902
                        },
                        {
                            "start": 1902,
                            "end": 2097
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 19,
                            "end": 23,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 200,
                            "end": 203,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 471,
                            "end": 474,
                            "matchedPaperCorpusId": "257622671"
                        },
                        {
                            "start": 1037,
                            "end": 1040,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 1301,
                            "end": 1304,
                            "matchedPaperCorpusId": "264439578"
                        },
                        {
                            "start": 1506,
                            "end": 1509,
                            "matchedPaperCorpusId": "257622671"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65966796875
                }
            ],
            "relevance_judgement": 0.74609375,
            "relevance_judgment_input_expanded": "# Title: Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers\n# Venue: arXiv.org\n# Authors: Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu\n## Abstract\nAccommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n## Introduction\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8,21], and more recently, constructing large language models (LLMs) [9,69].Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1,20,19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.\n\nMany recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,61] allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42,78].(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters.Ainslie et al. [1]  KV pairs are scored by u.SPARSEK computes a threshold for each query (\u03c4 (u)) such that the sum of normalized scores is k, which is 3 in this example.We select top-k KV pairs (orange cells) to perform attention.Right: the SPARSEK attention module.We fuse selection and attention in one kernel for efficiency.incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders.Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.\n...\nMeanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.\n\nTo tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1.Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top-k mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time.It is worth noting that our method draws inspiration from the concept of top-k attention [32,1].Unfortunately, conventional top-k attention is non-differentiable and therefore cannot be used to train the scoring network.With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows.\n\nIncremental KV Selection.\n\nThe SPARSEK operator ( \u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder.Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results.\n\nComputational and Memory Efficiency.SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32,2,47] to linear time and achieves constant memory cost in inference.This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference.Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs.This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately ( \u00a7 3.2).\n\nExtension with IO-awareness.FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness.However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application.\n\n## Related Work\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context.Numerous efficient approaches have emerged, spanning state-space models [30,62], recurrent neural networks [45,52,49], linear attention [55,38] and low-rank approximations of self-attention [75,14,53], which replace the self-attention with novel linear blocks for long-context modeling.Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29,77].Besides, a few studies combine the Transformer with block-wise recurrence [17,35,36,12] or key-value compression [60,59,18].In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix.This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15,27,42].\n\nSparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56,51], dilated sliding windows [4,22], combination of patterns [34,13], or domain-specific patterns [31].Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81,42,27].However, these static methods often prove suboptimal in various scenarios [66,2].Alternatively, sparse patterns can be learned in a data-driven manner.For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens.Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs.Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference.",
            "reference_string": "[270703226 | Lou et al. | 2024 | Citations: 26]"
        },
        {
            "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 25,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.01160",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.01160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2435537",
                    "name": "Matteo Pagliardini"
                },
                {
                    "authorId": "50552613",
                    "name": "Daniele Paliotta"
                },
                {
                    "authorId": "2456863",
                    "name": "Martin Jaggi"
                },
                {
                    "authorId": "116272138",
                    "name": "Franccois Fleuret"
                }
            ],
            "abstract": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.",
            "corpus_id": 259063695,
            "sentences": [
                {
                    "corpus_id": "259063695",
                    "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
                    "text": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.",
                    "score": 0.7163339166427519,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.732421875
                }
            ],
            "relevance_judgement": 0.732421875,
            "relevance_judgment_input_expanded": "# Title: Faster Causal Attention Over Large Sequences Through Sparse Flash Attention\n# Venue: arXiv.org\n# Authors: Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret\n## Abstract\nTransformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n",
            "reference_string": "[259063695 | Pagliardini et al. | 2023 | Citations: 25]"
        },
        {
            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 32,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155315840",
                    "name": "Guanzheng Chen"
                },
                {
                    "authorId": "40613621",
                    "name": "Xin Li"
                },
                {
                    "authorId": "3451645",
                    "name": "Zaiqiao Meng"
                },
                {
                    "authorId": "3279808",
                    "name": "Shangsong Liang"
                },
                {
                    "authorId": "2211459675",
                    "name": "Li Bing"
                }
            ],
            "abstract": "Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x or almost 8x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
            "corpus_id": 264451707,
            "sentences": [
                {
                    "corpus_id": "264451707",
                    "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
                    "text": "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information. Additionally, modifications to the model architecture make these methods challenging to apply to existing pre-trained LLMs. Conversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity of models to tack the entire long sequences without explicit drops of context information. \n\nLength Extrapolation. Building on the foundation laid by ALiBi (Press et al., 2022), a series of works (Sun et al., 2023;Chi et al., 2022;2023) seek to train the Transformer-based models on a short length, while directly testing on longer counterparts. These methods substitute the position embedding with bias introduced into attention scores, thereby incorporating positional information. Notably, the bias typically gives higher profits to closer tokens. This mechanism intuitively amplifies the local context for each token at the expense of distant information. Consequently, these length-extrapolation methods encounter challenges in effectively handling long contexts in practical applications (Pal et al., 2023). However, our CLEX demonstrates remarkable effectiveness in practical tasks such as summarization, indicating the de facto extrapolation ability for applications. \n\nPosition Embedding (PE) Scaling. Recent research has sought to extend the context length of Transformers through the scaling of the extensively employed RoPE.",
                    "score": 0.5842022294971193,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 21877,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 46,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1268
                        },
                        {
                            "start": 1271,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1837
                        },
                        {
                            "start": 1838,
                            "end": 1991
                        },
                        {
                            "start": 1992,
                            "end": 2153
                        },
                        {
                            "start": 2156,
                            "end": 2188
                        },
                        {
                            "start": 2189,
                            "end": 2314
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 114,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 304,
                            "end": 326,
                            "matchedPaperCorpusId": "250526424"
                        },
                        {
                            "start": 1334,
                            "end": 1354,
                            "matchedPaperCorpusId": "237347130"
                        },
                        {
                            "start": 1374,
                            "end": 1392,
                            "matchedPaperCorpusId": "15641339"
                        },
                        {
                            "start": 1392,
                            "end": 1409,
                            "matchedPaperCorpusId": "248965309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72216796875
                }
            ],
            "relevance_judgement": 0.72216796875,
            "relevance_judgment_input_expanded": "# Title: CLEX: Continuous Length Extrapolation for Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Li Bing\n## Abstract\nTransformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x or almost 8x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.\n## RELATED WORK\nHierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information. Additionally, modifications to the model architecture make these methods challenging to apply to existing pre-trained LLMs. Conversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity of models to tack the entire long sequences without explicit drops of context information. \n\nLength Extrapolation. Building on the foundation laid by ALiBi (Press et al., 2022), a series of works (Sun et al., 2023;Chi et al., 2022;2023) seek to train the Transformer-based models on a short length, while directly testing on longer counterparts. These methods substitute the position embedding with bias introduced into attention scores, thereby incorporating positional information. Notably, the bias typically gives higher profits to closer tokens. This mechanism intuitively amplifies the local context for each token at the expense of distant information. Consequently, these length-extrapolation methods encounter challenges in effectively handling long contexts in practical applications (Pal et al., 2023). However, our CLEX demonstrates remarkable effectiveness in practical tasks such as summarization, indicating the de facto extrapolation ability for applications. \n\nPosition Embedding (PE) Scaling. Recent research has sought to extend the context length of Transformers through the scaling of the extensively employed RoPE.",
            "reference_string": "[264451707 | Chen et al. | 2023 | Citations: 32]"
        },
        {
            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266686678",
                    "name": "Jinghan Yao"
                },
                {
                    "authorId": "2297768912",
                    "name": "Sam Ade Jacobs"
                },
                {
                    "authorId": "2226706029",
                    "name": "Masahiro Tanaka"
                },
                {
                    "authorId": "2537545",
                    "name": "Olatunji Ruwase"
                },
                {
                    "authorId": "1685408",
                    "name": "A. Shafi"
                },
                {
                    "authorId": "1802958",
                    "name": "H. Subramoni"
                },
                {
                    "authorId": "2222521323",
                    "name": "Dhabaleswar K. Panda"
                }
            ],
            "abstract": "Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.",
            "corpus_id": 272310078,
            "sentences": [
                {
                    "corpus_id": "272310078",
                    "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
                    "text": "The substantial memory demands of Transformers have spurred extensive research into memory-efficient attention mechanisms to facilitate their application to longer sequences. FlashAttention (Dao et al., 2022) utilizes an online softmax computation technique (Milakov & Gimelshein, 2018) and reduces memory requirements of self-attention from O(n 2 ) to O(n) (Rabe & Staats, 2021) while preserving the accuracy. Other notable strategies include lowrank approximations (Katharopoulos et al., 2020;Wang et al., 2020), kernel-based methods (Kitaev et al., 2020;Lu et al., 2021;Xiong et al., 2021), and sparse attention mechanisms (Child et al., 2019), which approximate or selectively compute attention to minimize memory consumption. Furthermore, techniques that combine local and global contexts (Ainslie et al., 2020;Beltagy et al., 2020;Liu et al., 2021;Zaheer et al., 2020) enable superior performance on tasks involving long sequences or large-scale inputs while maintaining computational efficiency. Our work is inspired by FlashAttention and builds upon it by introducing hierarchical blockwise computation, leveraging the memory hierarchy in modern systems, which significantly enhances scalability and reduces memory requirements.",
                    "score": 0.9461071049865798,
                    "section_title": "Memory-efficient Transformer",
                    "char_start_offset": 5461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1236
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 190,
                            "end": 208,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 467,
                            "end": 495,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 557,
                            "end": 573,
                            "matchedPaperCorpusId": "239616022"
                        },
                        {
                            "start": 573,
                            "end": 592,
                            "matchedPaperCorpusId": "231847231"
                        },
                        {
                            "start": 837,
                            "end": 854,
                            "matchedPaperCorpusId": "232352874"
                        },
                        {
                            "start": 854,
                            "end": 874,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71826171875
                }
            ],
            "relevance_judgement": 0.71826171875,
            "relevance_judgment_input_expanded": "# Title: Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer\n# Venue: arXiv.org\n# Authors: Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, A. Shafi, H. Subramoni, Dhabaleswar K. Panda\n## Abstract\nLarge Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.\n## Memory-efficient Transformer\nThe substantial memory demands of Transformers have spurred extensive research into memory-efficient attention mechanisms to facilitate their application to longer sequences. FlashAttention (Dao et al., 2022) utilizes an online softmax computation technique (Milakov & Gimelshein, 2018) and reduces memory requirements of self-attention from O(n 2 ) to O(n) (Rabe & Staats, 2021) while preserving the accuracy. Other notable strategies include lowrank approximations (Katharopoulos et al., 2020;Wang et al., 2020), kernel-based methods (Kitaev et al., 2020;Lu et al., 2021;Xiong et al., 2021), and sparse attention mechanisms (Child et al., 2019), which approximate or selectively compute attention to minimize memory consumption. Furthermore, techniques that combine local and global contexts (Ainslie et al., 2020;Beltagy et al., 2020;Liu et al., 2021;Zaheer et al., 2020) enable superior performance on tasks involving long sequences or large-scale inputs while maintaining computational efficiency. Our work is inspired by FlashAttention and builds upon it by introducing hierarchical blockwise computation, leveraging the memory hierarchy in modern systems, which significantly enhances scalability and reduces memory requirements.",
            "reference_string": "[272310078 | Yao et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Denoising Self-Attentive Sequential Recommendation",
            "venue": "ACM Conference on Recommender Systems",
            "year": 2022,
            "reference_count": 60,
            "citation_count": 50,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.04120",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.04120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1504511015",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "2410838",
                    "name": "Yusan Lin"
                },
                {
                    "authorId": "29913565",
                    "name": "Menghai Pan"
                },
                {
                    "authorId": "2127181454",
                    "name": "Lan Wang"
                },
                {
                    "authorId": "3056465",
                    "name": "Chin-Chia Michael Yeh"
                },
                {
                    "authorId": "2185014510",
                    "name": "Xiaoting Li"
                },
                {
                    "authorId": "2185013996",
                    "name": "Yan Zheng"
                },
                {
                    "authorId": "2148956204",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2145058012",
                    "name": "Hao Yang"
                }
            ],
            "abstract": "Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies. This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence. However, real-world item sequences are often noisy, which is particularly true for implicit feedback. For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. As such, the current user action only depends on a subset of items, not on the entire sequences. Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items. This may lead to sub-optimal performance if Transformers are not regularized properly. Here we propose the Rec-denoiser model for better training of self-attentive recommender systems. In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction. To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions. This largely purifies item-item dependencies and provides better model interpretability. In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations. Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences. Our Rec-denoiser is a general plugin that is compatible to many Transformers. Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines.",
            "corpus_id": 252216464,
            "sentences": [
                {
                    "corpus_id": "252216464",
                    "title": "Denoising Self-Attentive Sequential Recommendation",
                    "text": "Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,18,29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts. \n\nAnother line of work is to use learnable attention distributions [12,36,38,40]. Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks. This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments. Our Rec-denoiser is related to this line of work. Instead of using sparsemax, we design a trainable binary mask for the self-attention network. As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way.",
                    "score": 0.6348468524312815,
                    "section_title": "Sparse Transformer",
                    "char_start_offset": 10558,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 924
                        },
                        {
                            "start": 927,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1311
                        },
                        {
                            "start": 1312,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1553
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 169,
                            "matchedPaperCorpusId": "209009596"
                        },
                        {
                            "start": 316,
                            "end": 320,
                            "matchedPaperCorpusId": "209009596"
                        },
                        {
                            "start": 992,
                            "end": 996,
                            "matchedPaperCorpusId": "202538495"
                        },
                        {
                            "start": 996,
                            "end": 999,
                            "matchedPaperCorpusId": "44005113"
                        },
                        {
                            "start": 999,
                            "end": 1002,
                            "matchedPaperCorpusId": "153313159"
                        },
                        {
                            "start": 1002,
                            "end": 1005,
                            "matchedPaperCorpusId": "159041867"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71435546875
                }
            ],
            "relevance_judgement": 0.71435546875,
            "relevance_judgment_input_expanded": "# Title: Denoising Self-Attentive Sequential Recommendation\n# Venue: ACM Conference on Recommender Systems\n# Authors: Huiyuan Chen, Yusan Lin, Menghai Pan, Lan Wang, Chin-Chia Michael Yeh, Xiaoting Li, Yan Zheng, Fei Wang, Hao Yang\n## Abstract\nTransformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies. This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence. However, real-world item sequences are often noisy, which is particularly true for implicit feedback. For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. As such, the current user action only depends on a subset of items, not on the entire sequences. Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items. This may lead to sub-optimal performance if Transformers are not regularized properly. Here we propose the Rec-denoiser model for better training of self-attentive recommender systems. In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction. To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions. This largely purifies item-item dependencies and provides better model interpretability. In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations. Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences. Our Rec-denoiser is a general plugin that is compatible to many Transformers. Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines.\n## Sparse Transformer\nRecently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,18,29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts. \n\nAnother line of work is to use learnable attention distributions [12,36,38,40]. Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks. This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments. Our Rec-denoiser is related to this line of work. Instead of using sparsemax, we design a trainable binary mask for the self-attention network. As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way.",
            "reference_string": "[252216464 | Chen et al. | 2022 | Citations: 50]"
        },
        {
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 111,
            "citation_count": 2285,
            "influential_citation_count": 213,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "24593911",
                    "name": "Tri Dao"
                },
                {
                    "authorId": "49577833",
                    "name": "Daniel Y. Fu"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "1755572",
                    "name": "A. Rudra"
                },
                {
                    "authorId": "2061444681",
                    "name": "Christopher R'e"
                }
            ],
            "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
            "corpus_id": 249151871,
            "sentences": [
                {
                    "corpus_id": "249151871",
                    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                    "text": "Our block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23,38,39,55,76]. For model training, the lottery tickets hypothesis [28,29,30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterfly pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks. \n\nEfficient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [22] and computer vision [24,91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12,54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52,57,79,89]. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details. \n\nThere are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31,36,37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models.",
                    "score": 0.6475957005448896,
                    "section_title": "A Related Work",
                    "char_start_offset": 28686,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 748
                        },
                        {
                            "start": 749,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1210
                        },
                        {
                            "start": 1211,
                            "end": 1385
                        },
                        {
                            "start": 1386,
                            "end": 1503
                        },
                        {
                            "start": 1504,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1701
                        },
                        {
                            "start": 1704,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1987
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "2134321"
                        },
                        {
                            "start": 230,
                            "end": 233,
                            "matchedPaperCorpusId": "38486148"
                        },
                        {
                            "start": 289,
                            "end": 293,
                            "matchedPaperCorpusId": "53388625"
                        },
                        {
                            "start": 296,
                            "end": 299,
                            "matchedPaperCorpusId": "209324341"
                        },
                        {
                            "start": 876,
                            "end": 880,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 880,
                            "end": 883,
                            "matchedPaperCorpusId": "231719476"
                        },
                        {
                            "start": 1132,
                            "end": 1136,
                            "matchedPaperCorpusId": "209315300"
                        },
                        {
                            "start": 1147,
                            "end": 1151,
                            "matchedPaperCorpusId": "222290917"
                        },
                        {
                            "start": 1202,
                            "end": 1206,
                            "matchedPaperCorpusId": "222067132"
                        },
                        {
                            "start": 1317,
                            "end": 1321,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1336,
                            "end": 1339,
                            "matchedPaperCorpusId": "248498407"
                        },
                        {
                            "start": 1364,
                            "end": 1368,
                            "matchedPaperCorpusId": "235743105"
                        },
                        {
                            "start": 1379,
                            "end": 1383,
                            "matchedPaperCorpusId": "235829099"
                        },
                        {
                            "start": 1489,
                            "end": 1493,
                            "matchedPaperCorpusId": "202888986"
                        },
                        {
                            "start": 1496,
                            "end": 1499,
                            "matchedPaperCorpusId": "159041867"
                        },
                        {
                            "start": 1499,
                            "end": 1502,
                            "matchedPaperCorpusId": "59310641"
                        },
                        {
                            "start": 1615,
                            "end": 1619,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 1648,
                            "end": 1652,
                            "matchedPaperCorpusId": "207930593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7119140625
                },
                {
                    "corpus_id": "249151871",
                    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                    "text": "FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification [13]. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X [80] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the first model that can achieve better-than-chance performance on Path-256. \n\n\u2022 Benchmarking Attention. FlashAttention is up to 3\u00d7 faster than the standard attention implementation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of.",
                    "score": 0.8565319215319852,
                    "section_title": "Introduction",
                    "char_start_offset": 5641,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 612
                        },
                        {
                            "start": 615,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 366,
                            "end": 370,
                            "matchedPaperCorpusId": "260440449"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6484375
                },
                {
                    "corpus_id": "249151871",
                    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                    "text": "We analyze the IO complexity [1] of FlashAttention, proving that it requires  ( 2  2  \u22121 ) HBM accesses where  is the head dimension and  is the size of SRAM, as compared to \u03a9(  +  2 ) of standard attention. For typical values of  and , FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9\u00d7 fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes. \n\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4\u00d7 faster than even FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive. 1 e empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. \n\n\u2022 Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2 (seq. length 1K) 3\u00d7 faster than baseline implementations from HuggingFace [87] and Megatron-LM [77], and long-range arena (seq. length 1K-4K) 2.4\u00d7 faster than baselines. \n\n\u2022 Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities.",
                    "score": 0.6335070528842772,
                    "section_title": "Introduction",
                    "char_start_offset": 3743,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 515
                        },
                        {
                            "start": 518,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1500
                        },
                        {
                            "start": 1503,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1621
                        },
                        {
                            "start": 1622,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1870
                        },
                        {
                            "start": 1873,
                            "end": 1897
                        },
                        {
                            "start": 1898,
                            "end": 2012
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 29,
                            "end": 32,
                            "matchedPaperCorpusId": "6264984"
                        },
                        {
                            "start": 1690,
                            "end": 1694,
                            "matchedPaperCorpusId": "203642156"
                        },
                        {
                            "start": 1775,
                            "end": 1779,
                            "matchedPaperCorpusId": "269498086"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6474609375
                },
                {
                    "corpus_id": "249151871",
                    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                    "text": "Transformer models [82] have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger [5] and deeper [83], but equipping them with longer context remains difficult [80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences. \n\nMany approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [51,74] to low-rank approximation [12,50,84], and their combinations [3,9,92]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). \n\nIn this paper, we argue that a missing principle is making attention algorithms IO-aware [1]-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1  In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. \n\nFlashAttention does not read and write the large  \u00d7  attention matrix to HBM, resulting in an 7.6\u00d7 speedup on the attention computation. \n\nGPUs, compute speed has out-paced memory speed [61,62,63], and most operations in Transformers are bottlenecked by memory accesses [43].",
                    "score": 0.591260805876638,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 549
                        },
                        {
                            "start": 552,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1146
                        },
                        {
                            "start": 1149,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1804
                        },
                        {
                            "start": 1807,
                            "end": 1943
                        },
                        {
                            "start": 1946,
                            "end": 2082
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 186,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 261,
                            "end": 265,
                            "matchedPaperCorpusId": "260440449"
                        },
                        {
                            "start": 704,
                            "end": 708,
                            "matchedPaperCorpusId": "209315300"
                        },
                        {
                            "start": 708,
                            "end": 711,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 738,
                            "end": 742,
                            "matchedPaperCorpusId": "222067132"
                        },
                        {
                            "start": 742,
                            "end": 745,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 776,
                            "end": 778,
                            "matchedPaperCorpusId": "248498407"
                        },
                        {
                            "start": 778,
                            "end": 781,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1238,
                            "end": 1241,
                            "matchedPaperCorpusId": "6264984"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59423828125
                }
            ],
            "relevance_judgement": 0.7119140625,
            "relevance_judgment_input_expanded": "# Title: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n# Venue: Neural Information Processing Systems\n# Authors: Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e\n## Abstract\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n## Introduction\nTransformer models [82] have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger [5] and deeper [83], but equipping them with longer context remains difficult [80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences. \n\nMany approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [51,74] to low-rank approximation [12,50,84], and their combinations [3,9,92]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). \n\nIn this paper, we argue that a missing principle is making attention algorithms IO-aware [1]-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1  In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. \n\nFlashAttention does not read and write the large  \u00d7  attention matrix to HBM, resulting in an 7.6\u00d7 speedup on the attention computation. \n\nGPUs, compute speed has out-paced memory speed [61,62,63], and most operations in Transformers are bottlenecked by memory accesses [43].\n...\nWe analyze the IO complexity [1] of FlashAttention, proving that it requires  ( 2  2  \u22121 ) HBM accesses where  is the head dimension and  is the size of SRAM, as compared to \u03a9(  +  2 ) of standard attention. For typical values of  and , FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9\u00d7 fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes. \n\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4\u00d7 faster than even FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive. 1 e empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. \n\n\u2022 Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2 (seq. length 1K) 3\u00d7 faster than baseline implementations from HuggingFace [87] and Megatron-LM [77], and long-range arena (seq. length 1K-4K) 2.4\u00d7 faster than baselines. \n\n\u2022 Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities.\n...\nFlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification [13]. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X [80] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the first model that can achieve better-than-chance performance on Path-256. \n\n\u2022 Benchmarking Attention. FlashAttention is up to 3\u00d7 faster than the standard attention implementation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of.\n\n## A Related Work\nOur block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23,38,39,55,76]. For model training, the lottery tickets hypothesis [28,29,30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterfly pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks. \n\nEfficient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [22] and computer vision [24,91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12,54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52,57,79,89]. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details. \n\nThere are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31,36,37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models.",
            "reference_string": "[249151871 | Dao et al. | 2022 | Citations: 2285]"
        },
        {
            "title": "HLogformer: A Hierarchical Transformer for Representing Log Data",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319897053",
                    "name": "Zhichao Hou"
                },
                {
                    "authorId": "2329558200",
                    "name": "Mina Ghashami"
                },
                {
                    "authorId": "2318981072",
                    "name": "Mikhail Kuznetsov"
                },
                {
                    "authorId": "2937550",
                    "name": "MohamadAli Torkamani"
                }
            ],
            "abstract": "Transformers have gained widespread acclaim for their versatility in handling diverse data structures, yet their application to log data remains underexplored. Log data, characterized by its hierarchical, dictionary-like structure, poses unique challenges when processed using conventional transformer models. Traditional methods often rely on manually crafted templates for parsing logs, a process that is labor-intensive and lacks generalizability. Additionally, the linear treatment of log sequences by standard transformers neglects the rich, nested relationships within log entries, leading to suboptimal representations and excessive memory usage. To address these issues, we introduce HLogformer, a novel hierarchical transformer framework specifically designed for log data. HLogformer leverages the hierarchical structure of log entries to significantly reduce memory costs and enhance representation learning. Unlike traditional models that treat log data as flat sequences, our framework processes log entries in a manner that respects their inherent hierarchical organization. This approach ensures comprehensive encoding of both fine-grained details and broader contextual relationships. Our contributions are threefold: First, HLogformer is the first framework to design a dynamic hierarchical transformer tailored for dictionary-like log data. Second, it dramatically reduces memory costs associated with processing extensive log sequences. Third, comprehensive experiments demonstrate that HLogformer more effectively encodes hierarchical contextual information, proving to be highly effective for downstream tasks such as synthetic anomaly detection and product recommendation.",
            "corpus_id": 272310408,
            "sentences": [
                {
                    "corpus_id": "272310408",
                    "title": "HLogformer: A Hierarchical Transformer for Representing Log Data",
                    "text": "Global Memory Tokens in Transformers. Models like Longformer (Beltagy, Peters, and Cohan 2020), ETC (Extended Transformer Construction) (Ainslie et al. 2020), and Big Bird (Zaheer et al. 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms. Sparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al. 2021) and (Kitaev, Kaiser, and Levskaya 2020), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure. Segment-based Recurrence. Segment-based recurrence methods, such as Transformer-XL (Dai et al. 2019) and Compressive Transformer (Rae et al. 2019), introduce mechanisms to maintain and leverage contextual information across segments, significantly reducing memory and computational costs. \n\nDespite their effectiveness, these approaches are not specifically tailored to the unique characteristics of log data, which often exhibit a hierarchical, dictionary-like structure. This gap underscores the need for models designed to capture and leverage the intrinsic structure of log data.",
                    "score": 0.5484572016564052,
                    "section_title": "Efficient Transformers",
                    "char_start_offset": 6293,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 37
                        },
                        {
                            "start": 38,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1310
                        },
                        {
                            "start": 1313,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1605
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 61,
                            "end": 94,
                            "matchedPaperCorpusId": "215737171"
                        },
                        {
                            "start": 136,
                            "end": 157,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 172,
                            "end": 192,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 800,
                            "end": 816,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 822,
                            "end": 857,
                            "matchedPaperCorpusId": "232352874"
                        },
                        {
                            "start": 1151,
                            "end": 1168,
                            "matchedPaperCorpusId": "5987139"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: HLogformer: A Hierarchical Transformer for Representing Log Data\n# Venue: arXiv.org\n# Authors: Zhichao Hou, Mina Ghashami, Mikhail Kuznetsov, MohamadAli Torkamani\n## Abstract\nTransformers have gained widespread acclaim for their versatility in handling diverse data structures, yet their application to log data remains underexplored. Log data, characterized by its hierarchical, dictionary-like structure, poses unique challenges when processed using conventional transformer models. Traditional methods often rely on manually crafted templates for parsing logs, a process that is labor-intensive and lacks generalizability. Additionally, the linear treatment of log sequences by standard transformers neglects the rich, nested relationships within log entries, leading to suboptimal representations and excessive memory usage. To address these issues, we introduce HLogformer, a novel hierarchical transformer framework specifically designed for log data. HLogformer leverages the hierarchical structure of log entries to significantly reduce memory costs and enhance representation learning. Unlike traditional models that treat log data as flat sequences, our framework processes log entries in a manner that respects their inherent hierarchical organization. This approach ensures comprehensive encoding of both fine-grained details and broader contextual relationships. Our contributions are threefold: First, HLogformer is the first framework to design a dynamic hierarchical transformer tailored for dictionary-like log data. Second, it dramatically reduces memory costs associated with processing extensive log sequences. Third, comprehensive experiments demonstrate that HLogformer more effectively encodes hierarchical contextual information, proving to be highly effective for downstream tasks such as synthetic anomaly detection and product recommendation.\n## Efficient Transformers\nGlobal Memory Tokens in Transformers. Models like Longformer (Beltagy, Peters, and Cohan 2020), ETC (Extended Transformer Construction) (Ainslie et al. 2020), and Big Bird (Zaheer et al. 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms. Sparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al. 2021) and (Kitaev, Kaiser, and Levskaya 2020), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure. Segment-based Recurrence. Segment-based recurrence methods, such as Transformer-XL (Dai et al. 2019) and Compressive Transformer (Rae et al. 2019), introduce mechanisms to maintain and leverage contextual information across segments, significantly reducing memory and computational costs. \n\nDespite their effectiveness, these approaches are not specifically tailored to the unique characteristics of log data, which often exhibit a hierarchical, dictionary-like structure. This gap underscores the need for models designed to capture and leverage the intrinsic structure of log data.",
            "reference_string": "[272310408 | Hou et al. | 2024 | Citations: 0]"
        },
        {
            "title": "AdaSplash: Adaptive Sparse Flash Attention",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350815234",
                    "name": "Nuno Gon\u00e7alves"
                },
                {
                    "authorId": "2309007068",
                    "name": "Marcos V. Treviso"
                },
                {
                    "authorId": "2346669007",
                    "name": "Andr'e F. T. Martins"
                }
            ],
            "abstract": "The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.",
            "corpus_id": 276421279,
            "sentences": [
                {
                    "corpus_id": "276421279",
                    "title": "AdaSplash: Adaptive Sparse Flash Attention",
                    "text": "The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.",
                    "score": 0.5474943345319095,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: AdaSplash: Adaptive Sparse Flash Attention\n# Venue: arXiv.org\n# Authors: Nuno Gon\u00e7alves, Marcos V. Treviso, Andr'e F. T. Martins\n## Abstract\nThe computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.\n",
            "reference_string": "[276421279 | Goncalves et al. | 2025 | Citations: 0]"
        },
        {
            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345187428",
                    "name": "Akul Datta"
                }
            ],
            "abstract": "This paper reviews the development of the Receptance Weighted Key Value (RWKV) architecture, emphasizing its advancements in efficient language modeling. RWKV combines the training efficiency of Transformers with the inference efficiency of RNNs through a novel linear attention mechanism. We examine its core innovations, adaptations across various domains, and performance advantages over traditional models. The paper also discusses challenges and future directions for RWKV as a versatile architecture in deep learning.",
            "corpus_id": 273821735,
            "sentences": [
                {
                    "corpus_id": "273821735",
                    "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
                    "text": "The success of Transformers, coupled with their limitations, led to research into more efficient attention mechanisms. These mechanisms aim to reduce the quadratic complexity of standard self-attention while retaining its ability to model long-range dependencies. Some notable examples include: \n\n\u2022 Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers [Katharopoulos et al., 2020] reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird [Zaheer et al., 2020] combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices. \n\nThese approaches aim to maintain the strong performance of Transformers while reducing their computational and memory requirements, especially for long sequences.",
                    "score": 0.688227297368159,
                    "section_title": "Efficient Attention Mechanisms",
                    "char_start_offset": 4184,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 294
                        },
                        {
                            "start": 297,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1897
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 727,
                            "end": 755,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 1120,
                            "end": 1141,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7080078125
                }
            ],
            "relevance_judgement": 0.7080078125,
            "relevance_judgment_input_expanded": "# Title: The Evolution of RWKV: Advancements in Efficient Language Modeling\n# Venue: arXiv.org\n# Authors: Akul Datta\n## Abstract\nThis paper reviews the development of the Receptance Weighted Key Value (RWKV) architecture, emphasizing its advancements in efficient language modeling. RWKV combines the training efficiency of Transformers with the inference efficiency of RNNs through a novel linear attention mechanism. We examine its core innovations, adaptations across various domains, and performance advantages over traditional models. The paper also discusses challenges and future directions for RWKV as a versatile architecture in deep learning.\n## Efficient Attention Mechanisms\nThe success of Transformers, coupled with their limitations, led to research into more efficient attention mechanisms. These mechanisms aim to reduce the quadratic complexity of standard self-attention while retaining its ability to model long-range dependencies. Some notable examples include: \n\n\u2022 Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers [Katharopoulos et al., 2020] reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird [Zaheer et al., 2020] combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices. \n\nThese approaches aim to maintain the strong performance of Transformers while reducing their computational and memory requirements, especially for long sequences.",
            "reference_string": "[273821735 | Datta | 2024 | Citations: 1]"
        },
        {
            "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315166927",
                    "name": "Guoxia Wang"
                },
                {
                    "authorId": "2072984835",
                    "name": "Jinle Zeng"
                },
                {
                    "authorId": "2325510350",
                    "name": "Xiyuan Xiao"
                },
                {
                    "authorId": "2323811334",
                    "name": "Siming Wu"
                },
                {
                    "authorId": "2323896653",
                    "name": "Jiabin Yang"
                },
                {
                    "authorId": "2324060184",
                    "name": "Lujing Zheng"
                },
                {
                    "authorId": "2323781886",
                    "name": "Zeyu Chen"
                },
                {
                    "authorId": "2324064223",
                    "name": "Jiang Bian"
                },
                {
                    "authorId": "2315250077",
                    "name": "Dianhai Yu"
                },
                {
                    "authorId": "2323851061",
                    "name": "Haifeng Wang"
                }
            ],
            "abstract": "The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $O(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity $O(N)$, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.",
            "corpus_id": 273026224,
            "sentences": [
                {
                    "corpus_id": "273026224",
                    "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
                    "text": "The attention mechanism, as formulated in Equation 2, presents significant computational and memory challenges, particularly in the computation of   . As the sequence length  increases, the resultant attention scores matrix grows quadratically, leading to a complexity of O ( 2 ). To address this scalability issue, researchers have proposed various optimization techniques, focusing on both memory efficiency and computational speed. \n\nMemory Efficient Attention (MEA) Rabe & Staats (2021) marks a notable advancement in model training optimizations. By leveraging Online Softmax Milakov & Gimelshein (2018) and chunking techniques, MEA reduces memory requirements from O ( 2 ) to O ( \u221a ), enabling the use of larger models or extended sequence lengths within existing hardware constraints. Building upon this foundation, FlashAttention Dao et al. (2022); Dao (2023) focuses on reducing attention latency through IO-aware memory read/write optimizations. Utilizing tiling techniques during computation, FlashAttention achieves a memory overhead of O (), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process. \n\nFor scenarios requiring specific attention masks, several tailored solutions have emerged. Sparse Causal Flash Attention (SCFA) Pagliardini et al. (2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed. However, there remains room for optimization, particularly for complex masking patterns. \n\nOur proposed method, FLASHMASK, builds upon these advancements to support customized complex attention masks. FLASHMASK reduces memory complexity from O ( 2 ) to O () while leveraging sparsity in the attention mask to skip masked blocks.",
                    "score": 0.5982207599829131,
                    "section_title": "ATTENTION OPTIMIZATION TECHNIQUES",
                    "char_start_offset": 4696,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 434
                        },
                        {
                            "start": 437,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1405
                        },
                        {
                            "start": 1408,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1857
                        },
                        {
                            "start": 1858,
                            "end": 2062
                        },
                        {
                            "start": 2063,
                            "end": 2151
                        },
                        {
                            "start": 2154,
                            "end": 2263
                        },
                        {
                            "start": 2264,
                            "end": 2391
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 838,
                            "end": 855,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 1536,
                            "end": 1561,
                            "matchedPaperCorpusId": "259063695"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.701171875
                }
            ],
            "relevance_judgement": 0.701171875,
            "relevance_judgment_input_expanded": "# Title: FlashMask: Efficient and Rich Mask Extension of FlashAttention\n# Venue: International Conference on Learning Representations\n# Authors: Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, Haifeng Wang\n## Abstract\nThe computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $O(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose FlashMask, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves linear memory complexity $O(N)$, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate FlashMask's performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that FlashMask surpasses the latest counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s, achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle and integrated into PaddleNLP, supporting models with over 100 billion parameters for contexts up to 128K tokens.\n## ATTENTION OPTIMIZATION TECHNIQUES\nThe attention mechanism, as formulated in Equation 2, presents significant computational and memory challenges, particularly in the computation of   . As the sequence length  increases, the resultant attention scores matrix grows quadratically, leading to a complexity of O ( 2 ). To address this scalability issue, researchers have proposed various optimization techniques, focusing on both memory efficiency and computational speed. \n\nMemory Efficient Attention (MEA) Rabe & Staats (2021) marks a notable advancement in model training optimizations. By leveraging Online Softmax Milakov & Gimelshein (2018) and chunking techniques, MEA reduces memory requirements from O ( 2 ) to O ( \u221a ), enabling the use of larger models or extended sequence lengths within existing hardware constraints. Building upon this foundation, FlashAttention Dao et al. (2022); Dao (2023) focuses on reducing attention latency through IO-aware memory read/write optimizations. Utilizing tiling techniques during computation, FlashAttention achieves a memory overhead of O (), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process. \n\nFor scenarios requiring specific attention masks, several tailored solutions have emerged. Sparse Causal Flash Attention (SCFA) Pagliardini et al. (2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed. However, there remains room for optimization, particularly for complex masking patterns. \n\nOur proposed method, FLASHMASK, builds upon these advancements to support customized complex attention masks. FLASHMASK reduces memory complexity from O ( 2 ) to O () while leveraging sparsity in the attention mask to skip masked blocks.",
            "reference_string": "[273026224 | Wang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.16754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356854731",
                    "name": "Kwangseob Ahn"
                }
            ],
            "abstract": "Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5>= 0.80 and R@50>= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.",
            "corpus_id": 278000642,
            "sentences": [
                {
                    "corpus_id": "278000642",
                    "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations",
                    "text": "Sparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression. However, empirical studies reveal utility drops once the prompt surpasses ~128 K tokens, reaffirming that size alone is insufficient for faithful recall.",
                    "score": 0.5963812789303806,
                    "section_title": "Long-Context Transformers",
                    "char_start_offset": 2684,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 854
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.693359375
                }
            ],
            "relevance_judgement": 0.693359375,
            "relevance_judgment_input_expanded": "# Title: HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations\n# Venue: arXiv.org\n# Authors: Kwangseob Ahn\n## Abstract\nLarge language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5>= 0.80 and R@50>= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.\n## Long-Context Transformers\nSparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression. However, empirical studies reveal utility drops once the prompt surpasses ~128 K tokens, reaffirming that size alone is insufficient for faithful recall.",
            "reference_string": "[278000642 | Ahn | 2025 | Citations: 0]"
        },
        {
            "title": "ORKG-Leaderboards: a systematic workflow for mining leaderboards as a knowledge graph",
            "venue": "International Journal on Digital Libraries",
            "year": 2023,
            "reference_count": 56,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.11068",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1591123106",
                    "name": "Salomon Kabongo KABENAMUALU"
                },
                {
                    "authorId": "1789682566",
                    "name": "J. D\u2019Souza"
                },
                {
                    "authorId": "145044578",
                    "name": "S. Auer"
                }
            ],
            "abstract": "The purpose of this work is to describe the orkg -Leaderboard software designed to extract leaderboards defined as task\u2013dataset\u2013metric tuples automatically from large collections of empirical research papers in artificial intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the open research knowledge graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus, the systemsss output, when integrated within the ORKG\u2019s supported Semantic Web infrastructure of representing machine-actionable \u2018resources\u2019 on the Web, enables: (1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and (2) specifically, enables researchers to track the progress in AI with an overview of the state-of-the-art across the most common AI tasks and their corresponding datasets via dynamic ORKG frontend views leveraging tables and visualization charts over the machine-actionable data. Our best model achieves performances above 90% F1 on the leaderboard extraction task, thus proving orkg -Leaderboards a practically viable tool for real-world usage. Going forward, in a sense, orkg -Leaderboards transforms the leaderboard extraction task to an automated digitalization task, which has been, for a long time in the community, a crowdsourced endeavor.",
            "corpus_id": 258762176,
            "sentences": [
                {
                    "corpus_id": "258762176",
                    "title": "ORKG-Leaderboards: a systematic workflow for mining leaderboards as a knowledge graph",
                    "text": "BigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle [16]. BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) [16].",
                    "score": 0.6543811496150708,
                    "section_title": "BigBird",
                    "char_start_offset": 29760,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 792
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 269,
                            "end": 273,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 787,
                            "end": 791,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6875
                }
            ],
            "relevance_judgement": 0.6875,
            "relevance_judgment_input_expanded": "# Title: ORKG-Leaderboards: a systematic workflow for mining leaderboards as a knowledge graph\n# Venue: International Journal on Digital Libraries\n# Authors: Salomon Kabongo KABENAMUALU, J. D\u2019Souza, S. Auer\n## Abstract\nThe purpose of this work is to describe the orkg -Leaderboard software designed to extract leaderboards defined as task\u2013dataset\u2013metric tuples automatically from large collections of empirical research papers in artificial intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the open research knowledge graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus, the systemsss output, when integrated within the ORKG\u2019s supported Semantic Web infrastructure of representing machine-actionable \u2018resources\u2019 on the Web, enables: (1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and (2) specifically, enables researchers to track the progress in AI with an overview of the state-of-the-art across the most common AI tasks and their corresponding datasets via dynamic ORKG frontend views leveraging tables and visualization charts over the machine-actionable data. Our best model achieves performances above 90% F1 on the leaderboard extraction task, thus proving orkg -Leaderboards a practically viable tool for real-world usage. Going forward, in a sense, orkg -Leaderboards transforms the leaderboard extraction task to an automated digitalization task, which has been, for a long time in the community, a crowdsourced endeavor.\n## BigBird\nBigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle [16]. BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) [16].",
            "reference_string": "[258762176 | KABENAMUALU et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 181,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.14962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1930733",
                    "name": "D. Hagos"
                },
                {
                    "authorId": "2312327093",
                    "name": "Rick Battle"
                },
                {
                    "authorId": "2260694752",
                    "name": "Danda B. Rawat"
                }
            ],
            "abstract": "The emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.",
            "corpus_id": 271329267,
            "sentences": [
                {
                    "corpus_id": "271329267",
                    "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
                    "text": "It combines local windowed attention with task-motivated global attention. Local attention is primarily used to build contextual representations, while global attention allows Longformer to create full sequence representations for prediction [116]. In standard transformers, the self-attention mechanism considers interactions between all pairs of positions in the input sequence, leading to quadratic complexity. \n\nSparse Transformers. The standard transformer's attention mechanism calculates attention scores for all pairs of positions in a sequence, leading to quadratic time complexity [18]. Sparse Transformers address this issue by considering only a subset of positions during attention computation [117]. This introduces sparsity, significantly reducing memory requirements and computational load, making them suitable for longer sequences [117]. As shown in Equation 27, the sparse Transformer is a modified version of the standard attention mechanism used in transformers [117]. Sparse Transformer's attention, given a sequence of input embeddings X with dimensions N \u00d7 d, where N is the sequence length and d the embedding dimension, the attention scores for position i attending to position j can be computed as shown in Equation 27, where Sp represents the sparse attention, Q i represents the query vector for position i, K j denotes the key vector for position j, Q i K T j represents the dot product of query and the key vectors, capturing the pairwise interactions between positions in the input sequence, V j represents the value vector for position j, and M ij denotes a binary mask element indicating whether vector position i attends to vector position j. This demonstrates that the attention mechanism is computed for each pair of positions i and j based on their corresponding query, key, and value vectors. In global sparse attention, the mask M ij is generated by randomly selecting a fixed number of positions for each position i to attend to. This introduces sparsity by limiting the attention to a small subset of positions in the sequence [117]. However, for local sparse attention, the mask M ij ensures that each position attends to a nearby local neighborhood. This reduces the computational complexity associated with attending to all positions and helps capture short-range dependencies efficiently [117]. The division by \u221a d k serves as a scaling factor for numerical stability, where d k represents the dimensionality of the key vectors.",
                    "score": 0.6427710298835392,
                    "section_title": "F. Long Sequence Language Models",
                    "char_start_offset": 64149,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 413
                        },
                        {
                            "start": 416,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 1970
                        },
                        {
                            "start": 1971,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2193
                        },
                        {
                            "start": 2194,
                            "end": 2340
                        },
                        {
                            "start": 2341,
                            "end": 2474
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 591,
                            "end": 595,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68310546875
                }
            ],
            "relevance_judgement": 0.68310546875,
            "relevance_judgment_input_expanded": "# Title: Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: D. Hagos, Rick Battle, Danda B. Rawat\n## Abstract\nThe emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.\n## F. Long Sequence Language Models\nIt combines local windowed attention with task-motivated global attention. Local attention is primarily used to build contextual representations, while global attention allows Longformer to create full sequence representations for prediction [116]. In standard transformers, the self-attention mechanism considers interactions between all pairs of positions in the input sequence, leading to quadratic complexity. \n\nSparse Transformers. The standard transformer's attention mechanism calculates attention scores for all pairs of positions in a sequence, leading to quadratic time complexity [18]. Sparse Transformers address this issue by considering only a subset of positions during attention computation [117]. This introduces sparsity, significantly reducing memory requirements and computational load, making them suitable for longer sequences [117]. As shown in Equation 27, the sparse Transformer is a modified version of the standard attention mechanism used in transformers [117]. Sparse Transformer's attention, given a sequence of input embeddings X with dimensions N \u00d7 d, where N is the sequence length and d the embedding dimension, the attention scores for position i attending to position j can be computed as shown in Equation 27, where Sp represents the sparse attention, Q i represents the query vector for position i, K j denotes the key vector for position j, Q i K T j represents the dot product of query and the key vectors, capturing the pairwise interactions between positions in the input sequence, V j represents the value vector for position j, and M ij denotes a binary mask element indicating whether vector position i attends to vector position j. This demonstrates that the attention mechanism is computed for each pair of positions i and j based on their corresponding query, key, and value vectors. In global sparse attention, the mask M ij is generated by randomly selecting a fixed number of positions for each position i to attend to. This introduces sparsity by limiting the attention to a small subset of positions in the sequence [117]. However, for local sparse attention, the mask M ij ensures that each position attends to a nearby local neighborhood. This reduces the computational complexity associated with attending to all positions and helps capture short-range dependencies efficiently [117]. The division by \u221a d k serves as a scaling factor for numerical stability, where d k represents the dimensionality of the key vectors.",
            "reference_string": "[271329267 | Hagos et al. | 2024 | Citations: 26]"
        },
        {
            "title": "Scorch: A Library for Sparse Deep Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303621454",
                    "name": "Bobby Yan"
                },
                {
                    "authorId": "2303402046",
                    "name": "Alexander J Root"
                },
                {
                    "authorId": "2303401237",
                    "name": "Trevor Gale"
                },
                {
                    "authorId": "2303401552",
                    "name": "David Broman"
                },
                {
                    "authorId": "2303400919",
                    "name": "Fredrik Kjolstad"
                }
            ],
            "abstract": "The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms. Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations. To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs. Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures. Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference. Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability. More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations. This flexibility is crucial for exploring novel sparse architectures. We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains. With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks. Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem. We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries.",
            "corpus_id": 270063477,
            "sentences": [
                {
                    "corpus_id": "270063477",
                    "title": "Scorch: A Library for Sparse Deep Learning",
                    "text": "We evaluate the performance of sparse autoencoders on four datasets: MNIST [35], CIFAR-10 [34], CIFAR-100 [34], and CelebA [39].The datasets are preprocessed as follows:\n\n\u2022 MNIST: The images are converted to tensors and flattened into a 1D vector of size 784 (28\u00d728).\n\nBigBird is designed to handle long sequences while maintaining a manageable computational complexity.It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer.The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer.\n\nSparse transformers have been successfully applied to various natural language processing tasks, such as text classification, question answering, and language modeling, where the input sequences can be very long.It has also shown promising results in other domains, such as genomics and time series analysis, where the ability to handle long sequences is crucial.\n\nWe evaluate the inference performance of the BigBird model [61] on three text classification datasets: AG News [64], IMDB [42], and Yahoo Answers [64].The model architecture and hyperparameters are listed in Table 3.We use the AdamW optimizer [40] with a learning rate of 0.001 to train the models for 5 epochs with a batch size of 64.The sparse attention is configured to use a block size of 16, 2 global tokens, 2 random blocks, and 2 sliding blocks.\n\nFor each dataset, we train the models on the training set and evaluate the inference time on the test set using an Apple M1 Ultra CPU.The experiments are implemented in PyTorch 2.2.1 and Scorch 0.1.0.",
                    "score": 0.6129229161035157,
                    "section_title": "E.3 Sparse Autoencoders",
                    "char_start_offset": 46879,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 128,
                            "end": 169
                        },
                        {
                            "start": 171,
                            "end": 267
                        },
                        {
                            "start": 269,
                            "end": 370
                        },
                        {
                            "start": 370,
                            "end": 535
                        },
                        {
                            "start": 535,
                            "end": 604
                        },
                        {
                            "start": 606,
                            "end": 690
                        },
                        {
                            "start": 690,
                            "end": 805
                        },
                        {
                            "start": 805,
                            "end": 895
                        },
                        {
                            "start": 897,
                            "end": 1092
                        },
                        {
                            "start": 1094,
                            "end": 1306
                        },
                        {
                            "start": 1306,
                            "end": 1457
                        },
                        {
                            "start": 1459,
                            "end": 1610
                        },
                        {
                            "start": 1610,
                            "end": 1675
                        },
                        {
                            "start": 1675,
                            "end": 1794
                        },
                        {
                            "start": 1794,
                            "end": 1911
                        },
                        {
                            "start": 1913,
                            "end": 2047
                        },
                        {
                            "start": 2047,
                            "end": 2113
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 75,
                            "end": 79,
                            "matchedPaperCorpusId": "14542261"
                        },
                        {
                            "start": 123,
                            "end": 127,
                            "matchedPaperCorpusId": "459456"
                        },
                        {
                            "start": 1518,
                            "end": 1522,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1570,
                            "end": 1574,
                            "matchedPaperCorpusId": "368182"
                        },
                        {
                            "start": 1581,
                            "end": 1585,
                            "matchedPaperCorpusId": "1428702"
                        },
                        {
                            "start": 1605,
                            "end": 1609,
                            "matchedPaperCorpusId": "368182"
                        },
                        {
                            "start": 1702,
                            "end": 1706,
                            "matchedPaperCorpusId": "53592270"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.681640625
                }
            ],
            "relevance_judgement": 0.681640625,
            "relevance_judgment_input_expanded": "# Title: Scorch: A Library for Sparse Deep Learning\n# Venue: arXiv.org\n# Authors: Bobby Yan, Alexander J Root, Trevor Gale, David Broman, Fredrik Kjolstad\n## Abstract\nThe rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms. Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations. To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs. Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures. Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference. Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability. More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations. This flexibility is crucial for exploring novel sparse architectures. We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains. With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks. Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem. We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries.\n## E.3 Sparse Autoencoders\nWe evaluate the performance of sparse autoencoders on four datasets: MNIST [35], CIFAR-10 [34], CIFAR-100 [34], and CelebA [39].The datasets are preprocessed as follows:\n\n\u2022 MNIST: The images are converted to tensors and flattened into a 1D vector of size 784 (28\u00d728).\n\nBigBird is designed to handle long sequences while maintaining a manageable computational complexity.It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer.The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer.\n\nSparse transformers have been successfully applied to various natural language processing tasks, such as text classification, question answering, and language modeling, where the input sequences can be very long.It has also shown promising results in other domains, such as genomics and time series analysis, where the ability to handle long sequences is crucial.\n\nWe evaluate the inference performance of the BigBird model [61] on three text classification datasets: AG News [64], IMDB [42], and Yahoo Answers [64].The model architecture and hyperparameters are listed in Table 3.We use the AdamW optimizer [40] with a learning rate of 0.001 to train the models for 5 epochs with a batch size of 64.The sparse attention is configured to use a block size of 16, 2 global tokens, 2 random blocks, and 2 sliding blocks.\n\nFor each dataset, we train the models on the training set and evaluate the inference time on the test set using an Apple M1 Ultra CPU.The experiments are implemented in PyTorch 2.2.1 and Scorch 0.1.0.",
            "reference_string": "[270063477 | Yan et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348389336",
                    "name": "Yujiao Yang"
                },
                {
                    "authorId": "2282559167",
                    "name": "Jing Lian"
                },
                {
                    "authorId": "2244250357",
                    "name": "Linhui Li"
                }
            ],
            "abstract": "We propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement selective routing on input data and experts. Our approach advances MoE design with four key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch-wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and efficient transformers (including the model architecture of recently proposed DeepSeek-V3) in several tasks across image and natural language domains. In language modeling tasks, we achieve an average reduction of 2.38 in perplexity compared to the best-performed MoE method with an average of 76% FLOPs. In Long Range Arena benchmark, we recorded an average score that is at least 0.68% higher than all comparison models including Full Attention, MoEs, and transformer variants, with only 50% FLOPs of the best MoE method. In image classification, our model yielded an average accuracy improvement of 1.75% than the best model while maintaining comparable FLOPs. The source codes are available at https://github.com/YujiaoYang-work/UoE.",
            "corpus_id": 276775748,
            "sentences": [
                {
                    "corpus_id": "276775748",
                    "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
                    "text": "Transformer [5] architecture represents a significant advancement in artificial intelligence, bringing forth unparalleled capabilities alongside the challenge of resourceintensive training and serving processes. Significant enthusiasm has been ignited for actively enhancing the efficiency of transformer architectures. A preponderance of the works concentrate on reducing memory usage and improving computational efficiency. Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence. Linformer [7] and Lightning-Attention [8,9] leverages linear-complexity self-attention mechanisms, catering to large-scale data and lengthy sequences. Reformer [10] mitigates memory constraints by employing reversible layers and locality-sensitive hashing techniques. Flash Attention [11] reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion. DeepSeek-V2 [13] introduces the Multi-Head Latent Attention (MLA) mechanism, which employs low-rank joint compression to enhance training efficiency and reduce the KV cache size during inference. Tensor Product Attention (TPA) [14] dynamically constructs QKV as context-dependent decomposed tensors, enabling adaptive adjustments and facilitating seamless integration with effective Rotary Position Embedding. \n\nThe other works aims to enhance the modeling capabilities of long sequences. Transformer-XL [15] introducing segmentlevel recurrence to enhance sequence modeling beyond the scope of the original Transformer. Sinkhorn Transformer [16] fuses Sinkhorn algorithm with self-attention mechanisms to improve sequence modeling accuracy. Long-Short-Term Memory Transformer [17] combines Transformer with Long Short-Term Memory (LSTM) mechanisms and thus enhance the modeling capability for long sequences. SeerAttention [18] integrates a learnable gating mechanism into standard attention mechanism, enabling adaptive selection of salient blocks within the attention map. Although These innovations have notably improved the performance of the transformer frameworks in their respective domains, they are typically model-specific and may not be universally applicable. By comparison, our work provides a model-free method. By simply adding selection mechanism to existing model parallel transformers, we can train a model more efficiently while maintaining or even improving its performance.",
                    "score": 0.6258420034529558,
                    "section_title": "A. Variants of Transformer",
                    "char_start_offset": 6646,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1405
                        },
                        {
                            "start": 1408,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1904
                        },
                        {
                            "start": 1905,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2267
                        },
                        {
                            "start": 2268,
                            "end": 2321
                        },
                        {
                            "start": 2322,
                            "end": 2490
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 12,
                            "end": 15,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 832,
                            "end": 836,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 1637,
                            "end": 1641,
                            "matchedPaperCorpusId": "211505992"
                        },
                        {
                            "start": 1772,
                            "end": 1776,
                            "matchedPaperCorpusId": "250628527"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6806640625
                }
            ],
            "relevance_judgement": 0.6806640625,
            "relevance_judgment_input_expanded": "# Title: Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer\n# Venue: arXiv.org\n# Authors: Yujiao Yang, Jing Lian, Linhui Li\n## Abstract\nWe propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement selective routing on input data and experts. Our approach advances MoE design with four key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch-wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and efficient transformers (including the model architecture of recently proposed DeepSeek-V3) in several tasks across image and natural language domains. In language modeling tasks, we achieve an average reduction of 2.38 in perplexity compared to the best-performed MoE method with an average of 76% FLOPs. In Long Range Arena benchmark, we recorded an average score that is at least 0.68% higher than all comparison models including Full Attention, MoEs, and transformer variants, with only 50% FLOPs of the best MoE method. In image classification, our model yielded an average accuracy improvement of 1.75% than the best model while maintaining comparable FLOPs. The source codes are available at https://github.com/YujiaoYang-work/UoE.\n## A. Variants of Transformer\nTransformer [5] architecture represents a significant advancement in artificial intelligence, bringing forth unparalleled capabilities alongside the challenge of resourceintensive training and serving processes. Significant enthusiasm has been ignited for actively enhancing the efficiency of transformer architectures. A preponderance of the works concentrate on reducing memory usage and improving computational efficiency. Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence. Linformer [7] and Lightning-Attention [8,9] leverages linear-complexity self-attention mechanisms, catering to large-scale data and lengthy sequences. Reformer [10] mitigates memory constraints by employing reversible layers and locality-sensitive hashing techniques. Flash Attention [11] reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion. DeepSeek-V2 [13] introduces the Multi-Head Latent Attention (MLA) mechanism, which employs low-rank joint compression to enhance training efficiency and reduce the KV cache size during inference. Tensor Product Attention (TPA) [14] dynamically constructs QKV as context-dependent decomposed tensors, enabling adaptive adjustments and facilitating seamless integration with effective Rotary Position Embedding. \n\nThe other works aims to enhance the modeling capabilities of long sequences. Transformer-XL [15] introducing segmentlevel recurrence to enhance sequence modeling beyond the scope of the original Transformer. Sinkhorn Transformer [16] fuses Sinkhorn algorithm with self-attention mechanisms to improve sequence modeling accuracy. Long-Short-Term Memory Transformer [17] combines Transformer with Long Short-Term Memory (LSTM) mechanisms and thus enhance the modeling capability for long sequences. SeerAttention [18] integrates a learnable gating mechanism into standard attention mechanism, enabling adaptive selection of salient blocks within the attention map. Although These innovations have notably improved the performance of the transformer frameworks in their respective domains, they are typically model-specific and may not be universally applicable. By comparison, our work provides a model-free method. By simply adding selection mechanism to existing model parallel transformers, we can train a model more efficiently while maintaining or even improving its performance.",
            "reference_string": "[276775748 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Attention mechanism in neural networks: where it comes and where it goes",
            "venue": "Neural computing & applications (Print)",
            "year": 2022,
            "reference_count": 194,
            "citation_count": 166,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.13154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1409173278",
                    "name": "Derya Soydaner"
                }
            ],
            "abstract": "A long time ago in the machine learning literature, the idea of incorporating a mechanism inspired by the human visual system into neural networks was introduced. This idea is named the attention mechanism, and it has gone through a long development period. Today, many works have been devoted to this idea in a variety of tasks. Remarkable performance has recently been demonstrated. The goal of this paper is to provide an overview from the early work on searching for ways to implement attention idea with neural networks until the recent trends. This review emphasizes the important milestones during this progress regarding different tasks. By this way, this study aims to provide a road map for researchers to explore the current development and get inspired for novel approaches beyond the attention.",
            "corpus_id": 248427085,
            "sentences": [
                {
                    "corpus_id": "248427085",
                    "title": "Attention mechanism in neural networks: where it comes and where it goes",
                    "text": "The attention scheme is determined by considering only queries and keys from the same cluster. Thus, queries are routed to keys belonging to the same cluster [175].\n\nSparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight [177]. By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods [178]. Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method [179]. A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear [180]. Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering.\n\nMusic Transformer [181] shows that self-attention can also be useful for modeling music. This study emphasizes the infeasibility of the relative position representations introduced by [103] for long sequences because of the quadratic intermediate relative information in the sequence length. Therefore, this study presents an extended version of relative attention named relative local attention that improves the relative attention for longer musical compositions by reducing its intermediate memory requirement to linear in the sequence length. A softmax-free Transformer (SOFT) is presented to improve the computational efficiency of ViT. It uses Gaussian kernel function instead of the dot-product similarity [182].\n\nAdditionally, various approaches have been presented in Hierarchical Visual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Perceiver [185], and Performer [186]. Image Transformer based on the crosscovariance matrix between keys and queries is applied [187], and a new vision Transformer is proposed [188]. Furthermore, a Bernoulli sampling attention mechanism",
                    "score": 0.655982017049573,
                    "section_title": "What about complexity?",
                    "char_start_offset": 34548,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 158,
                            "end": 163,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 748,
                            "end": 753,
                            "matchedPaperCorpusId": "202538495"
                        },
                        {
                            "start": 962,
                            "end": 967,
                            "matchedPaperCorpusId": "232105052"
                        },
                        {
                            "start": 1045,
                            "end": 1050,
                            "matchedPaperCorpusId": "240354799"
                        },
                        {
                            "start": 1173,
                            "end": 1178,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1323,
                            "end": 1328,
                            "matchedPaperCorpusId": "54477714"
                        },
                        {
                            "start": 2018,
                            "end": 2023,
                            "matchedPaperCorpusId": "239616022"
                        },
                        {
                            "start": 2114,
                            "end": 2119,
                            "matchedPaperCorpusId": "232290833"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6787109375
                }
            ],
            "relevance_judgement": 0.6787109375,
            "relevance_judgment_input_expanded": "# Title: Attention mechanism in neural networks: where it comes and where it goes\n# Venue: Neural computing & applications (Print)\n# Authors: Derya Soydaner\n## Abstract\nA long time ago in the machine learning literature, the idea of incorporating a mechanism inspired by the human visual system into neural networks was introduced. This idea is named the attention mechanism, and it has gone through a long development period. Today, many works have been devoted to this idea in a variety of tasks. Remarkable performance has recently been demonstrated. The goal of this paper is to provide an overview from the early work on searching for ways to implement attention idea with neural networks until the recent trends. This review emphasizes the important milestones during this progress regarding different tasks. By this way, this study aims to provide a road map for researchers to explore the current development and get inspired for novel approaches beyond the attention.\n## What about complexity?\nThe attention scheme is determined by considering only queries and keys from the same cluster. Thus, queries are routed to keys belonging to the same cluster [175].\n\nSparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight [177]. By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods [178]. Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method [179]. A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear [180]. Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering.\n\nMusic Transformer [181] shows that self-attention can also be useful for modeling music. This study emphasizes the infeasibility of the relative position representations introduced by [103] for long sequences because of the quadratic intermediate relative information in the sequence length. Therefore, this study presents an extended version of relative attention named relative local attention that improves the relative attention for longer musical compositions by reducing its intermediate memory requirement to linear in the sequence length. A softmax-free Transformer (SOFT) is presented to improve the computational efficiency of ViT. It uses Gaussian kernel function instead of the dot-product similarity [182].\n\nAdditionally, various approaches have been presented in Hierarchical Visual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Perceiver [185], and Performer [186]. Image Transformer based on the crosscovariance matrix between keys and queries is applied [187], and a new vision Transformer is proposed [188]. Furthermore, a Bernoulli sampling attention mechanism",
            "reference_string": "[248427085 | Soydaner | 2022 | Citations: 166]"
        },
        {
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 47,
            "citation_count": 55,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.acl-long.227.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.15688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2092641069",
                    "name": "Siyu Ding"
                },
                {
                    "authorId": "40861754",
                    "name": "Junyuan Shang"
                },
                {
                    "authorId": "104463827",
                    "name": "Shuohuan Wang"
                },
                {
                    "authorId": "2117103617",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "50007795",
                    "name": "Hao Tian"
                },
                {
                    "authorId": "40354707",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "144270731",
                    "name": "Haifeng Wang"
                }
            ],
            "abstract": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",
            "corpus_id": 229923177,
            "sentences": [
                {
                    "corpus_id": "229923177",
                    "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                    "text": "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u2126(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use. In this study, we adopt a different approach to adapting Recurrence Transformers for a pretraining-then-finetuning setting, to model a long document.\n\nRecurrence Transformers Rae et al., 2019) have been successfully applied in generative language modeling. They employ the Transformer decoder as a parametric model for each conditional distribution in p(x) = L t=1 p(x t |x <t ), where x denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism . This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer (Rae et al., 2019) adds a",
                    "score": 0.5799167425863927,
                    "section_title": "Related Work",
                    "char_start_offset": 5099,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 382,
                            "end": 403,
                            "matchedPaperCorpusId": "209315300"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6767578125
                }
            ],
            "relevance_judgement": 0.6767578125,
            "relevance_judgment_input_expanded": "# Title: ERNIE-Doc: A Retrospective Long-Document Modeling Transformer\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang\n## Abstract\nTransformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.\n## Related Work\nSparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u2126(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use. In this study, we adopt a different approach to adapting Recurrence Transformers for a pretraining-then-finetuning setting, to model a long document.\n\nRecurrence Transformers Rae et al., 2019) have been successfully applied in generative language modeling. They employ the Transformer decoder as a parametric model for each conditional distribution in p(x) = L t=1 p(x t |x <t ), where x denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism . This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer (Rae et al., 2019) adds a",
            "reference_string": "[229923177 | Ding et al. | 2021 | Citations: 55]"
        },
        {
            "title": "Extending LLMs' Context Window with 100 Samples",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.07004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279654115",
                    "name": "Yikai Zhang"
                },
                {
                    "authorId": "2278801242",
                    "name": "Junlong Li"
                },
                {
                    "authorId": "2256991660",
                    "name": "Pengfei Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.",
            "corpus_id": 266999285,
            "sentences": [
                {
                    "corpus_id": "266999285",
                    "title": "Extending LLMs' Context Window with 100 Samples",
                    "text": "The vanilla attention mechanism in the Transformer architecture is known for its quadratic time and space complexity, which poses significant resource demands for transformer models when processing lengthy inputs. Various works have focused on conquering the complexity issue and proposing more efficient Transformers. Sparse transformers (Child et al., 2019;Ye et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020;Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020;Lewis et al., 2020;Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022;Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs.",
                    "score": 0.6050179780332292,
                    "section_title": "More Efficient Transformers",
                    "char_start_offset": 24824,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1837
                        },
                        {
                            "start": 1838,
                            "end": 1986
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 375,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 437,
                            "end": 457,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 995,
                            "end": 1013,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 1013,
                            "end": 1032,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1867,
                            "end": 1885,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67529296875
                }
            ],
            "relevance_judgement": 0.67529296875,
            "relevance_judgment_input_expanded": "# Title: Extending LLMs' Context Window with 100 Samples\n# Venue: arXiv.org\n# Authors: Yikai Zhang, Junlong Li, Pengfei Liu\n## Abstract\nLarge Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.\n## More Efficient Transformers\nThe vanilla attention mechanism in the Transformer architecture is known for its quadratic time and space complexity, which poses significant resource demands for transformer models when processing lengthy inputs. Various works have focused on conquering the complexity issue and proposing more efficient Transformers. Sparse transformers (Child et al., 2019;Ye et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020;Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020;Lewis et al., 2020;Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022;Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs.",
            "reference_string": "[266999285 | Zhang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10348571.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3340854?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3340854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189279577",
                    "name": "Le Zhou"
                }
            ],
            "abstract": "Efficient Transformer models typically employ local and global attention methods, or utilize hierarchical or recurrent architectures, to process long text inputs in natural language processing tasks. However, these models face challenges in terms of sacrificing either efficiency, accuracy, or compatibility to develop their application in longer sequences. To maintain both the accuracy of global attention and the efficiency of local attention, while keeping a good compatibility to be easily applied to an existing pre-trained model, in this paper, we propose multi-level local attention (Mulla attention), which is a hierarchical local attention that acts on both the input sequence and multiple pooling sequences of different granularity simultaneously, thus performing long-range modeling while maintaining linear or log-linear complexity. We apply Mulla attention to LongT5 and implement our LongT5-Mulla sequence-to-sequence model, without introducing new parameters except for positional embeddings. Experiments show that our model can surpass all baseline models, including two original variants of LongT5, in the 8~16k-input long text summarization task on the Multi-News, arXiv and WCEP-10 datasets, with improvements of at least +0.22, +0.01, +0.52 percentage points (pp) averaged Rouge scores respectively, while at the meantime being able to effectively process longer sequences that have 16~48k tokens with at least 52.6% lower memory consumption than LongT5-tglobal, and +0.56~1.62 pp averaged Rouge scores higher than LongT5-local. These results demonstrate that our proposed LongT5-Mulla model can effectively process long sequences and extend the maximum input length for long text tasks from 16k to 48k while maintaining accuracy and efficiency.",
            "corpus_id": 266110855,
            "sentences": [
                {
                    "corpus_id": "266110855",
                    "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
                    "text": "Transformer [1] models have achieved dominant results in numerous text-oriented tasks and have become the foundational architecture in natural language processing (NLP). However, standard Transformer models [1], [2], [3], [4], [5] with full attention cannot efficiently handle long texts due to their quadratic complexity in relation to the input length, and consequently cause the maximum input length of 1\u223c4k for lowering computational consumption. To relieve the limitation, considerable efforts of research called efficient Transformers [6] have been done in past few years, which \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Okyay Kaynak . \n\nincluding transformers with sparse attention and transformers with hierarchical or recurrent architectures. \n\nAs the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention [8] if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird [9], LongT5 [10] have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy. \n\nAs another category, transformers with hierarchical or recurrent architectures [11], [12], [13], [14], [15] are based on the concept of divide-and-conquer.",
                    "score": 0.5558716870242997,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 702
                        },
                        {
                            "start": 705,
                            "end": 812
                        },
                        {
                            "start": 815,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1665
                        },
                        {
                            "start": 1666,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2080
                        },
                        {
                            "start": 2083,
                            "end": 2238
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 212,
                            "end": 215,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 217,
                            "end": 220,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 222,
                            "end": 225,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 541,
                            "end": 544,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 1301,
                            "end": 1304,
                            "matchedPaperCorpusId": "221845203"
                        },
                        {
                            "start": 1465,
                            "end": 1468,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1477,
                            "end": 1481,
                            "matchedPaperCorpusId": "245144820"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.669921875
                }
            ],
            "relevance_judgement": 0.669921875,
            "relevance_judgment_input_expanded": "# Title: LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence\n# Venue: IEEE Access\n# Authors: Le Zhou\n## Abstract\nEfficient Transformer models typically employ local and global attention methods, or utilize hierarchical or recurrent architectures, to process long text inputs in natural language processing tasks. However, these models face challenges in terms of sacrificing either efficiency, accuracy, or compatibility to develop their application in longer sequences. To maintain both the accuracy of global attention and the efficiency of local attention, while keeping a good compatibility to be easily applied to an existing pre-trained model, in this paper, we propose multi-level local attention (Mulla attention), which is a hierarchical local attention that acts on both the input sequence and multiple pooling sequences of different granularity simultaneously, thus performing long-range modeling while maintaining linear or log-linear complexity. We apply Mulla attention to LongT5 and implement our LongT5-Mulla sequence-to-sequence model, without introducing new parameters except for positional embeddings. Experiments show that our model can surpass all baseline models, including two original variants of LongT5, in the 8~16k-input long text summarization task on the Multi-News, arXiv and WCEP-10 datasets, with improvements of at least +0.22, +0.01, +0.52 percentage points (pp) averaged Rouge scores respectively, while at the meantime being able to effectively process longer sequences that have 16~48k tokens with at least 52.6% lower memory consumption than LongT5-tglobal, and +0.56~1.62 pp averaged Rouge scores higher than LongT5-local. These results demonstrate that our proposed LongT5-Mulla model can effectively process long sequences and extend the maximum input length for long text tasks from 16k to 48k while maintaining accuracy and efficiency.\n## I. INTRODUCTION\nTransformer [1] models have achieved dominant results in numerous text-oriented tasks and have become the foundational architecture in natural language processing (NLP). However, standard Transformer models [1], [2], [3], [4], [5] with full attention cannot efficiently handle long texts due to their quadratic complexity in relation to the input length, and consequently cause the maximum input length of 1\u223c4k for lowering computational consumption. To relieve the limitation, considerable efforts of research called efficient Transformers [6] have been done in past few years, which \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Okyay Kaynak . \n\nincluding transformers with sparse attention and transformers with hierarchical or recurrent architectures. \n\nAs the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention [8] if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird [9], LongT5 [10] have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy. \n\nAs another category, transformers with hierarchical or recurrent architectures [11], [12], [13], [14], [15] are based on the concept of divide-and-conquer.",
            "reference_string": "[266110855 | Zhou | 2023 | Citations: 0]"
        },
        {
            "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351045310",
                    "name": "Ruyi Xu"
                },
                {
                    "authorId": "2046958974",
                    "name": "Guangxuan Xiao"
                },
                {
                    "authorId": "2351511467",
                    "name": "Haofeng Huang"
                },
                {
                    "authorId": "2325916768",
                    "name": "Junxian Guo"
                },
                {
                    "authorId": "2249530374",
                    "name": "Song Han"
                }
            ],
            "abstract": "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention.",
            "corpus_id": 277151262,
            "sentences": [
                {
                    "corpus_id": "277151262",
                    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
                    "text": "The overhead involved in determining block importance can negate the gains achieved through sparsity, rendering these methods impractical for real-world deployment. This leads us to a question: Can we design a block-sparse attention mechanism that dramatically accelerates longcontext Transformers without compromising accuracy, truly unlocking their potential for real-world applications? Attention is computed only on the selected blocks (red blocks on the right), achieving substantial computational savings. This example uses a sequence length of 24. \n\nWe answer this question by introducing XAttention, a novel plug-and-play framework designed to significantly improve the efficiency of block-sparse attention in long-context Transformers. XAttention is based on the key insight that the sum of antidiagonal values within the attention matrix can serve as a powerful, yet computationally efficient, indicator of block importance. Unlike existing methods that primarily rely on computationally intensive and lossy solutions like token pooling to identify important blocks, XAttention leverages this simple score to offer a potentially more streamlined and direct approach for rapidly and accurately identifying critical attention blocks. This antidiagonal scoring algorithm allows XAttention to aggressively find and prune non-essential computations, achieving substantial sparsity without sacrificing accuracy. We extensively evaluate XAttention on challenging longcontext benchmarks, including RULER and LongBench for natural language processing, VideoMME for video understanding, and VBench for video generation. Across these benchmarks, XAttention achieves accuracy comparable to full attention while delivering substantial computational gains, demonstrating up to 13.5\u00d7 acceleration in attention computation during pre-filling. These results underscore XAttention's ability to unlock the practical potential of block-sparse attention, paving the way for scalable and efficient deployment of long-context Transformers in demanding applications, especially in the expanding field of multimodal AI.",
                    "score": 0.5672277841889608,
                    "section_title": "Introduction",
                    "char_start_offset": 2146,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 554
                        },
                        {
                            "start": 557,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1836
                        },
                        {
                            "start": 1837,
                            "end": 2104
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65234375
                }
            ],
            "relevance_judgement": 0.65234375,
            "relevance_judgment_input_expanded": "# Title: XAttention: Block Sparse Attention with Antidiagonal Scoring\n# Venue: arXiv.org\n# Authors: Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, Song Han\n## Abstract\nLong-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention.\n## Introduction\nThe overhead involved in determining block importance can negate the gains achieved through sparsity, rendering these methods impractical for real-world deployment. This leads us to a question: Can we design a block-sparse attention mechanism that dramatically accelerates longcontext Transformers without compromising accuracy, truly unlocking their potential for real-world applications? Attention is computed only on the selected blocks (red blocks on the right), achieving substantial computational savings. This example uses a sequence length of 24. \n\nWe answer this question by introducing XAttention, a novel plug-and-play framework designed to significantly improve the efficiency of block-sparse attention in long-context Transformers. XAttention is based on the key insight that the sum of antidiagonal values within the attention matrix can serve as a powerful, yet computationally efficient, indicator of block importance. Unlike existing methods that primarily rely on computationally intensive and lossy solutions like token pooling to identify important blocks, XAttention leverages this simple score to offer a potentially more streamlined and direct approach for rapidly and accurately identifying critical attention blocks. This antidiagonal scoring algorithm allows XAttention to aggressively find and prune non-essential computations, achieving substantial sparsity without sacrificing accuracy. We extensively evaluate XAttention on challenging longcontext benchmarks, including RULER and LongBench for natural language processing, VideoMME for video understanding, and VBench for video generation. Across these benchmarks, XAttention achieves accuracy comparable to full attention while delivering substantial computational gains, demonstrating up to 13.5\u00d7 acceleration in attention computation during pre-filling. These results underscore XAttention's ability to unlock the practical potential of block-sparse attention, paving the way for scalable and efficient deployment of long-context Transformers in demanding applications, especially in the expanding field of multimodal AI.",
            "reference_string": "[277151262 | Xu et al. | 2025 | Citations: 15]"
        },
        {
            "title": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 28,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.07027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "74884989",
                    "name": "Davis Yoshida"
                },
                {
                    "authorId": "37907837",
                    "name": "Allyson Ettinger"
                },
                {
                    "authorId": "1700980",
                    "name": "Kevin Gimpel"
                }
            ],
            "abstract": "Fine-tuning a pretrained transformer for a downstream task has become a standard method in NLP in the last few years. While the results from these models are impressive, applying them can be extremely computationally expensive, as is pretraining new models with the latest architectures. We present a novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time. An additional benefit is that our method removes the fixed context size constraint that most transformer models have, allowing for more flexible use. When applied to the GPT-2 language model, we find that our method attains better perplexity than an unmodified GPT-2 model on the PG-19 and WikiText-103 corpora, for a given amount of computation or memory.",
            "corpus_id": 221140187,
            "sentences": [
                {
                    "corpus_id": "221140187",
                    "title": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size",
                    "text": "Recently, many methods have been proposed which lower the memory footprint or computation time of transformer language models, or allow them to be used on larger contexts. The Transformer-XL (Dai et al., 2019) allows a position within an attention window to attend to tokens from the previous windows by introducing relative position embeddings. While that mechanism like ours, allows information to flow between windows of text, existing BERT and GPT-2 models do not use relative position embeddings, so training from scratch would be necessary to take advantage of this architecture. \n\nOther methods modify the attention function to reduce the quadratic memory footprint down to a manageable amount. Child et al. (2019) modify the transformer architecture to replace the standard attention with a sparse one. Qiu et al. (2019) enforce a block-sparse structure on the attention matrix. Kitaev et al. (2019) also introduce sparsity, but instead do so by using locality sensitive hashing to select positions over which a full attention is computed, reducing the memory cost from quadratic to O(T log T ) for an input of size T . Rae et al. ( 2019) introduce a memory compression technique that allows much longer contexts to be attended to in memory. Beltagy et al. (2020) replace the standard attention with a combination of dilated sliding windows, and global attention from selected tokens that. Sukhbaatar et al. (2019) learn a masking function such that not all tokens attend to every previous position. Tay et al. (2020) learn synthetic attention weights, removing the need for token-token interactions. Wu et al. (2019) replace the full self-attention with a dynamic convolution depending only on the current timestep, yielding a linear dependence on length instead of a quadratic dependence. Figure 1: Augmenting a pretrained transformer with a small recurrence module, allowing reduction of attention computation as well as simpler processing of longer contexts. \n\nWhile the above methods all allow for a reduction in required computational resources, they also all require one to train a model from scratch. Our method's goal is to allow more efficient and powerful use of the wide array of existing pre-trained models that cover many domains.",
                    "score": 0.6474354262609757,
                    "section_title": "Related Work",
                    "char_start_offset": 3348,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 585
                        },
                        {
                            "start": 588,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1507
                        },
                        {
                            "start": 1508,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 1970
                        },
                        {
                            "start": 1973,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2252
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 191,
                            "end": 209,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 887,
                            "end": 907,
                            "matchedPaperCorpusId": "209315300"
                        },
                        {
                            "start": 1398,
                            "end": 1422,
                            "matchedPaperCorpusId": "159041867"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6376953125
                }
            ],
            "relevance_judgement": 0.6376953125,
            "relevance_judgment_input_expanded": "# Title: Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size\n# Venue: arXiv.org\n# Authors: Davis Yoshida, Allyson Ettinger, Kevin Gimpel\n## Abstract\nFine-tuning a pretrained transformer for a downstream task has become a standard method in NLP in the last few years. While the results from these models are impressive, applying them can be extremely computationally expensive, as is pretraining new models with the latest architectures. We present a novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time. An additional benefit is that our method removes the fixed context size constraint that most transformer models have, allowing for more flexible use. When applied to the GPT-2 language model, we find that our method attains better perplexity than an unmodified GPT-2 model on the PG-19 and WikiText-103 corpora, for a given amount of computation or memory.\n## Related Work\nRecently, many methods have been proposed which lower the memory footprint or computation time of transformer language models, or allow them to be used on larger contexts. The Transformer-XL (Dai et al., 2019) allows a position within an attention window to attend to tokens from the previous windows by introducing relative position embeddings. While that mechanism like ours, allows information to flow between windows of text, existing BERT and GPT-2 models do not use relative position embeddings, so training from scratch would be necessary to take advantage of this architecture. \n\nOther methods modify the attention function to reduce the quadratic memory footprint down to a manageable amount. Child et al. (2019) modify the transformer architecture to replace the standard attention with a sparse one. Qiu et al. (2019) enforce a block-sparse structure on the attention matrix. Kitaev et al. (2019) also introduce sparsity, but instead do so by using locality sensitive hashing to select positions over which a full attention is computed, reducing the memory cost from quadratic to O(T log T ) for an input of size T . Rae et al. ( 2019) introduce a memory compression technique that allows much longer contexts to be attended to in memory. Beltagy et al. (2020) replace the standard attention with a combination of dilated sliding windows, and global attention from selected tokens that. Sukhbaatar et al. (2019) learn a masking function such that not all tokens attend to every previous position. Tay et al. (2020) learn synthetic attention weights, removing the need for token-token interactions. Wu et al. (2019) replace the full self-attention with a dynamic convolution depending only on the current timestep, yielding a linear dependence on length instead of a quadratic dependence. Figure 1: Augmenting a pretrained transformer with a small recurrence module, allowing reduction of attention computation as well as simpler processing of longer contexts. \n\nWhile the above methods all allow for a reduction in required computational resources, they also all require one to train a model from scratch. Our method's goal is to allow more efficient and powerful use of the wide array of existing pre-trained models that cover many domains.",
            "reference_string": "[221140187 | Yoshida et al. | 2020 | Citations: 7]"
        },
        {
            "title": "On the Role of Attention Masks and LayerNorm in Transformers",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 12,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303845787",
                    "name": "Xinyi Wu"
                },
                {
                    "authorId": "2468364",
                    "name": "A. Ajorlou"
                },
                {
                    "authorId": "2303611770",
                    "name": "Yifei Wang"
                },
                {
                    "authorId": "2242248678",
                    "name": "Stefanie Jegelka"
                },
                {
                    "authorId": "1688304",
                    "name": "A. Jadbabaie"
                }
            ],
            "abstract": "Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.",
            "corpus_id": 270094757,
            "sentences": [
                {
                    "corpus_id": "270094757",
                    "title": "On the Role of Attention Masks and LayerNorm in Transformers",
                    "text": "Sparse and local Attention While many existing transformer models and LLMs do not use sparse attention, sparse and local attention is gaining popularity due to the demand for efficiency, particular for long-context tasks. For example, sparse attention was populated by Longformer [4] and OpenAI [10] and nowadays popular LLMs like Mistral 7B [22] use sliding window attention by default. Other popular sparse attention models include, but are not limited to BigBird [42], Recurrent Memory Transformers (RMTs) [7], and Streaming Attention [38]. Besides language tasks, sparse attention is also common in vision transformers [19,25,28].",
                    "score": 0.5659929546692641,
                    "section_title": "Related work",
                    "char_start_offset": 7308,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 634
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 509,
                            "end": 512,
                            "matchedPaperCorpusId": "250526424"
                        },
                        {
                            "start": 623,
                            "end": 627,
                            "matchedPaperCorpusId": "248178045"
                        },
                        {
                            "start": 627,
                            "end": 630,
                            "matchedPaperCorpusId": "232352874"
                        },
                        {
                            "start": 630,
                            "end": 633,
                            "matchedPaperCorpusId": "258048654"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63720703125
                }
            ],
            "relevance_judgement": 0.63720703125,
            "relevance_judgment_input_expanded": "# Title: On the Role of Attention Masks and LayerNorm in Transformers\n# Venue: Neural Information Processing Systems\n# Authors: Xinyi Wu, A. Ajorlou, Yifei Wang, Stefanie Jegelka, A. Jadbabaie\n## Abstract\nSelf-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.\n## Related work\nSparse and local Attention While many existing transformer models and LLMs do not use sparse attention, sparse and local attention is gaining popularity due to the demand for efficiency, particular for long-context tasks. For example, sparse attention was populated by Longformer [4] and OpenAI [10] and nowadays popular LLMs like Mistral 7B [22] use sliding window attention by default. Other popular sparse attention models include, but are not limited to BigBird [42], Recurrent Memory Transformers (RMTs) [7], and Streaming Attention [38]. Besides language tasks, sparse attention is also common in vision transformers [19,25,28].",
            "reference_string": "[270094757 | Wu et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Sliding Window Attention Training for Efficient Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275537250",
                    "name": "Zichuan Fu"
                },
                {
                    "authorId": "2347893589",
                    "name": "Wentao Song"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2344958511",
                    "name": "Yingying Zhang"
                },
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2298206411",
                    "name": "Xuetao Wei"
                },
                {
                    "authorId": "2151647484",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                }
            ],
            "abstract": "Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at https://github.com/Fzkuji/swat-attention.",
            "corpus_id": 276618265,
            "sentences": [
                {
                    "corpus_id": "276618265",
                    "title": "Sliding Window Attention Training for Efficient Large Language Models",
                    "text": "While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility.",
                    "score": 0.6038951614631637,
                    "section_title": "Efficient Transformers",
                    "char_start_offset": 20831,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 201,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 777
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63427734375
                }
            ],
            "relevance_judgement": 0.63427734375,
            "relevance_judgment_input_expanded": "# Title: Sliding Window Attention Training for Efficient Large Language Models\n# Venue: arXiv.org\n# Authors: Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao\n## Abstract\nRecent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at https://github.com/Fzkuji/swat-attention.\n## Efficient Transformers\nWhile architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility.",
            "reference_string": "[276618265 | Fu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3492102",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3492102?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3492102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292320440",
                    "name": "Linh Manh Pham"
                },
                {
                    "authorId": "2330416501",
                    "name": "Hoang Cao the"
                }
            ],
            "abstract": "Transformer-based models, such as Bidirectional Encoder Representations from Transformers (BERT), cannot process long sequences because their self-attention operation scales quadratically with the sequence length. To remedy this, we introduce the Look Near and Look Far BERT (LNLF-BERT) with a two-level self-attention mechanism at the sentence and document levels, which can handle document classifications with thousands of tokens. The self-attention mechanism of LNLF-BERT retains some of the benefits of full self-attention at each level while reducing the complexity of not using full self-attention on the whole document. Our theoretical analysis shows that the LNLF-BERT mechanism is an approximator of the full self-attention model. We pretrain the LNLF-BERT from scratch and fine-tune it on downstream tasks. The experiments were also conducted to demonstrate the feasibility of LNLF-BERT in long text processing. Moreover, LNLF-BERT effectively balances local and global attention, allowing for efficient document-level understanding. Compared to other long-sequence models like Longformer and BigBird, LNLF-BERT shows competitive performance in both accuracy and computational efficiency. The architecture is scalable to various downstream tasks, making it adaptable for different applications in natural language processing.",
            "corpus_id": 273856640,
            "sentences": [
                {
                    "corpus_id": "273856640",
                    "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels",
                    "text": "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) [3]. \n\nBIGBIRD takes the view that full attention is a fully connected graph that allows us to exploit existing graph theory to help reduce its complexity. The problem of reducing the second-order complexity of self-attention can now be viewed as a graph distribution problem. The authors point out the important role of three perspectives: \n\n\u2022 The first viewpoint: build the shortest path between pairs of nodes using random edge construction \n\n\u2022 The second viewpoint: most contexts in NLP use a lot of local references \n\n\u2022 The last viewpoint: use global tokens (star network) While a complete Transformer based on a quadratic attention mechanism is Turing complete [16], the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine [3]. And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) [3]. From this, we can see that BIGBIRD has a pretty solid mathematical foundation to underpin the model's operation. \n\nCombining the knowledge in Longformer and LongT5 models, we can see that a Transformer model using sparse self-attention will have to include at least the following elements: local self-attention, global self-attention, and shortest path between tokens.",
                    "score": 0.5565312503816268,
                    "section_title": "F. BIGBIRD",
                    "char_start_offset": 12783,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 371
                        },
                        {
                            "start": 374,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 707
                        },
                        {
                            "start": 710,
                            "end": 810
                        },
                        {
                            "start": 813,
                            "end": 887
                        },
                        {
                            "start": 890,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1623
                        },
                        {
                            "start": 1626,
                            "end": 1879
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 367,
                            "end": 370,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1034,
                            "end": 1038,
                            "matchedPaperCorpusId": "57825721"
                        },
                        {
                            "start": 1180,
                            "end": 1183,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1506,
                            "end": 1509,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.615234375
                }
            ],
            "relevance_judgement": 0.615234375,
            "relevance_judgment_input_expanded": "# Title: LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels\n# Venue: IEEE Access\n# Authors: Linh Manh Pham, Hoang Cao the\n## Abstract\nTransformer-based models, such as Bidirectional Encoder Representations from Transformers (BERT), cannot process long sequences because their self-attention operation scales quadratically with the sequence length. To remedy this, we introduce the Look Near and Look Far BERT (LNLF-BERT) with a two-level self-attention mechanism at the sentence and document levels, which can handle document classifications with thousands of tokens. The self-attention mechanism of LNLF-BERT retains some of the benefits of full self-attention at each level while reducing the complexity of not using full self-attention on the whole document. Our theoretical analysis shows that the LNLF-BERT mechanism is an approximator of the full self-attention model. We pretrain the LNLF-BERT from scratch and fine-tune it on downstream tasks. The experiments were also conducted to demonstrate the feasibility of LNLF-BERT in long text processing. Moreover, LNLF-BERT effectively balances local and global attention, allowing for efficient document-level understanding. Compared to other long-sequence models like Longformer and BigBird, LNLF-BERT shows competitive performance in both accuracy and computational efficiency. The architecture is scalable to various downstream tasks, making it adaptable for different applications in natural language processing.\n## F. BIGBIRD\nThe BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) [3]. \n\nBIGBIRD takes the view that full attention is a fully connected graph that allows us to exploit existing graph theory to help reduce its complexity. The problem of reducing the second-order complexity of self-attention can now be viewed as a graph distribution problem. The authors point out the important role of three perspectives: \n\n\u2022 The first viewpoint: build the shortest path between pairs of nodes using random edge construction \n\n\u2022 The second viewpoint: most contexts in NLP use a lot of local references \n\n\u2022 The last viewpoint: use global tokens (star network) While a complete Transformer based on a quadratic attention mechanism is Turing complete [16], the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine [3]. And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) [3]. From this, we can see that BIGBIRD has a pretty solid mathematical foundation to underpin the model's operation. \n\nCombining the knowledge in Longformer and LongT5 models, we can see that a Transformer model using sparse self-attention will have to include at least the following elements: local self-attention, global self-attention, and shortest path between tokens.",
            "reference_string": "[273856640 | Pham et al. | 2024 | Citations: 3]"
        },
        {
            "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.04013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329374652",
                    "name": "Themistoklis Haris"
                }
            ],
            "abstract": "Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar, Vaswani, Grangier, 2021] enabling each token to attend to only its $k$ closest tokens. While $k$NN attention has shown empirical success in making Transformers more efficient, its exact approximation guarantees have not been theoretically analyzed. In this work, we establish a theoretical framework for $k$NN attention, reformulating self-attention as expectations over softmax distributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017] with $k$NN indices for efficient approximation. Building on this framework, we also propose novel sub-quadratic algorithms that approximate self-attention gradients by leveraging efficient sampling techniques, such as Markov Chain-based estimation. Finally, we demonstrate the practical effectiveness of these algorithms through empirical experiments, showcasing their benefits in both training and inference.",
            "corpus_id": 273850602,
            "sentences": [
                {
                    "corpus_id": "273850602",
                    "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
                    "text": "Transformer models have become the dominant neural architecture across language, vision, and other domains [Vas17, DBK + 20]. However, scaling them to handle larger input sequences remains a significant challenge [TDA + 20], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [KRA + 21] and time-series forecasting [WZZ + 22, ZCZX23, ZZP + 21]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this \"compute divide\" is vital for democratizing AI [AW20]. \n\nEfficient computation of self-attention has been a focal point of research in recent years [FCA23]. Flash Attention [DFE + 22] and related work [SY24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF24], or adaptive techniques [CNM19]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [WLK + 20, TBM + 21, XZC + 21, KVPF20, CLD + 20]. On the negative side, recent finegrained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [KWH23,AS24a]. \n\nIn this work, we focus on sparse attention methods where each token vector q i \u2208 R d attends to the k tokens k j \u2208 R d with the largest inner products q T i k j [GDG + 21, WWW + 22], a paradigm we refer to as kNN Attention.",
                    "score": 0.6443264249266176,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 733
                        },
                        {
                            "start": 736,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1706
                        },
                        {
                            "start": 1709,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 827,
                            "end": 834,
                            "matchedPaperCorpusId": "232380042"
                        },
                        {
                            "start": 1692,
                            "end": 1699,
                            "matchedPaperCorpusId": "252198880"
                        },
                        {
                            "start": 1699,
                            "end": 1705,
                            "matchedPaperCorpusId": "257219595"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61474609375
                }
            ],
            "relevance_judgement": 0.61474609375,
            "relevance_judgment_input_expanded": "# Title: $k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers\n# Venue: \n# Authors: Themistoklis Haris\n## Abstract\nDespite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar, Vaswani, Grangier, 2021] enabling each token to attend to only its $k$ closest tokens. While $k$NN attention has shown empirical success in making Transformers more efficient, its exact approximation guarantees have not been theoretically analyzed. In this work, we establish a theoretical framework for $k$NN attention, reformulating self-attention as expectations over softmax distributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017] with $k$NN indices for efficient approximation. Building on this framework, we also propose novel sub-quadratic algorithms that approximate self-attention gradients by leveraging efficient sampling techniques, such as Markov Chain-based estimation. Finally, we demonstrate the practical effectiveness of these algorithms through empirical experiments, showcasing their benefits in both training and inference.\n## Introduction\nTransformer models have become the dominant neural architecture across language, vision, and other domains [Vas17, DBK + 20]. However, scaling them to handle larger input sequences remains a significant challenge [TDA + 20], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [KRA + 21] and time-series forecasting [WZZ + 22, ZCZX23, ZZP + 21]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this \"compute divide\" is vital for democratizing AI [AW20]. \n\nEfficient computation of self-attention has been a focal point of research in recent years [FCA23]. Flash Attention [DFE + 22] and related work [SY24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF24], or adaptive techniques [CNM19]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [WLK + 20, TBM + 21, XZC + 21, KVPF20, CLD + 20]. On the negative side, recent finegrained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [KWH23,AS24a]. \n\nIn this work, we focus on sparse attention methods where each token vector q i \u2208 R d attends to the k tokens k j \u2208 R d with the largest inner products q T i k j [GDG + 21, WWW + 22], a paradigm we refer to as kNN Attention.",
            "reference_string": "[273850602 | Haris | 2024 | Citations: 0]"
        },
        {
            "title": "Efficient Streaming Language Models with Attention Sinks",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 790,
            "influential_citation_count": 135,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.17453",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2046958974",
                    "name": "Guangxuan Xiao"
                },
                {
                    "authorId": "2249538771",
                    "name": "Yuandong Tian"
                },
                {
                    "authorId": "2249538643",
                    "name": "Beidi Chen"
                },
                {
                    "authorId": "2249530374",
                    "name": "Song Han"
                },
                {
                    "authorId": "2249533054",
                    "name": "Mike Lewis"
                }
            ],
            "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
            "corpus_id": 263310483,
            "sentences": [
                {
                    "corpus_id": "263310483",
                    "title": "Efficient Streaming Language Models with Attention Sinks",
                    "text": "Sparse Transformers. The literature on efficient Transformer models primarily focuses on reducing the computational and memory complexity of the self-attention mechanism. A relevant line of work involves sparsifying the attention matrix by restricting the field of view to fixed, predefined patterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Transformer (Child et al., 2019)  Building on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alternative, utilizing global tokens, local sliding window attentions, and random attention. However, these methods have several limitations. First, Sparse Transformer and ETC require custom GPU kernels for a specific block-sparse variant of matrix-matrix multiplication. Second, LongFormer, ETC, and BigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models. Third, these methods are incompatible with pre-trained models, necessitating retraining from scratch. In contrast, our method offers ease of implementation using standard GPU kernels and is compatible with pre-trained autoregressive language models using dense attention, which are prevalent in the NLP community. This compatibility provides a significant advantage, allowing for the leveraging of existing pre-trained models without any fine-tuning. \n\nConcurrent Works. Our research coincides with the work of Han et al., who conducted a theoretical study on the length generalization failure of language models, identifying three out-of-distribution factors. Their approach, inspired by this analysis, involves employing a \"\u039b\"-shaped attention pattern and reconfiguring position encoding distances to enhance length generalization in LLMs. This approach bears a resemblance to our methodology. However, our work uncovers the \"attention sink\" phenomenon, wherein Transformer models tend to assign high attention scores to initial tokens with small semantics. This phenomenon extends beyond the scope of length generalization failure, indicating a more pervasive issue in Transformer models. We observe this \"attention sink\" behavior not only in auto-regressive language models but also in encoder Transformers such as BERT (see Section H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in Transformer architectures.",
                    "score": 0.5882096542239414,
                    "section_title": "B ADDITIONAL RELATED WORKS",
                    "char_start_offset": 27608,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 20
                        },
                        {
                            "start": 21,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1369
                        },
                        {
                            "start": 1372,
                            "end": 1389
                        },
                        {
                            "start": 1390,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2110
                        },
                        {
                            "start": 2111,
                            "end": 2377
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 360,
                            "end": 378,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 446,
                            "end": 468,
                            "matchedPaperCorpusId": "220831004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6142578125
                }
            ],
            "relevance_judgement": 0.6142578125,
            "relevance_judgment_input_expanded": "# Title: Efficient Streaming Language Models with Attention Sinks\n# Venue: International Conference on Learning Representations\n# Authors: Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis\n## Abstract\nDeploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n## B ADDITIONAL RELATED WORKS\nSparse Transformers. The literature on efficient Transformer models primarily focuses on reducing the computational and memory complexity of the self-attention mechanism. A relevant line of work involves sparsifying the attention matrix by restricting the field of view to fixed, predefined patterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Transformer (Child et al., 2019)  Building on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alternative, utilizing global tokens, local sliding window attentions, and random attention. However, these methods have several limitations. First, Sparse Transformer and ETC require custom GPU kernels for a specific block-sparse variant of matrix-matrix multiplication. Second, LongFormer, ETC, and BigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models. Third, these methods are incompatible with pre-trained models, necessitating retraining from scratch. In contrast, our method offers ease of implementation using standard GPU kernels and is compatible with pre-trained autoregressive language models using dense attention, which are prevalent in the NLP community. This compatibility provides a significant advantage, allowing for the leveraging of existing pre-trained models without any fine-tuning. \n\nConcurrent Works. Our research coincides with the work of Han et al., who conducted a theoretical study on the length generalization failure of language models, identifying three out-of-distribution factors. Their approach, inspired by this analysis, involves employing a \"\u039b\"-shaped attention pattern and reconfiguring position encoding distances to enhance length generalization in LLMs. This approach bears a resemblance to our methodology. However, our work uncovers the \"attention sink\" phenomenon, wherein Transformer models tend to assign high attention scores to initial tokens with small semantics. This phenomenon extends beyond the scope of length generalization failure, indicating a more pervasive issue in Transformer models. We observe this \"attention sink\" behavior not only in auto-regressive language models but also in encoder Transformers such as BERT (see Section H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in Transformer architectures.",
            "reference_string": "[263310483 | Xiao et al. | 2023 | Citations: 790]"
        },
        {
            "title": "InAttention: Linear Context Scaling for Transformers",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325097804",
                    "name": "Joseph Eisner"
                }
            ],
            "abstract": "VRAM requirements for transformer models scale quadratically with context length due to the self-attention mechanism. In this paper we modify the decoder-only transformer, replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states. Benchmarking shows that InAttention significantly reduces VRAM usage during inference, enabling handling of long sequences on consumer GPUs. We corroborate that fine-tuning extends context length efficiently, improving performance on long sequences without high training costs. InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.",
            "corpus_id": 273228328,
            "sentences": [
                {
                    "corpus_id": "273228328",
                    "title": "InAttention: Linear Context Scaling for Transformers",
                    "text": "Decoder-based transformer stacks [16] have demonstrated syntactic and semantic understanding of language and other time-series data, achieving state-of-the-art few-shot [3] performance across nearly any natural language task. They exhibit predictable scaling laws with respect to number of parameters and the amount of training data they ingest: bigger is better and we generally know by how much [8]. \n\nThese Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques: ...For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks... while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token. \n\nPerhaps the most human-interpretable form of sparse attention is sliding window attention (e.g. Mistral's [1]), which uses a lower-diagonal banded matrix as the attention mask.",
                    "score": 0.580984630268669,
                    "section_title": "Background and Introduction",
                    "char_start_offset": 30,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 401
                        },
                        {
                            "start": 404,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1966
                        },
                        {
                            "start": 1969,
                            "end": 2064
                        },
                        {
                            "start": 2065,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 169,
                            "end": 172,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6142578125
                }
            ],
            "relevance_judgement": 0.6142578125,
            "relevance_judgment_input_expanded": "# Title: InAttention: Linear Context Scaling for Transformers\n# Venue: arXiv.org\n# Authors: Joseph Eisner\n## Abstract\nVRAM requirements for transformer models scale quadratically with context length due to the self-attention mechanism. In this paper we modify the decoder-only transformer, replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states. Benchmarking shows that InAttention significantly reduces VRAM usage during inference, enabling handling of long sequences on consumer GPUs. We corroborate that fine-tuning extends context length efficiently, improving performance on long sequences without high training costs. InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.\n## Background and Introduction\nDecoder-based transformer stacks [16] have demonstrated syntactic and semantic understanding of language and other time-series data, achieving state-of-the-art few-shot [3] performance across nearly any natural language task. They exhibit predictable scaling laws with respect to number of parameters and the amount of training data they ingest: bigger is better and we generally know by how much [8]. \n\nThese Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques: ...For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks... while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token. \n\nPerhaps the most human-interpretable form of sparse attention is sliding window attention (e.g. Mistral's [1]), which uses a lower-diagonal banded matrix as the attention mask.",
            "reference_string": "[273228328 | Eisner | 2024 | Citations: 0]"
        },
        {
            "title": "Exploring Attention Sparsity to Accelerate Transformer Training on GPUs",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3425638",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3425638?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3425638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244621763",
                    "name": "Bokyeong Yoon"
                },
                {
                    "authorId": "2311079315",
                    "name": "Ah-Hyun Lee"
                },
                {
                    "authorId": "2311077789",
                    "name": "Jinsung Kim"
                },
                {
                    "authorId": "2244620979",
                    "name": "Gordon Euhyun Moon"
                }
            ],
            "abstract": "The computational complexity required for training a Transformer model quadratically increases as the length of the input sequence increases. Therefore, to accelerate the training of a large-scale Transformer with long sequences, it is crucial to reduce the number of operations for the multi-head attention computations, which dominate the overall Transformer training process. Previous approaches have sought to sparsify the multi-head attention before training by statically selecting the critical elements in the attention score matrix. However, since the critical elements in the attention score matrix can vary across different model tasks and datasets, dynamically considering the critical elements is essential for achieving better model quality. In this paper, we propose a new sparsity-aware Transformer that captures task- and input-dependent sparsity pattern in the attention score matrix during a small number of steps of the standard training of the Transformer. Then the identified sparsity pattern is utilized in the sparse training, transferred from the standard training, based on the degree of skewness and distance values of the attention score matrices. Experimental results demonstrate that our approach significantly reduces the number of operations in the multi-head attention operations, achieving up to $2.84\\times $ training speedup, $6.87\\times $ memory reduction and better accuracy compared to state-of-the-art sparse Transformer models.",
            "corpus_id": 271143408,
            "sentences": [
                {
                    "corpus_id": "271143408",
                    "title": "Exploring Attention Sparsity to Accelerate Transformer Training on GPUs",
                    "text": "The performance of original encoder-only Transformer and three state-of-the-art sparse Transformers, including Longformer, BigBird, and LSG Attention, is compared with our SAT model. \n\n\u2022 Original encoder-only Transformer [1]: This implementation is based on the original Transformer architecture and performs the original dense MHA operation during entire training. \n\n\u2022 Longformer [5]: This model utilizes a sparse attention mechanism based on dilated sliding windows. It is evaluated using a sliding windows size of 64. \n\n\u2022 BigBird [4]: This model incorporates sparse attention mechanisms, including sliding window attention, global attention, and random attention. It is evaluated using a block size of 32 and 3 random blocks. \n\n\u2022 LSG Attention [14]: This model incorporates local attention, sparse attention, and global attention. Max norm is chosen for the sparse attention to evaluate. It is evaluated using a block size of 32 and a sparse block size of 32, with a sparsity factor of 2. \n\n\u2022 SAT: This model is our proposed method, which utilizes learnable parameters to capture the general sparse pattern of the training dataset. \n\nIn our SAT, an embedding dimension (D) of size 64 was employed, and the batch size was determined by the available memory size, resulting in batch sizes of 512. The block size (B) for the learnable parameter is set to 32. We set the skewness threshold (\u03b3 ) to 1.7 for the AG News, CIFAR-10, and iNaturalist datasets, and a value of 1.3 for the Yelp Review dataset. Additionally, the difference threshold (d) was consistently fixed at 1.3 across all datasets. Note that all the experiment results presented in this section are averaged over five different executions.",
                    "score": 0.5599913125191881,
                    "section_title": "2) MODELS COMPARED",
                    "char_start_offset": 29820,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 185,
                            "end": 365
                        },
                        {
                            "start": 368,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 520
                        },
                        {
                            "start": 523,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 728
                        },
                        {
                            "start": 731,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 991
                        },
                        {
                            "start": 994,
                            "end": 1134
                        },
                        {
                            "start": 1137,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1703
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 533,
                            "end": 536,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 747,
                            "end": 751,
                            "matchedPaperCorpusId": "253157377"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.611328125
                }
            ],
            "relevance_judgement": 0.611328125,
            "relevance_judgment_input_expanded": "# Title: Exploring Attention Sparsity to Accelerate Transformer Training on GPUs\n# Venue: IEEE Access\n# Authors: Bokyeong Yoon, Ah-Hyun Lee, Jinsung Kim, Gordon Euhyun Moon\n## Abstract\nThe computational complexity required for training a Transformer model quadratically increases as the length of the input sequence increases. Therefore, to accelerate the training of a large-scale Transformer with long sequences, it is crucial to reduce the number of operations for the multi-head attention computations, which dominate the overall Transformer training process. Previous approaches have sought to sparsify the multi-head attention before training by statically selecting the critical elements in the attention score matrix. However, since the critical elements in the attention score matrix can vary across different model tasks and datasets, dynamically considering the critical elements is essential for achieving better model quality. In this paper, we propose a new sparsity-aware Transformer that captures task- and input-dependent sparsity pattern in the attention score matrix during a small number of steps of the standard training of the Transformer. Then the identified sparsity pattern is utilized in the sparse training, transferred from the standard training, based on the degree of skewness and distance values of the attention score matrices. Experimental results demonstrate that our approach significantly reduces the number of operations in the multi-head attention operations, achieving up to $2.84\\times $ training speedup, $6.87\\times $ memory reduction and better accuracy compared to state-of-the-art sparse Transformer models.\n## 2) MODELS COMPARED\nThe performance of original encoder-only Transformer and three state-of-the-art sparse Transformers, including Longformer, BigBird, and LSG Attention, is compared with our SAT model. \n\n\u2022 Original encoder-only Transformer [1]: This implementation is based on the original Transformer architecture and performs the original dense MHA operation during entire training. \n\n\u2022 Longformer [5]: This model utilizes a sparse attention mechanism based on dilated sliding windows. It is evaluated using a sliding windows size of 64. \n\n\u2022 BigBird [4]: This model incorporates sparse attention mechanisms, including sliding window attention, global attention, and random attention. It is evaluated using a block size of 32 and 3 random blocks. \n\n\u2022 LSG Attention [14]: This model incorporates local attention, sparse attention, and global attention. Max norm is chosen for the sparse attention to evaluate. It is evaluated using a block size of 32 and a sparse block size of 32, with a sparsity factor of 2. \n\n\u2022 SAT: This model is our proposed method, which utilizes learnable parameters to capture the general sparse pattern of the training dataset. \n\nIn our SAT, an embedding dimension (D) of size 64 was employed, and the batch size was determined by the available memory size, resulting in batch sizes of 512. The block size (B) for the learnable parameter is set to 32. We set the skewness threshold (\u03b3 ) to 1.7 for the AG News, CIFAR-10, and iNaturalist datasets, and a value of 1.3 for the Yelp Review dataset. Additionally, the difference threshold (d) was consistently fixed at 1.3 across all datasets. Note that all the experiment results presented in this section are averaged over five different executions.",
            "reference_string": "[271143408 | Yoon et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-acl.546.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.546?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "4874730",
                    "name": "Xuanyu Zhang"
                },
                {
                    "authorId": "2068974",
                    "name": "Zhepeng Lv"
                },
                {
                    "authorId": "2149535351",
                    "name": "Qing Yang"
                }
            ],
            "abstract": ",",
            "corpus_id": 259858862,
            "sentences": [
                {
                    "corpus_id": "259858862",
                    "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
                    "text": "Different from traditional sparse Transformer (Martins and Astudillo, 2016;Correia et al., 2019;Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered. \n\nTo address these issues, we propose A 2 -Former with adaptive attention to model longer sequences in this paper. It can select useful tokens automatically in sparse attention by learnable position vectors, which consist of meta position and offset position vectors. Because each element in the learnable offset position vector is not an integer, we utilize linear interpolation to gather discrete vectors from original the input embedding matrix. Position visualization further shows that traditional attention patterns are not enough to cover the valuable positions automatically selected by models. Experiments on Long Range Arena, a systematic and unified benchmark with different tasks, show that our model has achieved further improvement in performance compared with other sparse-based Transformers. \n\nOverall, the main contributions are as follows: \n\n\u2022 We propose a novel efficient Transformer, A 2 -Former, which replaces hand-crafted attention patterns with learnable adaptive attention in sparse attention. Besides, position visualization (Figure 3) further shows that traditional attention patterns are not enough to cover the useful positions automatically selected by models. \n\n\u2022 We adopt an interpolation technique to help the model gather discrete positions with a continuous weight matrix. By combining the meta position and generated offset position, the position of tokens can be selected dynamically according to the context. \n\n\u2022 Experiments on different long sequence tasks validate the effectiveness of our model.",
                    "score": 0.5486559572993824,
                    "section_title": "Introduction",
                    "char_start_offset": 1517,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1503
                        },
                        {
                            "start": 1504,
                            "end": 1708
                        },
                        {
                            "start": 1711,
                            "end": 1758
                        },
                        {
                            "start": 1761,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2091
                        },
                        {
                            "start": 2094,
                            "end": 2208
                        },
                        {
                            "start": 2209,
                            "end": 2347
                        },
                        {
                            "start": 2350,
                            "end": 2437
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 46,
                            "end": 75,
                            "matchedPaperCorpusId": "16432551"
                        },
                        {
                            "start": 75,
                            "end": 96,
                            "matchedPaperCorpusId": "202538495"
                        },
                        {
                            "start": 96,
                            "end": 116,
                            "matchedPaperCorpusId": "153313159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60986328125
                }
            ],
            "relevance_judgement": 0.60986328125,
            "relevance_judgment_input_expanded": "# Title: Adaptive Attention for Sparse-based Long-sequence Transformer\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Xuanyu Zhang, Zhepeng Lv, Qing Yang\n## Abstract\n,\n## Introduction\nDifferent from traditional sparse Transformer (Martins and Astudillo, 2016;Correia et al., 2019;Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered. \n\nTo address these issues, we propose A 2 -Former with adaptive attention to model longer sequences in this paper. It can select useful tokens automatically in sparse attention by learnable position vectors, which consist of meta position and offset position vectors. Because each element in the learnable offset position vector is not an integer, we utilize linear interpolation to gather discrete vectors from original the input embedding matrix. Position visualization further shows that traditional attention patterns are not enough to cover the valuable positions automatically selected by models. Experiments on Long Range Arena, a systematic and unified benchmark with different tasks, show that our model has achieved further improvement in performance compared with other sparse-based Transformers. \n\nOverall, the main contributions are as follows: \n\n\u2022 We propose a novel efficient Transformer, A 2 -Former, which replaces hand-crafted attention patterns with learnable adaptive attention in sparse attention. Besides, position visualization (Figure 3) further shows that traditional attention patterns are not enough to cover the useful positions automatically selected by models. \n\n\u2022 We adopt an interpolation technique to help the model gather discrete positions with a continuous weight matrix. By combining the meta position and generated offset position, the position of tokens can be selected dynamically according to the context. \n\n\u2022 Experiments on different long sequence tasks validate the effectiveness of our model.",
            "reference_string": "[259858862 | Zhang et al. | 2023 | Citations: 7]"
        },
        {
            "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
            "venue": "",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.17678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312344360",
                    "name": "Xihui Lin"
                },
                {
                    "authorId": "2253427836",
                    "name": "Yunan Zhang"
                },
                {
                    "authorId": "2284984440",
                    "name": "Suyu Ge"
                },
                {
                    "authorId": "27419446",
                    "name": "Barun Patra"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "2301764251",
                    "name": "Xia Song"
                }
            ],
            "abstract": "Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM.",
            "corpus_id": 271431900,
            "sentences": [
                {
                    "corpus_id": "271431900",
                    "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
                    "text": "Transformer-based LLMs have opened up fresh opportunities to both research and applications (Ope-nAI, 2023;Touvron et al., 2023). Their quadratic overhead imposes prohibitive overhead in training and serving these models. For example, training Llama 2 (Touvron et al., 2023) 70B with a 4K context length on 2 trillion tokens takes 23 days on 2048 A100 GPUs Rucinski (2024). When serving the model, the model's KV cache consumes 343 GB GPU memory with a batch size 32 and 4K context length. There is an urgent need to train LLMs efficiently and serve them cost-effectively. \n\nMany works aim to improve efficiency of attention through various sparse attention techniques (Tay et al., 2023;Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), where only a subset of the   tokens in the context are attended to. Despite their promising on-paper FLOP savings compared to full-context dense attention, these methods often fail to deliver real-world efficiency gains. As pointed out by the seminal work FlashAttention (Dao et al., 2022;Dao, 2023), GPU memory access, rather than computation, is the primary bottleneck for attention. Dense attention has benefited from CUDA-level implementations specifically optimized for more efficient memory IO, an significant optimization that sparse attention methods have yet to receive. The absence of a flexible, efficient, and easy-to-use library for optimized implementations for sparse attention has become a major roadblock for research in this area, slowing the progress in improving LLMs' training and serving efficiency. \n\nWith Sparsely-Sharded(S2) Attention, we aim to bridge this gap. S2-Attention is a Triton library that provides kernel optimization for sparse attention. It is highly flexible, allowing practitioners to explore various sparse attention strategies and customize different attention patterns across attention heads and context ranges. The main challenge in implementing a fused kernel for general sparse attention arises from the sparsity itself.",
                    "score": 0.5410240490916385,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 572
                        },
                        {
                            "start": 575,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1570
                        },
                        {
                            "start": 1573,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1904
                        },
                        {
                            "start": 1905,
                            "end": 2016
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 727,
                            "end": 747,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1020,
                            "end": 1038,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60498046875
                }
            ],
            "relevance_judgement": 0.60498046875,
            "relevance_judgment_input_expanded": "# Title: S2-Attention: Hardware-Aware Context Sharding Among Attention Heads\n# Venue: \n# Authors: Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Xia Song\n## Abstract\nSparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM.\n## INTRODUCTION\nTransformer-based LLMs have opened up fresh opportunities to both research and applications (Ope-nAI, 2023;Touvron et al., 2023). Their quadratic overhead imposes prohibitive overhead in training and serving these models. For example, training Llama 2 (Touvron et al., 2023) 70B with a 4K context length on 2 trillion tokens takes 23 days on 2048 A100 GPUs Rucinski (2024). When serving the model, the model's KV cache consumes 343 GB GPU memory with a batch size 32 and 4K context length. There is an urgent need to train LLMs efficiently and serve them cost-effectively. \n\nMany works aim to improve efficiency of attention through various sparse attention techniques (Tay et al., 2023;Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), where only a subset of the   tokens in the context are attended to. Despite their promising on-paper FLOP savings compared to full-context dense attention, these methods often fail to deliver real-world efficiency gains. As pointed out by the seminal work FlashAttention (Dao et al., 2022;Dao, 2023), GPU memory access, rather than computation, is the primary bottleneck for attention. Dense attention has benefited from CUDA-level implementations specifically optimized for more efficient memory IO, an significant optimization that sparse attention methods have yet to receive. The absence of a flexible, efficient, and easy-to-use library for optimized implementations for sparse attention has become a major roadblock for research in this area, slowing the progress in improving LLMs' training and serving efficiency. \n\nWith Sparsely-Sharded(S2) Attention, we aim to bridge this gap. S2-Attention is a Triton library that provides kernel optimization for sparse attention. It is highly flexible, allowing practitioners to explore various sparse attention strategies and customize different attention patterns across attention heads and context ranges. The main challenge in implementing a fused kernel for general sparse attention arises from the sparsity itself.",
            "reference_string": "[271431900 | Lin et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 71,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.21079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344790336",
                    "name": "Yifei Xia"
                },
                {
                    "authorId": "2348282342",
                    "name": "Suhan Ling"
                },
                {
                    "authorId": "46182701",
                    "name": "Fangcheng Fu"
                },
                {
                    "authorId": "2167500394",
                    "name": "Yujie Wang"
                },
                {
                    "authorId": "2108525422",
                    "name": "Huixia Li"
                },
                {
                    "authorId": "2319391688",
                    "name": "Xuefeng Xiao"
                },
                {
                    "authorId": "2313408987",
                    "name": "Bin Cui"
                }
            ],
            "abstract": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is often hindered by significant latency, primarily due to the computational demands of attention mechanisms. For instance, generating an 8-second 720p video (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500 PFLOPs consumed by attention computations. To address this issue, we propose AdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention method. Firstly, to realize the Dynamic Pattern, we introduce a blockified pattern to efficiently capture the hierarchical sparsity inherent in DiTs. This is based on our observation that sparse characteristics of DiTs exhibit hierarchical and blockified structures between and within different modalities. This blockified approach significantly reduces the complexity of attention computation while maintaining high fidelity in the generated videos. Secondly, to enable Online Precise Search, we propose the Fused LSE-Cached Search with Head-adaptive Hierarchical Block Sparse Attention. This method is motivated by our finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and heads, but remain invariant across denoising steps. By leveraging this invariance across denoising steps, it adapts to the dynamic nature of DiTs and allows for precise, real-time identification of sparse indices with minimal overhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can be integrated seamlessly with existing DiTs, requiring neither additional fine-tuning nor a dataset-dependent profiling. Extensive experiments validate that AdaSpa delivers substantial acceleration across various models while preserving video quality, establishing itself as a robust and scalable approach to efficient video generation.",
            "corpus_id": 276725385,
            "sentences": [
                {
                    "corpus_id": "276725385",
                    "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
                    "text": "In the self-attention mechanism [58], tokens are projected into the query, key, and value matrices Q, K, V \u2208 R H\u00d7L\u00d7D , where H is the number of attention heads, L is the input length, and D is the hidden dimension of each head. The attention weights matrix W attn \u2208 R L\u00d7L is computed as: \n\nwhich quantifies token-to-token interactions across the sequence. To maintain numerical stability during the exponentiation, the Log-Sum-Exp (LSE) [2] trick is commonly employed. Let Z = QK \u22a4 \u221a d and denote by z j the j-th component of a row z. Then, LSE can be written as: \n\nUsing this, the safe Softmax can be expressed as: \n\nand the entire dense attention distribution in a numerically stable form is: \n\nThis operation, however, requires constructing an L \u00d7 L attention matrix, leading to O(L 2 ) time and memory complexity, which becomes prohibitive for long sequences. \n\nFlashAttention [12,13,50] addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention.",
                    "score": 0.5819239138066779,
                    "section_title": "FlashAttention",
                    "char_start_offset": 9079,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 287
                        },
                        {
                            "start": 290,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 563
                        },
                        {
                            "start": 566,
                            "end": 615
                        },
                        {
                            "start": 618,
                            "end": 694
                        },
                        {
                            "start": 697,
                            "end": 863
                        },
                        {
                            "start": 866,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1305
                        },
                        {
                            "start": 1308,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1576
                        },
                        {
                            "start": 1579,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1858
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 885,
                            "end": 888,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 888,
                            "end": 891,
                            "matchedPaperCorpusId": "271098045"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.603515625
                }
            ],
            "relevance_judgement": 0.603515625,
            "relevance_judgment_input_expanded": "# Title: Training-free and Adaptive Sparse Attention for Efficient Long Video Generation\n# Venue: arXiv.org\n# Authors: Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, Bin Cui\n## Abstract\nGenerating high-fidelity long videos with Diffusion Transformers (DiTs) is often hindered by significant latency, primarily due to the computational demands of attention mechanisms. For instance, generating an 8-second 720p video (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500 PFLOPs consumed by attention computations. To address this issue, we propose AdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention method. Firstly, to realize the Dynamic Pattern, we introduce a blockified pattern to efficiently capture the hierarchical sparsity inherent in DiTs. This is based on our observation that sparse characteristics of DiTs exhibit hierarchical and blockified structures between and within different modalities. This blockified approach significantly reduces the complexity of attention computation while maintaining high fidelity in the generated videos. Secondly, to enable Online Precise Search, we propose the Fused LSE-Cached Search with Head-adaptive Hierarchical Block Sparse Attention. This method is motivated by our finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and heads, but remain invariant across denoising steps. By leveraging this invariance across denoising steps, it adapts to the dynamic nature of DiTs and allows for precise, real-time identification of sparse indices with minimal overhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can be integrated seamlessly with existing DiTs, requiring neither additional fine-tuning nor a dataset-dependent profiling. Extensive experiments validate that AdaSpa delivers substantial acceleration across various models while preserving video quality, establishing itself as a robust and scalable approach to efficient video generation.\n## FlashAttention\nIn the self-attention mechanism [58], tokens are projected into the query, key, and value matrices Q, K, V \u2208 R H\u00d7L\u00d7D , where H is the number of attention heads, L is the input length, and D is the hidden dimension of each head. The attention weights matrix W attn \u2208 R L\u00d7L is computed as: \n\nwhich quantifies token-to-token interactions across the sequence. To maintain numerical stability during the exponentiation, the Log-Sum-Exp (LSE) [2] trick is commonly employed. Let Z = QK \u22a4 \u221a d and denote by z j the j-th component of a row z. Then, LSE can be written as: \n\nUsing this, the safe Softmax can be expressed as: \n\nand the entire dense attention distribution in a numerically stable form is: \n\nThis operation, however, requires constructing an L \u00d7 L attention matrix, leading to O(L 2 ) time and memory complexity, which becomes prohibitive for long sequences. \n\nFlashAttention [12,13,50] addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention.",
            "reference_string": "[276725385 | Xia et al. | 2025 | Citations: 11]"
        },
        {
            "title": "Linear attention based spatiotemporal multi graph GCN for traffic flow prediction",
            "venue": "Scientific Reports",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41598-025-93179-y",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11893777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292350600",
                    "name": "Yanping Zhang"
                },
                {
                    "authorId": "2222497764",
                    "name": "Wenjin Xu"
                },
                {
                    "authorId": "2222463969",
                    "name": "Benjiang Ma"
                },
                {
                    "authorId": "2349410310",
                    "name": "Dan Zhang"
                },
                {
                    "authorId": "2349423969",
                    "name": "Fanli Zeng"
                },
                {
                    "authorId": "2349818655",
                    "name": "Jiayu Yao"
                },
                {
                    "authorId": "2244143401",
                    "name": "Hongning Yang"
                },
                {
                    "authorId": "2243983469",
                    "name": "Zhenzhen Du"
                }
            ],
            "abstract": "Intelligent Transportation Systems (ITSs) have become pivotal in urban traffic management by utilizing traffic flow prediction, which aids in alleviating congestion and facilitating route planning. This study introduces the Linear Attention Based Spatial-Temporal Multi-Graph Convolutional Neural Network (LASTGCN), a novel deep learning model tailored for traffic flow prediction. LASTGCN incorporates a Multifactor Fusion Unit (MFF-unit) to dynamically integrate meteorological factors, an advanced multi-graph convolutional network for spatial correlations, and the Receptance Weighted Key Value (RWKV) block, which employs a linear attention mechanism for efficient processing of historical traffic data.The model achieves computational efficiency by using RWKV, which offers advantages over Transformer-based models in handling large-scale data while capturing complex dependencies. The model is designed to achieve computational efficiency, making it suitable for mid-term traffic management scenarios and potentially adaptable to real-time applications with further optimization. Experimental results using real-world highway traffic datasets indicate that LASTGCN outperforms several state-of-the-art methods in terms of accuracy and robustness, especially in long-term predictions. Additionally, integrating external factors such as weather conditions was found to significantly enhance the model\u2019s predictive accuracy.",
            "corpus_id": 276906703,
            "sentences": [
                {
                    "corpus_id": "276906703",
                    "title": "Linear attention based spatiotemporal multi graph GCN for traffic flow prediction",
                    "text": "There are two main strategies to mitigate the memory and computational complexity of transformers, which inherently scale quadratically with sequence length. The first strategy focuses on refining the Attention Mechanism 13 . A plethora of transformer derivatives, termed \"x-formers\" have been developed, employing tactics such as sparse attention [36][37][38] , full attention matrix approximation 19,39,40 , and chunked attention with gating 41 . Notably, recent innovations like FlashAttention 17 improve memory efficiency, but time complexity remains an issue, either remaining quadratic or influenced by hidden factors like chunk size. Conversely, RWKV 20 exemplifies superior space-time complexity trade-offs during inference by ingeniously casting linear attention as an RNN. \n\nThe second strategy involves supplanting the attention mechanism with alternative modules for scalability in long sequences. For instance, MLP-Mixer 42,43 advocates replacing attention with Multi-Layer Perceptrons in computer vision. Additionally, the Attention Free Transformer (AFT) 44 replaces dot-product self-attention with an efficient variant resembling multi-head attention in its feature dimension correspondence. Concurrently, RNN-based components have been adapted to extend context length, such as the Recurrent Memory Transformer 45 and Linear Recurrent Units 46 .",
                    "score": 0.742423064366025,
                    "section_title": "Optimizing attention mechanism",
                    "char_start_offset": 5842,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 782
                        },
                        {
                            "start": 785,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 356,
                            "end": 360,
                            "matchedPaperCorpusId": "245144820"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6015625
                }
            ],
            "relevance_judgement": 0.6015625,
            "relevance_judgment_input_expanded": "# Title: Linear attention based spatiotemporal multi graph GCN for traffic flow prediction\n# Venue: Scientific Reports\n# Authors: Yanping Zhang, Wenjin Xu, Benjiang Ma, Dan Zhang, Fanli Zeng, Jiayu Yao, Hongning Yang, Zhenzhen Du\n## Abstract\nIntelligent Transportation Systems (ITSs) have become pivotal in urban traffic management by utilizing traffic flow prediction, which aids in alleviating congestion and facilitating route planning. This study introduces the Linear Attention Based Spatial-Temporal Multi-Graph Convolutional Neural Network (LASTGCN), a novel deep learning model tailored for traffic flow prediction. LASTGCN incorporates a Multifactor Fusion Unit (MFF-unit) to dynamically integrate meteorological factors, an advanced multi-graph convolutional network for spatial correlations, and the Receptance Weighted Key Value (RWKV) block, which employs a linear attention mechanism for efficient processing of historical traffic data.The model achieves computational efficiency by using RWKV, which offers advantages over Transformer-based models in handling large-scale data while capturing complex dependencies. The model is designed to achieve computational efficiency, making it suitable for mid-term traffic management scenarios and potentially adaptable to real-time applications with further optimization. Experimental results using real-world highway traffic datasets indicate that LASTGCN outperforms several state-of-the-art methods in terms of accuracy and robustness, especially in long-term predictions. Additionally, integrating external factors such as weather conditions was found to significantly enhance the model\u2019s predictive accuracy.\n## Optimizing attention mechanism\nThere are two main strategies to mitigate the memory and computational complexity of transformers, which inherently scale quadratically with sequence length. The first strategy focuses on refining the Attention Mechanism 13 . A plethora of transformer derivatives, termed \"x-formers\" have been developed, employing tactics such as sparse attention [36][37][38] , full attention matrix approximation 19,39,40 , and chunked attention with gating 41 . Notably, recent innovations like FlashAttention 17 improve memory efficiency, but time complexity remains an issue, either remaining quadratic or influenced by hidden factors like chunk size. Conversely, RWKV 20 exemplifies superior space-time complexity trade-offs during inference by ingeniously casting linear attention as an RNN. \n\nThe second strategy involves supplanting the attention mechanism with alternative modules for scalability in long sequences. For instance, MLP-Mixer 42,43 advocates replacing attention with Multi-Layer Perceptrons in computer vision. Additionally, the Attention Free Transformer (AFT) 44 replaces dot-product self-attention with an efficient variant resembling multi-head attention in its feature dimension correspondence. Concurrently, RNN-based components have been adapted to extend context length, such as the Recurrent Memory Transformer 45 and Linear Recurrent Units 46 .",
            "reference_string": "[276906703 | Zhang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 91,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.08618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50982080",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "2250363276",
                    "name": "Xiaoyang Wang"
                },
                {
                    "authorId": "2173531",
                    "name": "Sangwoo Cho"
                },
                {
                    "authorId": "2243367575",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2256336899",
                    "name": "Dong Yu"
                }
            ],
            "abstract": "This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.",
            "corpus_id": 266210450,
            "sentences": [
                {
                    "corpus_id": "266210450",
                    "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
                    "text": "Attention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020b), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021). Positional Embedding. In Transformer models, positional embeddings can be primarily categorized into two types: absolute and relative. Earlier versions of Transformers utilize absolute positional encoding. For instance, the vanilla Transformer (Vaswani et al., 2017) model adds sinusoidal positional embeddings to the word embeddings, whereas GPT (Radford et al., 2018) and BERT (Devlin et al., 2018) introduce learnable positional embeddings. Currently, it has become more common to use relative positional embedding. For instance, Transformer-XL (Dai et al., 2019) and T5 (Raffel et al., 2020) adds learnable attention logit bias into attention layers. Alibi (Press et al., 2022) biases attention scores based on the distance between key and query elements. RoPE (Su et al., 2023) multiplies the keys and queries of every attention layer by sinusoidal embeddings. The Alibi and RoPE methods are further improved through the incorporation of an additional bias term (Sun et al., 2022;Chi et al., 2023). LLM.",
                    "score": 0.5951584828994106,
                    "section_title": "Related Work",
                    "char_start_offset": 29719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 10
                        },
                        {
                            "start": 11,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1684
                        },
                        {
                            "start": 1685,
                            "end": 1790
                        },
                        {
                            "start": 1791,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 1933
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 482,
                            "end": 501,
                            "matchedPaperCorpusId": "211505992"
                        },
                        {
                            "start": 542,
                            "end": 564,
                            "matchedPaperCorpusId": "221845203"
                        },
                        {
                            "start": 578,
                            "end": 599,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1169,
                            "end": 1191,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1499,
                            "end": 1520,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1586,
                            "end": 1606,
                            "matchedPaperCorpusId": "237347130"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59375
                }
            ],
            "relevance_judgement": 0.59375,
            "relevance_judgment_input_expanded": "# Title: Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention\n# Venue: arXiv.org\n# Authors: Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, Dong Yu\n## Abstract\nThis paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.\n## Related Work\nAttention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020b), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021). Positional Embedding. In Transformer models, positional embeddings can be primarily categorized into two types: absolute and relative. Earlier versions of Transformers utilize absolute positional encoding. For instance, the vanilla Transformer (Vaswani et al., 2017) model adds sinusoidal positional embeddings to the word embeddings, whereas GPT (Radford et al., 2018) and BERT (Devlin et al., 2018) introduce learnable positional embeddings. Currently, it has become more common to use relative positional embedding. For instance, Transformer-XL (Dai et al., 2019) and T5 (Raffel et al., 2020) adds learnable attention logit bias into attention layers. Alibi (Press et al., 2022) biases attention scores based on the distance between key and query elements. RoPE (Su et al., 2023) multiplies the keys and queries of every attention layer by sinusoidal embeddings. The Alibi and RoPE methods are further improved through the incorporation of an additional bias term (Sun et al., 2022;Chi et al., 2023). LLM.",
            "reference_string": "[266210450 | Song et al. | 2023 | Citations: 7]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "272310078",
            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
            "text": "The substantial memory demands of Transformers have spurred extensive research into memory-efficient attention mechanisms to facilitate their application to longer sequences. FlashAttention (Dao et al., 2022) utilizes an online softmax computation technique (Milakov & Gimelshein, 2018) and reduces memory requirements of self-attention from O(n 2 ) to O(n) (Rabe & Staats, 2021) while preserving the accuracy. Other notable strategies include lowrank approximations (Katharopoulos et al., 2020;Wang et al., 2020), kernel-based methods (Kitaev et al., 2020;Lu et al., 2021;Xiong et al., 2021), and sparse attention mechanisms (Child et al., 2019), which approximate or selectively compute attention to minimize memory consumption. Furthermore, techniques that combine local and global contexts (Ainslie et al., 2020;Beltagy et al., 2020;Liu et al., 2021;Zaheer et al., 2020) enable superior performance on tasks involving long sequences or large-scale inputs while maintaining computational efficiency. Our work is inspired by FlashAttention and builds upon it by introducing hierarchical blockwise computation, leveraging the memory hierarchy in modern systems, which significantly enhances scalability and reduces memory requirements.",
            "score": 0.9461071049865798,
            "section_title": "Memory-efficient Transformer",
            "char_start_offset": 5461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 467,
                    "end": 495,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 557,
                    "end": 573,
                    "matchedPaperCorpusId": "239616022"
                },
                {
                    "start": 573,
                    "end": 592,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 837,
                    "end": 854,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 854,
                    "end": 874,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification [13]. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X [80] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the first model that can achieve better-than-chance performance on Path-256. \n\n\u2022 Benchmarking Attention. FlashAttention is up to 3\u00d7 faster than the standard attention implementation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of.",
            "score": 0.8565319215319852,
            "section_title": "Introduction",
            "char_start_offset": 5641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1145
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 370,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "233307400",
            "title": "When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting",
            "text": "Since the dense attention mechanism learns weights for all pairs of source and target words, its space complexity is O(n 2 ) in the source sequence length. Several sparse attention architectures have been proposed in literature to enable the translation of longer source sequences by making the space complexity O(n). \n\nChild et al. [Chi+19] proposed the Sparse Transformer architecture, which factorized the dense attention using p separate attention heads to learn only O(n\u2022 p \u221a n) weights. They showed that the resulting model could use larger context sizes and achieved significantly better results than Transformers on density modeling tasks. \n\nFollowing the success of Sparse Transformers, Beltagy, Peters, Cohan [BPC20] proposed the Longformer architecture, which reduced the number of attention weights to O(n) and achieved significantly better results than Transformers on multiple long document tasks including question answering, coreference resolution, and classification. \n\nFinally, Zaheer et al. [Zah+20] proposed the BigBird architecture. Like Longformers, BigBird also used O(n) weights. Unlike Longformers, BigBird has been shown to be a Turing-complete universal approximator. Although attention enabled the recollection of long-range memories, sparse attention made it computationally tractable to do so.",
            "score": 0.7632728442014963,
            "section_title": "Sparse attention",
            "char_start_offset": 4772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1323
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "276906703",
            "title": "Linear attention based spatiotemporal multi graph GCN for traffic flow prediction",
            "text": "There are two main strategies to mitigate the memory and computational complexity of transformers, which inherently scale quadratically with sequence length. The first strategy focuses on refining the Attention Mechanism 13 . A plethora of transformer derivatives, termed \"x-formers\" have been developed, employing tactics such as sparse attention [36][37][38] , full attention matrix approximation 19,39,40 , and chunked attention with gating 41 . Notably, recent innovations like FlashAttention 17 improve memory efficiency, but time complexity remains an issue, either remaining quadratic or influenced by hidden factors like chunk size. Conversely, RWKV 20 exemplifies superior space-time complexity trade-offs during inference by ingeniously casting linear attention as an RNN. \n\nThe second strategy involves supplanting the attention mechanism with alternative modules for scalability in long sequences. For instance, MLP-Mixer 42,43 advocates replacing attention with Multi-Layer Perceptrons in computer vision. Additionally, the Attention Free Transformer (AFT) 44 replaces dot-product self-attention with an efficient variant resembling multi-head attention in its feature dimension correspondence. Concurrently, RNN-based components have been adapted to extend context length, such as the Recurrent Memory Transformer 45 and Linear Recurrent Units 46 .",
            "score": 0.742423064366025,
            "section_title": "Optimizing attention mechanism",
            "char_start_offset": 5842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1362
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "245144820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims about training time and model accuracy, and report attention runtime and memory benchmarks. \n\n\u2022 Training Speed. FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and speeds up GPT-2 up to 3\u00d7 over HuggingFace [87] and 1.8\u00d7 over Megatron [77] over standard Transformers. \n\nFlashAttention speeds up the long-range arena (LRA) benchmark 2.4\u00d7. \u2022 Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAttention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two longdocument classification tasks. Finally, FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K). \n\n\u2022 Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3\u00d7 faster than standard attention for common seq. lengths (up to 2K). We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines. Additional experiment details are in Appendix E.",
            "score": 0.7206906295338221,
            "section_title": "Experiments",
            "char_start_offset": 18789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1116
                },
                {
                    "start": 1119,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1637
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "203642156"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5302734375
        },
        {
            "corpus_id": "259063695",
            "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
            "text": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.",
            "score": 0.7163339166427519,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "273323477",
            "title": "Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures",
            "text": "The transformer uses the concept of attention, which in loose terms can be thought of as similar to human attention; i.e., identifying parts of the input that are interesting and the connection between them. Early transformers, such as the original transformer (Vaswani et al., 2017), BERT (Kenton and Toutanova, 2019), and BART (Lewis et al., 2020) were severely limited by input lengths, often 512 or 1024 tokens, due to the O(n 2 ) time and space complexity required by the self-attention mechanism in the standard transformer. Several modifications to the transformer have been proposed to work around this, with one popular family of strategies using a sparse self-attention where many of the input pairs are selectively ignored. Some examples of this family include Big Bird (Zaheer et al., 2020), which uses a sliding window attention combined with randomly selected global attention; Longformer (Beltagy et al., 2020), which uses a sliding window attention and a dilated sliding window attention, along with some global attention tokens; and Smart Bird (Wu et al., 2021), which uses a low-dimension full attention transformer to determine which attentions to calculate in the full-depth transformer. A representation of these different sparsity patterns is depicted in Figure 2 for an original transformer, Longformer, Big Bird, and our modifications to Longformer. The diagrams are meant to be illustrative only and exact numbers of cells may not be proportional to the actual sparse attention matrices used in each case. \n\nOne area of concern with these sparse transformer methods, as highlighted by (Tay et al., 2020), is that they can lack long range context. The tasks laid out in (Tay et al., 2020) are varied and substantially different from summarization, but the concept is still similar; if relevant information is contained at a longer distance (or lags) in the input sequence than the sliding window, the model will not be able to compute the attention between those inputs. Other NLP specific tasks that require longer range context have also been proposed, such as LAMBADA, which tries to predict a word given context from sentences preceding the sentence missing the word.",
            "score": 0.7104210736410359,
            "section_title": "Introduction",
            "char_start_offset": 1768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 283,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 329,
                    "end": 349,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1610,
                    "end": 1628,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1694,
                    "end": 1712,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "271431900",
            "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
            "text": "Efficient Transformers Numerous of attempts have been made to make the training and serving of LLMs more efficient. FlashAttention family (Dao et al., 2022;Dao, 2023) are the most widelyadopted attention acceleration framework. Dao et al. (2022) breaks down the attention computation into smaller blockwise computation to reduce the IO between SRAM and HBM. Dao (2023) further improve the performance by reducing the non-matmul operations, extra parallelization over sequence length, and better warp organization. Our implementation is based on the FlashAttention kernel, and leverage its parallelization over the number of heads. \n\nSparse Self-Attention reduce the computational complexity of self-attention by only attending to a subset of tokens (Child et al., 2019;Katharopoulos et al., 2020;Kitaev et al., 2020;Zaheer et al., 2020;Beltagy et al., 2020). Child et al. (2019) factorized attention computation into local and stride patterns to reduce the computation. Ho et al. (2019) saves computation by applying multiple attentions along each axis of the input tensor. Beltagy et al. (2020) adds more sparsity on top of Child et al. (2019) by taking advantage of the receptive field accumulation with dilated sliding window. Zaheer et al. (2020) replaces the one-to-all attention in transformers with sliding window attention and random attention. Qiu et al. (2020) chunks the sequence into blocks, which reduces FLOPs by performing attention on a larger granularity. However, many of these methods can't bring wall-clock speed-up due to ignorance of realistic memory access cost (Dao et al., 2022). Meanwhile, we argue that some sparsity pattern are not inference-friendly for generative models due to inefficient KV caching. Also, as they underutilized the parallelization over head, there's actually great room to further reduce FLOPs while improving context coverage. We will discuss this further in the following sections.",
            "score": 0.7089022492747976,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 156,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 228,
                    "end": 245,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 796,
                    "end": 816,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 816,
                    "end": 836,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1230,
                    "end": 1250,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1353,
                    "end": 1370,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 1585,
                    "end": 1603,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482177734375
        },
        {
            "corpus_id": "257038940",
            "title": "Neural Attention Memory",
            "text": "Transformers' attention mechanism can be understood as a memory where all hidden activations are stored and selectively read. Since storing the entire sequence is inefficient, there have been studies to efficiently scale to longer context by adding memory architectures to Transformers. Transformer XL (Dai et al., 2019) has attempted to overcome the limited context length of Transformers by using segmentlevel recurrence and relative positional encoding. Compressive Transformer (Rae et al., 2019) and \u221e-former (Martins et al., 2022) further have extended the idea by compressing the recurrent segment into a smaller memory and using lessprecise unbounded continuous memory, respectively. While these works effectively address the scalability problem of Transformers, they have not extended their ideas to generic memory architecture that can be freely read and written.",
            "score": 0.7048972481879621,
            "section_title": "Transformer and Memory",
            "char_start_offset": 7943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 872
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 499,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 513,
                    "end": 535,
                    "matchedPaperCorpusId": "249350332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1619873046875
        },
        {
            "corpus_id": "264426102",
            "title": "REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization",
            "text": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, Zaheer et al. (2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware. Due to the capability to handle longer contexts, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization.",
            "score": 0.7039680529539772,
            "section_title": "A Model Detail",
            "char_start_offset": 27504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1058
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 307,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "265220849",
            "title": "Striped Attention: Faster Ring Attention for Causal Transformers",
            "text": "While the feedforward layers of transformers easily achieve high utilization on modern accelerators, attention has historically had lower device efficiency. Many approaches have attempted to close this gap. Perhaps the most common approach is the introduction of computational approximations of attention. While approaches such as Linformer (Wang et al., 2020) and the Sparse Transformer (Child et al., 2019) attempted to use approximate the computation of attention with linear and sparse equivalents, such approaches never achieved the popularity of the original attention implementation. (Rabe & Staats, 2021) proposed a memory efficient algorithm for self attention that reduced the memory requirement from O(n 2 ) to O(1), as well as a practical implementation that required O( \u221a n) memory. FlashAttention (Dao et al., 2022) provided an efficient implementation of this algorithm that leverage custom CUDA kernels and several optimizations accounting for the GPU memory hierarchy. FlashAttention2 (Dao, 2023) further optimized the parallelization of computations across the GPU process hierarchy, better dividing work between and within thread blocks.",
            "score": 0.7031280077676549,
            "section_title": "Efficient Attention Implementations",
            "char_start_offset": 25270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1156
                }
            ],
            "ref_mentions": [
                {
                    "start": 811,
                    "end": 829,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09600830078125
        },
        {
            "corpus_id": "238407884",
            "title": "ABC: Attention with Bounded-memory Control",
            "text": "Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.",
            "score": 0.6999479631353763,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.173095703125
        },
        {
            "corpus_id": "271162170",
            "title": "Inference Optimization of Foundation Models on AI Accelerators",
            "text": "Figure 2: Flash Attention by Dao et al. [24]. The outer loop iterates over K and V blocks and loads them to fast SRAM. In each block, inner loops iterates over Q blocks, loading them to SRAM, and writing the attention output back to HBM. \n\nModern LLMs have extended the support of context length from the order of thousands to millions within a few years from less than 1k (e.g., GPT-2 [13]) to 200k+ (e.g., Claude 3 [8]). The main challenge of expanding the context window lies in the extensive computational requirements and memory consumption for the attention computation. As the model considers more tokens simultaneously, the compute/time complexity and memory demands of calculations increase significantly, scaling quadratically with the size of the context window. FlashAttention [23,24] was introduced to address these challenges, which reformulates the attention computation as a sequence of matrix multiplications and applies block-sparse decomposition. By processing attention in smaller blocks, FlashAttention reduces the memory footprint of attention computation, avoiding the need to materialize the entire attention matrix in memory at once. The key advantage of FlashAttention is its ability to minimize data movement between different memory hierarchies. By carefully selecting the block size based on the memory hierarchy and capacity of the device, FlashAttention ensures that the data can be efficiently processed without requiring multiple transfers between memory levels. For example, on GPUs, the block size is typically small to fit within the L2 cache, minimizing expensive memory accesses. In contrast, devices like AWS Trainium or Google TPU, which have a large scratchpad memory in the tens of megabytes (MBs), can leverage larger block sizes to maximize computational efficiency by processing more data in parallel. \n\nFor large context, Blockwise Parallel Transformer (BPT) [72] further minimize memory consumption on feedforward network by computing them in a block-wise manner. Enhancing BPT, Ring Attention [73] utilizes blockwise computation for self-attention and feedforward processes to distribute extended sequences across multiple devices by dividing the input text into smaller, more manageable blocks.",
            "score": 0.6959656211243852,
            "section_title": "Efficient Attention Computation",
            "char_start_offset": 14618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 793,
                    "end": 796,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1905,
                    "end": 1909,
                    "matchedPaperCorpusId": "266351737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435302734375
        },
        {
            "corpus_id": "276575796",
            "title": "GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices",
            "text": "Transformers have become the backbone of many GenAI models, but their multi-head self-attention mechanism can dominate runtime and memory usage. Therefore, researchers have explored a range of strategies to optimize attention on hardware and algorithmic levels. \n\nHardware-based. FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations. \n\nAlgorithmic-based. Work on sparse attention reduces the quadratic complexity of self-attention by ignoring parts of the input that do not affect the result significantly. Child et al. (Child et al. 2019) pioneered this approach by limiting attention to strided patterns using sparse factorizations of the attention matrix to reduce computation cost while maintaining performance on sequence models. Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns.",
            "score": 0.6942539759035261,
            "section_title": "Attention Optimization",
            "char_start_offset": 22202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 2166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "273821735",
            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
            "text": "The success of Transformers, coupled with their limitations, led to research into more efficient attention mechanisms. These mechanisms aim to reduce the quadratic complexity of standard self-attention while retaining its ability to model long-range dependencies. Some notable examples include: \n\n\u2022 Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers [Katharopoulos et al., 2020] reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird [Zaheer et al., 2020] combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices. \n\nThese approaches aim to maintain the strong performance of Transformers while reducing their computational and memory requirements, especially for long sequences.",
            "score": 0.688227297368159,
            "section_title": "Efficient Attention Mechanisms",
            "char_start_offset": 4184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 755,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1120,
                    "end": 1141,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "276961696",
            "title": "Radar: Fast Long-Context Decoding for Any Transformer",
            "text": "Context reduction. Previous literature shows that Transformers with sparse attention patterns can generate with fewer context tokens while not sacrificing the generation quality (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Roy et al., 2020;Chen et al., 2021). In addition to the success of sparse attention architectures, researchers also empirically demonstrate that the vanilla Transformers do not need all tokens in the context to correctly generate the current token (O'Connor & Andreas, 2021;Sun et al., 2021;Liu et al., 2024). Our work entails this discovery. \n\nBased on the discovery, there are some closely related methods proposed Han et al. (2023). For instance, StreamingLLM (Xiao et al., 2024) uses only the local sliding window along with a few beginning tokens to generate the current token. H 2 O (Zhang et al., 2023) proposes to additionally retain middle tokens based on a scoring function to maximize hit rate. SnapKV (Li et al., 2024) proposes to retain middle tokens based on the attention pooling in a parallel manner. All these methods, albeit reducing the context length, cannot recover the lost information once the context token is evicted. Moreover, they still suffer from the quadratic time complexity. By contrast, our method is able to dynamically retrieve any tokens in the context with a low time complexity. Recently, Zandieh et al. (2024) propose to compress the KV cache for memory and time efficiency, posing an opportunity for better algorithm designs. \n\nEfficient attention. The quadratic computing time of the dot-product attention becomes a bottleneck in long-context generation. Searching for an efficient design of the attention mechanisms is a long-standing topic for Transformers (Wang et al., 2020;Kitaev et al., 2020). Recently, Landmark Attention (Mohtashami & Jaggi, 2023) proposes to use grouped softmax with a special token to reduce computation.",
            "score": 0.6865106473628043,
            "section_title": "RELATED WORK",
            "char_start_offset": 19516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 513,
                    "matchedPaperCorpusId": "235446334"
                },
                {
                    "start": 513,
                    "end": 530,
                    "matchedPaperCorpusId": "237572264"
                },
                {
                    "start": 656,
                    "end": 673,
                    "matchedPaperCorpusId": "263831240"
                },
                {
                    "start": 702,
                    "end": 721,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 952,
                    "end": 969,
                    "matchedPaperCorpusId": "269303164"
                },
                {
                    "start": 1758,
                    "end": 1778,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4208984375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Transformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.",
            "score": 0.6851752688225068,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 110,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 110,
                    "end": 112,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "237260635"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 300,
                    "end": 302,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "231939146"
                },
                {
                    "start": 940,
                    "end": 943,
                    "matchedPaperCorpusId": "237260635"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 946,
                    "end": 948,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1334,
                    "end": 1337,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 2119,
                    "end": 2123,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51708984375
        },
        {
            "corpus_id": "269982604",
            "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
            "text": "While we cover a comparison of rather seminal works in this paper that we believe are representative of the efficient transformer space, there are a massive amount of efficient transformer designs (Tay et al., 2022) such that it would be impractical to compare our proposed approach to them all.Just for sparsity-focused transformers alone, there are numerous options with varying degrees of complexity.Sparse Transformer (Child et al., 2019), for example, represents an initial attempt at reducing complexity in this manner alongside its many descendants (Parmar et al., 2018;Zhong et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;Tay et al., 2020) eventually culminating in the proposal of BigBird (Zaheer et al., 2020), although new sparsity-focused methods are still likely being developed.\n\nAlong the lines of sparse transformer development, other classes of efficient transformers exhibit similar progressions.\n\nRegarding softmax approximations with random/sampling-based methods, Random Fourier Attention (Peng et al., 2021) is an alternative to Performer (Choromanski et al., 2020).Skyformer (Chen et al., 2021) was preceded by Nystromformer (Xiong et al., 2021).TransNormer (Qin et al., 2022a) is a more recent approach that combines blocked local attention with kernalized linear attention, across two disparate sets of transformer layers (e.g.layers 1-4 might contain one type, layers 5-12 might contain another), although that makes it slightly architecture reliant.",
            "score": 0.6843311354026962,
            "section_title": "A.5. Vast Efficient Transformer Design Space",
            "char_start_offset": 33103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 800
                },
                {
                    "start": 802,
                    "end": 922
                },
                {
                    "start": 924,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1484
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 215,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 577,
                    "end": 596,
                    "matchedPaperCorpusId": "53108335"
                },
                {
                    "start": 1106,
                    "end": 1125,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 1189,
                    "end": 1207,
                    "matchedPaperCorpusId": "252992749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1378173828125
        },
        {
            "corpus_id": "270370979",
            "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
            "text": "The primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements.This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models.Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to O(n).These models achieve a balance between maintaining sufficient context for understanding while managing computational load.For achieving complexity of O(n log n), several approaches have been proposed.Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window.Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity.LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead.\n\nEquipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token.This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts.Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited.This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications.",
            "score": 0.6746960254522978,
            "section_title": "Long-context Transformers",
            "char_start_offset": 7163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1242
                },
                {
                    "start": 1244,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "matchedPaperCorpusId": "253157377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "259936734",
            "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
            "text": "Scaling up the context length of Transformers [18] is a challenge, since the attention layer at their heart has runtime and memory requirements quadratic in the input sequence length. Ideally, we would like to go beyond the standard 2k sequence length limit to train models to understand books, high resolution images, and long-form videos. Just within the last year, there have been several language models with much longer context than before: GPT-4 [12] with context length 32k, MosaicML's MPT with context length 65k, and Anthropic's Claude with context length 100k. Emerging use cases such as long document querying and story writing have demonstrated a need for models with such long context. \n\nTo reduce the computational requirement of attention on such long context, there have been numerous methods proposed to approximate attention [2,3,4,8,9,14,19,20]. Though these methods have seen some use cases, as far as we know, most large-scale training runs still use standard attention. Motivated by this, Dao et al. [5] proposed to reorder the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. This yields 2-4\u00d7 wall-clock time speedup over optimized baselines, up to 10-20\u00d7 memory saving, with no approximation, and as a result FlashAttention has seen wide adoption in large-scale training and inference of Transformers. \n\nHowever, context length increases even more, FlashAttention is still not nearly as efficient as other primitives such as matrix-multiply (GEMM). In particular, while FlashAttention is already 2-4\u00d7 faster than a standard attention implementation, the forward pass only reaches 30-50% of the theoretical maximum FLOPs/s of the device (Fig. 5), while the backward pass is even more challenging, reaching only 25-35% of maximum throughput on A100 GPU (Fig. 6). In contrast, optimized GEMM can reach up to 80-90% of the theoretical maximum device throughput.",
            "score": 0.6734934102387016,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 846,
                    "end": 848,
                    "matchedPaperCorpusId": "248498407"
                },
                {
                    "start": 848,
                    "end": 850,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 850,
                    "end": 852,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 852,
                    "end": 854,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "229924221",
            "title": "Shortformer: Better Language Modeling using Shorter Inputs",
            "text": "Many recent papers have explored how to improve the efficiency of transformers by reducing the quadratic cost of self-attention, motivated by scaling to longer sequences (Kitaev et al., 2020;Roy et al., 2020;Tay et al., 2020). We show that longer sequences can be harmful and instead demonstrate improved results with shorter sequences, which naturally also improves efficiency.\n\nOne way to reduce the memory consumption of a transformer model is to sparsify the attention matrix by letting each token attend only to a subset of the nearby tokens Beltagy et al., 2020). Our approach, training on shorter subsequence lengths, is much more efficient since we do not sparsify an attention matrix: we use multiple, but much smaller, attention matrices, which are dense. Since attention uses memory and computation in a way that scales quadratically with input size, by splitting the inputs into multiple subsequences that are each processed independently, we use much less memory and run faster. The model of Beltagy et al. (2020) uses different, sparse attention patterns at every layer, with the number of neighboring tokens that each tokens attends to growing in each subsequent layers. In our method, we let each layer attend to the same number of tokens at every stage. Similar to our method, Beltagy et al. (2020) let each token attend a growing number of neighbors as training progresses, but they use five stages, which we found not to be superior to the two stages in our method.\n\nThe adaptive attention span model of Sukhbaatar et al. (2019) learns the maximum effective context window sizes for each head at each layer inde-pendently. Like our method, the context window sizes are smaller at the beginning of training and get longer as training progresses. We show that a simple approach of manually choosing two subsequence lengths is highly effective. In addition, keeping subsequence lengths equal across all heads and layers lets us save memory and runtime.",
            "score": 0.6627027194946694,
            "section_title": "Staged Training",
            "char_start_offset": 27425,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3037109375
        },
        {
            "corpus_id": "271097926",
            "title": "HDT: Hierarchical Document Transformer",
            "text": "HDT resolves these problems by simultaneous contextualization over all hierarchy levels in a single layer using an efficient hierarchical attention pattern.\n\nDue to the quadratic memory and time complexity of the self-attention block of Transformer models (Vaswani et al., 2017), major efforts have been devoted to reducing complexity, in particular when applying attention to longer sequences.One branch of work focuses on using sparse attention patterns to reduce computation while maintaining expressiveness including Sparse Transformer (Rewon et al., 2019), ETC (Ainslie et al., 2020), BigBird (Zaheer et al., 2020), Longformer (Beltagy et al., 2020) and CoLT5 (Ainslie et al., 2023) among others.Another branch of work aims at speeding up attention computation by considering GPU hardware characteristics.FlashAttention 1 & 2 (Dao et al., 2022;Dao, 2023) propose tiling with block-wise attention to reduce memory IOs and compute attention blocks in parallel in Static Random Access Memory (SRAM).Matteo et al. (2023) extends the idea of FlashAttention to key/query dropping and hashing-based attention that was originally proposed by Kitaev et al. (2020).Our work builds upon both branches: We propose a dynamic sample-dependent sparse attention pattern which exploits the structure of text documents.For efficiency, we follow Dao et al. (2022) and implement this pattern as a customized memory-aware kernel.",
            "score": 0.6618156356048639,
            "section_title": "Related Work",
            "char_start_offset": 5946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 158,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1413
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 278,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 566,
                    "end": 587,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 831,
                    "end": 849,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1139,
                    "end": 1159,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1332,
                    "end": 1349,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "248427085",
            "title": "Attention mechanism in neural networks: where it comes and where it goes",
            "text": "The attention scheme is determined by considering only queries and keys from the same cluster. Thus, queries are routed to keys belonging to the same cluster [175].\n\nSparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight [177]. By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods [178]. Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method [179]. A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear [180]. Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering.\n\nMusic Transformer [181] shows that self-attention can also be useful for modeling music. This study emphasizes the infeasibility of the relative position representations introduced by [103] for long sequences because of the quadratic intermediate relative information in the sequence length. Therefore, this study presents an extended version of relative attention named relative local attention that improves the relative attention for longer musical compositions by reducing its intermediate memory requirement to linear in the sequence length. A softmax-free Transformer (SOFT) is presented to improve the computational efficiency of ViT. It uses Gaussian kernel function instead of the dot-product similarity [182].\n\nAdditionally, various approaches have been presented in Hierarchical Visual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Perceiver [185], and Performer [186]. Image Transformer based on the crosscovariance matrix between keys and queries is applied [187], and a new vision Transformer is proposed [188]. Furthermore, a Bernoulli sampling attention mechanism",
            "score": 0.655982017049573,
            "section_title": "What about complexity?",
            "char_start_offset": 34548,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 163,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 748,
                    "end": 753,
                    "matchedPaperCorpusId": "202538495"
                },
                {
                    "start": 962,
                    "end": 967,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 1045,
                    "end": 1050,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 1173,
                    "end": 1178,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1323,
                    "end": 1328,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 2018,
                    "end": 2023,
                    "matchedPaperCorpusId": "239616022"
                },
                {
                    "start": 2114,
                    "end": 2119,
                    "matchedPaperCorpusId": "232290833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "278237209",
            "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing",
            "text": "The quadratic cost of attention in the 2017 transformer model [1] has led to a wide body of research on efficient attention variants [49,59]. Popular alternatives are different linear attention variants that typically use a fixed vector or matrix memory and update it recurrently. The 1992 unnormalised linear Transformers [16,14,15] trade performance for better computational efficiency. State space models [6,7,8] are popular alternatives that offer efficient, parallel training while keeping linear cost and efficient inference. The parallel training requirement forces only a linear recurrent relation between the timesteps. A common characteristic of such models is the relatively small, fixed memory that \u2021 See: https://github.com/lucidrains/routing-transformer?tab=readme-ov-file#issues requires extreme compression. Despite recent progress, these models still underperform quadratic attention on many benchmarks [21,22]. \n\nSparse attention methods aim to mitigate the quadratic cost of full attention by computing attention scores for only a subset of token pairs rather than the full attention matrix. These methods typically employ various heuristics to strategically identify which tokens and token relationships are the most important to process. This is often done by introducing special tokens that serve as higher-level representations of entire chunks of tokens, or by assuming emergent hierarchical structures within the attention patterns. For example, SepLLM [60] uses separators in the sentence as special tokens that sparse attention focuses on. Sparse Transformer [18] uses static attention patterns to reduce computational complexity. Longformer [20] combines sliding window attention with additionally selected tokens globally available. BigBird [19] combines sliding window attention and global attention on selected tokens, while additionally including randomly selected tokens in the attention. \n\nStreaming LLM [48] discovers and preserves attention sinks as a necessary component despite their inefficiency and combines them with sliding window attention. Some methods [32,33,34] focus on post-training attention reduction, motivated by KV-cache reduction. Hash Attention [61] uses top-k selection in the attention scores to induce sparsity and improve efficiency.",
            "score": 0.6552657467939811,
            "section_title": "Related Work",
            "char_start_offset": 37484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 330,
                    "end": 333,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 408,
                    "end": 411,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 411,
                    "end": 413,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 920,
                    "end": 924,
                    "matchedPaperCorpusId": "266149332"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "267406617"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1938,
                    "end": 1942,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 2097,
                    "end": 2101,
                    "matchedPaperCorpusId": "258947558"
                },
                {
                    "start": 2101,
                    "end": 2104,
                    "matchedPaperCorpusId": "269303164"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "258832459",
            "title": "RWKV: Reinventing RNNs for the Transformer Era",
            "text": "Recently, a number of techniques have been proposed to address the limitations of transformers. \n\nOptimizing Attention Mechanism Many transformer variants (\"x-formers\") have been introduced to reduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;Kitaev et al., 2020;Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020;Ma et al., 2021;Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other efficient methods (Katharopoulos et al., 2020;Jaegle et al., 2021). Some recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022;Jang et al., 2019) share similarities with RWKV's chunked computation scheme. Despite being memory-efficient, their time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV achieves better space and time complexity during inference by formulating a linear attention as an RNN. \n\nAttention Free Models Another line of research replaces the attention mechanism with other modules to scale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021;Liu et al., 2021) propose replacing attention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer (AFT) (Zhai et al., 2021) and HrrFormer (Alam et al., 2023) replaces dot-product self-attention with a computationally efficient alternative. None of these models have been successfully scaled to the point where drawing comparisons with transformer-based large language models makes sense. \n\nThere has also been substantial research into state space models (SSM) (Gu et al., 2021) and its variants (Dao et al., 2022b;Gupta et al., 2022;Poli et al., 2023). In contrast to the preceding models, SSM and its successors have shown substantial progress towards efficient scaling.",
            "score": 0.6548453375621213,
            "section_title": "C Additional Related Work",
            "char_start_offset": 24771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 398,
                    "end": 414,
                    "matchedPaperCorpusId": "235313355"
                },
                {
                    "start": 481,
                    "end": 498,
                    "matchedPaperCorpusId": "252439127"
                },
                {
                    "start": 527,
                    "end": 555,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 555,
                    "end": 575,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 669,
                    "end": 687,
                    "matchedPaperCorpusId": "189818910"
                },
                {
                    "start": 1713,
                    "end": 1732,
                    "matchedPaperCorpusId": "247762199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.448486328125
        },
        {
            "corpus_id": "258762176",
            "title": "ORKG-Leaderboards: a systematic workflow for mining leaderboards as a knowledge graph",
            "text": "BigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle [16]. BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) [16].",
            "score": 0.6543811496150708,
            "section_title": "BigBird",
            "char_start_offset": 29760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 792
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "251442728",
            "title": "Investigating Efficiently Extending Transformers for Long Input Summarization",
            "text": "We first investigate whether using an efficient Transformer encoder allows models to incorporate longer input sequences while consuming reasonable amounts of device memory. We consider two encoder architectures that exemplify different approaches to efficient attention. Big Bird (Zaheer et al., 2020) uses sparse attention computation, combining sliding-window and random attention, and a set of global-attention tokens. Conversely, Performer (Choromanski et al., 2021) factorizes attention matrices via orthogonal random features. \n\nBoth model also performed well on the LRA tasks (Tay et al., 2021). For this experiment, we perform both pretraining and fine-tuning with the same encoder architecture to avoid the issue of mismatch between pretraining and fine-tuning architectures. In addition, we also introduce two simple variants of local attention Transformer encoders. First, we use a simple block-local Transformer (Local), where encoder input tokens are divided into nonoverlapping blocks, and tokens can only attend to other tokens within the block. Second, we extend the local Transformer by adding a set of global tokens with learned embeddings, that can attend to and be attended from every encoder token (Global-Local). These components are similar to the sliding window attention and global token attention of Big Bird, ETC (Ainslie et al., 2020) and Longformer (Beltagy et al., 2020). However, we opt for the simpler block-local attention rather than sliding window attention, and compensate for the lack of overlapping blocks by staggering the local attention blocks, which we elaborate on in Section 3.2. As we show below, the performance is highly competitive despite its simplicity. \n\nResults on short and long summarization tasks are shown in Table 1, with the relative training steps per second and memory consumed per device for fine-tuning on arXiv shown in the right-most columns. Among the short tasks, the full-attention Transformer performs best, followed by BigBird. On the long tasks, Big Bird and Global-Local models perform best, but Big Bird consumes significantly more memory and trains much more slowly than the other architectures.",
            "score": 0.6507857378610309,
            "section_title": "Encoder architectures",
            "char_start_offset": 5445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 301,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 444,
                    "end": 470,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "237260051",
            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
            "text": "Since the vanilla Transformer is inefficient in processing long sequences, many methods explore to improve the efficiency of Transformer in different ways (Tay et al., 2020). One direction is computing a sparse attention matrix rather than a dense one by only computing attention on a sparse number of query and key vector pairs (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Zhang et al., 2021). For example, Sparse Transformer (Child et al., 2019) is a Transformer variant that combines local self-attention and stride attention. It uses half of the attention heads to attend tokens within a region and the rest attention heads to attend tokens with certain strides. Longformer (Beltagy et al., 2020) combines sliding window attention and global attention at certain positions to capture local and global contexts, respectively. BigBird (Zaheer et al., 2020) further introduces a random attention mechanism that attends several randomly selected pairs of tokens. However, the token pairs that are randomly sampled or selected by fixed rules may not be helpful for context modeling, which limits the performance of these methods. There are also many other ways to accelerate Transformers (Kitaev et al., 2020;Wang et al., 2020b). For example, Reformer (Kitaev et al., 2020) uses hashing techniques to cluster input embeddings into different buckets based on their similarities, and then chunks the buckets using a certain length. The tokens only attend to same bucket in their own chunk and previous chunk. Linformer (Wang et al., 2020b) assumes that the selfattention matrix is low-rank, and it approximates the self-attention mechanism by using low-rank attention key and value projected by separate linear transformations. Linear Transformer (Katharopoulos et al., 2020) uses kernel functions to approximate the self-attention mechanism. It derives a kernel-based formulation of self-attention based on the matrix multiplication associative property and designs a simple kernel function to approximate the computation. However, these methods do not fully consider the characteristics of natural language and may be suboptimal in text understanding.",
            "score": 0.6491906287120446,
            "section_title": "Related Work",
            "char_start_offset": 4611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 409,
                    "matchedPaperCorpusId": "234339280"
                },
                {
                    "start": 1760,
                    "end": 1788,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "Our block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23,38,39,55,76]. For model training, the lottery tickets hypothesis [28,29,30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterfly pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks. \n\nEfficient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [22] and computer vision [24,91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12,54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52,57,79,89]. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details. \n\nThere are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31,36,37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models.",
            "score": 0.6475957005448896,
            "section_title": "A Related Work",
            "char_start_offset": 28686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "38486148"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "231719476"
                },
                {
                    "start": 1132,
                    "end": 1136,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "222290917"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "248498407"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "235743105"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1496,
                    "end": 1499,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 1499,
                    "end": 1502,
                    "matchedPaperCorpusId": "59310641"
                },
                {
                    "start": 1615,
                    "end": 1619,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1648,
                    "end": 1652,
                    "matchedPaperCorpusId": "207930593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "221140187",
            "title": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size",
            "text": "Recently, many methods have been proposed which lower the memory footprint or computation time of transformer language models, or allow them to be used on larger contexts. The Transformer-XL (Dai et al., 2019) allows a position within an attention window to attend to tokens from the previous windows by introducing relative position embeddings. While that mechanism like ours, allows information to flow between windows of text, existing BERT and GPT-2 models do not use relative position embeddings, so training from scratch would be necessary to take advantage of this architecture. \n\nOther methods modify the attention function to reduce the quadratic memory footprint down to a manageable amount. Child et al. (2019) modify the transformer architecture to replace the standard attention with a sparse one. Qiu et al. (2019) enforce a block-sparse structure on the attention matrix. Kitaev et al. (2019) also introduce sparsity, but instead do so by using locality sensitive hashing to select positions over which a full attention is computed, reducing the memory cost from quadratic to O(T log T ) for an input of size T . Rae et al. ( 2019) introduce a memory compression technique that allows much longer contexts to be attended to in memory. Beltagy et al. (2020) replace the standard attention with a combination of dilated sliding windows, and global attention from selected tokens that. Sukhbaatar et al. (2019) learn a masking function such that not all tokens attend to every previous position. Tay et al. (2020) learn synthetic attention weights, removing the need for token-token interactions. Wu et al. (2019) replace the full self-attention with a dynamic convolution depending only on the current timestep, yielding a linear dependence on length instead of a quadratic dependence. Figure 1: Augmenting a pretrained transformer with a small recurrence module, allowing reduction of attention computation as well as simpler processing of longer contexts. \n\nWhile the above methods all allow for a reduction in required computational resources, they also all require one to train a model from scratch. Our method's goal is to allow more efficient and powerful use of the wide array of existing pre-trained models that cover many domains.",
            "score": 0.6474354262609757,
            "section_title": "Related Work",
            "char_start_offset": 3348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 887,
                    "end": 907,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1398,
                    "end": 1422,
                    "matchedPaperCorpusId": "159041867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "245117808",
            "title": "Couplformer: Rethinking Vision Transformer with Coupling Attention Map",
            "text": "Depending on the self-attention mechanism, the Transformer model has already become prevalent in many fields. As described above, the standard self-attention operation relies on dot-production multiplication. Sequentially, this leads to the problem that self-attention is the quadratic time and memory complexity [38]. Moreover, when the input sequence length grows, the poorly scales become the bottleneck in Transformer. The reason is that the dot product between the feature matrix Q and matrix K generates a massive matrix to present the token-token interaction. In such a situation, it is unavoidable to exist exhaustive and redundant computation in the standard self-attention operation. \n\nIn order to address this problem, the researchers proposed several novel Transformer architectures to improve the original self-attention mechanism. Because all of them attempt to reduce the computational costs to let the model to perform efficiently, these models are named \"efficient Transformers\" [38]. The complexity of efficient Transformers are listed in Tab. 1. These approaches could be divided into three categories. \n\nThe first solution is to sparsify the self-attention layers. According to the sparse attention, the attention map can only be computed by limited pairs in a particular predefined manner. For example, By limiting and fixing the field of view, [33] and [32] employed the fixed block local attention patterns to constrain the pair for the score. Similarly, [7] and [2] leverage fixed strided attention patterns to achieve the cost reduction. Kitaev et al. [29] proposed a learnable approach by using hash-based similarity to replace the token-to-token interaction. Moreover, Zaheer et al. [51] proposed their sparsity pattern, which is a universal approximation and Turing completeness.",
            "score": 0.6469869938921705,
            "section_title": "Efficient Transformers",
            "char_start_offset": 6905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2490234375
        },
        {
            "corpus_id": "263608118",
            "title": "SEA: Sparse Linear Attention with Estimated Attention Mask",
            "text": "A.7 DISCUSSION ABOUT DIFFERENCE BETWEEN SEA AND FLASHATTENTION Fig. 8, we show two quadratic attention baselines, vanilla, and FlashAttention (Dao et al., 2022). While FlashAttention consumes memory linearly, it scales quadratically in terms of computation complexity. FlashAttention is an efficient implementation of original quadratic attention that eliminates most of the memory space and bandwidth requirement of the attention mechanism by fusing attention probability calculation and context vector calculation. FlashAttention has linear memory complexity and quadratic time complexity because it does not require space for storing attention score matrix to compute softmax probability. The lack of attention probability storage is an advantage in terms of memory bandwidth consumption. However, this characteristic may be a downside when attention probability is required in usage scenarios where we are concerned, such as token importance analysis for token compression. Therefore, FlashAttention has some of the same limitations as previous linear attention methods. However, our SEA attention does not have such limitations by providing an estimated attention matrix, while showing linear complexity in both memory and computation. \n\nA.8 EVALUATION OF SEA ON LONGER CONTEXT MODEL We evaluate our method with a longer context on Wikitext2. However, OPT only supports context lengths up to 2048, therefore we used positional embedding interpolation that was introduced in previous work (Dosovitskiy et al., 2021), using bilinear interpolation. After interpolating the positional embedding, we perform a few optimization steps (750 steps, 1.5M tokens) on the model with the causal language modeling task loss. In longer context length, 4096, our method outperforms quadratic attention in both latency and memory cost. Also, the experiment result shows our model is much stronger than baselines after positional embedding interpolation. This result is interesting, as we think this shows that sparse attention masking helps to preserve the important attention relations by masking out non-important attention relations. We picked the SEA-OPT result from Table A.5, and details are following. In Table A.5, we show the performance trade of the longer context model using our post-training compression techniques: query skipping and dynamic k control.",
            "score": 0.6457567294578586,
            "section_title": "A.6 FLOPS COMPARISON",
            "char_start_offset": 35469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 160,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1493,
                    "end": 1519,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5625
        },
        {
            "corpus_id": "259858862",
            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
            "text": "Recently, Transformer (Vaswani et al., 2017) and its variants (Devlin et al., 2019;Radford et al., 2018;Liu et al., 2019;Yang et al., 2019) have been widely used in natural language processing (OpenAI, 2023;Zhang et al., 2023;OpenAI, 2022;Zhang and Yang, 2021a;Zhang, 2020;Zhang and Wang, 2020;Zhang, 2019), computer vision (Dosovitskiy et al., 2021;Zhu et al., 2021;Touvron et al., 2021;Zhang and Yang, 2021b), speech (Dong et al., 2018;Li et al., 2019;Zhang et al., 2020) and other domains. To improve computational and memory efficiency, a dizzying number of efficient Transformers (Child et al., 2019;Ho et al., 2019;Rae et al., 2020;Zhao et al., 2019;Kitaev et al., 2020;Tay et al., 2020;Beltagy et al., 2020;Choromanski et al., 2020;Wang et al., 2020;Zaheer et al., 2020;Roy et al., 2021;Xiong et al., 2021;Tay et al., 2021a;Ma et al., 2021;Chen, 2021;Zhu and Soricut, 2021;Liu et al., 2022) have been proposed recently, which can be roughly divided into two directions: sparse attention, low-rank and kernel methods. \n\nSparse attention methods usually limit the field of view to fixed or random patterns. These patterns can also be used in combination. For example, Sparse Transformer (Child et al., 2019) combines stride and fixed factorized attention by assigning half of its heads to the pattern for reducing the complexity of a traditional Transformer. Longformer (Beltagy et al., 2020) integrates a windowed localcontext self-attention and task-oriented global attention that encodes inductive bias about the corresponding task. BigBird (Zaheer et al., 2020) incorporates random attention besides global attention and local window attention. Random attention means that each query attends to a small number of random keys.",
            "score": 0.6457187720073266,
            "section_title": "Related Work",
            "char_start_offset": 4105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 44,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 62,
                    "end": 83,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 121,
                    "end": 139,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 239,
                    "end": 261,
                    "matchedPaperCorpusId": "240230618"
                },
                {
                    "start": 261,
                    "end": 273,
                    "matchedPaperCorpusId": "213300129"
                },
                {
                    "start": 273,
                    "end": 294,
                    "matchedPaperCorpusId": "219182274"
                },
                {
                    "start": 367,
                    "end": 388,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 388,
                    "end": 410,
                    "matchedPaperCorpusId": "239011964"
                },
                {
                    "start": 438,
                    "end": 454,
                    "matchedPaperCorpusId": "59413863"
                },
                {
                    "start": 621,
                    "end": 638,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 656,
                    "end": 676,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 676,
                    "end": 693,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 777,
                    "end": 794,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 794,
                    "end": 813,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 813,
                    "end": 831,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 831,
                    "end": 847,
                    "matchedPaperCorpusId": "235313355"
                },
                {
                    "start": 847,
                    "end": 858,
                    "matchedPaperCorpusId": "237421288"
                },
                {
                    "start": 858,
                    "end": 880,
                    "matchedPaperCorpusId": "236428421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "273850602",
            "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
            "text": "Transformer models have become the dominant neural architecture across language, vision, and other domains [Vas17, DBK + 20]. However, scaling them to handle larger input sequences remains a significant challenge [TDA + 20], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [KRA + 21] and time-series forecasting [WZZ + 22, ZCZX23, ZZP + 21]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this \"compute divide\" is vital for democratizing AI [AW20]. \n\nEfficient computation of self-attention has been a focal point of research in recent years [FCA23]. Flash Attention [DFE + 22] and related work [SY24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF24], or adaptive techniques [CNM19]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [WLK + 20, TBM + 21, XZC + 21, KVPF20, CLD + 20]. On the negative side, recent finegrained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [KWH23,AS24a]. \n\nIn this work, we focus on sparse attention methods where each token vector q i \u2208 R d attends to the k tokens k j \u2208 R d with the largest inner products q T i k j [GDG + 21, WWW + 22], a paradigm we refer to as kNN Attention.",
            "score": 0.6443264249266176,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 827,
                    "end": 834,
                    "matchedPaperCorpusId": "232380042"
                },
                {
                    "start": 1692,
                    "end": 1699,
                    "matchedPaperCorpusId": "252198880"
                },
                {
                    "start": 1699,
                    "end": 1705,
                    "matchedPaperCorpusId": "257219595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "269009907",
            "title": "Softmax Attention with Constant Cost per Token",
            "text": "The conventional attention mechanism of Transformers (Vaswani et al., 2017) has become the predominant method for capturing sequential dependencies, but its cost is quadratic in sequence length n, because it applies a Softmax function over the rows of an n \u00d7 n matrix of scaled dotproducts.FlashAttention (Dao et al., 2022) reduces memory use from quadratic to linear by computing, normalizing, and reducing the matrix in an incremental fashion, i.e., without ever storing it in full, but the compute cost remains quadratic.\n\nNumerous alternatives to conventional attention have been proposed to reduce its quadratic cost, including linearized, low-rank, and sparse approximations, mechanisms that slide context windows, and convolutions in the frequency domain.We cannot summarize all proposals fairly in the space of a paragraph.Notable examples include the methods proposed by Child et al. (2019), Wang et al. (2020), Kitaev et al. (2020), Katharopoulos et al. (2020), Zhai et al. (2021), Roy et al. (2020), Lee-Thorp et al. (2021), andPoli et al. (2023).More recently, generalized state space models that build on previous research (Martin and Cundy, 2017) (Gu et al., 2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state (Peng et al., 2023) (Gu and Dao, 2023) (Katsch, 2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention (Jelassi et al., 2024).",
            "score": 0.6440797412928102,
            "section_title": "Summary",
            "char_start_offset": 10,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 524
                },
                {
                    "start": 526,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1588
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258544921875
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "It combines local windowed attention with task-motivated global attention. Local attention is primarily used to build contextual representations, while global attention allows Longformer to create full sequence representations for prediction [116]. In standard transformers, the self-attention mechanism considers interactions between all pairs of positions in the input sequence, leading to quadratic complexity. \n\nSparse Transformers. The standard transformer's attention mechanism calculates attention scores for all pairs of positions in a sequence, leading to quadratic time complexity [18]. Sparse Transformers address this issue by considering only a subset of positions during attention computation [117]. This introduces sparsity, significantly reducing memory requirements and computational load, making them suitable for longer sequences [117]. As shown in Equation 27, the sparse Transformer is a modified version of the standard attention mechanism used in transformers [117]. Sparse Transformer's attention, given a sequence of input embeddings X with dimensions N \u00d7 d, where N is the sequence length and d the embedding dimension, the attention scores for position i attending to position j can be computed as shown in Equation 27, where Sp represents the sparse attention, Q i represents the query vector for position i, K j denotes the key vector for position j, Q i K T j represents the dot product of query and the key vectors, capturing the pairwise interactions between positions in the input sequence, V j represents the value vector for position j, and M ij denotes a binary mask element indicating whether vector position i attends to vector position j. This demonstrates that the attention mechanism is computed for each pair of positions i and j based on their corresponding query, key, and value vectors. In global sparse attention, the mask M ij is generated by randomly selecting a fixed number of positions for each position i to attend to. This introduces sparsity by limiting the attention to a small subset of positions in the sequence [117]. However, for local sparse attention, the mask M ij ensures that each position attends to a nearby local neighborhood. This reduces the computational complexity associated with attending to all positions and helps capture short-range dependencies efficiently [117]. The division by \u221a d k serves as a scaling factor for numerical stability, where d k represents the dimensionality of the key vectors.",
            "score": 0.6427710298835392,
            "section_title": "F. Long Sequence Language Models",
            "char_start_offset": 64149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2340
                },
                {
                    "start": 2341,
                    "end": 2474
                }
            ],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "269741084",
            "title": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing",
            "text": "Since one of the bottlenecks of transformers is the quadratic computational complexity of selfattention, a natural approach is sparsifying attention computation. A naive sparse attention pattern is the sliding window attention (Kovaleva et al., 2019), where each token attends to neighbors within a local window. However, this neglects longrange interaction between words. Existing works such as Longformer (Beltagy et al., 2020) and Poolingformer (Zhang et al., 2021) extend the sliding window attention by adding global attending tokens and applying pooling to expand the receptive field area. Unlimiformer (Bertsch et al., 2023) adopts the retrieval-augmented generative method by searching the top K most important tokens for the incoming sequence. It then applies attention to just those tokens in the decoders, resulting in pruned computations with minor losses. Nevertheless, the contribution of less relevant tokens may accumulate over time and impact the overall sequence generation. Although these methods extend the attainable context length, they cannot prevent increasing memory consumption as the input length increases. Alternatively, compressing past tokens using a recurrent sequence model can potentially reduce memory consumption by condensing the information into a fixed-size embedding.",
            "score": 0.6377165682223536,
            "section_title": "Long Context Transformers",
            "char_start_offset": 4732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "234339280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435791015625
        },
        {
            "corpus_id": "268820218",
            "title": "On Difficulties of Attention Factorization through Shared Memory",
            "text": "In the era of big data and natural language processing, handling long-form text is crucial.Transformers (Vaswani et al., 2017) have shown promise in some tasks, but they do not scale well with longer inputs due to their quadratic time and memory complexity inherent in their attention framework.This challenge has given rise to multiple approaches designed to handle sequences exceeding typical input lengths, including attention reformulation for efficient computing (Rabe & Staats, 2021;Dao et al., 2022), exploration of weight sharing techniques (Dehghani et al., 2018;Raffel et al., 2019), heavy use of quantization (Shen et al., 2019) or replacing the attention operation itself with a faster alternative.\n\nIn the present work, we focus on designs that alter the Transformer architecture to lower the computational demands by leveraging an external memory in the form of a set of learnable vectors.Models like Linear Unified Nested Attention (Luna; Ma et al., 2021) or Perceiver (Jaegle et al., 2021) use it to factorize an attention operation into a sequence of attentions with a linear complexity, while the Memory Augmented Transformer (Wu et al., 2022) processes long inputs chunk-by-chunk using the memory as a hidden state to carry information between chunks.While these models adopt different perspectives on long input processing, they all leverage the attention mechanism as an interface for communication between the input and memory.The latter can be used as a convenient fixed-length dense representation of sparse inputs such as texts.\n\nGiven the properties of the attention operation, we discover the phenomenon which does not allow to utilize multiple memory cells properly, which we call memory degradation.Overcoming it may significantly improve the performance of the named models, and we propose several tweaks which lead to noticeable performance gains on the considered benchmarks.",
            "score": 0.6368957403291063,
            "section_title": "INTRODUCTION & RELATED WORK",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 91,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 710
                },
                {
                    "start": 712,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1553
                },
                {
                    "start": 1555,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 126,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1144,
                    "end": 1161,
                    "matchedPaperCorpusId": "248157404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "253399443",
            "title": "Multi-dimensional patient acuity estimation with longitudinal EHR tokenization and flexible transformer networks",
            "text": "They have shown to before the same or better for vision tasks, while also reducing vision-specific induction bias Han et al. (24). For video data, they can be used for trajectory tracking of objects like balls Patrick et al. (25) using attention on objects in images, as well as approximate self attention to reduce quadratic dependency. \n\nWhile the Transformer is in one sense more efficient than its sequential counterparts due to its ability to parallelize computations at each layer, one of the main drawbacks is its required memory consumption. Since each input element of a sequence of length n must be compared with every other input element in the sequence, typical Transformer implementations require memory on the order of O(n 2 ). While acceptable for relatively short sequences, the memory consumption quickly becomes problematic for very long sequences. Decreasing the memory requirement of Transformers is an area of ongoing research. \n\nOne potential solution was proposed by Beltagy et al. (18) in their Longformer architecture. Rather than computing full n 2 self-attentions, they propose a sliding self-attention window of specified width, where each input sequence element is compared only with neighboring sequence elements within the window. They extend this to include user-specified global attention patterns (such as on the special [CLS] tokens for classification) that are always compared with every element in the sequence. Through several NLP experiments, they demonstrate the promising ability of the Longformer to approximate results from a full Transformer model.",
            "score": 0.6364502313554365,
            "section_title": "Principal findings",
            "char_start_offset": 29725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1592
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "236986986"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "235390605"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "215737171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06671142578125
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Transformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8,21], and more recently, constructing large language models (LLMs) [9,69].Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1,20,19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.\n\nMany recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,61] allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42,78].(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters.Ainslie et al. [1]  KV pairs are scored by u.SPARSEK computes a threshold for each query (\u03c4 (u)) such that the sum of normalized scores is k, which is 3 in this example.We select top-k KV pairs (orange cells) to perform attention.Right: the SPARSEK attention module.We fuse selection and attention in one kernel for efficiency.incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders.Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.",
            "score": 0.6363882906227577,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 642
                },
                {
                    "start": 644,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "257622671"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "264439578"
                },
                {
                    "start": 1506,
                    "end": 1509,
                    "matchedPaperCorpusId": "257622671"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "235368340",
            "title": "A Survey of Transformers",
            "text": "of which is associated with a local memory block. All the queries in a query block attend to only the keys in the corresponding memory block. Fig. 4(e) depicts a commonly used case where the memory blocks are identical to their corresponding query blocks.  for Question Answering tasks. They also replace some of the band attention heads in upper layers with dilated window attention to increase the receptive field without increasing computation. As a concurrent work to Longformer [10], Extended Transformer Construction (ETC) [1] utilizes combination of band attention and external global-node attention. ETC also includes a masking mechanism to handle structured inputs and adapt Contrastive Predictive Coding (CPC) [135] for pre-training. In addition to the band and global attention, BigBird [163] uses additional random attention to approximate full attention. Their theoretical analysis also reveals that the usage of a sparse encoder and sparse decoder can simulate any Turing Machine, which explains the success of those sparse attention models.\n\nSparse Transformer [17] uses a factorized attention where different sparse patterns are designed for different types of data. For data with a periodic structure (e.g., images), it uses a composition of band attention and strided attention. Whereas for data without a periodic structure (e.g., text), it uses a composition of block local attention combined with global attention, where global nodes are from fixed positions in the input sequence.",
            "score": 0.6355867556753736,
            "section_title": "Position-based Sparse Attention.",
            "char_start_offset": 19738,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48046875
        },
        {
            "corpus_id": "252216464",
            "title": "Denoising Self-Attentive Sequential Recommendation",
            "text": "Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,18,29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts. \n\nAnother line of work is to use learnable attention distributions [12,36,38,40]. Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks. This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments. Our Rec-denoiser is related to this line of work. Instead of using sparsemax, we design a trainable binary mask for the self-attention network. As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way.",
            "score": 0.6348468524312815,
            "section_title": "Sparse Transformer",
            "char_start_offset": 10558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1553
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "209009596"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "209009596"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "202538495"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "44005113"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "153313159"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "159041867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "261125712",
            "title": "Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs",
            "text": "Jiang et al. [13] designs NUMA-aware thread scheduling for the variable-length problem on ARM platforms. ReTransformer [30] proposes a new sub-matrix pipeline design for multi-head self-attention in Processing-in-Memory. E.T. [4] proposed a kernel fusion design for the attention module. However, constrained by the shared memory capacity, it can only outperform FasterTransformer in a very limited range and related methods will become unusable as the sequence length becomes long. Besides optimizing the computation, Synthesizer [25], Reformer [15], and FlashAttention [7] improve the vanilla self-attention for less computation and more expressive. At present, FlashAttention starts to be applied in large-scale models. FlashAttention is a fast and memory-efficient exact attention algorithm. It is based on the principle of making attention mechanism IO-aware, which uses tiling to reduce the number of memory accesses between GPU high bandwidth memory and on-chip SRAM. Similarly, Narang et al. [19] study to make recurrent neural network sparse and remove the redundancy, which is a promising direction for improving transformer models.",
            "score": 0.634366884827506,
            "section_title": "RELATED WORK",
            "char_start_offset": 47971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1142
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "255775620"
                },
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "227221318"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "239037005"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244140625
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "We analyze the IO complexity [1] of FlashAttention, proving that it requires  ( 2  2  \u22121 ) HBM accesses where  is the head dimension and  is the size of SRAM, as compared to \u03a9(  +  2 ) of standard attention. For typical values of  and , FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9\u00d7 fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes. \n\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4\u00d7 faster than even FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive. 1 e empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. \n\n\u2022 Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2 (seq. length 1K) 3\u00d7 faster than baseline implementations from HuggingFace [87] and Megatron-LM [77], and long-range arena (seq. length 1K-4K) 2.4\u00d7 faster than baselines. \n\n\u2022 Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities.",
            "score": 0.6335070528842772,
            "section_title": "Introduction",
            "char_start_offset": 3743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1870
                },
                {
                    "start": 1873,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "6264984"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "203642156"
                },
                {
                    "start": 1775,
                    "end": 1779,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "263902277",
            "title": "Superior protein thermophilicity prediction with protein language model embeddings",
            "text": "As BigBird uses a sparse attention mechanism scaling linearly with the sequence length instead of quadratically, the average pooling is not needed. Hence, comparing the results of vanilla-Transformer and BigBird can be used to evaluate whether using the full sequence length as input of the transformer-based layers is beneficial. vanilla-Transformer and BigBird consist of an optimized number of transformer layers, each comprising multiple self-attention heads, with the number of heads considered as a hyperparameter (see Supplementary Table S8). The transformer layers of both prediction models follow a pre-layer normalization design, with layer normalization followed by a multi-head self-attention and a feedforward layer, as well as skip connections around the self-attention and feedforward parts ( 56 ). However, vanilla-Transformer and BigBird differ regarding the self-attention mechanism. For vanilla-Transformer, we employ the regular self-attention calculation with each sequence element attending to all others. BigBird instead applies an approximation, where each sequence element only attends to a subset of the other sequence elements, realized via local and global tokens. Similarly to ProLaTherm , we apply an average pooling along the sequence length on the output of the last transformer layer in both cases. Finally, we use the head classifier design of ProLaTherm . \n\nAll sequence-based models are trained from scratch to minimize the cross-entropy loss using the Adam optimizer with an optimized learning rate and a learning rate reduction on a validation loss plateau. In addition to dropout, we apply early stopping for regularization.",
            "score": 0.6308678732502824,
            "section_title": "Purely sequence-based comparison partners",
            "char_start_offset": 21210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 812,
                    "matchedPaperCorpusId": "211082816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400390625
        },
        {
            "corpus_id": "248177894",
            "title": "Revisiting Transformer-based Models for Long Document Classification",
            "text": "Vanilla transformer relies on the multi-head selfattention mechanism, which scales poorly with the length of the input sequence, requiring quadratic computation time and memory to store all scores that are used to compute the gradients during back-propagation (Qiu et al., 2020). Several Transformer-based models (Kitaev et al., 2020;Tay et al., 2020;Choromanski et al., 2021)  in-between a window of neighbour (consecutive) tokens. Global attention relies on the idea of global tokens that are able to attend and be attended by any other token in the sequence (Figure 3). BigBird of Zaheer et al. (2020) is another sparse-attention based Transformer that uses a combination of a local, global and random attention, i.e., all tokens also attend a number of random tokens on top of those in the same neighbourhood. Both models are warm-started from the public RoBERTa checkpoint and are further pre-trained on masked language modelling. They have been reported to outperform RoBERTa on a range of tasks that require modelling long sequences. \n\nWe choose Longformer (Beltagy et al., 2020) in this study and refer readers to Xiong et al. (2021) for a systematic comparison of recent proposed efficient attention variants.",
            "score": 0.6307684116058963,
            "section_title": "Sparse-Attention Transformers",
            "char_start_offset": 6042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1218
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 278,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 351,
                    "end": 376,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465087890625
        },
        {
            "corpus_id": "268378933",
            "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
            "text": "Due to attention's computational bottleneck in transformers, some methods aim to explore efficient attention mechanisms. Solutions include trading accuracy for speed, e.g., Longformer [14] employs sliding window attention, expanding the receptive field with a dilated sliding pattern and optionally integrating global attention. BP-Transformer [16] balances complexity and capacity with fine-tocoarse attention across multiple scales using binary partitioning. Linformer [17] approximates selfattention with a low-rank matrix, simplifying operations to linear ones. LongLoRA [18] uses blockwise attention and token shifting to enhance communication between blocks. Another solution lies in system-level optimizations, e.g., FlashAttention [19,20] optimizes memory access by perceptually reading and writing, improving efficiency without sacrificing accuracy. However, these methods don't preserve dialogue history or expand the context window sufficiently for prolonged dialogue with long-term memory.",
            "score": 0.6290639768066633,
            "section_title": "Efficient transformers",
            "char_start_offset": 758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1001
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21630859375
        },
        {
            "corpus_id": "271916182",
            "title": "Macformer: Transformer with Random Maclaurin Feature Attention",
            "text": "One common motivation shared across several studies is the need to scale transformers to process long sequences efficiently. To address this limitation, various approaches have been proposed. \n\nSoftmax-Free Linear Transformers (Lu et al. 2024) aim to approximate self-attention at linear complexity by replacing the dot-product similarity with a Gaussian kernel function. This innovation allows for a full self-attention matrix to be approximated under low-rank matrix decomposition. Linformer (Wang et al. 2020) approximates the selfattention mechanism with a low-rank matrix, reducing its complexity from O(n 2 ) to O(n). Methods such as Swin Transformer (Liu et al. 2021) have explored shifted windowbased self-attention to reduce computational costs. However, while Swin Transformer addresses some of the challenges associated with traditional transformers, it may still struggle when training on small datasets. ESwin Transformer (Yao and Shao 2023) further improves upon this by redesigning the modules of Swin Transformer and introducing simple convolutional components, enhancing the model's performance on small datasets. \n\nSparse attention patterns have been another area of focus for optimizing transformer models (Qiu et al. 2019;Ho et al. 2019) by limiting the reception field of attention computation. Efficient Transformer models in domains such as speech recognition (Luo 2023), audio tagging (Schmid, Koutini, and Widmer 2023), and remote sensing image segmentation (Xu et al. 2021) often leverage techniques like knowledge distillation, quantization, and sparse attention (Child et al. 2019) patterns. Informer (Zhou et al. 2020) introduces the ProbSparse self-attention mechanism and selfattention distillation techniques, reducing time complexity and memory usage to O(n log n). \n\nRandom feature attention (RFA) (Peng et al. 2021) proposes reducing computational costs by decomposing Softmax into linear operations. This method uses random feature map (Rahimi and Recht 2007) to approximate the attention.",
            "score": 0.628587164262914,
            "section_title": "Related Work",
            "char_start_offset": 2889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 242,
                    "matchedPaperCorpusId": "250334278"
                },
                {
                    "start": 935,
                    "end": 954,
                    "matchedPaperCorpusId": "260363331"
                },
                {
                    "start": 1383,
                    "end": 1393,
                    "matchedPaperCorpusId": "266480229"
                },
                {
                    "start": 1409,
                    "end": 1443,
                    "matchedPaperCorpusId": "253420462"
                },
                {
                    "start": 1483,
                    "end": 1498,
                    "matchedPaperCorpusId": "238244662"
                },
                {
                    "start": 1629,
                    "end": 1647,
                    "matchedPaperCorpusId": "229156802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "267334813",
            "title": "LOCOST: State-Space Models for Long Document Abstractive Summarization",
            "text": "In this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. \n\nMemory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022;Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. \n\nProfuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. \n\nIn contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. \n\nAttention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT).",
            "score": 0.6285465661379903,
            "section_title": "Related Work",
            "char_start_offset": 3900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 343,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 484,
                    "end": 507,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1212,
                    "end": 1233,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1374,
                    "end": 1392,
                    "matchedPaperCorpusId": "245144820"
                },
                {
                    "start": 1899,
                    "end": 1926,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1928,
                    "end": 1945,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 2086,
                    "end": 2104,
                    "matchedPaperCorpusId": "234742218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "267770290",
            "title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution Images",
            "text": "Transformers, and their key component, attention, have been integral to the success and improvements in generative models in recent years [Vaswani et al., 2023]. Their global receptive fields, ability to compute dynamically based on input context, and large capacities have made them useful building blocks across many tasks [Khan et al., 2022]. The main drawback of Transformer architectures is their quadratic scaling of computational complexity with sequence length, affecting both time and memory requirements. When looking to generate a Stable Diffusion image at 2048 \u00d7 2048 resolution, the attention maps of the largest U-Net blocks incur a memory cost of approximately 69 GB in half-precision, calculated as (1 batch\u00d78 heads\u00d7(256 2 tokens) 2 \u00d72 bytes). This exceeds the capabilities of most consumer GPUs [Zhuang et al., 2023]. Specialized kernels, such as those used in Flash Attention, have greatly improved speed and reduced memory costs [Dao et al., 2022], however, challenges due to the unfavorable quadratic scaling with sequence length are persistent. \n\nIn the quest for computational efficiency, the concept of sparse attention has gained traction. Methods like Token Merging (ToMe) [Bolya et al., 2023] and its application in latent image diffusion models [Bolya and Hoffman, 2023] have reduced the computation time required by condensing tokens with high similarity, thereby retaining the essence of Figure 1: A visualization of our method. From a given latent or image, we subsample positions on the grid in a strided fashion for the keys and values used in attention maintaining the full set of query tokens. Link to demo video is here. \n\nthe information with fewer tokens. Similarly, approaches like Neighborhood Attention [Hassani et al., 2023] and Focal Transformers [Yang et al., 2021] have introduced mechanisms where query tokens attend only to a select neighborhood, balancing the trade-off between receptive field and computational load. These strategies aim to efficiently approximate the attention mechanism's output. While performant, these methods typically require training-time modifications to be successful, incurring significant logistical overheads to leverage their optimizations.",
            "score": 0.6266932538852318,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 344,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 812,
                    "end": 833,
                    "matchedPaperCorpusId": "235694438"
                },
                {
                    "start": 948,
                    "end": 966,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1198,
                    "end": 1218,
                    "matchedPaperCorpusId": "252968113"
                },
                {
                    "start": 1272,
                    "end": 1297,
                    "matchedPaperCorpusId": "252968113"
                },
                {
                    "start": 1743,
                    "end": 1765,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "276775748",
            "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
            "text": "Transformer [5] architecture represents a significant advancement in artificial intelligence, bringing forth unparalleled capabilities alongside the challenge of resourceintensive training and serving processes. Significant enthusiasm has been ignited for actively enhancing the efficiency of transformer architectures. A preponderance of the works concentrate on reducing memory usage and improving computational efficiency. Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence. Linformer [7] and Lightning-Attention [8,9] leverages linear-complexity self-attention mechanisms, catering to large-scale data and lengthy sequences. Reformer [10] mitigates memory constraints by employing reversible layers and locality-sensitive hashing techniques. Flash Attention [11] reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion. DeepSeek-V2 [13] introduces the Multi-Head Latent Attention (MLA) mechanism, which employs low-rank joint compression to enhance training efficiency and reduce the KV cache size during inference. Tensor Product Attention (TPA) [14] dynamically constructs QKV as context-dependent decomposed tensors, enabling adaptive adjustments and facilitating seamless integration with effective Rotary Position Embedding. \n\nThe other works aims to enhance the modeling capabilities of long sequences. Transformer-XL [15] introducing segmentlevel recurrence to enhance sequence modeling beyond the scope of the original Transformer. Sinkhorn Transformer [16] fuses Sinkhorn algorithm with self-attention mechanisms to improve sequence modeling accuracy. Long-Short-Term Memory Transformer [17] combines Transformer with Long Short-Term Memory (LSTM) mechanisms and thus enhance the modeling capability for long sequences. SeerAttention [18] integrates a learnable gating mechanism into standard attention mechanism, enabling adaptive selection of salient blocks within the attention map. Although These innovations have notably improved the performance of the transformer frameworks in their respective domains, they are typically model-specific and may not be universally applicable. By comparison, our work provides a model-free method. By simply adding selection mechanism to existing model parallel transformers, we can train a model more efficiently while maintaining or even improving its performance.",
            "score": 0.6258420034529558,
            "section_title": "A. Variants of Transformer",
            "char_start_offset": 6646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2490
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 15,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 832,
                    "end": 836,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "250628527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "The second approach is to leverage low-rank approximation and kernelization. Linformer (Wang et al. 2020) projects the key and value matrices to a reduced dimension to exploit the low-rankness of self-attention, as illustrated in Fig. 2(d), thereby reducing the complexity to O(n). Performer (Choromanski et al. 2021) achieves linear complexity O(n) by approximating the softmax attention kernel using orthogonal random features. \n\nLastly, there are neural memory approaches. The neural memory has additional trainable special tokens to comprehend the entire sequence with a reduced computational complexity. The neural memory was originally used by BERT (Kenton and Toutanova 2019) and Vision Transformer (Dosovitskiy et al. 2021) as a form of [CLS] tokens to aggregate information from the entire context. To address the limitation of local attention at fixed sparse pattern techniques, Longformer (Beltagy, Peters, and Cohan 2020) and BigBird (Zaheer et al. 2020) incorporated additional 'global attention' tokens as a neural memory to enable effective computation on global contextual information. Particularly, BigBird combined window-based local attention, global attention, and random attention, as shown in Fig. 2(e). Set Transformer (Lee et al. 2019), illustrated in Fig. 2(f), also uses the neural memory technique. It replaces self-attention with two cross-attentions between the learned 'inducing points' and the input sequence. The input sequence is encoded and subsequently decoded, effectively optimizing self-attention computations. \n\nNystr\u00f6mformer (Xiong et al. 2021) uses a downsampling approach rather than training a neural memory. It derives landmarks from input tokens via pooling, which are then employed to reconstruct the softmax matrix in an approximated self-attention mechanism. Given that the count of landmarks is considerably less than the sequence length, a linear complexity is achieved. Nonetheless, the method employed for landmark sampling is susceptible to redundancy. Furthermore, Nystr\u00f6mformer incurs a significant computational overhead when calculating the pseudo-inverse of the attention matrix between the landmarks.",
            "score": 0.623072292809416,
            "section_title": "Background and Motivation",
            "char_start_offset": 7085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 316,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 946,
                    "end": 965,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1565,
                    "end": 1583,
                    "matchedPaperCorpusId": "231847231"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22021484375
        },
        {
            "corpus_id": "276961169",
            "title": "Speedy MASt3R",
            "text": "Traditional self-attention mechanisms in transformers suffer from quadratic complexity with respect to sequence length, making them inefficient for large-scale feature matching tasks. FlashAttention [4] optimizes memory access by using an I/O-aware algorithm that avoids materializing the full attention matrix, significantly reducing both computation and memory costs. It achieves this by tiling the attention computation, ensuring that intermediate values fit within high-bandwidth memory (SRAM) on GPUs. FlashAttention v2 [3] improves upon this by further optimizing work partitioning and parallelization, achieving significant speedup compared to naive attention implementations.",
            "score": 0.6200831933165603,
            "section_title": "Efficient Attention Mechanisms and FlashAttention",
            "char_start_offset": 5761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 683
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322998046875
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "Fig. 1 illustrates a comparative overview of the performance, inference throughput, and memory footprint of various existing efficient transformer models on the Long Range Arena (LRA) benchmark (Tay et al. 2021). BigBird achieves a better throughput than the standard transformer. However, its irregular sparse attention pattern leads to a limited throughput enhancement on real hardware platforms. Reformer, Linformer, and Performer have effectively optimized attention operations in terms of throughput and memory footprint. Nevertheless, they can be vulnerable to specific tasks and show lower accuracy compared to the standard transformer model. \n\nThe most successful technique to date for creating efficient transformers is Nystr\u00f6mformer, which achieves superior results in both accuracy and inference throughput. We analyzed Nystr\u00f6mformer in depth and found several limitations. Firstly, Nystr\u00f6mformer selects landmarks by sampling (i.e., average pooling) for the input sequence tokens without considering the redundancy among the selected landmarks. Secondly, the presence of a pseudo-inverse process during the approximation stage can lead to computation overhead. \n\nWe introduce an advanced variant of Nystr\u00f6mformer, called Proxyformer. It introduces trainable special tokens, termed proxy tokens, to establish effective landmarks without sampling. These learned proxy tokens are used as landmarks to decompose attention operations with the Nystr\u00f6m method. To ensure minimal redundancy between these proxy tokens, we employ contrastive loss during their training. Additionally, we uses an input injection module at the first layer to adapt the learned tokens based on the information of input tokens. Each layer passes its own proxy token outputs to the following layer, ensuring every layer remains contextually aware of the input. \n\nSimilar approaches, such as Set Transformer (Lee et al. 2019) and Luna (Ma et al. 2021), have used learned tokens (inducing points) to decompose attention operations. However, these methods rely on two nested-attention blocks, requiring intricate layer designs. This deviates from the conventional structure of standard transformer models, hindering the adoption of pre-trained weights. Notably, Set Transformer lags in performance compared to Nystr\u00f6mformer due to its inducing points being oblivious to input context, as shown in Fig. 1.",
            "score": 0.6200812471699116,
            "section_title": "Introduction",
            "char_start_offset": 1969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2382
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 211,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306640625
        },
        {
            "corpus_id": "254725259",
            "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion",
            "text": "Some models reduces the computational complexity by introducing sparse attention mechanism. In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird [18] is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length. \n\nSimilar to the above models, Routing Transformer [19] also adopts a sparse attention to achieve the complexity of 1.5 power of input sequence length. Routing Transformer selects sparsity patterns without computation of a full attention matrix. Lite Transformer [4] adopts long-short range attention. The long-short range attention separately captures sparse and diagonal patterns in the attention maps, and reduces model size and computation cost. \n\nLogSparse Transformer [20] reduces the linear complexity of each attention layer to the logarithmic complexity to relieve the quadratic total complexity. To achieve this, in the self-attention of LogSparse Transformer, each cell only attends to its previous cells with an exponential step size. LogSparse Transformer also applies convolutional self-attention to manipulate queries and keys to catch more local context and enhance the overall performance. \n\nSparse Sinkhorn transformer [17] is another sparse efficient model. Tokens in the vanilla self-attention only attend to tokens within the same block but each token attends to tokens in the sorted block in Sparse Sinkhorn attention. Then, other tokens far away in the other block can be considered. It enables the model to compute quasi-global attention and improves memory efficiency. \n\nMeanwhile, some models directly reduce the number of parameters. For example, DeLighT [3] lowers the number of parameters by using block-wise scaling. Similarly, Informer [21] adopts ProbSparse-attention to reduce the number of queries used, and enhance memory and calculation efficiency.",
            "score": 0.6197107786670373,
            "section_title": "B. SPARSE ATTENTION",
            "char_start_offset": 6896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2284
                },
                {
                    "start": 2285,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "195766887"
                },
                {
                    "start": 1775,
                    "end": 1779,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 2305,
                    "end": 2309,
                    "matchedPaperCorpusId": "229156802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "258888224",
            "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
            "text": "The first replaces the attention mechanism with an alternative operation that features more favorable scaling with the input sequence length [Peng et al., 2021, Katharopoulos et al., 2020, Choromanski et al., 2020a, Schlag et al., 2021]. While several recent methods in this category show promise, none have emerged as a definitive winner, and most state-of-the-art language models still rely on the standard attention mechanism [Touvron et al., 2023, Chowdhery et al., 2022]. The second approach proposed to compress the length of the input context, controlling the complexity of the attention operation but unavoidably sacrificing potentially relevant information from the original input [Lee et al., 2019, Wang et al., 2020, Jaegle et al., 2021]. The third approach involves pruning the attention matrix, preventing each token from attending to every other token within the context [Zaheer et al., 2020, Martins et al., 2020, Lee et al., 2023]. This line of research is motivated by the theoretical finding highlighting that sparse Transformers retain the expressivity of their dense counterparts [Yun et al., 2020]. Many methods in this category employ specially designed attention masks that aim to zero out as many entries as possible, often based on principles of locality, randomness, or a combination of both. The main drawback of these methods is their mostly static nature, meaning that every token is compelled to attend to a fixed context window and disregard the rest of the context regardless of its specific role within the input sequence. Our approach falls within this last category, and enables dynamic sparsification of the attention matrix for decoder models, without resorting to any potentially restricting inductive biases about its structure. \n\nImplementation Speed-up Recently, hardware-optimized implementations [Dao et al., 2022, Touvron et al., 2023] have been proposed with the aim of optimizing computational resources during the training phase of Transformers [Hoffmann et al., 2022].",
            "score": 0.6179929767801222,
            "section_title": "Related Work",
            "char_start_offset": 7302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 187,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 885,
                    "end": 905,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 905,
                    "end": 927,
                    "matchedPaperCorpusId": "219636190"
                },
                {
                    "start": 1100,
                    "end": 1118,
                    "matchedPaperCorpusId": "219558319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1480712890625
        },
        {
            "corpus_id": "266110855",
            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
            "text": "Standard Transformer [1] adopts the full attention mechanism, that is, each token in the input sequence needs to be calculated attention from all the other tokens, and consequently causes O(N 2 ) complexity and consumes huge resources when processing long sequences. For this reason, these models that are built on vanilla Transformer, for example, BERT [2], BART [3], GPT2 [5], and T5 [4], generally process no more than input length of 1\u223c4k at a time. \n\nTo deal with this problem, a lot of research has been done to make the attention mechanism more efficient and have lower complexity, so called efficient Transformers [6]. Except for low-rank kernel optimizations such as Linformer [19], Performer [20], and down-sampling models such as Charformer [21], BP Transformer [22], a great branch of this research is about Transformers with sparse attention, which directly deletes some attention relations from the original attention computation, and makes the attention matrix appear sparse. \n\nIn these contributions to Transformers with sparse attention, LED [7] and ETC [8] concurrently propose sliding window local attention and a small-scale variant of full attention called global attention, which makes attention focus only on neighboring and selected tokens. BigBird [9] jointly uses local, global and random attention to cover as much useful information as possible over short and long distances, and achieves state-of-the-art results on many question answering and summarization tasks. LongT5 [10] proposes a modified local-global attention called transient global attention that dynamically constructs global tokens and discards them after the attention operation. However, a major challenge of these works is how to maintain efficiency and accuracy while scaling up the input length. Because in the scenarios of longer sequences with length of 16\u223c48k, they are either computationally intensive due to a linear increasing global memory (e.g. global attention), or informationally lossy because of a fixed pattern that only fits for common long texts of 8\u223c16k (e.g. local attention and random attention).",
            "score": 0.6170954554368326,
            "section_title": "II. RELATED WORK A. TRANSFORMERS WITH SPARSE ATTENTION",
            "char_start_offset": 5891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 622,
                    "end": 625,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "235624202"
                },
                {
                    "start": 1071,
                    "end": 1074,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "245144820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "276768585",
            "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
            "text": "We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). \n\nWe show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Notably, it does not require any positional embeddings. It also retains Transformers' long-context retrieval abilities and achieves near-perfect accuracy in the needlein-the-haystack test (Kamradt, 2023) within the training context length. In contrast, all the tested recurrent sequence models fail. We also introduce a \"Pro\" block design that integrates several architectural components commonly used in recurrent sequence models, which significantly improves the performance of FoX and the baseline Transformer. Finally, we show that FoX can be implemented in a hardware-aware way with a simple modification to the FlashAttention (Dao, 2023) algorithm.",
            "score": 0.6165101316434619,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1195068359375
        },
        {
            "corpus_id": "277634383",
            "title": "Adaptive Computation Pruning for the Forgetting Transformer",
            "text": "Transformers (Vaswani et al., 2017) have quadratic time complexity with respect to context length, resulting in significant computational costs over long sequences. The recently proposed Forgetting Transformer (FoX) (Lin et al., 2025) features a modified softmax attention mechanism with a forget gate, which allows some attention heads to downweight distant dependencies and focus mainly on the local context. FoX has been shown to consistently achieve better or on-par performance compared to the standard RoPE-based (Su et al., 2024) Transformer in various tasks, including long-context language modeling and downstream tasks such as the needle-in-a-haystack test (Kamradt, 2023). It is also compatible with the FlashAttention (Dao, 2024) algorithm, which allows efficient processing of long sequences. Lin et al. (2025) show that many attention heads in FoX tend to forget quickly. For these heads, the dependencies between distant input-output pairs are extremely weak and can potentially be ignored. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. The computations to be pruned are determined with a dynamically set threshold that guarantees that the pruned attention weights are negligible. As shown in Figure 1, this can be easily achieved by identifying a pruning boundary across the grid of computations in FlashAttention with a linear time complexity algorithm. After identifying the pruning boundary, we only visit the remaining blocks for the FlashAttention iterations, and thus no computations are wasted on the pruned dependencies. \n\nWe apply ACP to language model pretraining with FoX with sizes from 125M to 760M parameters and training context lengths from 4k to 16k tokens. We find that ACP consistently prunes around 70% of the FLOPs in softmax attention across the tested model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. In particular, longer context lengths lead to greater flop savings and throughput improvements. These speed improvements are achieved without affecting language modeling loss and downstream task performance.",
            "score": 0.6160345900866585,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 216,
                    "end": 234,
                    "matchedPaperCorpusId": "276768585"
                },
                {
                    "start": 519,
                    "end": 536,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 730,
                    "end": 741,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 806,
                    "end": 823,
                    "matchedPaperCorpusId": "276768585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "276106883",
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "text": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
            "score": 0.6147172778546091,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78125
        },
        {
            "corpus_id": "275458476",
            "title": "ELFATT: Efficient Linear Fast Attention for Vision Transformers",
            "text": "Transformers have achieved great success in large language models (ChatGPT (OpenAI et al., 2024) and Llama (Dubey et al., 2024)) and large vision models (SAM (Kirillov et al., 2023) and Sora (Brooks et al., 2024)). The core technique of the transformer, the vanilla softmax-based attention (Va-niATT) mechanism, is capable of capturing the relationship between any two tokens (Wu et al., 2024). In complex tasks, the sequence length becomes longer and longer, usually much longer than the embedding dimension. The softmax operation after the multiplication of query and key matrices makes VaniATT quadratic computational complexity with respect to the sequence length. It has become a main obstacle to computational efficiency. \n\nAcceleration methods to speed up the attention computation can be classified into two categories: (1) memory-efficient methods; (2) computation-efficient methods. Memoryefficient methods focus on optimizing memory input/output (I/O) operations to achieve almost linear complexity (Dao et al., 2022;Dao, 2024;Shah et al., 2024;Ramapuram et al., 2024). FlashAttention (Dao et al., 2022;Dao, 2024;Shah et al., 2024) and FlashSigmoid (Ramapuram et al., 2024) are representatives of memory-efficient methods.",
            "score": 0.6130179315142807,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1233
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1038,
                    "matchedPaperCorpusId": "259936734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2255859375
        },
        {
            "corpus_id": "255545908",
            "title": "DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching",
            "text": "In the vanilla Transformer, the memory cost is quadratic to the length of sequences due to the matrix multiplication, which has become a bottleneck for Transformer when dealing with long sequences. Recently, several approaches have been proposed to improve the efficiency of Transformer [30], [44], [45]. Linear Transformer [30]   linear dot product of kernel feature maps and makes use of the associativity property of matrix products to reduce the computational complexity. BigBird [44] combines local attention and global attention at certain positions and utilizes random attention on several randomly selected token pairs. FastFormer [45] uses additive attention mechanism to model global contexts, achieving effective context modeling with linear complexity.",
            "score": 0.6129548149508486,
            "section_title": "C. Efficient Transformer",
            "char_start_offset": 11184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 764
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28564453125
        },
        {
            "corpus_id": "270063477",
            "title": "Scorch: A Library for Sparse Deep Learning",
            "text": "We evaluate the performance of sparse autoencoders on four datasets: MNIST [35], CIFAR-10 [34], CIFAR-100 [34], and CelebA [39].The datasets are preprocessed as follows:\n\n\u2022 MNIST: The images are converted to tensors and flattened into a 1D vector of size 784 (28\u00d728).\n\nBigBird is designed to handle long sequences while maintaining a manageable computational complexity.It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer.The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer.\n\nSparse transformers have been successfully applied to various natural language processing tasks, such as text classification, question answering, and language modeling, where the input sequences can be very long.It has also shown promising results in other domains, such as genomics and time series analysis, where the ability to handle long sequences is crucial.\n\nWe evaluate the inference performance of the BigBird model [61] on three text classification datasets: AG News [64], IMDB [42], and Yahoo Answers [64].The model architecture and hyperparameters are listed in Table 3.We use the AdamW optimizer [40] with a learning rate of 0.001 to train the models for 5 epochs with a batch size of 64.The sparse attention is configured to use a block size of 16, 2 global tokens, 2 random blocks, and 2 sliding blocks.\n\nFor each dataset, we train the models on the training set and evaluate the inference time on the test set using an Apple M1 Ultra CPU.The experiments are implemented in PyTorch 2.2.1 and Scorch 0.1.0.",
            "score": 0.6129229161035157,
            "section_title": "E.3 Sparse Autoencoders",
            "char_start_offset": 46879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 169
                },
                {
                    "start": 171,
                    "end": 267
                },
                {
                    "start": 269,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 604
                },
                {
                    "start": 606,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 895
                },
                {
                    "start": 897,
                    "end": 1092
                },
                {
                    "start": 1094,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1457
                },
                {
                    "start": 1459,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1911
                },
                {
                    "start": 1913,
                    "end": 2047
                },
                {
                    "start": 2047,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 79,
                    "matchedPaperCorpusId": "14542261"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "459456"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "matchedPaperCorpusId": "1428702"
                },
                {
                    "start": 1605,
                    "end": 1609,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1702,
                    "end": 1706,
                    "matchedPaperCorpusId": "53592270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.681640625
        },
        {
            "corpus_id": "237260051",
            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
            "text": "Transformer (Vaswani et al., 2017) has achieved great success in NLP by serving as the basic architecture of many popular models such as BERT (Devlin et al., 2019), RoBERTa (Radford et al., 2019) and XLM (Conneau and Lample, 2019). In addition, Transformers also show great potentials in other fields like computer vision (Dosovitskiy et al., 2021) and speech recognition (Dong et al., 2018) to understand data in other modalities. Self-attention is the core of Transformer (Parikh et al., 2016). It aims to model the interaction between each pair of tokens to help capture their contexts, where the   computational complexity is quadratic with respect to the input sequence length (Vaswani et al., 2017). Thus, Transformer is inefficient in handling long sequences (Tay et al., 2020). \n\nAn intuitive way to improve the efficiency of Transformer is computing a sparse self-attention matrix to reduce the number of tokens to be attended (Child et al., 2019). For example, Beltagy et al. (2020) proposed Longformer, which computes sliding window attention to capture local contexts and global attention at a few positions to capture global contexts. Zaheer et al. (2020) proposed BidBird, which further incorporates random attention that models the interactions between each token with a certain number of randomly selected tokens. However, attending to the tokens that are randomly sampled or selected by certain rules may not be actually helpful for context modeling. \n\nInstead of attending to the heuristically or randomly selected tokens, attending to those potentially important tokens may help build higherquality sparse attention for text modeling. Fortunately, these potentially important tokens can be efficiently and effectively identified by a tiny Transformer model with very low dimensions. Fig. 1 show the attention heatmaps learned by a tiny arXiv:2108.09193v3 [cs.CL] 2 Sep 2021 \n\nTransformer with 4 hidden dimensions and a standard 256-dim Transformer. We find the attention heatmap produced by the low-dimensional Transformer is very similar to the heatmap learned by a Transformer with a much larger size.",
            "score": 0.609265448929072,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 142,
                    "end": 163,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 173,
                    "end": 195,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 204,
                    "end": 230,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 322,
                    "end": 348,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 474,
                    "end": 495,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 682,
                    "end": 704,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48876953125
        },
        {
            "corpus_id": "271720097",
            "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
            "text": "\u2022 The transformers' attention mechanism requires significant computational resources. \u2022 To optimize the key-value cache, it can be divided into blocks that are stored in non-contiguous parts of physical memory. \u2022 As the length of a transformer sequence increases, the complexity of attention increases exponentially. \n\n\u2022 Sliding windows and attention sinks can help reduce this complexity. \n\n\u2022 Flash Attention minimized expensive data movement by regrouping operations and keeping relevant data in memory. \n\n\u2022 Speculative Decoding samples generations from a more efficient model and achieves its speed up by reducing the needed runs from the larger model.",
            "score": 0.6082958537394806,
            "section_title": "In Conclusion.",
            "char_start_offset": 37697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 655
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11993408203125
        },
        {
            "corpus_id": "274598006",
            "title": "Flex Attention: A Programming Model for Generating Optimized Attention Kernels",
            "text": "High-performance self-attention kernels, at the heart of Transformers (Vaswani et al., 2017), have become one of the most important building blocks in deep learning. FlashAttention (Dao et al., 2022;Dao, 2024), the standard kernel strategy for attention, fuses the tensor operations in selfattention into one kernel, drastically improving the runtime and memory consumption. These optimizations are essential in enabling efficient training and decoding, particularly for long sequences. Unfortunately, this performance comes at the cost of flexibility. In exchange for the performance and memory benefits, users are restricted to only the handful of popular attention variants supported. \n\nHowever, new attention mechanisms continue to be a focus of significant research. The goal of these changes vary widely, including reductions in computational complexity (Sliding Window Attention (Beltagy et al., 2020b)), improvements to training stability (Softcapping (Team et al., 2024)), ability to work with sequences of differing lengths (Document Masking), better length extrapolation (ALiBI (Press et al., 2022)), applying attention to other domains such as images (Neighborhood Attention (Hassani & Shi, 2022)), modifications for better inference throughput (PagedAttention (Kwon et al., 2023a)), etc. Furthermore, users often want combinations of these (such as Sliding Window Attention combined with ALiBI), leading to a combinatorial explosion of possible attention kernels. These modifications are not merely speculative either -many of the most popular Large Language Models (LLMs) released use these variants, such as Sliding Window Attention in Mistral-7B, Softcapping in Gemma-2, or ALiBI in MPT-7B. \n\nThe importance of FlashAttention combined with its limited ability to support all attention variants poses a problem for researchers aiming to try new attention variants -a \"software lottery\" of sorts. If a particular attention variant is not supported by the existing kernels, further exploration can be hindered by slow runtime and memory limitations. This problem is exacerbated by the difficulty of automatically generating fused attention kernels, resisting traditional compiler-based approaches.",
            "score": 0.6071790536858614,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 92,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 199,
                    "end": 209,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 1273,
                    "end": 1292,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "232380042",
            "title": "A Practical Survey on Faster and Lighter Transformers",
            "text": "Fig. 14. The connectivity matrices of the block-wise attention [90] for   = 3 blocks. The corresponding permutations are written below the connectivity matrices. \n\nBeltagy et al. [5] introduced the Longformer which further reduces the complexity to O () using a combination of sliding window and global attentions (see Figure 15). The assumption behind the sliding window attention is that the most useful information is located in each position's neighbourhood. The sliding window attention is limited in that it requires O ( \u221a ) layers to model long-range dependencies. Thus, a few preselected tokens have a global attention: they can attend to every position and be attended by every position. Consequently, the maximum path length between any two positions is equal to 2. Zaheer et al. [139] introduced BigBird, which also achieves a linear complexity using a combination of random, sliding window, and global attentions (see Figure 15). BigBird has two configurations that the authors referred to as internal transformer construction (ITC) and extended transformer construction (ETC). Similarly to the Longformer, the former uses existing positions for global attention, while the latter uses additional tokens, increasing the model's capacity and performance. Interestingly, the extra location of ETC may be seen as a form of memory. The authors proved that their sparse factorization preserves the theoretical properties of Transformers with the full attention: the model is both a universal approximator of sequence functions and Turing complete. However, BigBird without random attention outperformed BigBird with it in most of their experiments.",
            "score": 0.6058598043551398,
            "section_title": "Input indices",
            "char_start_offset": 57497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 790,
                    "end": 795,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2117919921875
        },
        {
            "corpus_id": "266999285",
            "title": "Extending LLMs' Context Window with 100 Samples",
            "text": "The vanilla attention mechanism in the Transformer architecture is known for its quadratic time and space complexity, which poses significant resource demands for transformer models when processing lengthy inputs. Various works have focused on conquering the complexity issue and proposing more efficient Transformers. Sparse transformers (Child et al., 2019;Ye et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020;Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020;Lewis et al., 2020;Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022;Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs.",
            "score": 0.6050179780332292,
            "section_title": "More Efficient Transformers",
            "char_start_offset": 24824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 375,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 437,
                    "end": 457,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 995,
                    "end": 1013,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1013,
                    "end": 1032,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1867,
                    "end": 1885,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "250113485",
            "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
            "text": "This linear complexity significantly alleviates the memory burden, making it possible to train models with long sequences. However, since the hybrid sparse attention mechanism is not directly supported by the highly optimized GEMM kernels on CPU and GPU, the inference speed is limited, requiring efficient hardware acceleration to achieve a higher speed. \n\nExisting attention accelerators [8][9][10]14] meet performance bottlenecks given long input sequences. FTRANS [9] compresses weights of transformers while computing the full attention without sparsity. SpAtten [14] applies coarse-grained pruning to remove tokens and heads with a relatively low pruning ratio, which cannot effectively shorten the sequence length.  3 [8] and Sanger [10] accelerate the attention by utilizing dynamic sparse patterns, which can incur large additional memory/computation overhead in the case of long sequences. To this end, we propose SALO to efficiently accelerate the attention in transformers for tasks with long input sequences. The main contributions are summarized as follows: \n\n\u2022 We survey popular sparse attention mechanisms and explore the common computation patterns they share. Moreover, we analyze their potential of data reuse during computation.",
            "score": 0.6049445734528724,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "220633179"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "239012114"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "220633179"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "239012114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.374755859375
        },
        {
            "corpus_id": "276618265",
            "title": "Sliding Window Attention Training for Efficient Large Language Models",
            "text": "While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility.",
            "score": 0.6038951614631637,
            "section_title": "Efficient Transformers",
            "char_start_offset": 20831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 777
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "207930593",
            "title": "Compressive Transformers for Long-Range Sequence Modelling",
            "text": "There have been a variety of recent attempts to extend the range of attention, particularly in the Transformer, or to replace the attention operation with something less expensive. Wu et al. (2019) show that a convolution-like operator that runs in linear time can actually exceed the performance of the quadratic-time self-attention layer in the Transformer at sentence-to-sentence translation and sentence-level language modelling. However such a mechanism inhibits the flow of information across a large number of time-steps for a given layer, and has not shown to be beneficial for longrange sequence modelling. Dai et al. (2019) propose the TransformerXL, which keeps past activations around in memory. They also propose a novel relative positional embedding scheme which they see outperforms the Transformer's original absolute positional system. Our model incorporates both of these ideas, the use of a memory to preserve prior activations and their relative positional embedding scheme. \n\nThe Sparse Transformer (Child et al., 2019) uses fixed sparse attention masks to attend to roughly \u221a n locations in memory. This approach still requires keeping all memories around during training, however with careful re-materialization of activations and custom kernels, the authors are able to train the model with a reasonable budget of memory and compute. When run on Enwik8, the much larger attention window of 8, 000 improves model performance, but overall it does not significantly outperform a simpler TransformerXL with a much smaller attention window. \n\nThe use of dynamic attention spans is explored in Sukhbaatar et al. (2019). Different attention heads can learn to have shorter or longer spans of attention -and they observe this achieves state-ofthe-art in character-based language modelling. This idea could easily be combined with our contribution -a compressive memory. However an efficient implementation is not possible on current dense-linear-algebra accelerators, such as Google's TPUs, due to the need for dynamic and sparse computation. Our approach builds on simple dense linear algebra components, such as convolutions.",
            "score": 0.6030203949817339,
            "section_title": "RELATED WORK",
            "char_start_offset": 4641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 633,
                    "matchedPaperCorpusId": "57759363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3857421875
        },
        {
            "corpus_id": "277065826",
            "title": "Intra-neuronal attention within language models Relationships between activation and semantics",
            "text": "Studies on attention mechanisms have shown that some heads are specialized, while others are redundant. Luong et al. [129] demonstrated that certain heads capture precise syntactic relations, whereas others focus on global semantic relationships. [130,131] observed that removing several heads does not significantly impact model performance, suggesting compensatory mechanisms among the remaining heads. \n\nStatistical mechanics approaches have been used to analyze interactions between attention paths. [132] modeled the contribution of attention heads via a kernel decomposition : \n\nEach kernel K i corresponds to a specific head, allowing an evaluation of its role in the model's final representation. Results indicate that some heads play a structuring role, while others can be eliminated without significant impact. This observation paves the way for transformer architecture optimizations by reducing redundant heads and improving model interpretability. \n\nThe computational efficiency of transformers has been extensively researched. Sparse Transformers [23] reduce attention complexity to O(n log n) by introducing a sparse attention structure. Reformer [133] optimizes memory management via key-value factorization and local attention. Performer [134] replaces standard attention with a linear approximation, reducing complexity to O(n). This method relies on random projections of keys and queries into a lower-dimensional space, where dot products are computed approximately using kernels favoring efficient factorization. This avoids costly dense matrix multiplications while maintaining high accuracy, making attention scalable even for long sequences. Longformer and BigBird [135] combine local and global attentions to efficiently process long sequences. \n\nOther studies have analyzed attention head specialization in specific contexts. Clark et al. [136] examined BERT attention matrices and found that some heads learn specific syntactic relationships, such as subject-verb dependencies or anaphoric relations. Transformer-XL [137] introduced a recurrent memory mechanism that captures longer-term dependencies, improving text generation and dialogue modeling. \n\nFinally, attention mechanisms have extended to other domains, including computer vision with Vision Transformer (ViT) [138] and Swin Transformer [139], as well as neuroscience and cognitive process modeling [140].",
            "score": 0.6030024048947673,
            "section_title": "body",
            "char_start_offset": 2435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2178
                },
                {
                    "start": 2181,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 122,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1163,
                    "end": 1168,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1256,
                    "end": 1261,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1690,
                    "end": 1695,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1866,
                    "end": 1871,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 2326,
                    "end": 2331,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43994140625
        },
        {
            "corpus_id": "273507316",
            "title": "FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs",
            "text": "Large Transformers: Large Transformers, characterized by their extensive parameters and layers, are primarily employed for complex tasks such as natural language processing (NLP) and computer vision [38]. In these models, particularly in LLMs like GPT [1], the attention mechanism plays a pivotal role, consuming a significant portion of computational resources. Although models such as Vision Transformers (ViT) and Diffusion Transformers [33,40] also incorporate attention mechanisms, the proportion of computation dedicated to attention in these models is relatively small. Consequently, the FlashAttention series is more specifically tailored to large transformer models, where attention computations are more prominent. \n\nFlashAttention series algorithms: FlashAttention employs tiling and recomputation to minimize the number of memory access between the on-chip SRAM (a.k.a shared memory) and high bandwidth memory (HBM). It introduces frequent data flow via SRAM between Tensor Core and Cube Core [9,20]. FlashAttention2 further optimizes the workflow of FlashAttention, exhibiting better parallelism and work partitioning. Based on the characters of newer GPU architectures, such as Hopper and Blackwell, FlashAttention3 hides the the non-GEMM operations under asynchronous General Matrix Multiplication (GEMM) with asynchronous instructions to further improve performance. Appendix A provides a more detailed description. However, FlashAttention2/3 only supports the resource-rich GPUs, neglecting low-resource GPUs and powerful NPUs. What's more, FlashAttention series lacks the capability to reduce the memory occupied by attention_mask and decrease the communication overhead introduced by AllReduce. \n\nUltra-long sequence inference: Limited device memory poses a significant constraint, rendering FlashAttention series incapable of supporting inference with ultra-long sequences (e.g., 256K) on a single node. Notably, the offloading is generally coupled with attention optimization for efficient memory utilization. For instance, both FlexGen [30] and DeepSpeed-Inference [4] design a classical offloading strategies that schedules data among GPUs, CPUs, and disks but lacks fine-grained pipeline design.",
            "score": 0.6020060054415961,
            "section_title": "Related Work",
            "char_start_offset": 3447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "3557281"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "1096373"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "231719476"
                },
                {
                    "start": 1005,
                    "end": 1008,
                    "matchedPaperCorpusId": "233137473"
                },
                {
                    "start": 1008,
                    "end": 1011,
                    "matchedPaperCorpusId": "3887305"
                },
                {
                    "start": 2058,
                    "end": 2062,
                    "matchedPaperCorpusId": "257495837"
                },
                {
                    "start": 2087,
                    "end": 2090,
                    "matchedPaperCorpusId": "250243681"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0855712890625
        },
        {
            "corpus_id": "267627468",
            "title": "FAST: Factorizable Attention for Speeding up Transformers",
            "text": "Transformers are the single deep learning architecture that underpin many recent successful applications in diverse fields, including in natural language processing, speech, computer vision, and biology. \n\nTransformers incorporate a Softmax-based all-to-all score computation mechanism denoted as \"attention\". While this mechanism has proven to be extremely effective in learning tasks, they have a cost that is quadratic in the length of the input (N ) and in the data dimension (D), and need a similar amount of memory. Our goal is to develop a more efficient transformer implementation that is as expressive using a novel attention formulation described in \u00a7 2. quadratic dependence on N , the problem of \"long range attention\" that arises needs to be tackled, and is dealt with in the literature in various ways. \n\n1. Algorithmic: Faster attention algorithms via various approaches, including spectral matrix decompositions, kernel method approximations, and sparsification via locality sensitive hashing have been proposed [6,20,10,11,9,1], but these solutions have not appeared to have gained traction. This appears to be for the most part due to perceived lower expressivity of the resulting transformers. These algorithms are further discussed in \u00a74. \n\n2. Parallelization: Efficient attention via careful parallelization and minimizing communication between the CPU and the GPU [5,4]. This is widely used, but is still quadratic. There are also quadratic frameworks to extend training across multiple nodes and GPUs to allow larger problems to fit in memory [15,16]. \n\n3. Non-transformer Models: New learning architectures are being proposed as an alternative to Transformers. Recent ones which have generated considerable interest include Mamba [8], \n\nRetentive networks [17], and CRATE [23].These do require re-engineering the model pipeline. \n\nIn practice, the really large models use quadratic vanilla attention strategies, and work on the data in batches at various stride-lengths and stitch together the results to reach token lengths to the trillions [3,19]. Previously proposed fast attention mechanisms do not appear to have been integrated into these works.",
            "score": 0.6010576354825629,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1386,
                    "end": 1389,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "221191193"
                },
                {
                    "start": 1570,
                    "end": 1573,
                    "matchedPaperCorpusId": "239768728"
                },
                {
                    "start": 2066,
                    "end": 2069,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1453857421875
        },
        {
            "corpus_id": "268384915",
            "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
            "text": "Transformers (Vaswani et al., 2017) have emerged as the dominant architectures for large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022) due to their remarkable capacities to understand complex text and generate controllable responses.Empirically, the power of Transformers lies largely in their multi-head attention modules, which enable Transformers to capture rich semantic information from textual contexts effectively.For every plus, there is a minus.Despite the success of Transformers' attention modules, these modules exhibit quadratic time and memory complexity concerning sequence length, posing challenges in terms of both computing time and memory overheads as sequence length increases.\n\nVarious efforts have been devoted to making attention modules more efficient and enabling LLMs to process longer sequences.One direction is taking full advantage of a single device's compute and storage units (e.g., a GPU) to process long sequences, such as FlashAttention (Dao et al., 2022).FlashAttention can significantly accelerate the computation of attention modules by using more efficient static random access memory (SRAM) instead of high-bandwidth memory (HBM) in devices to store intermediate attention states.Another direction is using distributed clusters containing multiple devices (e.g., multiple GPUs) to process long sequences, such as RingAttention (Li et al., 2021).RingAttention divides sequences into multiple subsequences and processes subsequences separately on different devices.\n\nAll the above improvements orienting to speedup attention operation have achieved promising results, each targeting different bottleneck.However an intuitive problem is raised -whether we can combine these improvements to achieve a more efficient attention solution.The concept is straightforward, yet in a distributed setting, simple combination of two methods may not benefit from their strength.Moreover , the RingAttention approach cannot directly incorporate with online softmax, and the FlashAttention implementation focuses exclusively on optimizing the computation of attention on a single device.To address these challenges,this paper introduces an efficient distributed attention framework to handle extremely long sequences named \"BurstAttention\".BurstAttention can take full advantage of the",
            "score": 0.5987347850768316,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 718
                },
                {
                    "start": 720,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1012
                },
                {
                    "start": 1012,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1524
                },
                {
                    "start": 1526,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1924
                },
                {
                    "start": 1924,
                    "end": 2131
                },
                {
                    "start": 2131,
                    "end": 2284
                },
                {
                    "start": 2284,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 993,
                    "end": 1011,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16943359375
        },
        {
            "corpus_id": "258987361",
            "title": "Lightening-Transformer: A Dynamically-Operated Optically-Interconnected Photonic Transformer Accelerator",
            "text": "The sparsity opportunity in Transformers mainly lies in three folds [12]. (1) Redundancy of the attention head/token. Attention head/token pruning directly removes unimportant heads or tokens entirely [57]. (2) Redundancy of the token channels. Token channel pruning removes some channels of the token embedding. (3) Redundancy of the attention map. A number of Transformers have proposed to explore sparsity inside the attention map so as to make the attention module more efficient. For example, BigBird [65] and BlockBERT [41] propose structured/block-wise sparse attention patterns, e.g., window-and global-attentions. The sparse attention features sparse computation in both QK T and AV . \n\nOur Lightening-Transformer can be easily extended to support the (1) and ( 2) opportunities, as they remove head/token/channel entirely, resulting in regular dense GEMM. We can also support hardware-friendly structured/block-wise (3) sparse attention patterns by reformulating sparse computation into small chunked dense matrix-matrix/vector-matrix multiplication, as illustrated in Figure 16. To generate sparse attention A = QK T efficiently, we can blockify Q/K matrices based on structured sparse patterns and form groups of matrix-matrix multiplication. Take the block-wise window local attention [65] as an example. Assume the number of tokens is n, the window size is w and the block size is b. The token i will only attends to key matrix K with index i \u2212 (w \u2212 1)/2 to i + (w \u2212 1)/2. We can blockify the Q and K matrix based on the block size b, resulting in \u2308n/b\u2309 chunked Q and K matrices. Based on the window pattern, each chunked Q matrices will compute with only w chunked K matrices, still featuring dense matrix-matrix multiplication. To support AV , we can first compress the sparse attention A row by row. This approach generates dense A chunked matrices/vectors based on whether the sparsity is block-wise or not. Then, we can form vector-matrix/matrix-matrix multiplication between the compressed A with the corresponding rows of V .",
            "score": 0.598643377995583,
            "section_title": "A. Sparsity Support",
            "char_start_offset": 46965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "253523276"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35693359375
        },
        {
            "corpus_id": "273026224",
            "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
            "text": "The attention mechanism, as formulated in Equation 2, presents significant computational and memory challenges, particularly in the computation of   . As the sequence length  increases, the resultant attention scores matrix grows quadratically, leading to a complexity of O ( 2 ). To address this scalability issue, researchers have proposed various optimization techniques, focusing on both memory efficiency and computational speed. \n\nMemory Efficient Attention (MEA) Rabe & Staats (2021) marks a notable advancement in model training optimizations. By leveraging Online Softmax Milakov & Gimelshein (2018) and chunking techniques, MEA reduces memory requirements from O ( 2 ) to O ( \u221a ), enabling the use of larger models or extended sequence lengths within existing hardware constraints. Building upon this foundation, FlashAttention Dao et al. (2022); Dao (2023) focuses on reducing attention latency through IO-aware memory read/write optimizations. Utilizing tiling techniques during computation, FlashAttention achieves a memory overhead of O (), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process. \n\nFor scenarios requiring specific attention masks, several tailored solutions have emerged. Sparse Causal Flash Attention (SCFA) Pagliardini et al. (2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed. However, there remains room for optimization, particularly for complex masking patterns. \n\nOur proposed method, FLASHMASK, builds upon these advancements to support customized complex attention masks. FLASHMASK reduces memory complexity from O ( 2 ) to O () while leveraging sparsity in the attention mask to skip masked blocks.",
            "score": 0.5982207599829131,
            "section_title": "ATTENTION OPTIMIZATION TECHNIQUES",
            "char_start_offset": 4696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2151
                },
                {
                    "start": 2154,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 838,
                    "end": 855,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1536,
                    "end": 1561,
                    "matchedPaperCorpusId": "259063695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.701171875
        },
        {
            "corpus_id": "253158021",
            "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost",
            "text": "The Transformer [40] architecture has been the go-to method for encoding sequential data, due to its superior performance in various tasks such as machine translation [30], image classification [15], and protein language modeling [34]. Its key strength stems from the multi-head attention module, where a so-called attention score matrix computes how contextually important one token is to another for all possible token pairs. Each Transformer layer simultaneously pools the token representations based on the attention scores, eventually returning contextualized features without sequentially traversing through the input sequence as its recurrent neural network-based predecessors [18]. \n\nA well-known drawback of the original Transformer is its high computational cost in time and memory that increases quadratically with sequence length. This is due to the full pairwise computation of attention scores, which prohibits applying it in tasks involving long-range dependencies such as document summarization [19] or high-resolution image processing [53]. Many works have thus focused on developing more efficient alternatives by exploiting fixed or learnable attention sparsity patterns [9,51,22,13], low-rank approximations [45,48], or kernelized attention modules [21,10]. \n\nEven though the efficient alternatives hold theoretical expressibility guarantees [50], they are far from sufficient, still failing to convince practitioners to replace the original Transformer. We believe this is mostly due to their lack of adaptability. They apply the same modifications to unanimously sparsify all the attention modules across layers, without considering the tasks at hand. Such strategy  In multi-head attention, each attention head samples a bipartite graph connecting queries to keys from an underlying SBM. The adjacency of the sampled graph is used as an attention mask to compute the dot products only for the sampled edges. \n\nimposes inductive bias too strongly and often leads to sub-optimal cost vs. performance trade-offs in downstream tasks [29]. In this work, we argue that to retain the utmost potential of Transformers, each attention module should have the ability to flexibly choose between sparse and full attention. This is especially evident when considering many state-of-the-art systems suggest the need for a mixture of dense and sparse attention layers.",
            "score": 0.5964827604788128,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "231939146"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "232404731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451171875
        },
        {
            "corpus_id": "278000642",
            "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations",
            "text": "Sparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression. However, empirical studies reveal utility drops once the prompt surpasses ~128 K tokens, reaffirming that size alone is insufficient for faithful recall.",
            "score": 0.5963812789303806,
            "section_title": "Long-Context Transformers",
            "char_start_offset": 2684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "237266377",
            "title": "Fastformer: Additive Attention Can Be All You Need",
            "text": "Transformer (Vaswani et al., 2017) and their variants have achieved great success in many fields. For example, Transformer is the backbone architecture of many state-of-the-art pre-trained language models in NLP, such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2019). Transformer also shows great promises in vision-related tasks (Dosovitskiy et al., 2021). The core of a Transformer model is self-attention mechanism, which allows the Transformer to model the contexts within an input sequence (Parikh et al., 2016). However, since self-attention computes the dot-product between the input representations at each pair of positions, its complexity is quadratic to the input sequence length (Vaswani et al., 2017). Thus, it is difficult for standard Transformer models to efficiently handle long input sequences (Tay et al., 2020). \n\nThere are many methods to accelerate the Transformer model (Beltagy et al., 2020;Zaheer et al., 2020;Wang et al., 2020b;Kitaev et al., 2020;Tay et al., 2021). For example, BigBird (Zaheer et al., 2020) computes sparse attention instead of a dense one. It uses a combination of local attention, global attention at certain positions and random attention between a certain number of tokens. However, sparse attention usually cannot fully model the global context (Wu et al., 2021b). Linformer (Wang et al., 2020b) exploits the low-rank characteristic of the self-attention matrix by computing approximated ones. It projects attention key and value into low-dimensional matrices that are independent of the sequence length. However, the approximation is in fact context-agnostic, which may weaken the context modeling ability of Transformer. In addition, these methods are not efficient enough when the input sequence length is very long. \n\nIn this paper we propose Fastformer 1 , which is an efficient Transformer variant based on additive attention that can achieve effective context modeling in linear complexity. In Fastformer, we first use additive attention mechanism to summarize the input attention query matrix into a global query vector.",
            "score": 0.5956217259613382,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 256,
                    "end": 278,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 342,
                    "end": 368,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 507,
                    "end": 528,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 703,
                    "end": 725,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 986,
                    "end": 1003,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1307,
                    "end": 1325,
                    "matchedPaperCorpusId": "235294151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "266210450",
            "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
            "text": "Attention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020b), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021). Positional Embedding. In Transformer models, positional embeddings can be primarily categorized into two types: absolute and relative. Earlier versions of Transformers utilize absolute positional encoding. For instance, the vanilla Transformer (Vaswani et al., 2017) model adds sinusoidal positional embeddings to the word embeddings, whereas GPT (Radford et al., 2018) and BERT (Devlin et al., 2018) introduce learnable positional embeddings. Currently, it has become more common to use relative positional embedding. For instance, Transformer-XL (Dai et al., 2019) and T5 (Raffel et al., 2020) adds learnable attention logit bias into attention layers. Alibi (Press et al., 2022) biases attention scores based on the distance between key and query elements. RoPE (Su et al., 2023) multiplies the keys and queries of every attention layer by sinusoidal embeddings. The Alibi and RoPE methods are further improved through the incorporation of an additional bias term (Sun et al., 2022;Chi et al., 2023). LLM.",
            "score": 0.5951584828994106,
            "section_title": "Related Work",
            "char_start_offset": 29719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 482,
                    "end": 501,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 542,
                    "end": 564,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 578,
                    "end": 599,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1169,
                    "end": 1191,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1499,
                    "end": 1520,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1586,
                    "end": 1606,
                    "matchedPaperCorpusId": "237347130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "Transformer-based models (Vaswani et al. 2017) such as BERT (Kenton and Toutanova 2019) and GPT-3 (Brown et al. 2020) have demonstrated state-of-the-art performance in various Natural Language Processing (NLP) tasks, including question answering (Rajpurkar et al. 2016), summarization (Miller 2019), and language modeling (Child et al. 2019). Recently, they have extended their influence to a wide range of applications, including image processing (Touvron et al. 2021) and generative modeling (Child et al. 2019). The most significant factor contributing to the successful performance of Transformer models is the self-attention mechanism. This key component enables transformer models to effectively understand interactions between tokens in the input sequence, model long-range dependencies, and capture contextual information from the entire sequence. However, a well-known scalability issue arises from the quadratic dependency (i.e., O(n 2 )) of self-attention operations on the input sequence length n, leading to slow and memoryintensive processing for long sequence inputs. \n\nTo tackle the scalability challenge, several Efficient Transformers (Tay et al. 2022) have been recently introduced, aiming to reduce the quadratic dependency to a subquadratic level. For example, BigBird (Zaheer et al. 2020) exploits the sparsity of attention to reduce the complexity of attention operation. Reformer (Kitaev, Kaiser, and Levskaya 2020) learns the sparse attention pattern in a datadriven fashion. Linformer (Wang et al. 2020) leverages the low-rankness of the attention map. Nystr\u00f6mformer (Xiong et al. 2021) decomposes the softmax matrix of self-attention with the Nystr\u00f6m method (Wang and Zhang 2013). For an efficient approximation, the decomposed matrix uses landmarks sampled from the input sequence. \n\nAlthough these techniques have all demonstrated successful optimization of self-attention operations, they have their respective limitations. Fig. 1 illustrates a comparative overview of the performance, inference throughput, and memory footprint of various existing efficient transformer models on the Long Range Arena (LRA) benchmark (Tay et al. 2021).",
            "score": 0.594517221211506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 117,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 246,
                    "end": 269,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 285,
                    "end": 298,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 1153,
                    "end": 1169,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1290,
                    "end": 1310,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1404,
                    "end": 1439,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1593,
                    "end": 1612,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1685,
                    "end": 1706,
                    "matchedPaperCorpusId": "6204627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "263835309",
            "title": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and Learned Token Pruning",
            "text": "There are several research papers also making attempts at sparse attention similar to Longformer by combining different global (e.g., random attention generated with graph) Fig. 4 A demonstration of sliding window mechanism for local attention in Transformer, where the token length is n, the window size is w and i is the i-th token in the sequence. \n\nand local attention mechanisms to Transformer-based models; e.g., Extended Transformer Construction (ETC) [2], BigBird [62] and Global Memory Augmentation for Transformers (GMAT) [25]. Most of them are pre-trained on long documents in NLP tasks.",
            "score": 0.5944215141562434,
            "section_title": "Attention Mechanisms of Transformers",
            "char_start_offset": 23208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 598
                }
            ],
            "ref_mentions": [
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3955078125
        },
        {
            "corpus_id": "276961696",
            "title": "Radar: Fast Long-Context Decoding for Any Transformer",
            "text": "Recently, Landmark Attention (Mohtashami & Jaggi, 2023) proposes to use grouped softmax with a special token to reduce computation. Another line of work reduces the time complexity is through attention kernelization, which reformulates the dot-product attention mechanism as a kernel (Tsai et al., 2019;Katharopoulos et al., 2020;Peng et al., 2021;Zandieh et al., 2023). These methods can achieve linear complexity in both training and inference by recurrence. However, the parameters are required to be trained/fine-tuned to fit the new architectures. On the contrary, our method is training-free and can be applied to any pre-trained Transformers. \n\nInstead of modifying the vanilla attention, recent literature also focuses on improving the efficiency in implementation. For example, Flash Attention series (Dao et al., 2022;Dao, 2023) leverage the tiling technique to avoid memory-bound attention operations. Memory-efficient attention (Rabe & Staats, 2021) reorders the attention calculation to allocate only the constant space for any context length. Our work is orthogonal to these methods and can be combined together. \n\nLong-context ability via retrieval. In certain cases, Transformers can gain the long-context ability by retrieving related documents like retrieval-augmented generation (Lewis et al., 2020) based on kNN search. For instance, Memorizing Transformer (Wu et al., 2022) recurrently caches previous tokens in a kNN index. Unlimiformer (Bertsch et al., 2023) uses a kNN index to retrieve the encoded chunks in the encoder-decoder attention. However, these methods can only be used with the specialized architectures after training. They may also suffer from the exponential search quality degradation when the dimension increases (Beyer et al., 1999). Unlike these methods, Radar is a training-free method with a rigorous performance guarantee independent of the dimension.",
            "score": 0.5927595782275925,
            "section_title": "RELATED WORK",
            "char_start_offset": 21296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 55,
                    "matchedPaperCorpusId": "258887482"
                },
                {
                    "start": 284,
                    "end": 303,
                    "matchedPaperCorpusId": "201698358"
                },
                {
                    "start": 303,
                    "end": 330,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 330,
                    "end": 348,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 348,
                    "end": 369,
                    "matchedPaperCorpusId": "263831240"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1298,
                    "end": 1318,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1377,
                    "end": 1394,
                    "matchedPaperCorpusId": "247519194"
                },
                {
                    "start": 1459,
                    "end": 1481,
                    "matchedPaperCorpusId": "258436892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37255859375
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "Transformer models [82] have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger [5] and deeper [83], but equipping them with longer context remains difficult [80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences. \n\nMany approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [51,74] to low-rank approximation [12,50,84], and their combinations [3,9,92]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). \n\nIn this paper, we argue that a missing principle is making attention algorithms IO-aware [1]-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1  In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. \n\nFlashAttention does not read and write the large  \u00d7  attention matrix to HBM, resulting in an 7.6\u00d7 speedup on the attention computation. \n\nGPUs, compute speed has out-paced memory speed [61,62,63], and most operations in Transformers are bottlenecked by memory accesses [43].",
            "score": 0.591260805876638,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 704,
                    "end": 708,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 742,
                    "end": 745,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 776,
                    "end": 778,
                    "matchedPaperCorpusId": "248498407"
                },
                {
                    "start": 778,
                    "end": 781,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1238,
                    "end": 1241,
                    "matchedPaperCorpusId": "6264984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "273549477",
            "title": "The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI",
            "text": "Attention computation is arguably the most critical component of the Transformer model [33]. Due to the quadratic nature of the attention computation complexity, a lot of efforts have been made to speed up the attention computation such as the efforts by OpenAI team in [4]. One widely-used attention computation approach is called FlashAttention in [6,7]. The essence of FlashAttention method for attention computation in Transformer model for large language model (LLM) is the application of tiling by splitting a large matrix into tiles to have a finer granularity to deal with I/O with differentiated memory (HBM, SRAM, etc.) constraint hierarchy in GPU. \n\nFlashAttention [6,7] is of exact attention computation in its current form. As discussed in [22], fast causal attention computation for sparse FlashAttention can be more efficient. We considers attention computation in Transformer model for LLM in a probabilistic way. The presented probability density function (PDF) with respect to the block/tile distance in the matrix follows a constrained harmonic deduction philosophy. The presented PrFlashAttention dynamically and probabilistically skips less-related rows/columns in Query/Key (Q/K) matrix along a tensor dimension, say the number of Head dimension of , in the tensor shape of (Batch, Head, Context Length, Head Dimension) during attention computation while supporting causal masks for auto-regressive models by reshaping the tensors. \n\n]),",
            "score": 0.5910324647954153,
            "section_title": "Probabilistic FlashAttetnion",
            "char_start_offset": 42612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1459
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27880859375
        },
        {
            "corpus_id": "269899568",
            "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
            "text": "The attention mechanism in transformer-based language models is a slow and memory hungry process.State-of-theart optimization mechanisms, such as FlashAttention-2, have cleverly addressed this challenge; however, they fail to adapt to the computationally distinct phases of inference.We observe that FlashAttention-2 fails at parallelization across the contextlength dimension of operation during the decode phase of inference, thus resulting in low occupancy of the underlying hardware.As state-of-the-art models continue to push the limits on supporting increasingly long context lengths, optimization techniques that optimally parallelize over this dimension will become increasingly important.\n\nTo address this challenge, we propose LeanAttention, a scalable, exact-attention execution mechanism, specifically designed to optimize the decode phase of generative transformer models, though general enough to optimize the prefill-phase as well.LeanAttention utilizes the associative property of online softmax calculation as a reductive property and extrapolates the state-of-the-art \"stream-K\" matrix decomposition technique to the attention mechanism.This allows us to efficiently parallelize and execute the attention mechanism for the decode phase of inference.Our measurements show that LeanAttention delivers an average speedup of 2.6x over FlashAttention-2 and offers up to 8.33x speedup for 512k context sizes compared to FlashAttention-2.Notably, in a multi-GPU execution scenario with a large number of attention heads, the speedup realized by LeanAttention continues to increase as context length increases, providing 1.7x speedup over FlashAttention-2 (and FlashDecoding) at 512k context length.LeanAttention delivers near 100% processor occupancy, enabling the efficient scaling of next generation LLMs that leverage large context lengths.",
            "score": 0.5905562818128864,
            "section_title": "VII. CONCLUSION",
            "char_start_offset": 42934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 284
                },
                {
                    "start": 284,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 697
                },
                {
                    "start": 699,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "267522774",
            "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
            "text": "The main advantage of linear transformers is that their training latency remains the same across different context lengths given that we use the same \"batch size\" (tokens per training step) for all the context lengths. To show that it is the case, we report the training latencies (in terms of steps/sec) of our models and other attention mechanisms in Table 4. Using the same batch size across different context lengths, we note that the steps/sec of linear transformers such as Polysketch and Performer remain almost constant whereas the steps/sec of quadratic-time transformers decreases with increasing context lengths. The results show that, depending on the model structure, models using our Polysketch attention mechanism are significantly faster to train than models using a quadratic attention mechanism such as softmax (implemented via FlashAttention) at long context lengths.",
            "score": 0.590476360871814,
            "section_title": "E.1. Training Latency Comparison",
            "char_start_offset": 40087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 886
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1827392578125
        },
        {
            "corpus_id": "248377000",
            "title": "ChapterBreak: A Challenge Dataset for Long-Range Language Models",
            "text": "Bigbird (Zaheer et al., 2020) To reduce the quadratic complexity of self-attention in the standard Transformer, the Bigbird model employs a mixture of global, random and local attention mechanisms, which successfully reduce the complexity to linear. The idea is to insert each sequence O(1) global tokens, which attend to all other tokens. The rest tokens attend to their neighbor tokens, random tokens in the sequence as well as the inserted global tokens. A very similar idea is developed concurrently in the Longformer (Beltagy et al., 2020). The Bigbird model we fine-tuned is the decoder part of the released checkpoint. We fine-tune the model with causal LM objective on 14K books of PG-19 with peak learning rate 0.0001 for 100K steps. We set attention type to be \"original_full\" instead of using \"block_sparse\" during fine-tuning. Training is completed on a single RTX8000 GPU for around 6 days.\n\nLocal Transformer Rather than implementing all three types of sparse attention in Bigbird, the Local Transformer relies only on the local attention, i.e., each token attends to neighbors within a local window. The maximum attainable sequence length scales linearly with the number of layers, e.g., with window size k, the token representation at layer l theoretically covers information in a range of k \u00d7 l tokens.\n\nRouting Transformer (Roy et al., 2021) Different from previously described models which use position-based sparse attention, the Routing Transformer employs content-based sparse attention. Namely, each token are routed to clusters and the attention is performed only within each cluster. The clustering operation effectively reduces the quadratic complexity in length L to O(L 1.5 ). Both the RT and LT checkpoint we used were trained on PG-19 (Rae et al., 2020). For both RT and LT, we evaluate on single RTX8000 GPU.\n\nGPT-2/3 The GPT models have a lot shorter maximum input length than the rest models we evaluated. While GPT-2 model does not use sparse attentions at all, GPT-3 model adopts alternated layers of sparse and dense self-attention. We use the GPT",
            "score": 0.5894183635155492,
            "section_title": "C Baselines",
            "char_start_offset": 19188,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 29,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "278481359",
            "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips",
            "text": "Jianlin [51] demonstrated that models incorporating RoPE achieved improved performance on long-sequence benchmarks compared to alternative approaches. \n\nEfficient Attention Computation: Even with enhanced positional encoding, the standard selfattention mechanism has a quadratic O(n 2 ) memory cost when processing large amounts of tokens. \n\nFlashAttention [52] is an exact attention computation method optimized to minimize memory reads and writes, effectively making the computation I/O-bound rather than memory-bound. FlashAttention introduces a tiling strategy to store intermediate results in high-speed on-chip memory, dramatically reducing the need for expensive GPU memory access. Thus, while naive self-attention scales quadratically with sequence length, FlashAttention uses memory linear in the sequence length and achieves significant speedups. \n\nAnother recent technique is NTK (Neural Tangent Kernel)-aware interpolation, which \"stretches\" the rotary positional embeddings during inference. This enables an extension of the context length without retraining. Alibaba's Qwen-7B/14B models, for example, utilize NTK-aware interpolation, log-scaled attention, and local window attention to achieve context lengths well beyond 8K tokens [53]. The underlying principle of RoPE is that when frequencies are not modified, extending the trained context beyond boundaries results in angles that the model has never encountered. However, NTK interpolation ensures the rotation angles are adjusted within a range that the model is familiar with the new maximum length. This stretching technique has been empirically demonstrated in code-generation models (e.g., CodeLlama was extended from 16K to 100K with minimal performance degradation using a similar approach). \n\nAdvanced LLMs employ several mechanisms to enable long-context reasoning. These include positional encoding techniques that facilitate extrapolation-such as ALiBi's distance-based linear bias and RoPE's rotational encoding-and efficient attention algorithms like FlashAttention that overcome the quadratic memory bottleneck. In combination, these mechanisms have significantly increased the effective context length from roughly 1K to as high as 100K tokens, thereby expanding the range of applications from reading long contracts and logs to maintaining coherent context over vast document collections.",
            "score": 0.58865466876237,
            "section_title": "Memory Integration for Long-Term Context",
            "char_start_offset": 44145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375
        },
        {
            "corpus_id": "258832477",
            "title": "FIT: Far-reaching Interleaved Transformers",
            "text": "The primary sources of computational complexity in transformers arise from self-attention, which has a complexity of O(L 2 d), and the feed-forward network (FFN) with a complexity of O(Ld 2 ). It is important to be aware of the typical scale of L (sequence length) and d (embedding dimension) that we may encounter. With the recent surge of large models, d can vary from 1024 to 18432 [3,13]. Longer contexts are often desired, leading to L ranging from a few hundred to millions or even higher. In cases where d and L are of comparable magnitudes, such as d = 4096 and L = 2048, approximations applied to the original quadratic attention may yield negligible gains as they usually shift the computational burden from O(L 2 d) to O(Ld 2 ). Furthermore, optimized implementations of self-attention, such as FlashAttention [16], can improve efficiency and eliminate the need for O(L 2 ) memory. Therefore, the full quadratic attention remains the most effective and efficient operation when dealing with relatively short sequences (e.g., in the order of hundreds, or thousands). \n\nConsidering the efficacy of full attention for shorter sequences and our objective of developing a single architecture capable of handling sequences of varying lengths, we will focus on reviewing techniques that integrate global attention into architectures with local quadratic attentions (such as window attention). One simple approach is to leverage global attention for only a few transformer layers. Although this primarily reduces the constant factor in terms of computational complexity, it has demonstrated practical effectiveness in certain applications [3,34]. Another approach is to employ techniques such as shifted window [35] or convolution [51] to propagate information across groups, with a disadvantage that it only affects adjacent groups at a time. A more general approach involves the use of sparse/axial attention [11,24], mixed or learned attention patterns [50,52,32,41,40]. These methods allow for more flexible attention patterns, but may involve sparse operations that are not always accelerator-friendly. Another family of methods [14,39,4,27] incorporates recurrent mechanisms to connect transformers on local windows.",
            "score": 0.5886046480675483,
            "section_title": "Background and related work",
            "char_start_offset": 2370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1642,
                    "end": 1645,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1645,
                    "end": 1648,
                    "matchedPaperCorpusId": "247793203"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "245353951"
                },
                {
                    "start": 1959,
                    "end": 1963,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1963,
                    "end": 1966,
                    "matchedPaperCorpusId": "235422255"
                },
                {
                    "start": 1969,
                    "end": 1972,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1972,
                    "end": 1975,
                    "matchedPaperCorpusId": "235829099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40869140625
        },
        {
            "corpus_id": "263310483",
            "title": "Efficient Streaming Language Models with Attention Sinks",
            "text": "Sparse Transformers. The literature on efficient Transformer models primarily focuses on reducing the computational and memory complexity of the self-attention mechanism. A relevant line of work involves sparsifying the attention matrix by restricting the field of view to fixed, predefined patterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Transformer (Child et al., 2019)  Building on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alternative, utilizing global tokens, local sliding window attentions, and random attention. However, these methods have several limitations. First, Sparse Transformer and ETC require custom GPU kernels for a specific block-sparse variant of matrix-matrix multiplication. Second, LongFormer, ETC, and BigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models. Third, these methods are incompatible with pre-trained models, necessitating retraining from scratch. In contrast, our method offers ease of implementation using standard GPU kernels and is compatible with pre-trained autoregressive language models using dense attention, which are prevalent in the NLP community. This compatibility provides a significant advantage, allowing for the leveraging of existing pre-trained models without any fine-tuning. \n\nConcurrent Works. Our research coincides with the work of Han et al., who conducted a theoretical study on the length generalization failure of language models, identifying three out-of-distribution factors. Their approach, inspired by this analysis, involves employing a \"\u039b\"-shaped attention pattern and reconfiguring position encoding distances to enhance length generalization in LLMs. This approach bears a resemblance to our methodology. However, our work uncovers the \"attention sink\" phenomenon, wherein Transformer models tend to assign high attention scores to initial tokens with small semantics. This phenomenon extends beyond the scope of length generalization failure, indicating a more pervasive issue in Transformer models. We observe this \"attention sink\" behavior not only in auto-regressive language models but also in encoder Transformers such as BERT (see Section H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in Transformer architectures.",
            "score": 0.5882096542239414,
            "section_title": "B ADDITIONAL RELATED WORKS",
            "char_start_offset": 27608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2377
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 378,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 446,
                    "end": 468,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "254123526",
            "title": "A Comparative Study of Pretrained Language Models for Long Clinical Text",
            "text": "Various attention mechanisms have been proposed to handle the large memory consumption of the attention operations in the vanilla transformer architecture. Transformer-XL [22] segmented a long sequence into multiple small chunks and then learned long-term dependencies with a leftto-right segment-level recurrence mechanism. Transformer-XL learns 5.5 times longer dependencies than the vanilla transformer models but loses the advantage of bidirectional representation of BERT-like models. In another study, Reformer [23] applied two techniques to reduce the complexity of transformer architecture by replacing dot-product attention operation with locality-sensitive hashing and sharing the activation function among layers. Reformer was able to process longer sequences at a faster speed and be more memory efficient. However, this enhancement improves space, time, and memory efficiency, but not accuracy on specific tasks. \n\nAlmost simultaneously, Longformer [13] and BigBird [14] were proposed to drastically alleviate the memory consumption of transformer models by replacing the pairwise full attention mechanisms with a combination of sliding window attention and global attention mechanisms. \n\nThey are slightly different regarding implementation and configuration of the global and local attention mechanism, where BigBird introduces additional contrastive predictive coding to train global tokens [14]. Both models support input sequences up to 4.096 tokens long (8 times the input sequence limit of BERT) and significantly improve performance on long-text question answering and summarization tasks. However, the adaptability of these long sequence transformers to the clinical and biomedical fields, where document length mostly exceeds the limits of BERT-like models, has not been investigated.",
            "score": 0.5880560313008887,
            "section_title": "Transformers for Long Sequences",
            "char_start_offset": 5614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47802734375
        },
        {
            "corpus_id": "236912844",
            "title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
            "text": "The attention mechanism was first introduced and used in conjunction with a recurrent network in [2] to help memorize long sequences in neural machine translation. The Transformer [17] further proposed the multi-head attention mechanism which performs multiple self-attention in parallel to achieve more effective learning at different scales. Recently, the Transformer architecture has extended its application from NLP to vision tasks, especially as ViT [8] has achieved the state-of-theart results for image classification with an architecture that closely resembles the initial NLP version. \n\nThe other examples of using the Transformer as a viable alternative to CNNs include object detection [3,7,14,28], video processing [24,26], image enhancement [4,22], etc. Due to the enormous computational and memory requirements of the transformer models, there has been increasing interest in transformer optimizations along several directions. \n\nThe regular transformer scales quadratically with the number of tokens N in the input sequence, which limits its usage in settings with limited resources. Several works [19,12,6,11] have proposed techniques to reduce the complexity from O(N 2 ) to as low as O(N ), yielding linear transformers. However, these techniques only outperform the regular method for very long sequences. \n\nAnother direction focuses on the sparsity in transformers. [5] exploits a factorized sparse representation of attention. [18] leverages token and head sparsities to reduce the attention computation and memory access for NLP tasks. For ViT, [27] is an early example that prunes channels according to the learnable importance scores. In our work, the reduction of the transformations in self-attention can be viewed as a fixed layer pruning scheme, which is unexplored by any of these works. \n\nEvolving the Transformer architecture can also significantly improve performance. For ViTs, the evolutions can be categorized by how the key components are implemented, including positional encoding [9,21], token embedding [23], projection for attention [20], and model hierarchy [20,21,9]. Selecting deployment-friendly operators for non-linearity and normalization can also greatly improve inference latency.",
            "score": 0.5865599847084462,
            "section_title": "Related Work",
            "char_start_offset": 2672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295166015625
        },
        {
            "corpus_id": "269033427",
            "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
            "text": "Memory serves as a cornerstone of intelligence, as it enables efficient computations tailored to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based LLMs (Brown et al., 2020;Touvron et al., 2023;Anil et al., 2023;Groeneveld et al., 2024) have a constrained context-dependent memory, due to the nature of the attention mechanism. \n\nFigure 1: Infini-attention has an additional compressive memory with linear attention for processing infinitely long contexts. {KV} s\u22121 and {KV} s are attention key and values for current and previous input segments, respectively and Q s the attention queries. PE denotes position embeddings. \n\nThe attention mechanism in Transformers exhibits quadratic complexity in both memory footprint and computation time. For example, the attention Key-Value (KV) states have 3TB memory footprint for a 500B model with batch size 512 and context length 2048 (Pope et al., 2023). Indeed, scaling LLMs to longer sequences (i.e. 1M tokens) is challenging with the standard Transformer architectures and serving longer and longer context models becomes costly financially. \n\nCompressive memory systems promise to be more scalable and efficient than the attention mechanism for extremely long sequences (Kanerva, 1988;Munkhdalai et al., 2019). Instead of using an array that grows with the input sequence length, a compressive memory primarily maintains a fixed number of parameters to store and recall information with a bounded storage and computation costs. In the compressive memory, new information is added to the memory by changing its parameters with an objective that this information can be recovered back later on. However, the LLMs in their current state have yet to see an effective, practical compressive memory technique that balances simplicity along with quality. \n\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively process infinitely long inputs with bounded memory footprint and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention (Figure 1).",
            "score": 0.5861339999147822,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 208,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 913,
                    "end": 932,
                    "matchedPaperCorpusId": "253420623"
                },
                {
                    "start": 1268,
                    "end": 1292,
                    "matchedPaperCorpusId": "198179407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153076171875
        },
        {
            "corpus_id": "239885427",
            "title": "Hierarchical Transformers Are More Efficient Language Models",
            "text": "Transformer models (Vaswani et al., 2017) are capable of solving many sequence modeling tasks, including classical NLP tasks (Devlin et al., 2019), summarization (Zhang et al., 2020), language modeling (Radford et al., 2019;Brown et al., 2020), code generation (Chen et al., 2021), or even music generation (Huang et al., 2018;Dhariwal et al., 2020) and image generation (Parmar et al., 2018;Chen et al., 2020;Ramesh et al., 2021). One compelling feature of Transformers is their ability to handle long contexts given as part of the input. This is particularly visible in tasks where the output depends on parts of the context that may not be * Equal contribution. Order determined by coin toss. close-by in the generated sequence, like in summarization, where the summary may need to refer to information scattered across the context, or in largescale image generation, where pixels belonging to the same object may be far apart in the generation order. Transformers excel at such tasks thanks to self-attention, and they are used with longer and longer contexts. Transformer-XL Hourglass Figure 1: Bits-per-character vs. training cost for baseline (orange) and hierarchical Transformers (green). We observe significant perplexity improvements on enwik8 over the vanilla Transformer-XL baseline, see text for details.\n\nThe ability of Transformers to handle long contexts comes at a price: each self-attention layer, at least in its original form, has complexity quadratic in the length of the context. When a stack of n Transformer layers is used, both memory and time complexity is equal to O(L 2 n) where L is a sequence length and n number of decoder blocks. Due to this limitation, vanilla transformers are infeasible to train on tasks with very long input sequences, for instance, on high-resolution images. This issue has been studied extensively, and a number of techniques were introduced that modify attention mechanism without changing overall transformer architecture (Child et al., 2019;Roy et al., 2020;Ren et al., 2021). These sparse attention mechanisms reduce the complexity of self-attention but still force the model to operate on",
            "score": 0.5860720981296832,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33984375
        },
        {
            "corpus_id": "235254329",
            "title": "An Attention Free Transformer",
            "text": "Self attention mechanisms, represented by Transformers [1], have driven the advancement of various machine learning problems, including language understanding [2,3] and computer vision applications [4][5][6]. Different from classic model architectures such as Convolutional Neural Nets (CNNs) or Recurrent Neural Nets (RNNs), Transformers enable direct interaction between every pair of elements within a sequence, which makes them especially powerful at capturing long term dependencies. \n\nHowever, Transformers require high computational costs. The cause of this challenge is the need to perform attention operations that have quadratic time and space complexity w.r.t. the context size. This makes it difficult for Transformers to scale to inputs with large context sizes. A number of recent works have been dedicated to addressing the scalability issue of Transformers [7][8][9][10][11][12][13]. The common idea here is to approximate the full attention operation, with the techniques ranging from sparsity, locality sensitive hashing, low rank decomposition, kernel approximation, etc.. In this paper, we propose a computational module that does not use or approximate the standard dot product attention. We hence name our model the attention free transformer (AFT). Similar to dot product attention, AFT is composed of the interaction of three quantities, namely the query, key and value (Q, K, V ). The difference is that, in AFT the key and value (context) are first combined Figure 1: Left: average relative 2d attention maps from a pretrained 12 layer 6 head ViT [5]. Right: relative position biases learned by a AFT-conv with comparable size. Each row represents a layer (with layer index ranging from {0, 2, 4, 6, 8, 10}); Each column represents a head. See the Appendix for a more complete version. \n\nTable 1: Complexity comparison with different Transformers: Reformer [8], Linear Transformer [11], Performer [13] (only variants that support the causal mode are shown). Here T, d denote the sequence length and feature dimension, respectively. together with a set of learned position biases. The query is then combined with the reduced context with element-wise multiplication. See Figure 2 for an illustration.",
            "score": 0.5853428767508506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1907,
                    "end": 1911,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11798095703125
        },
        {
            "corpus_id": "267522774",
            "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
            "text": "Transformer-based models (Vaswani et al., 2017) are stateof-the-art for many natural language tasks, leading to breakthroughs in machine translation, language understanding (Devlin et al., 2019), and language modeling (Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2023;Anil et al., 2023). However, the quadratic time and space complexity of the attention mechanism limits scalability for long context lengths. Numerous \"efficient transformers\" have been proposed to address this issue (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020;Han et al., 2023). These variants approximate2 the standard attention mechanism. A survey by Tay et al. (2022) provides a broad overview of these techniques. While many efficient transformer constructions achieve linear theoretical training complexity, the survey observes that practical training speedups are often less significant, with potential losses in model quality. This explains the continued dominance of vanilla transformers. \n\nIn this work, we focus on improving training latency for transformer models in decoding-only tasks, specifically language modeling trained via next-word prediction. Our techniques are generalizable to encoding-only and encoderdecoder transformers, with potential applications beyond language modeling. We will first briefly discuss existing approaches to make training of transformer models faster and then place our contributions in context. \n\nMemory efficient and I/O aware approach. Recent work by (Dao et al., 2022;Dao, 2023) on FlashAttention and FlashAttention-2 seeks to enable vanilla transformer training on long contexts. This is achieved through I/O-aware optimizations like blocking/tiling and rematerialization, significantly improving memory efficiency. While this reduces the O(n 2 )3 HBM (High-Bandwidth Memory) requirements of accelerators (GPUs/TPUs), enabling training on thousands of tokens, the computational cost per step remains O(n 2 ) (see Figure 1), and this remains a barrier to further scaling the context length. \n\nApproximate softmax attention via sparsification.",
            "score": 0.5848936538673353,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 173,
                    "end": 194,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 218,
                    "end": 238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 509,
                    "end": 536,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1501,
                    "end": 1519,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46826171875
        },
        {
            "corpus_id": "264451707",
            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
            "text": "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information. Additionally, modifications to the model architecture make these methods challenging to apply to existing pre-trained LLMs. Conversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity of models to tack the entire long sequences without explicit drops of context information. \n\nLength Extrapolation. Building on the foundation laid by ALiBi (Press et al., 2022), a series of works (Sun et al., 2023;Chi et al., 2022;2023) seek to train the Transformer-based models on a short length, while directly testing on longer counterparts. These methods substitute the position embedding with bias introduced into attention scores, thereby incorporating positional information. Notably, the bias typically gives higher profits to closer tokens. This mechanism intuitively amplifies the local context for each token at the expense of distant information. Consequently, these length-extrapolation methods encounter challenges in effectively handling long contexts in practical applications (Pal et al., 2023). However, our CLEX demonstrates remarkable effectiveness in practical tasks such as summarization, indicating the de facto extrapolation ability for applications. \n\nPosition Embedding (PE) Scaling. Recent research has sought to extend the context length of Transformers through the scaling of the extensively employed RoPE.",
            "score": 0.5842022294971193,
            "section_title": "RELATED WORK",
            "char_start_offset": 21877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 114,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 304,
                    "end": 326,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1334,
                    "end": 1354,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1374,
                    "end": 1392,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1392,
                    "end": 1409,
                    "matchedPaperCorpusId": "248965309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "258840932",
            "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability",
            "text": "There has been plenty of prior work to enable transformers to handle long input more effectively and efficiently. Since the inefficiency comes from the quadratic dependency on sequence length because of the dense attention operation, a large portion of research simulates the attention operation with certain approximations, for example, replacing the dense attention matrix with a sparse version, or assume that it satisfies certain low-rank structures. We briefly review some methods on this topic in this section. For a more detailed survey, we refer the readers to [36]. \n\nSparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention [27] and Multi-passage BERT [41] use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird [46] further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions. On the other front, Orthogonal Transformer [13] utilizes an iterative approach to construct an orthogonal vector basis in Euclidean space, then perform windowed attention on grouped tokens after orthogonal projection. \n\nLow-rank Approximation. The self-attention matrix, at the center of transformer, has been found to display low-rank behaviors after pre-training. Linformer [40] performed spectrum analysis on the pre-trained attention matrix, and the results indicate that the top 128 singular values composite 88%-96% of the entire 512 singular values across attention heads and layers. Based on this observation, Linformer added low-rank projection matrices in attention to approximate the original attention matrix. On a similar notion, Drone [4] extended the low-rank approximation scope to all matrices in transformer via data-driven optimal compression.",
            "score": 0.5832486805887114,
            "section_title": "Attention Methods",
            "char_start_offset": 23458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "258509623"
                },
                {
                    "start": 1918,
                    "end": 1921,
                    "matchedPaperCorpusId": "245003869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "270380294",
            "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
            "text": "However, due to the lack of hardware-aware efficient implementation, its actual wall-time training efficiency is often worse than the dense attention optimized with FlashAttention (Dao et al., 2022a;Dao, 2023;Shah et al., 2024). \n\nIn this work, we choose Sliding Window Attention, a simple static sparse attention pattern, because it can easily leverage the highly optimized FlashAttention kernels to enjoy an actual training speed-up over its dense self-attention counterpart. \n\nLength Extrapolation Many previous works have focused on extending the context length of pretrained Transformers to improve their performance on long-context tasks. Methods such as LM-Infinite (Han et al., 2023), StreamingLLM (Xiao et al., 2024), and LongLoRA (Chen et al., 2023b) achieve linear complexity for length extrapolation, but they can only stabilize perplexity beyond the training sequence length rather than significantly improve it. In contrast, we demonstrate that pretraining Transformers with Sliding Window Attention from scratch enables natural improvements in perplexity beyond the training sequence length. Other approaches, including LLaMA-2-Long (Xiong et al., 2023), LongLLaMA (Tworkowski et al., 2023), PI (Chen et al., 2023a), LongRoPE (Ding et al., 2024) and Self-Extend (Jin et al., 2024), attempt to extend the full attention through modifying position embedding or continual training strategies, but they typically retain quadratic complexity in the attention mechanism with additional computation or memory I/O overhead, therefore they do not scale well to very long sequences. Although these methods achieve an improved perplexity on a sequence length that is multiple times longer than the training sequence length, their perplexity still explodes if the sequence is extremely long. Our method achieves both linear complexity and superior extrapolation performance compared to zero-shot length extrapolation methods, such as Self-Extend, under the perplexity metric. However, we acknowledge that, in terms of zero-shot retrieval performance, our method still lags behind these approaches.",
            "score": 0.5828709325338959,
            "section_title": "A RELATED WORKS",
            "char_start_offset": 31100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 706,
                    "end": 725,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 740,
                    "end": 760,
                    "matchedPaperCorpusId": "262084134"
                },
                {
                    "start": 1180,
                    "end": 1205,
                    "matchedPaperCorpusId": "259360592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "267938077",
            "title": "Cross-resolution land cover classification using outdated products and transformers",
            "text": "To reduce computation complexity and memory bottlenecks in attention mechanisms, numerous methods have been proposed. In the Swin Transformer, attention is constrained to non-overlapping local windows, and a shifting window operation is introduced to enable information communication between adjacent windows. In order to achieve larger receptive fields within appropriate computational budgets, subsequent works have designed various sparse patterns, such as dilated windows [32] and cross-shaped windows. However, these fixed sparse patterns are handcrafted and lack adaptability to varying data content. By contrast, a BiFormer [33] is a dynamically sparse attention mechanism with query-aware ability, aiming to attend to each query using only a small portion of key-value pairs that are semantically relevant, without scattering attention on irrelevant keys. This effectively reduces computation and memory costs while focusing on the informative parts of the data.",
            "score": 0.5828030644013664,
            "section_title": "Land Cover Classification Based on Vision Transformers",
            "char_start_offset": 9703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 970
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "247939839"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "257532403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435791015625
        },
        {
            "corpus_id": "257532403",
            "title": "BiFormer: Vision Transformer with Bi-Level Routing Attention",
            "text": "Vision transformers. Transformers are a family of neural networks that adopt channel-wise MLP blocks for perlocation embedding (channel mixing) and attention [42] blocks for cross-location relation modeling (spatial mixing). Transformers were originally proposed for natural language processing [13,42] and then introduced to computer vision by pioneering works such as DETR [1] and ViT [15]. In comparison with CNNs, the biggest difference is that transformers use attention as an alternative to convolution to enable global context modeling. However, as vanilla attention computes pairwise feature affinity across all spatial locations, it incurs a high computation burden and heavy memory footprints, especially for high-resolution inputs. Hence, an important research direction is to seek more efficient attention mechanisms. \n\nEfficient attention mechanisms. A large volume of works have been proposed to reduce the computation and memory complexity bottlenecks of vanilla attention by utilizing sparse connection patterns [6], low-rank approximations [43] or recurrent operations [11]. A thorough survey of these attention variants can be found at [39]. In the scope of vision transformers, sparse attention gains its popularity recently due to the tremendous success of Swin transformer [29]. In Swin transformer, attention is restricted to non-overlapping local windows, and the shift window operation is introduced to enable inter-window communication between adjacent windows. To enable larger and even quasi-global receptive fields under a reasonable computation budget, several follow-up works introduce different handcrafted sparse patterns, such as dilated windows [41,46] or cross-shaped windows [14]. There are also works that try to make the sparse pattern adaptive to data, such as DAT [48], TCFormer [53] and DPT [5]. While these works reduce the number of key/value tokens via different merging or selection strategies, these key/value tokens are shared by all queries on an image. Instead, we explore query-aware key/value token selection. The key observation which motivates our work is that the attentive region for different queries may differ significantly according to the visualization of pretrained ViT [15] and DETR [1].",
            "score": 0.5822666498208571,
            "section_title": "Related Works",
            "char_start_offset": 5595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "247939839"
                },
                {
                    "start": 1683,
                    "end": 1686,
                    "matchedPaperCorpusId": "238531695"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "235694312"
                },
                {
                    "start": 1804,
                    "end": 1808,
                    "matchedPaperCorpusId": "245650206"
                },
                {
                    "start": 1819,
                    "end": 1823,
                    "matchedPaperCorpusId": "248239926"
                },
                {
                    "start": 1832,
                    "end": 1835,
                    "matchedPaperCorpusId": "236635525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "276725385",
            "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
            "text": "In the self-attention mechanism [58], tokens are projected into the query, key, and value matrices Q, K, V \u2208 R H\u00d7L\u00d7D , where H is the number of attention heads, L is the input length, and D is the hidden dimension of each head. The attention weights matrix W attn \u2208 R L\u00d7L is computed as: \n\nwhich quantifies token-to-token interactions across the sequence. To maintain numerical stability during the exponentiation, the Log-Sum-Exp (LSE) [2] trick is commonly employed. Let Z = QK \u22a4 \u221a d and denote by z j the j-th component of a row z. Then, LSE can be written as: \n\nUsing this, the safe Softmax can be expressed as: \n\nand the entire dense attention distribution in a numerically stable form is: \n\nThis operation, however, requires constructing an L \u00d7 L attention matrix, leading to O(L 2 ) time and memory complexity, which becomes prohibitive for long sequences. \n\nFlashAttention [12,13,50] addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention.",
            "score": 0.5819239138066779,
            "section_title": "FlashAttention",
            "char_start_offset": 9079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 885,
                    "end": 888,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "271098045"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "273228328",
            "title": "InAttention: Linear Context Scaling for Transformers",
            "text": "Decoder-based transformer stacks [16] have demonstrated syntactic and semantic understanding of language and other time-series data, achieving state-of-the-art few-shot [3] performance across nearly any natural language task. They exhibit predictable scaling laws with respect to number of parameters and the amount of training data they ingest: bigger is better and we generally know by how much [8]. \n\nThese Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques: ...For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks... while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token. \n\nPerhaps the most human-interpretable form of sparse attention is sliding window attention (e.g. Mistral's [1]), which uses a lower-diagonal banded matrix as the attention mask.",
            "score": 0.580984630268669,
            "section_title": "Background and Introduction",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "247187613",
            "title": "Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism",
            "text": "Existing efficient transformers usually sparsify the full attention mechanism to densely connected clusters [24,22,13,33] or approximate it with low-rank projection [29]. As our method is a good approximation of the full attention mechanism and brings wall time speedup at arbtrary sequence length, it can potentially be combined with the existing efficient transformers. 17. We observe that the computation circled in Figure 17 is identical to the standard attention mechanism, so it can be further accelerated with our method. More importantly, the two matrix multipliation involved are the two of the three largest m \u00d7 n matrices. It will be very beneficial to reduce their complexity. \n\nWe report the accuracy on Image (1K) on LRA [26]  ). The matrix multiplication complexity of the standard Nystromformer takes O(nm 2 + mnd v + m 3 + nmd v ). After applying our method, it can be reduced to O( nm 2 2 + nmd v 2 + m 3 + nmd v ). The memory footprint can be reduced from O(md q + nm + m 2 + nm + nd v ) to O(md q + nm + m 2 + nd v ). Given n m > d v \u2248 d p , this could be a significant improvement that allows us to use more landmarks m to better approximate the full attention mechanism. Besides Nystromformer, we also illustrate two possible combinations with BigBird [33] and Linformer [29] that can be explored in the future work. \n\nAs shown in Figure 18 (A), Zaheer et al. 33 use block sparsity with block size 64 and compute a full attention within each block. We can apply the 1:2 or 2:4 sparsity within each block to bring further speedup. \n\nFigure 18 (B) gives another example on how to combine our method with Linformer [29]. Linformer uses low-rank approximation on the attention mechanism as follows: \n\nwhere E, F \u2208 R n\u00d7k are linear projection matrices and k n. We can first prune E and F along with other weight matrices to have 1:2 or 2:4 sparsity offline following Mishra et al. 17. Then we compute EK and FV with Sparse Matrix-Matrix multiplication.",
            "score": 0.5809220681953888,
            "section_title": "A.7 Combination with the Existing Efficient Transformers",
            "char_start_offset": 51970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1274,
                    "end": 1278,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "229923177",
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
            "text": "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u2126(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use. In this study, we adopt a different approach to adapting Recurrence Transformers for a pretraining-then-finetuning setting, to model a long document.\n\nRecurrence Transformers Rae et al., 2019) have been successfully applied in generative language modeling. They employ the Transformer decoder as a parametric model for each conditional distribution in p(x) = L t=1 p(x t |x <t ), where x denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism . This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer (Rae et al., 2019) adds a",
            "score": 0.5799167425863927,
            "section_title": "Related Work",
            "char_start_offset": 5099,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 403,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "271270399",
            "title": "Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference",
            "text": "Transformer models, including BERT [1], GPT [2], T5 [3] and others [4] [5] , have transformed Natural Language Processing (NLP) with their attention mechanism, achieving top performance in tasks such as question-answering [6], text classification [3], and machine translation [7].The transformer architecture uses self-attention mechanism [8] and it is highly parallelizable on modern Graphical Processing Units (GPUs), providing major benefits over older models like Long Short Term Memories (LSTMs) and Recurrent Neural Networks (RNNs).This has led to fast progress in NLP, with models like BERT exceeding human performance in difficult tasks [9] and expanding their application to computer vision, including object recognition and detection [10], image classification [11], and segmentation [12].\n\nDeploying large transformer models on devices with limited resources is challenging due to their high computational and memory requirements.For example, BERT-Base Transformer needs 440 MB of memory and over 176 Giga FLoating Point Operations (GFLOPs) [13] .The computations are particularly difficult because of the complex attention operations and the quadratic computational complexity related to the length of input sequences [14].Attention operations in transformer models become increasingly dominant as the input sequence length grows.For BERT-Base Transformer deployed on edge platforms, with a sequence length of 512, the attention operations account for about half of the total execution time, and this figure rises to 70% when the sequence length extends to 768 [15].Therefore, finding efficient ways to handle attention operations is crucial for speeding up transformers.\n\nMany studies utilized sparsity to mitigate the quadratic time and space complexity issue.Some techniques save computational effort by using fixed or static sparse attention patterns [16] [14] [17], but their performance is limited [18], since the sparse pattern in attention is naturally dynamic, and depends only on the input.Other techniques focus on dynamic sparsity, meaning there's no fixed pattern for which parts are sparse (zero).",
            "score": 0.5798260374258427,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 799
                },
                {
                    "start": 801,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1683
                },
                {
                    "start": 1685,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "235829175"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "251554643"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "239016160"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "221702858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "265351802",
            "title": "Linear Log-Normal Attention with Unbiased Concentration",
            "text": "Transformer models, proposed by (Vaswani et al., 2017), have become widely used deep learning architectures that have achieved state-of-the-art performance in various fields, including Natural Language Processing (NLP) (Brown et al., 2020;Devlin et al., 2018), Computer Vision (CV) (Dosovitskiy et al., 2020), Neural Machine Translation (NMT) (Chen et al., 2018), Document Summarization (Zhang et al., 2019;Pilault et al., 2020), and Protein Structure Prediction (Bahdanau et al., 2015). The main component of the Transformer model is an attention mechanism that identifies complex dependencies between tokens and efficiently captures tokens' correlation. However, standard self-attention suffers from quadratic memory and computation complexity, which arises from the N \u00d7 N attention matrix, where N is the sequence length. This problem is particularly significant during training, as it requires storing the attention matrix for gradient computation. Consequently, this significantly hinders the training of Transformer models with long sequences. \n\nRecently, we have observed an increasing interest in training Transformer models with long sequences, especially when considering large language models (Scao et al., 2022;Zhang et al., 2022;Chowdhery et al., 2022). Various approaches address the quadratic memory issue in self-attention. One class of the methods is sparse-attention, which aims to perform only a subset of the attention computations while preserving the softmax function (Child et al., 2019;Zaheer et al., 2020). Another approach is Linearized Attention (LA), which replaces the softmax with a product of two functions (Choromanski et al., 2020;Katharopoulos et al., 2020). These methods reduce the computational and memory complexity of the attention mechanism while striving to maintain performance. One of LA's benefits is that it performs dense operations and does not require special HW or low-level implementation. However, despite their efficiency, LA methods often underperform compared to standard self-attention. Thus, understanding the reasons behind the superior performance of self-attention is crucial for designing an effective LA method.",
            "score": 0.5793987627093891,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 239,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 463,
                    "end": 486,
                    "matchedPaperCorpusId": "11212020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "268032287",
            "title": "Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study",
            "text": "The self-attention mechanism in Transformers is a key component of recent pretrained language models. It allows each token in an input sequence to interact independently and in parallel with all other tokens. However, the computational and memory demands of the Transformer's self-attention grow quadratically with the sequence length. This makes processing long sequences computationally expensive. Consequently, models like BERT and its variants are limited to input sequence lengths of 512 tokens, which can be limiting when working with long documents for specific tasks. \n\nVarious adaptation of the Transformers' selfattention mechanism have been proposed to reduce this complexity (Tay et al., 2021(Tay et al., , 2022)). One of the possible approaches is sparse attention and involves using non-full attention patterns to reduce the scope of attention. The sparse attention mechanisms implemented in various pretrained models often comprise several atomic patterns, including global attention, band attention (such as sliding window or local attention), dilated attention, random attention, and block local attention. \n\nThe Sinkhorn attention (Tay et al., 2020) combines a content-based block sparse attention along with block local attention. Longformer (Beltagy et al., 2020) rely on a sliding window and a global attention. In addition to sliding window and a global attention, BigBird (Zaheer et al., 2020) use an additional random attention to approximate full attention. More recently, the LSG (Local Sparse Global) attention was introduced (Condevaux and Harispe, 2023). This attention relies on three components: block local attention, sparse attention to capture extended context and global attention. All these attention mechanisms allow re-ducing the attention complexity from O(n 2 ) to O(nlogn) or O(n). Many other methods can improve the efficiency by replacing the self-attention matrix by a low-rank approximation, such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2020a,b), Random Feature Attention (Peng et al., 2021), and LUNA (Ma et al., 2021).",
            "score": 0.5786806811566549,
            "section_title": "Attention mechanism for long-sequence transformers",
            "char_start_offset": 2746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 687,
                    "end": 704,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1149,
                    "end": 1167,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1395,
                    "end": 1416,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1553,
                    "end": 1582,
                    "matchedPaperCorpusId": "253157377"
                },
                {
                    "start": 2042,
                    "end": 2061,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 2072,
                    "end": 2089,
                    "matchedPaperCorpusId": "235313355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "273098139",
            "title": "UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation",
            "text": "An effective approach to facilitating long context is to avoid the O(n 2 ) computational complexity of the standard attention mechanism by designing linear attention mechanisms, sparse attention mechanisms, or low-rank attention mechanisms. These works can be categorized into the following four types: i) Sparse Attention Mechanisms: Reduce the computational burden of attention by exploiting inherent patterns within the attention mechanism (Jiang et al., 2024a;Ribar et al., 2023;Chen et al., 2023), or alternatively, by pruning the KV cache (Liu et al., 2023b;Xiao et al., 2023a;Pang et al., 2024;Zhang et al., 2024). ii) The attention mechanism with linear complexity: This typically involves transforming models with O(n 2 ) complexity into O(n) or O(n log n) linear attention (Zheng et al., 2022;Kitaev et al., 2020;Qin et al., 2022;Katharopoulos et al., 2020), or efficient long-sequence recurrent neural networks (Gu & Dao, 2023;Dao & Gu, 2024;Peng et al., 2023a;Yang et al., 2023). iii) Memory-augmented attention mechanisms: This typically involves encoding long-context text using additional memory blocks (He et al., 2024;Bertsch et al., 2024;Wang et al., 2024). iv) Hardware-friendly attention mechanisms: FlashAttention (Dao et al., 2022;Dao, 2023;Shah et al., 2024) accelerates precise attention computations by optimizing reads and writes across different levels of GPU memory. FlashAttention is especially effective for processing longer sequences. Among previous works, memory-augmented LLMs are most relevant to this study, as they involve memory retrieval. However, they typically require modifying the original model architecture and retraining the LLM or fine-tuning some parameters, which can be an impractical overhead in certain settings.",
            "score": 0.5778465382797917,
            "section_title": "ATTENTION MECHANISMS IN LONG CONTEXTS",
            "char_start_offset": 5990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1764
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 564,
                    "matchedPaperCorpusId": "260815690"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "267627983"
                },
                {
                    "start": 601,
                    "end": 620,
                    "matchedPaperCorpusId": "259263947"
                },
                {
                    "start": 783,
                    "end": 803,
                    "matchedPaperCorpusId": "248085050"
                },
                {
                    "start": 840,
                    "end": 867,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1135,
                    "end": 1156,
                    "matchedPaperCorpusId": "248563058"
                },
                {
                    "start": 1156,
                    "end": 1174,
                    "matchedPaperCorpusId": "259137816"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "273026224",
            "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
            "text": "Attention mechanisms are fundamental to transformer-based models, with various mask types enabling different attention patterns. The vanilla attention mechanism, as shown in Equation 2, supports arbitrary mask types through a dense mask matrix: \n\nwhere , ,  \u2208 R  \u00d7 are input sequences,  \u2208 R  \u00d7  is the attention mask,  is the sequence length, and  is the head dimension. The mask  modulates token visibility through element-wise addition with . While this approach supports arbitrary mask types, it incurs a memory complexity of  ( 2 ), limiting its scalability for long sequences. \n\nFlashAttention Dao et al. (2022); Dao (2023) addresses this limitation through IO-aware read/write operations and tiling techniques, eliminating the need for the intermediate  \u2208 R  \u00d7  and explicit mask . However, FlashAttention only supports predetermined mask patterns within its kernel, such as causal, sliding window, causal document, and document masks, as shown in Figure 1. \n\nxFormers Lefaudeux et al. (2022) extends FlashAttention's capabilities, offering support for masks with diagonal offsets. It represents document masks using cumulative sequence lengths, achieving a memory complexity of  (). \n\nFlexAttention He et al. (2024) introduces a more flexible mask description method based on deep learning compiler techniques. By combining block masks with expression-based descriptions, it can support arbitrary mask types. While this approach significantly reduces memory overhead through block-based processing, its memory complexity remains  (  2     ). Our proposed method, FLASHMASK, extends FlashAttention's mask support capabilities. It introduces a flexible, column-wise sparse mask representation that covers the majority of mainstream Transformer modeling requirements. As illustrated in Figure 1(b), FLASHMASK expresses which intervals need to be masked on a per-column basis, achieving a memory complexity of  (). This approach bridges the gap between mask flexibility and computational efficiency, offering a more versatile solution for attention mechanisms in large-scale transformer models.",
            "score": 0.5778004192679436,
            "section_title": "ATTENTION MASK SUPPORTED",
            "char_start_offset": 2561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 616,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "245117808",
            "title": "Couplformer: Rethinking Vision Transformer with Coupling Attention Map",
            "text": "ViT [13] O(n 2 ) Reformer [29] O(nlog(n)) Longformer [2] O(n(k + m)) Linformer [44] O(kn) Performer [8] O(kn) Big Bird [51] O(kn) Synthesizer [37] O(n 2 ) Sparse Transformer [7] O(n \u221a n) Transformer-XL [11] O(n 2 ) Compression Transformer [34] O(n 2 ) Ours O(kn) Secondly, depending on the low rank prior, employing the kernel-based method to approximate the attention matrix could also reduce the complexity. In terms of approximation solution, [44], [8] and [27] utilized the kernel method to avoid explicitly implementing the dot production. They attempted to find a relatively low-rank structure to reduce the memory and computational complexity. Tay et al. [37] proposed the Synthesizer Transformer, which leverages Multi-Layer Perception(MLP) structure to approximate the dot-product multiplication of self-attention mechanism. \n\nLastly, there are some of the other efficient Transformer architectures different from the solutions mentioned above. Depending on the segment-based recurrence, [11] applies a hidden state to connect adjacent blocks with a recurrent mechanism [38]. Different from the [11], [34] utilizes a dual memory system to maintain a fine-grained memory of past segment activations. As one kind of efficient Transformer, our model could be classified into the approximation solution to reduce the computation complexity and memory consumption.",
            "score": 0.5773718152853434,
            "section_title": "Model Complexity",
            "char_start_offset": 8733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 4,
                    "end": 8,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 100,
                    "end": 103,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "207930593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "247011581",
            "title": "Transformer Quality in Linear Time",
            "text": "Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018;Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Transformers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. \n\nMany techniques have been proposed to speedup Transformers over extended context via more efficient attention mechanisms (Child et al., 2019;Dai et al., 2019;Rae et al., 2019;Choromanski et al., 2020;Wang et al., 2020;Katharopoulos et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020;Roy et al., 2021;Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in    state-of-the-art systems. Here we examine this issue from a practical perspective, and find existing efficient attention methods suffer from at least one of the following drawbacks: \n\n\u2022 Inferior Quality. Our studies reveal that vanilla Transformers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efficient attention methods often incur significant quality drop compared to augmented Transformers, and this drop outweighs their efficiency benefits. \n\n\u2022 Overhead in Practice. As efficient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. \n\n\u2022 Inefficient Auto-regressive Training. Most attention linearization techniques enjoy fast decoding during inference, but can be extremely slow to train on auto-regressive tasks such as language modeling.",
            "score": 0.5772859154664827,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 743,
                    "end": 770,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 791,
                    "end": 811,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 831,
                    "end": 848,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2017822265625
        },
        {
            "corpus_id": "273653873",
            "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "text": "Efficient transformers have adopted various attention approximation including sparse and low-rank methods to extend the context length that can be processed by one attention layer. For example, sparse Transformer (Child et al., 2019), Longformer (Beltagy et al., 2020), Bigbird (Zaheer et al., 2020) and Diffusers (Feng et al., 2022) adopt combinations of random, window, and global sparse attention targeting the long-sequence scenario. Dataadaptive sparsity has also been considered in recent works including Reformer (Kitaev et al., 2020) BiFormer (Zhu et al., 2023b), DAT (Xia et al., 2022), and SparseViT (Chen et al., 2023b). The proposed tensorization is compatible with such sparisty masks. Low-rank attention is based on the assumption that the attention matrix has an intrinsic low-rank structure. Linformer (Wang et al., 2020), Performer (Katharopoulos et al., 2020), Linear Transformer (Katharopoulos et al., 2020), Synthesizer (Tay et al., 2021), andLRT (Winata et al., 2020) achieve efficient attention with linear complexity by projecting attention matrix to a lower dimension in the column vector space along the query or key dimension. However, these low-rank assumptions are mostly based on the vector space by directly reducing q,k,v sequence length, ignoring the hierarchical structure embedded in the attention matrix. Given the observed structure of attention matrices, we show that attention representations in the tensor space which is constructed by a set of hierarchically decomposed attention blocks can be better approximated with low-rank matrices.",
            "score": 0.576951967759508,
            "section_title": "Context Window Extension",
            "char_start_offset": 4807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 299,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 576,
                    "end": 594,
                    "matchedPaperCorpusId": "245650206"
                },
                {
                    "start": 849,
                    "end": 877,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 898,
                    "end": 926,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 940,
                    "end": 963,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "248780077",
            "title": "Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences",
            "text": "DeBERTa (He et al., 2021): DeBERTa differs from others on this list in that it decouples attention by word semantics from attention by word location. Version 2 of the model used a form of adversarial training to improve model generalization and surpassed human performance on Super GLUE benchmarks. We have used the RoBERTa based versions in this analysis.\n\nOne problem with transformers is the quadratic memory, and computational growth as sequence length increases because every token attends to all other tokens. Some have dealt with this problem by modifying the attention patterns to approximate this full attention pattern without requiring all of the attention comparisons. BIGBIRD (Zaheer et al., 2020) is an example that uses this attention approximation. The model uses a combination of global, sparse, and random attention. Again, we have used the RoBERTa based version of the model.",
            "score": 0.5769216734372207,
            "section_title": "Alternative Attention Mechanisms",
            "char_start_offset": 20365,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 689,
                    "end": 710,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2047119140625
        },
        {
            "corpus_id": "220831004",
            "title": "Big Bird: Transformers for Longer Sequences",
            "text": "sizes, this requirement translates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like question answering [61], document summarization [21], etc.\n\nHowever, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [105] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [73] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?\n\nIn this paper, we address both the above questions and produce a sparse attention mechanism that improves performance on a multitude of tasks that require long contexts. We systematically develop BigBird, an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We take inspiration from graph sparsification methods and understand where the proof for expressiveness of Transformers breaks down when full-attention is relaxed to form the proposed attention pattern. This understanding helped us develop BigBird, which is theoretically as expressive and also empirically useful. In particular, our BigBird consists of three main part:\n\n\u2022 A set of g global tokens that attend on all parts of the sequence. \u2022 For each query q i , a set of r random keys that each query will attend to.\n\n\u2022 A block of local neighbors w so that each node attends on their local structure.\n\nOur design leads to empirically high performing attention mechanism scaling to much longer sequence lengths (8x) on standard hardware (\u223c16GB memory) for large size models. Use of gradient checkpointing [15] could potentially allow for handling even longer",
            "score": 0.5761543748547209,
            "section_title": "Introduction",
            "char_start_offset": 1968,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "237412971",
            "title": "\\infty-former: Infinite Memory Transformer",
            "text": "When reading or writing a document, it is important to keep in memory the information previously read or written. Humans have a remarkable ability to remember long-term context, keeping in memory the relevant details (Carroll, 2007;Kuhbandner, 2020). Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;Dai et al., 2019;Rae et al., 2019;Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word. \n\nSeveral variations have been proposed to address this problem (Tay et al., 2020b). Some propose using sparse attention mechanisms, either with data-dependent patterns (Kitaev et al., 2020;Vyas et al., 2020;Tay et al., 2020a;Roy et al., 2021;Wang et al., 2021) or data-independent patterns (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), reducing the self-attention complexity (Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021;Jaegle et al., 2021), and caching past representations in a memory (Dai et al., 2019;Rae et al., 2019). These models are able to reduce the attention complexity, and, consequently, to scale up to longer contexts. However, as their complexity still depends on the context length, they cannot deal with unbounded context.",
            "score": 0.575386745578823,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 249,
                    "matchedPaperCorpusId": "221320806"
                },
                {
                    "start": 405,
                    "end": 422,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 439,
                    "end": 458,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1263,
                    "end": 1284,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1284,
                    "end": 1302,
                    "matchedPaperCorpusId": "220424511"
                },
                {
                    "start": 1302,
                    "end": 1320,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1320,
                    "end": 1337,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1337,
                    "end": 1355,
                    "matchedPaperCorpusId": "221655697"
                },
                {
                    "start": 1487,
                    "end": 1515,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1515,
                    "end": 1540,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1540,
                    "end": 1558,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 1625,
                    "end": 1643,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1643,
                    "end": 1660,
                    "matchedPaperCorpusId": "207930593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "269982567",
            "title": "Dynamic Context Adaptation and Information Flow Control in Transformers: Introducing the Evaluator Adjuster Unit and Gated Residual Connections",
            "text": "Attention mechanisms, the core of transformer architectures, have seen various adaptations to enhance model performance and interpretability.[12] proposed the Reformer, which reduces memory consumption by limiting the self-attention computation to a subset of key elements.A significant advancement, known as the Sparse Transformer [5], employs sparse factorizations of the attention matrix, enabling the model to handle longer sequences efficiently without a corresponding rise in computational demands.[25] introduced Linformer, which projects the attention matrix into a lowerdimensional space, significantly reducing the computational complexity from quadratic to linear with respect to sequence length.This adaptation maintains performance while enhancing efficiency, making it suitable for longer sequences.[6] developed the Performer, which utilizes random feature maps through the Fast Attention Via positive Orthogonal Random features approach (FAVOR+) to approximate the softmax function in attention.This method allows the Performer to scale linearly in terms of memory and compute, irrespective of sequence length.",
            "score": 0.5738882635548497,
            "section_title": "Adaptations in attention mechanisms",
            "char_start_offset": 3767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 273
                },
                {
                    "start": 273,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1126
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3642578125
        },
        {
            "corpus_id": "248219570",
            "title": "Relation is an option for processing context information",
            "text": "The field of computer vision has employed attention mechanism for a long time. The first study to use it for context processing was conducted in 2017 (Vaswani et al., 2017). The attention mechanism was used to implement the encoderdecoder model, a model used earlier for machine translation and building dialogue agents. The authors named this attentionbased encoder-decoder model Transformer. Transformer has been used in various fields to solve a variety of problems. While the aforementioned BERT is a research achievement within the domain of natural language processing, for example, Vision Transformer (Dosovitskiy et al., 2021) andImage Transformer (Parmar et al., 2018) are applications of Transformer in the field of computer vision. Vision Transformer is almost the same as the original Transformer, and Image Transformer is a Transformer that incorporates techniques used in CNNs, a network architecture commonly used in computer vision. The development of these Transformers is an example of applied research on the attention mechanism. However, as was previously indicated, research has also been conducted from another perspective to reduce the computational complexity of Attention (Tay et al., 2020). Sparse Transformer (Child et al., 2019) improves the computational complexity to O(N \u221a N). Furthermore, Reformer (Kitaev et al., 2020) and Cluster-Former (Wang et al., 2021) have improved the computational complexity to O(N log N). Recently, methods have been developed that achieve linear computational complexity with respect to the sequence length. Linear Attention was one of the pioneer methods to achieve linear computational complexity by a simple modification of the computation of attention mechanism, where the order of the matrix multiplications required in the process of computing Attention was modified, as described in the following sections. Since then, research on reducing the computational complexity of attention mechanism has continued, and methods such as Performer (Choromanski et al., 2021), Linformer (Wang et al., 2020), Random Feature Attention (Peng et al., 2021), and other methods have achieved linear computational complexity of attention mechanism with similar prediction performance to Linear Attention. \n\n. Method . . Attention and Relation",
            "score": 0.5731665578971404,
            "section_title": ". Related work",
            "char_start_offset": 4359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2253
                },
                {
                    "start": 2256,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 608,
                    "end": 638,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1330,
                    "end": 1351,
                    "matchedPaperCorpusId": "1428702"
                },
                {
                    "start": 1371,
                    "end": 1390,
                    "matchedPaperCorpusId": "221655697"
                },
                {
                    "start": 2005,
                    "end": 2031,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 2043,
                    "end": 2062,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 2089,
                    "end": 2108,
                    "matchedPaperCorpusId": "1957433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17236328125
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
            "score": 0.5730414669291741,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "249395507",
            "title": "EAANet: Efficient Attention Augmented Convolutional Networks",
            "text": "Modern Computer Vision has been built on powerful image embedding learned on image classification tasks such as CIFAR-10 ( Krizhevsky et al., a), CIFAR-100 (Krizhevsky et al., b), and ImageNet (Deng et al., 2009). These datasets have been used as benchmarks for training better image embedding and network architectures across many tasks. Developing an efficient self-attention mechanism for highresolution image embedding is crucial in CV. A well-known deficiency of the self-attention mechanism is its quadratic computation time and memory complexity. Many efficient attention mechanisms have been developed for Transformers to reduce the computation and memory complexity, most for NLP tasks. These mechanisms can be grouped into four categories (Zhang et al., 2021). \n\n1. Sparse attention mechanism, including content-independent sparsity and content-dependent sparsity. Axial Transformer (Ho et al., 2019) and Image Transformer (Parmar et al., 2018) are among few sparsity-based efficient attentions that are developed for image generation. \n\n2. Memory-based mechanism, including Compressive Transformers (Rae et al., 2019) and Set Transformer (Lee et al., 2019). These models use some extra global tokens as static memory and allow all the other tokens to attend only to those global tokens. \n\n3. Low-rank-based mechanism. For example the Linformer (Wang et al., 2020) reduces the overall self-attention complexity from O(n 2 ) to O(n) by projecting key K and value V with N tokens to a low-dimensional representation. This low-rank approximation of the length dimension will decompose the original self-attention matrix from N \u00d7 N to N \u00d7 K. Similar to Linformer, the Spatial Reduction Attention (SRA) presented in (Wang et al., 2021) also conduct projection on key and value but using convolution layer with kernel size R and stride R. \n\n4. Kernel-based mechanism like Performer (Choromanski et al., 2020). \n\nLongformer (Beltagy et al., 2020) utilizes hybrid attention mechanisms.",
            "score": 0.5729849164425008,
            "section_title": "Efficient Self-attention Mechanisms",
            "char_start_offset": 4187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 749,
                    "end": 769,
                    "matchedPaperCorpusId": "232404731"
                },
                {
                    "start": 933,
                    "end": 954,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1721,
                    "end": 1740,
                    "matchedPaperCorpusId": "232035922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2958984375
        },
        {
            "corpus_id": "276574608",
            "title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
            "text": "The attention mechanism was first introduced within the context of machine translation by Bahdamau et al. [1], when it was used for dynamic alignment of words between input and output sequences. This work introduced the concept of \"additive attention\", in which two vectors are independently processed by their own feed-forward networks to produce scalars that are summed to calculate the final attention score. With Neural Attention, instead of processing each vector separately, we concatenate the two vectors and perform a nonlinear operation on the combined representation, projecting them jointly into a space that encodes their relationship. This approach captures richer dependencies between vectors, whereas additive attention ultimately relies on a linear combination of independently projected representations. \n\nBuilding upon this foundation, Vaswani et al. [9] proposed the transformer architecture, eliminating the need for recurrence while still allowing flexibility of input sequence length. This innovation significantly improved the ability of sequence processing to be parallelized, addressing some of the bottlenecks in recurrent models. In this work, they adopted Dot-Product Attention as the mechanism for calculating attention scores, citing its practical advantages over additive attention when considering computational and memory efficiency. However, their work did not explore whether additive attention could offer advantages in expressivity, leaving the representational limitations of Dot-Product Attention largely unexamined. Neural Attention addresses this gap by prioritizing enhanced expressivity, even as we tackle the associated complexity challenges, as explained in section 3. \n\nWhile these foundational innovations laid the groundwork for modern transformers, much of the subsequent research has shifted toward improving computational efficiency, rather than addressing representational limitations. Fastformer [11] explored additive attention to achieve faster processing, while SwiftFormer [7] revisited additive attention for real-time applications in mobile vision tasks. Similarly, Xu et al. [12] proposed additive and convolution-based attention mechanisms tailored for vision transformers, focusing on practical implementation benefits. Linformer [10] addressed the O(n 2 ) complexity of self-attention by employing low-rank approximations to reduce computational and memory costs. More recently, Mahmood and Huang [5] introduced a segmented attention approach, computing attention only on pairs of consecutive overlapping segments to further optimize computational efficiency.",
            "score": 0.5714869761513953,
            "section_title": "Related Work",
            "char_start_offset": 6162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2426
                },
                {
                    "start": 2427,
                    "end": 2622
                }
            ],
            "ref_mentions": [
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2030,
                    "end": 2033,
                    "matchedPaperCorpusId": "257766532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "254044711",
            "title": "Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges",
            "text": "Query to reduce dimensions. Longformer, Bigbird [254], ETC [255], and SWIN Transformer [256] are all in the same category using sparse attention techniques. Image Transformer [249] and Axial Transformer [257] are other extended sparse attention works focused on vision data",
            "score": 0.5713512969582172,
            "section_title": "Sparse attention Longformer [253] with linear complexity of O(n) leverages a global memory technique and is analogous to applying CNN. A combination of sliding window and global attention technique is applied to each",
            "char_start_offset": 52032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 273
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 53,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 59,
                    "end": 64,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 87,
                    "end": 92,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 175,
                    "end": 180,
                    "matchedPaperCorpusId": "3353110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18212890625
        },
        {
            "corpus_id": "273186301",
            "title": "On Efficient Variants of Segment Anything Model: A Survey",
            "text": "Researchers have also made significant progress in optimizing the attention mechanism. It has been observed that the softmax operation within the attention mechanism contributes significantly to the overall computational cost. In EfficientViT [10], a novel ReLU linear attention is proposed to achieve a global receptive field with higher efficiency. This efficient backbone is further adopted in [171] to accelerate SAM. Improvements to the attention mechanism have also been made at the hardware level. FlashAttention [25] significantly reduces computation costs through techniques such as tiling, kernel fusion, and recomputation. And it is utilized in SAM acceleration works [113,116] to reduce memory need and enhance computation efficiency.",
            "score": 0.5712067406134593,
            "section_title": "Efficient Backbone",
            "char_start_offset": 15112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 746
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "263608461",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
            "text": "Transformers [37] have become the backbone of many state-of-the-art AI systems that have demonstrated impressive performance across a wide range of AI problems. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. However, scaling up the context length of Transformers is a challenge [29], since the inherited architecture design of Transformers, i.e. the self-attention has memory cost quadratic in the input sequence length, which makes it challenging to scale to longer input sequences. Large context Transformers are essential for tackling a diverse array of AI challenges, ranging from processing books and high-resolution images to analyzing long videos and complex codebases. They excel at extracting information from the interconnected web and hyperlinked content, and are crucial for handling complex scientific experiment data. There have been emerging use cases of language models with significantly expanded context than before: GPT-3.5 [32] with context length 16K, GPT-4 [29] with context length 32k, MosaicML's MPT [25] with context length 65k, and Anthropic's Claude [1] with context length 100k. \n\nDriven by the significance, there has been surging research interests in reducing memory cost. One line of research leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [24] which has led to the development of blockwise computation of self-attention and feedforward [30,9,23] without making approximations. Despite the reduced memory, a significant challenge still arises from storing the output of each layer. This necessity arises from self-attention's inherent nature, involving interactions among all elements (n to n interactions). The subsequent layer's self-attention relies on accessing all of the prior layer's outputs. Failing to do so would increase computational costs cubically, as every output must be recomputed for each sequence element, rendering it impractical for longer sequences. [37], memory efficient transformers [30], and memory efficient attention and feedforward (blockwise parallel transformers) [23].",
            "score": 0.5710857660754393,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 1536,
                    "end": 1538,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "258987968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "276961169",
            "title": "Speedy MASt3R",
            "text": "The Vision Transformer (ViT) [6] encoder-decoder in MASt3R plays a crucial role in 3D scene reconstruction and image matching. However, the traditional attention mechanism in ViT suffers from high computational complexity, scaling quadratically with the sequence length of the input tokens O(n 2 ), and a significant memory footprint. This becomes a bottleneck for MASt3R, as 60% of the total inference latency is attributed to the ViT encoder-decoder, with attention being the primary contributor. Specifically, the memory-intensive nature of attention computation limits the scalability of MASt3R to high-resolution images and real-time applications. \n\nTo address these limitations, we integrate FlashAttention v2 [3] into the self-attention modules of 2 pairs of encoders and decoders in MASt3R. FlashAttention v2 is an optimized attention mechanism that reduces both computational complexity and memory footprint by leveraging tiling strategies and efficient memory access patterns. Its core idea is to decompose the attention computation into smaller blocks (tiles) that fit into the GPU's fast memory (SRAM), minimizing the need for costly global memory accesses.",
            "score": 0.5710738593021976,
            "section_title": "FlashMatch",
            "char_start_offset": 11809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2198486328125
        },
        {
            "corpus_id": "260611052",
            "title": "Attention-Driven Lightweight Model for Pigmented Skin Lesion Detection",
            "text": "Self-attention mechanism was first proposed in natural language processing to obtain contextually enhanced representations for each word [22]. Researchers recognized that the self-attention mechanism's ability to capture global dependencies could also be beneficial for computer vision tasks. The Vision Transformer (ViT) [23] is one of the pioneering models in this regard, showcasing how self-attention can be effectively applied to image classification tasks. Additionally, the Swin Transformer [24] introduces hierarchical attention mechanisms for improved efficiency. \n\nSelf-attention mechanism requires storing attention scores for all pairs of elements in the sequence causing a quadratic computational complexity. The high computational complexity can be prohibitive for mobile devices with limited computation power and the corresponding long latency makes it undesirable for realtime applications or interactive user experience. \n\nTo enable the application of self-attention mechanism for lightweight models, sparse approximation of the attention mechanism is desirable to achieve faster and memory-efficient computations. One such attention with hardware-friendly operations is the decoupled fully connected (DFC) attention [18]. There are two main contributions to the sparsity of the approximation of the attention mechanism in this approach. Firstly, we replace the self-attention operations with fully-connected (FC) layers to calculate the feature maps. This modification helps reduce the computational complexity and memory requirements compared to the traditional dense self-attention mechanism. Secondly, we decouple the token aggregation process into the vertical and horizontal directions (of the image). By doing so, we can capture long-range dependencies and interactions between tokens more efficiently while maintaining sparsity in the attention computation. More specifically, in our approach, we begin with feature maps  \u2208  \u00d7\u00d7 , where , , and  represent the feature height, feature width, and number of channels, respectively. We split these feature maps into  \u00d7  tokens (  ,  = 1,2, \u2026 ,  \u00d7 ), where each token contains C values, representing a local region of the input data. \n\nTo generate the attention map, the tokenized feature maps are fed through two fully-connected (FC) layers sequentially. The first FC layer operates horizontally, capturing interactions and dependencies between tokens along the width dimension (), and the second FC layer operates vertically, capturing interactions and dependencies between tokens along the height dimension ().",
            "score": 0.5697627930448586,
            "section_title": "Efficient Attention Mechanism",
            "char_start_offset": 10487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2203
                },
                {
                    "start": 2206,
                    "end": 2325
                },
                {
                    "start": 2326,
                    "end": 2583
                }
            ],
            "ref_mentions": [
                {
                    "start": 1235,
                    "end": 1239,
                    "matchedPaperCorpusId": "253801665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40283203125
        },
        {
            "corpus_id": "271892197",
            "title": "S$^\\text{3}$Attention: Improving Long Sequence Attention With Smoothed Skeleton Sketching",
            "text": "Longformer [49] uses dilated sliding windows to obtain a sparse Attention matrix. Sparse Transformers [50] consider approximating a dense Attention matrix by several sparse factorization methods. In addition, some methods reduce the complexity by clustering tokens. For example, Reformer [51] uses a hash similarity measure to cluster tokens, and Routing Transformer [52] uses k-means to cluster tokens. BigBird [53] proposes a generalized Attention mechanism described by a directed graph to reduce Attention complexity. [54] considers using 2D Fourier Transformation to mix the token matrix directly. [55] uses max pooling scheme to reduce the computation costs. \n\nLow-rank and Kernel Methods. Inducing low rankness into the Attention matrix can quickly reduce the complexity and the kernel approximation is widely applied in efficient low-rank approximation. Linformer [56] and Luna [30] approximate softmax with linear functions, which yield a linear time and space complexity. [24,57] use random features tricks and reach promising numerical performance. [58] proposes Low-Rank Transformer based on matrix factorization. FMMformer [29] combines the fast multipole method with the kernel method. Synthesizer [59] uses a random low-rank matrix to replace the Attention matrix. Nystr\u00f6mformer [60] adopts the Nystr\u00f6m method to approximate standard Attention. Linear Transformer [61] expresses Attention as a linear dot-product of kernel feature maps. [28] applies the Multigrid method to efficiently compute the Attention matrix recursively. Cosformer [27] develops a cosine-based re-weighting mechanism to linearize the softmax function. [25] proposes the Scatterbrain, which unifies localitysensitive hashing and the kernel method into Attention for accurate and efficient approximation. \n\nSequence Length Reduction. Reducing sequence length is an efficient means to reduce computational costs. Perceiver IO [62] encodes inputs to a latent space whose size is typically smaller than the inputs and outputs, making the process computationally scalable to even very large inputs and outputs. Funnel-Transformer [63] uses pooling tricks to reduce sequence length, significantly saving both FLOPs and memory.",
            "score": 0.5694262369562977,
            "section_title": "body",
            "char_start_offset": 6585,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "235313355"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1060,
                    "end": 1064,
                    "matchedPaperCorpusId": "204960988"
                },
                {
                    "start": 1136,
                    "end": 1140,
                    "matchedPaperCorpusId": "236924765"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1452,
                    "end": 1456,
                    "matchedPaperCorpusId": "236428421"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "248498407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2083740234375
        },
        {
            "corpus_id": "209315300",
            "title": "Reformer: The Efficient Transformer",
            "text": "The Transformer model introduced in (Vaswani et al., 2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018;Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018;Radford et al., 2019). \n\nGiven the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored. \n\nIn particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al., 2019). \n\nLocality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015;Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015).",
            "score": 0.5692502708648346,
            "section_title": "RELATED WORK",
            "char_start_offset": 17145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 224,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 860,
                    "end": 862,
                    "matchedPaperCorpusId": "8869447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "271244689",
            "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training",
            "text": "Since the advent of ChatGPT [58], Large Language Models (LLMs) have demonstrated remarkable proficiency in comprehending and generating natural language texts. Besides revolutionizing the field of language processing, which encompasses translation [103], coding [23,72,94], etc., transformer-based LLMs have also found applications in multi-modal scenarios, such as image processing [15,61], video stream analysis [73], and AI for science [2,5]. To accommodate novel applications that require lengthy contexts [98], LLMs have developed to support long context input, from 2K-4K [79,81] to 32K [29,80], 128K [18,58], or even millions of tokens [1,9,41]. Considering the extrapolation problem [43,66], which refers to the decline in LLM performance when input sequences exceed the training length, it is necessary to conduct long context training [7,17,28] or fine-tuning [14,62] to facilitate long sequence inference. Beyond natural language processing, increasing the context length is also essential across diverse domains, including video processing [101], protein properties prediction [6], weather forecasting [54], and health care [40]. \n\nMaximizing system performance with limited memory is a common and significant challenge in the data management community. Within this context, training LLMs with long sequence lengths poses difficulties due to restricted GPU memory. During training, a large amount of activations 1 must be stored for gradient computation during the backward pass, resulting in substantial memory consumption. Typically, it is well known that the self-attention module in the transformer architecture has a quadratic computation and memory complexity w.r.t. the sequence length. FlashAttention [11,12], now a standard technique for attention computation in LLM training, accelerates computation and shrinks the memory complexity to be linear w.r.t. the sequence length by scheduling memory I/O and recomputing necessary components during the backward pass. Except for attention, the remaining activation memory also scales linearly with the sequence length, which can become quite large in long context scenarios.",
            "score": 0.5688695728369613,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 253,
                    "matchedPaperCorpusId": "258048937"
                },
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "272045337"
                },
                {
                    "start": 383,
                    "end": 387,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "237581134"
                },
                {
                    "start": 442,
                    "end": 444,
                    "matchedPaperCorpusId": "271931441"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "267682361"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "263828829"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "267770308"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "261493986"
                },
                {
                    "start": 1089,
                    "end": 1092,
                    "matchedPaperCorpusId": "255966856"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "256231457"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 1725,
                    "end": 1728,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2039794921875
        },
        {
            "corpus_id": "232335426",
            "title": "Finetuning Pretrained Transformers into RNNs",
            "text": "Prior work suggested many other strategies to improve efficiency in transformers, such as weight sharing and factorization (Dehghani et al., 2019;Lan et al., 2020), weight and layer pruning (Michel et al., 2019;Fan et al., 2020), quantization (Zafrir et al., 2019;Shen et al., 2020), training on short sequences (Press et al., 2021), gradually reducing the input sequence length from layer to layer (Dai et al., 2020), clustering query vectors (Vyas et al., 2020), and modifying the combination of attention and feedforward sublayers (Press et al., 2020;Mandava et al., 2020). Some of these methods present orthogonal design choices and can be integrated into our T2R model to gain further efficiency. For a more comprehensive survey of efficient transformers, see Tay et al. (2020c). Below we describe several prior works along two major strategies that reduce time and memory overhead in the attention mechanism: compressing the attention context and sparsifying the attention patterns.\n\nAttention Context Compression This strand of methods compresses the context that is attended to, thereby reducing the time and memory overhead in the attention. RNN models that we converted pretrained transformers into (Katharopoulos et al., 2020;Choromanski et al., 2021) compress the context into a recurrent state. Other approaches include low rank approximation of the attention computation (Wang et al., 2020a;Tay et al., 2020a) and adding a memory module that can access multiple tokens at once (Liu et al., 2018;Dai et al., 2019;Lee et al., 2019;Ainslie et al., 2020;Rae et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020).\n\nSparse Attention Patterns Another approach to reducing the time and memory overhead from the attention computation is to limit the tokens that are attended to by sparsifying the attention patterns. These patterns can be set in advance or learned during training (Tay et al., 2020c). For example, prior works introduced fixed patterns of blockwise attention (Qiu et al.) and strided attention",
            "score": 0.5682940349574406,
            "section_title": "Efficient Transformers",
            "char_start_offset": 22790,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 163,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 190,
                    "end": 211,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 243,
                    "end": 264,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 264,
                    "end": 282,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "207853045"
                },
                {
                    "start": 1491,
                    "end": 1509,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1543,
                    "end": 1564,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1564,
                    "end": 1581,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 1602,
                    "end": 1622,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375732421875
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.\n\nTo tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1.Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top-k mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time.It is worth noting that our method draws inspiration from the concept of top-k attention [32,1].Unfortunately, conventional top-k attention is non-differentiable and therefore cannot be used to train the scoring network.With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows.\n\nIncremental KV Selection.\n\nThe SPARSEK operator ( \u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder.Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results.\n\nComputational and Memory Efficiency.SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32,2,47] to linear time and achieves constant memory cost in inference.This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference.Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs.This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately ( \u00a7 3.2).\n\nExtension with IO-awareness.FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness.However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application.",
            "score": 0.5677010592890795,
            "section_title": "Introduction",
            "char_start_offset": 1917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 197,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1109
                },
                {
                    "start": 1111,
                    "end": 1136
                },
                {
                    "start": 1138,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1385
                },
                {
                    "start": 1387,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1759
                },
                {
                    "start": 1759,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2020
                },
                {
                    "start": 2022,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2143
                },
                {
                    "start": 2143,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "235422257"
                },
                {
                    "start": 842,
                    "end": 844,
                    "matchedPaperCorpusId": "257622671"
                },
                {
                    "start": 1330,
                    "end": 1333,
                    "matchedPaperCorpusId": "257622671"
                },
                {
                    "start": 1529,
                    "end": 1532,
                    "matchedPaperCorpusId": "235422257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "273323943",
            "title": "A Framework to Enable Algorithmic Design Choice Exploration in DNNs",
            "text": "Determining the best algorithm for a convolution operation is a non-trivial task depending on many features of the operation such as input sizes, number of output channels, kernel dimensions, stride of the kernel and more [22]. \n\nSimilar to CNNs, transformer models spend the vast majority of their inferencing time in utilizing one mechanism, attention [13]. Attention is an operation used in DNNs for language processing, it is used as it provides the benefit of being able to predict the target word depending on the context associated with the source, the DNN is aware of what part of the source should receive the most attention. In response to this, many different algorithms have been developed in order to complete the operation. These include flash, bigbird, longformer, linformer attention and more. \n\n(a) Flash: An exact attention algorithm that uses tiling to increase the number of cache hits and decrease the number of memory read and writes between GPU high bandwidth memory. (b) LongFormer: A self-attention algorithm which scales linearly as opposed to quadradically with input sequence length. This is achieved by combining a smaller windowed attention mechanism with a larger global attention mechanism. (c) BigBird: A sparse attention mechanism which also reduces the quadratic dependency on input size to linear. BigBird is a universal approximator of sequence to sequence functions which preserves the properties of the quadratic full attention models. (d) Linformer: An algorithm that approximates the attention mechanism reducing the time and space complexity from quadratic to linear. The algorithm has been found to perform on par with standard attention implementations. \n\nAgain, the algorithm providing the best performance depends on many factors such as, input sequence length, memory limitations and required accuracy [23].",
            "score": 0.5676901868343827,
            "section_title": "B. Algorithmic Design Choices",
            "char_start_offset": 6442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1854
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "227247634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "249319674",
            "title": "Complexity of symbolic representation in working memory of Transformer correlates with the complexity of a task",
            "text": "The Extended Transformer Construction (ETC) (Ainslie et al., 2020) employs a global-local attention mechanism to handle long inputs.The model input is separated into two parts: long input, which is a standard Transformer input sequence, and a small set of auxiliary tokens called global input.Hidden representations of the global input tokens store summarized information about sets of long input tokens.Each part of the input is associated with its type of attention: full selfattention between global input tokens, full cross-attention between global and long inputs, and self-attention restricted to a fixed radius for long input tokens.\n\nBigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) sparsify attention from a quadratic to linear dependenc\u0435 on the sequence length by serving a number of pre-selected input tokens to store global representations.Global tokens are allowed to attend the entire sequence and, as a result, accumulate and redistribute global information.In addition, BigBird can use extra tokens to preserve contextual information.\n\nAn extension of the input sequence with extra memory-dedicated tokens is implemented in MemTransformer, MemCtrl Transformer, and MemBottleneck Transformer (Burtsev et al., 2021).In MemTransformer, specially reserved memory tokens are concatenated with the encoder input sequence to form the Transformer input.The model uses full self-attention over the memoryaugmented input sequence and processes inputs in a standard way.MemCtrl Transformer uses the same memory-augmented input sequence as MemTransformer and has a sub-network to control memory and original input sequence tokens separately.In MemBottleneck Transformer, full attention is allowed between the input and the memory only.So, to update the model, we firstly update memory representations, as presented in MemCtrl Transformer, and secondly update the sequence representation.\n\nIn our symbolic working memory model studied in this paper, input encoding follows the vanilla Transformer, but during the generation of an output sequence, a decoder decides whether to write the next token to the internal working memory or the output target prediction.",
            "score": 0.5675810857866643,
            "section_title": "Related work",
            "char_start_offset": 5837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 640
                },
                {
                    "start": 642,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1069
                },
                {
                    "start": 1071,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1494
                },
                {
                    "start": 1494,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 1910
                },
                {
                    "start": 1912,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 66,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3671875
        },
        {
            "corpus_id": "277151262",
            "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
            "text": "The overhead involved in determining block importance can negate the gains achieved through sparsity, rendering these methods impractical for real-world deployment. This leads us to a question: Can we design a block-sparse attention mechanism that dramatically accelerates longcontext Transformers without compromising accuracy, truly unlocking their potential for real-world applications? Attention is computed only on the selected blocks (red blocks on the right), achieving substantial computational savings. This example uses a sequence length of 24. \n\nWe answer this question by introducing XAttention, a novel plug-and-play framework designed to significantly improve the efficiency of block-sparse attention in long-context Transformers. XAttention is based on the key insight that the sum of antidiagonal values within the attention matrix can serve as a powerful, yet computationally efficient, indicator of block importance. Unlike existing methods that primarily rely on computationally intensive and lossy solutions like token pooling to identify important blocks, XAttention leverages this simple score to offer a potentially more streamlined and direct approach for rapidly and accurately identifying critical attention blocks. This antidiagonal scoring algorithm allows XAttention to aggressively find and prune non-essential computations, achieving substantial sparsity without sacrificing accuracy. We extensively evaluate XAttention on challenging longcontext benchmarks, including RULER and LongBench for natural language processing, VideoMME for video understanding, and VBench for video generation. Across these benchmarks, XAttention achieves accuracy comparable to full attention while delivering substantial computational gains, demonstrating up to 13.5\u00d7 acceleration in attention computation during pre-filling. These results underscore XAttention's ability to unlock the practical potential of block-sparse attention, paving the way for scalable and efficient deployment of long-context Transformers in demanding applications, especially in the expanding field of multimodal AI.",
            "score": 0.5672277841889608,
            "section_title": "Introduction",
            "char_start_offset": 2146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2104
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "268875948",
            "title": "How Sparse Attention Approximates Exact Attention? Your Attention is Naturally $n^C$-Sparse",
            "text": "Large Language Models (LLMs) [VSP + 17, RNS + 18, DCLT18, RWC + 19, BMR + 20, CND + 22, ZRG + 22, Cha22] have emerged as a cornerstone of contemporary artificial intelligence, exhibiting remarkable capabilities across a plethora of AI domains.Their prowess is grounded in their ability to comprehend and generate human language with a level of sophistication that is unprecedented.This has catalyzed transformative applications in natural language processing, including machine translation [HWL21], content creation [Cha22,Ope23], and beyond, underscoring the profound impact of LLMs on the field of AI.\n\nHowever, the architectural backbone of these models, particularly those built on the transformer framework [VSP + 17], presents a significant challenge: computational efficiency [TDBM22].The essence of the transformer architecture, the Attention mechanism, necessitates a computational and memory complexity of O(n 2 ), where n represents the sequence length.This quadratic dependency limits the scalability of LLMs, especially as we venture into processing longer sequences or expanding model capacities.\n\nIn an effort to mitigate this bottleneck, the AI research community has pivoted towards innovative solutions, one of which is sparse transformers [CGRS19,CNM19].Sparse attention mechanisms aim to approximate the results of the full attention computation by selectively focusing on a subset of the input data points.This is typically achieved by omitting certain interactions in the Query and Key multiplications within the attention mechanism, thereby inducing sparsity in the attention matrix.The overarching goal is to preserve the model's performance while alleviating the computational and memory demands.\n\nA noteworthy advancement in this domain is the introduction of the Reformer model [KKL20], which adeptly reduces the complexity from O(n 2 ) to O(n log n) through the adoption of the Locality Sensitive Hashing (LSH) technique.LSH enables the Reformer to efficiently approximate the attention mechanism, significantly curtailing the computational overhead without substantially compromising the model's efficacy.\n\nDespite these advancements, the theoretical underpinnings of sparse attention mechanisms and their implications on model performance and behavior remain an area of active inquiry.",
            "score": 0.5664588610902663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 603
                },
                {
                    "start": 605,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1721
                },
                {
                    "start": 1723,
                    "end": 1949
                },
                {
                    "start": 1949,
                    "end": 2134
                },
                {
                    "start": 2136,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 497,
                    "matchedPaperCorpusId": "243766712"
                },
                {
                    "start": 783,
                    "end": 791,
                    "matchedPaperCorpusId": "221702858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "275906620",
            "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention",
            "text": "Efficient Transformer The Transformer architecture (Vaswani et al., 2017) is foundational for sequence modeling, but its quadratic complexity limits efficiency with long sequences. Various efficient variants (Tay et al., 2022;2020;Chen et al., 2021;Qin et al., 2022b;Zhang et al., 2024) have been proposed as alternatives, mainly categorized into sparse, low-rank, and linear transformers. Sparse transformers, such as BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020), restrict attention to local windows or global tokens to achieve linear complexity. SparseAxial (Ho et al., 2020) further enhances this by combining sparse attention with axial mechanisms for high-dimensional inputs. Reformer (Kitaev et al., 2020) locality-sensitive hashing to handle variable-length sequences efficiently. Low-rank transformers like Linformer (Wang et al., 2020) reduce the attention matrix to a lower-dimensional space, reducing memory and computation costs. Linear transformers such as Performer (Choromanski et al., 2021) use kernel-based approximations for linear-time complexity, while Nystr\u00f6mformer (Xiong et al., 2021) leverages Nystr\u00f6m decomposition for near-linear performance. \n\nTop-k Attention (Gupta et al., 2021) falls under the category of sparse attention, reducing attention complexity by selecting only the top-k most relevant tokens at each layer, thereby focusing computational resources on the most critical interactions. Unlimiformer (Bertsch et al., 2023) enables transformers to handle arbitrarily long sequences by chunking inputs and using a retrieval mechanism to attend to relevant past contexts. Similarly, IceFormer (Mao et al., 2024) improves transformer efficiency by integrating a k-nearest-neighbor (KNN) search mechanism that focuses on the KNN results as the most relevant tokens during inference, bypassing the need to compute the full attention matrix. However, with causal masks, these approaches can not compute the outputs of a long sequences in parallel, making them less efficient for training models from scratch by not fully exploiting the parallel computing power of GPUs.",
            "score": 0.5660304228570296,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 73,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 208,
                    "end": 226,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 249,
                    "end": 267,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 267,
                    "end": 286,
                    "matchedPaperCorpusId": "267523164"
                },
                {
                    "start": 713,
                    "end": 734,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1003,
                    "end": 1029,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1110,
                    "end": 1130,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1210,
                    "end": 1230,
                    "matchedPaperCorpusId": "235422257"
                },
                {
                    "start": 1460,
                    "end": 1482,
                    "matchedPaperCorpusId": "258436892"
                },
                {
                    "start": 1650,
                    "end": 1668,
                    "matchedPaperCorpusId": "269605811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55322265625
        },
        {
            "corpus_id": "269899568",
            "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
            "text": "Transformer-based [31] language models [12], [23], [26], [30], [37] have revolutionized the field of natural language processing (NLP) and found applications across diverse domains [18], [29].These powerful models, fueled by massive amounts of data and sophisticated architectures, have become indispensable tools for tasks such as machine translation [15], question answering [7], text generation [7], and sentiment analysis.\n\nThe core of the transformer architecture is the powerful component of self-attention; however, execution of the selfattention mechanism is slow and suffers from a large memory footprint, especially when dealing with long context length.A standard implementation of self-attention demonstrates quadratic time and memory complexity with respect to total sequence length, which leads to scalability challenges as model sizes [10] and supported context lengths increase [8], [24], [36].Despite these scalability challenges, we see a trend toward stateof-the-art models supporting greater and greater context lengths, with some production models supporting context lengths of hundreds of thousands of tokens.Support for long context lengths can improve a model's utility by allowing for an increasingly rich context, which is particularly beneficial in a range of applications (e.g.RAG involving numerous or long documents) allowing improved relevance, coherence, and user experience.\n\nTo mitigate LLM scalability challenges, mechanisms like FlashAttention [14] and FlashAttention-2 [13] have been developed.FlashAttention brings IO-awareness to optimize computation in the attention mechanism in a way that reduces slow reads and writes to and from GPU high bandwidth memory [19] (via incrementally computing the softmax computation in SRAM, also known as tiling).It allows for parallelization over batch size and number of heads.FlashAttention-2 builds on FlashAttention to further optimize the attention mechanism by additionally reducing the number of non-matrix multiply operations to maximize GPU throughput, and it additionally enables parallelization across input sequence length (query length) as well.While these optimizations provide significant improvements, e.g.",
            "score": 0.5660046243922516,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 426
                },
                {
                    "start": 428,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1131
                },
                {
                    "start": 1131,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1407
                },
                {
                    "start": 1409,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1788
                },
                {
                    "start": 1788,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 43,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "220301566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "270094757",
            "title": "On the Role of Attention Masks and LayerNorm in Transformers",
            "text": "Sparse and local Attention While many existing transformer models and LLMs do not use sparse attention, sparse and local attention is gaining popularity due to the demand for efficiency, particular for long-context tasks. For example, sparse attention was populated by Longformer [4] and OpenAI [10] and nowadays popular LLMs like Mistral 7B [22] use sliding window attention by default. Other popular sparse attention models include, but are not limited to BigBird [42], Recurrent Memory Transformers (RMTs) [7], and Streaming Attention [38]. Besides language tasks, sparse attention is also common in vision transformers [19,25,28].",
            "score": 0.5659929546692641,
            "section_title": "Related work",
            "char_start_offset": 7308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 634
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "248178045"
                },
                {
                    "start": 627,
                    "end": 630,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "258048654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "259063695",
            "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
            "text": "State-of-the-art sequence models have very high computational requirements. As a consequence, a lot of effort has been invested into developing methods to reduce the memory footprint in Transformers. Many efficient Transformer variants have been developed, with the main goal of taming the quadratic complexity of the attention mechanism (Tay et al., 2020). Several methods rely on kernelized attention (Katharopoulos et al., 2020;Choromanski et al., 2020), while others endow the Transformer with some auxiliary memory to increase the context (Wu et al., 2022;Borgeaud et al., 2021). \n\nIn many cases, leveraging sparsity in the attention matrix has proven useful. The Sparse Transformer (Child et al., 2019) works with a factorized sparse representation of the attention. They employ several sparse attention patterns, where each output position only computes weightings from a subset of input positions. \n\nThe Reformer (Kitaev et al., 2020) uses locality-sensitive-hashing (LSH) to sparsify the attention matrix and allow queries to restrict their context window to keys that collide with the same hash. However, to allow GPU-efficient processing, complex machinery has to be developed where the queries and keys are split into fixed-sized chunks, with the attention being applied only within the chunk and the immediate neighbor. K Q QK-sparse Hash-sparse Figure 1: Proposed sparsification of the attention matrix for a given attention head. In each depicted attention matrix, black areas indicate coefficients to compute, patterned areas those forced to zero due to the causal masking, and white areas coefficients that are ignored. We consider two main dynamic strategies to sparsify the left attention matrix. The QK-sparse attention consists of dropping some keys and queries (top, the discarded keys and queries are indicated in red), and the Hash-sparse attention computes a hash code for each key and each query, and restricts the attention matrix to blocks of keys and queries of same hash code (bottom, the three hash values are indicated for each key or query with the colors blue/green/red).",
            "score": 0.5654840067752557,
            "section_title": "Related work",
            "char_start_offset": 1868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 431,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 921,
                    "end": 942,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "258888224",
            "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
            "text": "The introduction of Transformers [Vaswani et al., 2017] in Large Language Models (LLMs) has profoundly influenced the landscape of Natural Language Processing (NLP), due to their appealing scaling properties [Kaplan et al., 2020] and their ability to train efficiently on modern hardware architectures designed for extensive parallel computing. As LLMs grow larger and more complex, the challenges associated with training and deploying them become more prominent. Especially challenging is the quest for processing increasingly longer sequences, as pure self-attention layers scale quadratically in sequence length during train and inference. \n\nTo address this limitation, several efforts focus on efficient implementations of the attention mechanism on dedicated hardware [Dao et al., 2022, Touvron et al., 2023], or on algorithmic procedures to directly tackle the quadratic complexity. The latter direction has led to numerous variants sacrificing the generality of the standard attention mechanism in favor of more efficient alternatives [Tay et al., Correspondence sanagnos@inf.ethz.ch. Current Token Attended Tokens Masked Tokens \n\nFigure 1: Visualization of the causal attention weights associated with standard, local, sparse causal attention, and our approach. Adaptively sparse attention (rightmost) prunes weights dynamically for each token, and it does not impose any restricting inductive biases on the final attention structure. \n\n2020, Kitaev et al., 2020, Choromanski et al., 2020b, Katharopoulos et al., 2020, Zaheer et al., 2020, Shi et al., 2021, Lin et al., 2022, Zhu and Soricut, 2021, Dai et al., 2020], some of which are illustrated in Fig. 1. Specifically, a large number of these methods focus either on sparsifying the attention weights, reducing the size of the available context to each token, or compressing the number of tokens to reduce the size of the attention matrix. \n\nThese methods, however, are inherently static, in the sense that each token is either forced to attend to a fixed pre-specified context window, or the input context is compressed to a fixed dimensionality, regardless of the information content of the input sequence.",
            "score": 0.5653899970683215,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 774,
                    "end": 791,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1498,
                    "end": 1526,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1526,
                    "end": 1547,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1547,
                    "end": 1565,
                    "matchedPaperCorpusId": "232045912"
                },
                {
                    "start": 1606,
                    "end": 1625,
                    "matchedPaperCorpusId": "219401850"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23583984375
        },
        {
            "corpus_id": "220831004",
            "title": "Big Bird: Transformers for Longer Sequences",
            "text": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
            "score": 0.5653065411400622,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "250493276",
            "title": "DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation",
            "text": "The full token-wise attention operation in standard Transformer [51,9] poses high requirements on memory and significantly increases computational cost. Thus, a lot of works are devoted to designing efficient attention mechanisms for Transformer or by extension, graph-based methods [58,57]. On the one hand, some works rely on heuristic strategies to lead a current token only focus on those in a certain local context [4,6,26,3]. Recently, more strategies based on properties of images to boost the efficiency in vision Transformer are explored [60,39]. On the other hand, random sampling based Informer [68], locality sensitive hashing based Reformer [22], and approximated Softmax based Performer [5] achieve lower complexity with a fine theoretical guarantee. Wang et al. [53] only involve tokens with top K attention scores for feature aggregation. Similar strategy is also adopted in [69,63,44]. Although effective, it is not flexible enough to fix the number of attentive tokens, which fails to model the complex and changeable matching patterns in practice. Different from all these methods, the sparse mechanism in the attention module of this paper is based on prior knowledge in image matching, targeting at exemplar-guided image generation.",
            "score": 0.5651204576241218,
            "section_title": "Efficient Transformer",
            "char_start_offset": 7204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 68,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "214713848"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "222291105"
                },
                {
                    "start": 423,
                    "end": 425,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "195766887"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "244478080"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "244729033"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "229156802"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "235702956"
                },
                {
                    "start": 898,
                    "end": 901,
                    "matchedPaperCorpusId": "233462290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "273849947",
            "title": "LASER: Attention with Exponential Transformation",
            "text": "The attention mechanism was used in Bahdanau et al. (2015) to drastically improve machine translation performance compared to encoder-decoder recurrent neural networks (RNNs) (Cho, 2014). This was later adopted in Transformers (Vaswani et al., 2017), which introduced self-attention to improve the performance in machine translation even further. Efficient attention mechanisms have been an active area of research due to the quadratic computational complexity in sequence length of Attention, which prevents long-context language modeling. One notable contribution is Linear Attention (Katharopoulos et al., 2020), which reduces the quadratic complexity of self-attention to linear in sequence length by using kernel approximation of the softmax operation. Similarly, the Performer (Choromanski et al., 2021) develops an alternative kernel approximation using random feature maps to achieve linear complexity. \n\nThe Mamba architecture introduces state-space models (SSMs) as a replacement for traditional attention. Models like S6 (S4+selection+scan) (Gu & Dao, 2023) from Mamba and SSD from Mamba-2 (Dao & Gu, 2024) have linear computational complexity in sequence length without the use of attention. However, despite these innovations, attention-based models like Gemini 1.5 Flash (Gemini, 2024) and LLaMA 3 (Dubey et al., 2024) continue to dominate long context regime, particularly through advancements in context parallelism (Liu et al., 2023), which ensures scalability while maintaining the strengths of attention mechanisms in Transformer models. \n\nEfficient attention mechanisms have become critical in handling long sequences, especially in Transformer-based architectures. This mechanism is used for faster inference and training, particularly when scaling up to large sequence lengths. Sparse Transformers (Child et al., 2019) use fixed sparse attention patterns, enabling them to efficiently handle very long sequences by reducing the quadratic complexity of standard attention to linear or sub-quadratic in practice. Routing Transformers (Roy et al., 2021) take a different approach by introducing a mechanism that sparsifies attention through data-dependent sparsity patterns with subquadratic computational complexity in sequence length.",
            "score": 0.5647421854382524,
            "section_title": "RELATED WORK",
            "char_start_offset": 5651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 58,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 227,
                    "end": 249,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 586,
                    "end": 614,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 783,
                    "end": 809,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466796875
        },
        {
            "corpus_id": "273233193",
            "title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models",
            "text": "By limiting the tokens that attend to each other and ignoring the computation of certain attention scores, the complexity and memory access of MHSA are reduced. The sparsity pattern of sparse attention can be defined online or offline, diverging into static sparse attention, which is agnostic to the input data, and dynamic sparse attention, which depends on the input. \n\nStatic sparse attention applies pre-defined attention masks to set the corresponding attention scores to zero during inference. The static sparse pattern usually includes local, global, and random attention. In local attention, tokens attend only to their neighbors within a fixed window. In global attention, certain tokens attend to all other tokens, regardless of their position. In random attention, tokens attend to a set of random tokens, covering various types of dependencies. Longformer [204] utilizes a combination of local attention and global attention to specific tokens, while BigBird [205] further adds random attention on top of local and global attention, demonstrating its ability to encompass all sequence-to-sequence functions. Static sparse attention changes the operations in MHSA from",
            "score": 0.5645681041552835,
            "section_title": "C. Compression Methods and Accelerators",
            "char_start_offset": 58441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1180
                }
            ],
            "ref_mentions": [
                {
                    "start": 972,
                    "end": 977,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "\u2022 Downsampling -Another popular method of reducing computation cost is to reduce the resolution of the sequence, hence reducing computation costs by a commensurate factor. Examples of this class of models include Perceiver (Jaegle et al., 2021), Funnel Transformers (Dai et al., 2020), Swin Transformer (Liu et al., 2021b), and Charformer (Tay et al., 2021c) models. Notably, there might also be some form of overlap of this class of models with models that leverage memory tokens as models such as Set Transformer can also be viewed as a form of downsampling, albeit within the attention mechanism. The recent Nystr\u00f6mformer (Xiong et al., 2021b), on the surface, may seem like a low-rank or kernal-based approach. However, it is actually a downsampling approach where the 'landmarks' are simply strided based pooling -in similar spirit to Set Transformer, Funnel Transformer or Perceiever. \n\n\u2022 Sparse Models and Conditional Computation -While not targeted specifically at the attention modules, sparse models sparsely activate a subset of the parameters which generally improves the parameter to FLOPs ratio. Examples of this class of model includes Switch Transformers (Fedus et al., 2021), ST-MoE (Zoph et al., 2022), GShard (Lepikhin et al., 2020), Product-Key Memory Layers (Lample et al., 2019). Within the scope of our studied models, sparse models typically operate on an adaptive basis in which the sparsity is typically learned (via mixture-of-experts like mechanism). \n\nWithin this context, we can also consider sparsification of attention weights to fall under this paradigm. For this reason, we believe there is a close connection to fixed or learned patterns in attention. However, we believe that the emergence of an entire research direction (Roller et al., 2021;Lewis et al., 2021;Lepikhin et al., 2020;Du et al., 2021) based on sparse efficient should warrant a new category of efficient Transformers. \n\nWe note that these buckets are a broad characterization of the different efficient Transformer models.",
            "score": 0.5633783094555884,
            "section_title": "A Taxonomy of Efficient Transformers",
            "char_start_offset": 22542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1919
                },
                {
                    "start": 1922,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "219401850"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "235624202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "Self-attention operation. The self-attention operation (Vaswani et al. 2017) stands as a pivotal element within transformer models. Fig. 2 illustrates a comparison of various attention techniques. In the conventional approach, shown in Fig. 2(a), each token attends to every other token, including itself. In the self-attention operation, the hidden states X = (x 1 , x 2 , ..., x n ) \u2208 R n\u00d7d of an input sequence have n tokens in d dimensions. They are projected to query (Q), key (K), and value (V ) by three weight matrices, \n\n, respectively, as follows: \n\nThen, the attention values between tokens are computed as: \n\nHere, S(\u2022) denotes the row-wise softmax normalization operation, and A \u2208 R n\u00d7n is the attention matrix, which contains the attention weights between tokens. The selfattention operation represents the value of a token as a weighted sum of all token values, based on their attention weights. The computation in Eq. 2 exhibits a quadratic complexity of O(n 2 d k + n 2 d v ) for an input sequence length n. As the input sequence length grows, this computation turns into a significant computational and memory challenge for the transformer model, thereby restricting its ability to handle longer input sequences. \n\nEfficient Transformers. Numerous studies have been conducted on efficient transformers with the goal of alleviating the quadratic dependency on input sequence length. The first approach is to exploit the sparsity of attention maps. To learn the sparse access pattern from the data set, Reformer (Kitaev, Kaiser, and Levskaya 2020) trains the query and key matrices to be clustered into buckets by locality sensitive hashing and considers the relationship between tokens only within the same bucket at attention operations, as depicted in Fig. 2(c). It reduces dot-product operations and achieves a complexity of O(nlogn). There are also similar approaches using sorting or clustering-based methods (Tay et al. 2020;Vyas, Katharopoulos, and Fleuret 2020). The second approach is to leverage low-rank approximation and kernelization.",
            "score": 0.56316673049379,
            "section_title": "Background and Motivation",
            "char_start_offset": 5097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 1528,
                    "end": 1563,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1948,
                    "end": 1986,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "252683356",
            "title": "DARTFormer: Finding The Best Type Of Attention",
            "text": "Following Wang et al. [2021] we use masked validation accuracy drop as our metric for determining the performance of each attention type. \n\nFor clarity we refer to computation of the QKV linear projections, multi-head attention, and then concatenation and linear projection back down to the sequence features as the attention block. In a standard Transformer there is only a single attention block that has multiple heads. When training mixed attention models (either supernetworks or a final heterogeneous model) we use multiple attention blocks, each containing only a single attention type. Following Tay et al. [2020b], we use a representative mixture of different attention mechanisms as part of our search space in order to cover the main methods of achieving efficient attention. The specific attentions we use are: Bigbird [Zaheer et al., 2020], Linear Transformer [Katharopoulos et al., 2020], Linformer [Wang et al., 2020], Local attention [Liu et al., 2018b], Longformer [Beltagy et al., 2020], Performer [Choromanski et al., 2020], Reformer [Kitaev et al., 2020], Sparse Transformer [Child et al., 2019], and Synthesizer [Tay et al., 2021]. \n\nWe use our setup to investigate two key paradigms. The first is learning the best attention for a new task with a single layer Transformer, this means only one full-scale Transformer needs to be trained after a good attention mechanism is found. The second paradigm, illustrated in Figure 1, is using a single layer Transformer to find the best head-wise heterogeneous attention mixture for that task, and then using that mixture in each layer of a full Transformer model, making it layer-wise homogeneous. We test these paradigms on three different tasks. They are taken from Tay et al. \n\n[2020b] and were specifically designed to test the capabilities of efficient long range Transformers. \n\nIn this paper we make the following contributions: \n\n\u2022 We propose a DARTS-like framework to efficiently find the best attention for a task. \n\n\u2022 We extend this framework to building and searching for optimal heterogeneous attention Transformer models.",
            "score": 0.5626938663485772,
            "section_title": "Introduction",
            "char_start_offset": 1914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 873,
                    "end": 901,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1133,
                    "end": 1151,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18310546875
        },
        {
            "corpus_id": "238856876",
            "title": "How does momentum benefit deep neural networks architecture design? A few case studies",
            "text": "4) Transformers are the current state-of-the-art machine learning (ML) models for sequential learning [98], which processes the input sequence concurrently and can learn long-term dependencies effectively. However, transformers suffer from quadratic computational time and memory costs with respect to the input sequence length; see Section 4 for details. In response, efficient attention has been proposed leveraging sparse and low-rank approximation of the attention matrix [53,66,4,1,112,107,39,16], locality-sensitive hashing [44], clustered attention [100], and decomposed near-field and far-field attention [61].",
            "score": 0.5621388682080403,
            "section_title": "Introduction",
            "char_start_offset": 1777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 618
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 485,
                    "end": 487,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 556,
                    "end": 561,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1820068359375
        },
        {
            "corpus_id": "269899568",
            "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
            "text": "While these optimizations provide significant improvements, e.g.FlashAttention-2 realized 2x speedup over FlashAttention, these mechanisms only provide performance benefits for a subset of problem sizes (i.e.sequence length, batch size, and number of heads) because they overlook the distinct behavior of the attention mechanism during the decode phase versus the prefill-phase in decoder-only transformer models.\n\nIn decoder-only transformer models, the inference process for a single request involves multiple forward passes of the model where output tokens are generated sequentially [21].This inference procedure inherently comprises two distinct computational phases due to the practice of reusing (caching) the key-value tensors of the attention mechanism of the previously computed tokens [28].The first phase is the prompt computation phase (sometimes known as prefill phase) where all tokens from the input prompt undergo parallel forward passes through the model to generate the first output token.This phase is computationally intensive and demands high FLOPS/s (floating point operations per second) [21].Following the prompt computation, the decode phase (sometimes known as tokengeneration phase) begins in an auto-regressive manner [31].Each subsequent token is produced based on the forward pass of the preceding token and the cached context (kv-cache) from previous tokens in the sequence.With the push towards longer context lengths, this cached context can be long, exceeding Fig. 1.Execution schedule of FlashAttention-2 [13], FlashDecoding [4] (fixed-split), and LeanAttention across a hypothetical five SM GPU executing attention of 2 heads.LeanAttention splits the context into optimal LeanTiles (shown here with 5 tiles per head).more than hundreds of thousands of tokens in length [16], [22], [24], [36].Despite state-of-the-art batching techniques [34] and attention partitioning mechanisms [13], [14], the sequential processing of this long context length makes the decode phase slow, bound by memory bandwidth [32] and capacity [21].",
            "score": 0.5614124520593158,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 64,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 413
                },
                {
                    "start": 415,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1754
                },
                {
                    "start": 1754,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "253420623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "259991382",
            "title": "Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model",
            "text": "In this section, we review methods that improve the original transformer by scaling up input length, longer-term attention span, limiting the connection between tokens in self-attention, and reducing the memory consumption and computation time. The attention span of the original transformer is fixed and constrained. Therefore, no information can cross fixed-length segments. The Transformer-XL [16] extended the attention span over a longer period of time and across several segments by incorporating information from earlier hidden states. Moreover, to maintain a consistent flow of positional information across segments, it encoded a relative position rather than absolute position. It has been shown that relative position embeddings perform better in tasks requiring comprehension and the creation of natural language [14,16]. To reduce memory and computational requirements, Sparse Transformer [12] introduced factorized self-attention using predefined sparsity patterns such as local or stride attention, making it possible to train deeper networks on sequences of unprecedented length on current hardware. \n\nThe objective of research lines like Routing Transformer [17], Reformer [13], and Sinkhorn Transformer [18] is to acquire knowledge of sparsity patterns. In particular, Routing Transformer [17] implements a sparsity pattern learning method that is based on content similarity and employs k-means clustering. This approach allows it to generate attention queries and keys exclusively from the same cluster. On the other hand, the Reformer [13] model uses localitysensitive hashing (LSH) [19] to incorporate attention mechanisms and to compute attention only for query and key vectors within the same hash buckets. \n\nAnother line of study [8,10,11] showed both attention types (local and global attention) are essential. The local attention is mainly utilized to construct contextual representations, while the global attention can build whole sequence representations for prediction. Longformer [10] introduced a windowed local-context self-attention along with global attention to a few pre-selected input tokens for learning task-specific representations, such as CLS token for classification and all question tokens for MRC. Extended transformer construction (ETC) model [8] defined some additional global content embeddings that do not correspond to any of the input tokens.",
            "score": 0.5600538556676116,
            "section_title": "Related Work",
            "char_start_offset": 6937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "3725815"
                },
                {
                    "start": 1175,
                    "end": 1179,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "matchedPaperCorpusId": "9222460"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "271143408",
            "title": "Exploring Attention Sparsity to Accelerate Transformer Training on GPUs",
            "text": "The performance of original encoder-only Transformer and three state-of-the-art sparse Transformers, including Longformer, BigBird, and LSG Attention, is compared with our SAT model. \n\n\u2022 Original encoder-only Transformer [1]: This implementation is based on the original Transformer architecture and performs the original dense MHA operation during entire training. \n\n\u2022 Longformer [5]: This model utilizes a sparse attention mechanism based on dilated sliding windows. It is evaluated using a sliding windows size of 64. \n\n\u2022 BigBird [4]: This model incorporates sparse attention mechanisms, including sliding window attention, global attention, and random attention. It is evaluated using a block size of 32 and 3 random blocks. \n\n\u2022 LSG Attention [14]: This model incorporates local attention, sparse attention, and global attention. Max norm is chosen for the sparse attention to evaluate. It is evaluated using a block size of 32 and a sparse block size of 32, with a sparsity factor of 2. \n\n\u2022 SAT: This model is our proposed method, which utilizes learnable parameters to capture the general sparse pattern of the training dataset. \n\nIn our SAT, an embedding dimension (D) of size 64 was employed, and the batch size was determined by the available memory size, resulting in batch sizes of 512. The block size (B) for the learnable parameter is set to 32. We set the skewness threshold (\u03b3 ) to 1.7 for the AG News, CIFAR-10, and iNaturalist datasets, and a value of 1.3 for the Yelp Review dataset. Additionally, the difference threshold (d) was consistently fixed at 1.3 across all datasets. Note that all the experiment results presented in this section are averaged over five different executions.",
            "score": 0.5599913125191881,
            "section_title": "2) MODELS COMPARED",
            "char_start_offset": 29820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1703
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "253157377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "249395507",
            "title": "EAANet: Efficient Attention Augmented Convolutional Networks",
            "text": "Longformer (Beltagy et al., 2020) utilizes hybrid attention mechanisms. It combines the sparsity and memory-based mechanism with its unique sliding window approach during attention score calculation. Compared with vanilla self-attention, which attends all tokens in every iteration, Longformer has a fixed interval of tokens to reduce the computation expenses. This sliding window attention mechanism can be improved by adding some global tokens that attend every iteration. \n\nMulti-scale Vision Transformer (Zhang et al., 2021), to our knowledge, is the first work implementing efficient Transformers such as Longformer and Linformer in computer vision tasks. A comprehensive evaluation of different efficient Transformer models' performance is also presented in Multi-scale Vision Transformer repository. We also use the efficient Transformers implementation in Multi-scale Vision Transformer of Longformer and Linformer blocks in our EAANet.",
            "score": 0.5591649375687003,
            "section_title": "Efficient Self-attention Mechanisms",
            "char_start_offset": 6103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 528,
                    "matchedPaperCorpusId": "232404731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.220703125
        },
        {
            "corpus_id": "274822286",
            "title": "Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models",
            "text": "Attention on long contexts. Since the introduction of Self-Attention [33], a significant amount of research has been made to reduce its quadratic cost in the sequence length at training time. To achieve this, state of the art methods usually approximate the Attention mechanism with sparse/linearized versions. For example, Reformer [21] uses locality-sensitive-hashing to group tokens with similar embeddings, allowing the model to only attend to a subset of tokens rather than the entire sequence. Longformer [1] combines local Attention, whereby each token is restricted to attending only to a window of neighboring tokens, and global Attention, whereby a few tokens can attend to all tokens. Meanwhile, Linformer [35] uses projection matrices to project keys and values into a lower-dimensional space along the sequence dimension. Different from these approximate Attention methods, other works have proposed to endow Transformers models with 'compressed' memory tokens that are updated dynamically and causally over sliding windows on entire sequence chunks. For example, Transformer-XL [4] and Infini-Attention [24] segment an input sequence into chunks and process them sequentially while maintaining a complementary set of tokens whose purpose is to summarize the older ones. In contrast to these works, our Attention retrieves relevant tokens 'eidetically,' i.e., we retrieve tokens rather than maintain a compressed representation of the past. Most similar to our method is Landmark Attention [23], which inserts landmark tokens into the input at fixed block intervals and trains these tokens to act as summaries of their corresponding blocks via a grouped softmax attention; these summary tokens are then used to index and retrieve relevant input tokens when processing future segments. Our Attention aims to integrate retrieval natively, without the need for external landmark tokens or complex Softmax procedures. State Space Models. While a great effort has been made to improve the efficiency of Transformer models, a recent line of work has explored efficient alternative 'linear' architectures. In particular, State Space Models (SSMs) [13-15, 30, 36] have emerged as promising competitors to Transformer models due to their efficient scaling and strong empirical performance.",
            "score": 0.5590851468578336,
            "section_title": "Background and Related Work",
            "char_start_offset": 4119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25830078125
        },
        {
            "corpus_id": "257232619",
            "title": "A Survey on Long Text Modeling with Transformers",
            "text": "The recurrent Transformer is another approach to tackling the issue of limited context length. Unlike efficient Transformers that simplify the attention structure, recurrent Transformer keeps the full self-attention mechanism. Typically, a long document is split into a series of chunks as Section 3.2. In-stead of processing each chunk separately, we cache the history information of previous chunks. When the subsequent segment is fed into the model, the cached information can be exploited to mitigate the context fragmentation problem. \n\n-Transformer-XL [Dai et al., 2019] first proposes a recurrence mechanism for Transformers. The hidden states of the previous segment are concatenated with the current segment as the input to the next layer. However, those representations of very distant segments will be discarded due to limitations of storage cost. To alleviate this problem, Compressive Transformer [Rae et al., 2019] condenses past representations into more coarse-grained memories. Due to the uni-directional self-attention, Compressive Transformer and Transformer-XL still have a fixed memory size. In order to eliminate this limitation, Memformer [Wu et al., 2020] designs a memory system with multiple slots to store history information. Memory cross attention and memory slot attention are proposed to retrieve and update the memory dynamically. ERNIE-Doc [Ding et al., 2021] enhances recurrent mechanisms through the same-layer recurrence, i.e., passing the concatenation of the hidden states of the current segment and the previous segment at the next layer to the next layer, further extending the context length while maintaining fine-grained representations. In addition, a retrospective feed mechanism is applied, making the global context information of a document available for each segment.",
            "score": 0.5590680066831673,
            "section_title": "Recurrent Transformers",
            "char_start_offset": 20248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1816
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1373,
                    "end": 1392,
                    "matchedPaperCorpusId": "229923177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07159423828125
        },
        {
            "corpus_id": "253158021",
            "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost",
            "text": "We propose SBM-Transformer, an efficient Transformer that can data-adaptively choose its attention sparsity between sparse and full attention without the need to explicitly compute the full attention score matrix. Theoretically, we show that our model enjoys the same expressibility as the original Transformer due to the flexibility of the latent SBM. Empirical experiments on LRA and GLUE show that our model performs competitively against previous state-of-the-art efficient Transformers. \n\nNonetheless, there are limitations due to sparse tensor operations being less optimized on GPU kernels. \n\nIn the LRA experiments, we found that SBM-Transformer can result in longer runtimes compared to dense counterparts while its memory usage is much lower. While previous sparsity-based attention mechanisms with block-sparse attention are much more amenable for GPU computation [51,9,3], our work requires an architecture with better workload balancing and acceleration under unstructured sparsity, for which there is ongoing work [46,54]. \n\nWe still believe this work is valuable as it is the first approach to induce per-example attention sparsity, allowing the model to adjust its computational cost based on the input. The cost being dependent on the number of edges also allows practitioners to easily impose constraints based on the available computational resources. We hope to see more GPU-friendly tensor operations optimized for finegrained sparsity in the future, at which point the value of this work will increase even further. As we propose a foundational replacement for the scaled dot-product attention module in the Transformer architecture, we do not expect any immediate negative societal impact due to this work.",
            "score": 0.5584576707812592,
            "section_title": "Conclusion",
            "char_start_offset": 28777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1729
                }
            ],
            "ref_mentions": [
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "222297541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "269741142",
            "title": "A Lightweight Sparse Focus Transformer for Remote Sensing Image Change Captioning",
            "text": "Shuffle Transformer [26] and Msg-transformer [27] employed spatial shuffle operations as alternatives to shifted window partitioning, facilitating cross-window connections.FasterViT [28] introduced hierarchical attention, breaking down global self-attention into multi-level attention components. FLatten Transformer [29] integrated depth-wise convolution in conjunction with linear attention mechanisms to address the challenge of maintaining diversity in output features across different positions. \n\nA summary of the efficient attention work reveals that our sparse focus attention not only enhances local information in images but also enables the accumulation of long-distance context. Simultaneously, it is implemented with simplicity and efficiency in both its realization and speed.",
            "score": 0.5577201724821188,
            "section_title": "A. Efficient Attention Architecture",
            "char_start_offset": 6697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 790
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "235254672"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "260351423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1669921875
        },
        {
            "corpus_id": "267027729",
            "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
            "text": "LLMs such as GPT-3 [36] and LLaMA [35] commonly embrace the Transformer architecture [37]. The architectural homogeneity suggests that tailored system optimizations are applicable. As shown in Figure 2     has an attention block with D attention heads and an MLP layer. Input/output dimensions are B \u00d7 S \u00d7 H (B: micro-batch size, S: sequence length, H: hidden dimension). We show the memory required to store activations (ACT ) in a 16-bit floating point format for each element. The fused kernel of FlashAttention [32] is used for efficient multi-head attention (MHA) computation.",
            "score": 0.5575746051105619,
            "section_title": "Transformer Architecture",
            "char_start_offset": 6450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 581
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07080078125
        },
        {
            "corpus_id": "269899568",
            "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
            "text": "Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference. To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the\"stream-K\"style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.",
            "score": 0.5573113508100517,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294677734375
        },
        {
            "corpus_id": "249151952",
            "title": "What Dense Graph Do You Need for Self-Attention?",
            "text": "To the best of our knowledge, our paper is related to sparse attention and their analysis. \n\nTheoretical Analysis. Previous works mainly focus on the approximation of sparse attention to vanilla attention. BigBird (Zaheer et al., 2021) first proved sparse attention mechanism defined by any graph containing star graph is a universal approximator. They also showed Turing Completeness of sparse encoder and sparse decoder. Pixelated Butterfly (Chen et al., 2021) proved their flat butterfly matrices can approximate butterfly matrices which can tightly represent all structured matrices. Our paper focuses on finding graphs with better properties, not graphs to approximate complete graph. Axial Attention (Ho et al., 2019) mentioned having the full receptive field, which is similar to grabbing all interactions among tokens in the paper. (Li et al., 2019) proposed Information Capacity between two nodes based on the path transferring least information while we propose Information Payload based on all paths between two nodes. \n\nSparse Attention. Previous Transformers adapt sparse attention including Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019), Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021). Compared to all those patterns, our proposed Hypercube Transformer adapts a fixed simple sparse pattern and is easy to implement. The recent flat butterfly pattern (Chen et al., 2021) is also another simple sparse pattern with O(N log N ) complexity with input length N . NAS has been applied to learning sparse patterns like SparseBERT (Shi et al., 2021), but searching methods cannot be applied to long-context tasks. Besides, their learned sparse patterns are data-dependent and cannot generalize.",
            "score": 0.5565361873189534,
            "section_title": "Related Work",
            "char_start_offset": 21950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 857,
                    "matchedPaperCorpusId": "203033339"
                },
                {
                    "start": 1122,
                    "end": 1140,
                    "matchedPaperCorpusId": "209009596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2216796875
        },
        {
            "corpus_id": "273856640",
            "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels",
            "text": "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) [3]. \n\nBIGBIRD takes the view that full attention is a fully connected graph that allows us to exploit existing graph theory to help reduce its complexity. The problem of reducing the second-order complexity of self-attention can now be viewed as a graph distribution problem. The authors point out the important role of three perspectives: \n\n\u2022 The first viewpoint: build the shortest path between pairs of nodes using random edge construction \n\n\u2022 The second viewpoint: most contexts in NLP use a lot of local references \n\n\u2022 The last viewpoint: use global tokens (star network) While a complete Transformer based on a quadratic attention mechanism is Turing complete [16], the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine [3]. And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) [3]. From this, we can see that BIGBIRD has a pretty solid mathematical foundation to underpin the model's operation. \n\nCombining the knowledge in Longformer and LongT5 models, we can see that a Transformer model using sparse self-attention will have to include at least the following elements: local self-attention, global self-attention, and shortest path between tokens.",
            "score": 0.5565312503816268,
            "section_title": "F. BIGBIRD",
            "char_start_offset": 12783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "matchedPaperCorpusId": "57825721"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1506,
                    "end": 1509,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.615234375
        },
        {
            "corpus_id": "232380042",
            "title": "A Practical Survey on Faster and Lighter Transformers",
            "text": "In conclusion, there seem to be no simple and universal guidelines regarding the current Transformer alternatives. If the data and task are standard, we recommend looking in the literature or on the Papers With Code website for references on how the different methods compare and experiment with already pre-trained models. Otherwise, we recommend using a small vanilla Transformer with mixed-precision and gradient checkpointing as baseline, then experimenting with already implemented lower-complexity models. As a side note, one may also want to combine multiple specialized approaches. For instance, BigBird-ETC relies on additional tokens for global attention, a form of memory similar to the Compressive Transformer. Nonetheless, many combinations are unprincipled at best. For instance, one should not factorize a sparse attention: the complexity will be similar to that of the same factorization of the full attention, and the sparsity may lose valuable information that the factorization could have preserved.",
            "score": 0.5564633024933541,
            "section_title": "C PRACTICAL GUIDELINES -SPECIALIZED METHODS",
            "char_start_offset": 107411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0948486328125
        },
        {
            "corpus_id": "274776700",
            "title": "Advances in Transformers for Robotic Applications: A Review",
            "text": "Transformer [138] O(n) Differentiable sorting in attention for structured inputs. Mega [87] O(n) Combines sparse and memory-efficient attention for general long-sequence modeling. \n\n*Here, d represents the hidden dimension of the model. \n\n\u2022 Adaptive Attention Span Transformer [131]: Adjusts the attention span dynamically extends significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. Many researchers have considered different approaches across various applications some names include Universal Transformer [25], Dynamic Vision Transformer [151] and Feedback Transformer [30].",
            "score": 0.5564470586472556,
            "section_title": "Sinkhorn",
            "char_start_offset": 16273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 661
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24658203125
        },
        {
            "corpus_id": "230435805",
            "title": "Transformers in Vision: A Survey",
            "text": "Oscar [44] Transformer layer to jointly process triplet      complexity of O(nc) with respect to the input sequence length n (where c is the number of clusters). We refer interested readers to a survey on efficient Transformers in NLP [34]. Similar to the NLP domain, computer vision models also suffer from the high computational cost of Transformer models. For example, image generators that are based on sequence-based Transformers (e.g., iGPT) have a high compute cost limiting their applicability to high-resolution inputs. The time and memory cost of core self-attention operation in Transformers increases quadratically with the number of patches, i.e. O(n 2 ), for n image patches (in some applications, e.g., low-level vision, n = H \u00d7 W where H, W denote the height and width of the image). This is a major drawback of existing Transformers that hinders their application to most tasks involving high-resolution (HR) images, such as object detection and segmentation (in highlevel vision), and super-resolution, deblurring, denoising, etc. (in low-level vision). Numerous methods have been proposed that make special design choices to perform selfattention more 'efficiently', for instance employing pooling/downsampling in self-attention [97], [219], [249], local window-based attention [36], [250], axial-attention [179], [251], low-rank projection attention [38], [252], [253], ker-nelizable attention [254], [255], and similarity-clustering based methods [246], [256]. However, almost all of these approaches either come with a trade-off between complexity and accuracy, require special hardware specifications or are still not applicable to very large images. Therefore, there is a pressing need to develop an efficient self-attention mechanism that can be applied to HR images on resourcelimited systems without compromising accuracy. It will be interesting to explore how existing models can be extended to high-dimensional cases e.g., using a multi-scale transformer design with a somewhat local context modeling.",
            "score": 0.5563869701035182,
            "section_title": "Multi-Model Learning",
            "char_start_offset": 111437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 10,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 1303,
                    "end": 1308,
                    "matchedPaperCorpusId": "232320340"
                },
                {
                    "start": 1376,
                    "end": 1381,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1383,
                    "end": 1388,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1421,
                    "end": 1426,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1468,
                    "end": 1473,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1475,
                    "end": 1480,
                    "matchedPaperCorpusId": "211505992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0560302734375
        },
        {
            "corpus_id": "266110855",
            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
            "text": "Transformer [1] models have achieved dominant results in numerous text-oriented tasks and have become the foundational architecture in natural language processing (NLP). However, standard Transformer models [1], [2], [3], [4], [5] with full attention cannot efficiently handle long texts due to their quadratic complexity in relation to the input length, and consequently cause the maximum input length of 1\u223c4k for lowering computational consumption. To relieve the limitation, considerable efforts of research called efficient Transformers [6] have been done in past few years, which \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Okyay Kaynak . \n\nincluding transformers with sparse attention and transformers with hierarchical or recurrent architectures. \n\nAs the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention [8] if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird [9], LongT5 [10] have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy. \n\nAs another category, transformers with hierarchical or recurrent architectures [11], [12], [13], [14], [15] are based on the concept of divide-and-conquer.",
            "score": 0.5558716870242997,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2080
                },
                {
                    "start": 2083,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1465,
                    "end": 1468,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "245144820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "273025930",
            "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
            "text": "Since our implementation differs from traditional transformers in sparse attention computation only, LONGGEN can benefit from the tools developed to improve transformer efficiency, such as flash attention. In fact, we build our customized kernel upon flash attention. Similarly to FlashAttention (Dao et al., 2022), LONGGEN takes an attention mask matrix as input. The attention mask is represented in compressed sparse row format, which consumes O(N ) memory instead of the original O(N 2 ). Note that the sparse mask is static for all heads, so there is no overhead in building the dynamic masks. During forward and backward steps, we skip computing a position by avoiding loading it into HBM if its mask value is zero. This implementation enables FLOPs saving, significantly accelerating training and inference. \n\nDuring inference, the bottleneck is to load KV cache as fast as possible. Instead of loading the entire head dimension, we split the loading of the head dimension across different thread blocks. We empirically found this to be more efficient than the original FlashAttention, probably because of reduced SRAM usage per loading cycle. For the attention sink initialization, we follow Mistral to use the rolling buffer cache for local sliding window tokens (Jiang et al., 2023).",
            "score": 0.5553837148051066,
            "section_title": "KERNEL-LEVEL OPTIMIZATION TO IMPROVE EFFICIENCY",
            "char_start_offset": 12331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1293
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 314,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55322265625
        },
        {
            "corpus_id": "268385246",
            "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs",
            "text": "FlashAttention is a highly efficient general-purpose fused attention kernel that is particularly effective during context encoding, as it avoids materializing the expensive-to-read-and-write n \u00d7 n attention matrix in GPU memory.\n\nHowever, during incremental decoding with single-context batch sampling, native FlashAttention kernels are not as efficient because they are not designed to be context-aware.Specifically, if there are B batch indices of K,V cache that are duplicate in values due to the shared prefix, FlashAttention-2 (FA2) can use paged KV-Cache to refer and point them to the same KV-pairs for the prefix across a batch.Nevertheless, this does not prevent the FlashAttention kernel from performing multiple reads of the KV-pairs from the shared prefix.\n\nTable 6 shows that a context-aware approach such as bifurcated attention outperforms FlashAttention in parallel sampling scenarios, especially with an increasing number of parallel samples.Notably, the bifurcated attention kernel is utilized solely during the decode step, allowing the efficient FlashAttention2 kernel to be employed during the prefill step for context lengths up to 8192.Furthermore, while non-contiguous memory avoids out-of-memory issues during parallel sampling for non-context-aware kernels, bifurcated attention's memory setup, which maintains only one copy of the context and expands by reference across batch indices, achieves substantially lower latencies.However, the native FlashAttention2 implementation is not yet compatible with PyTorch's compilation capabilities.\n\nIn the future, it may be possible to combine bifurcated attention with FlashAttention to optimize the latency further.",
            "score": 0.5552285882895536,
            "section_title": "H.1. In Comparison with FlashAttention",
            "char_start_offset": 50594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 768
                },
                {
                    "start": 770,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1565
                },
                {
                    "start": 1567,
                    "end": 1685
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232177734375
        },
        {
            "corpus_id": "267499866",
            "title": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers",
            "text": "The Transformer architecture (Vaswani et al., 2017) has revolutionized many fields within machine learning such as translation (Vaswani et al., 2017), summarization (Miller, 2019), text generation (Chen et al., 2019), sentiment classification (Sun et al., 2019), and also tasks like image classification (Dosovitskiy et al., 2020), object detection (Liu et al., 2021b), and protein folding (Jumper et al., 2021). The self-attention mechanism stands at the core of its strengths. It allows the Transformer to directly model long-range dependencies within a sequence without the need for a hidden state like in recurrent neural networks (Hochreiter & Schmidhuber, 1997). However, the self-attention mechanism has an inherent large memory cost, since its complexity grows quadratically with the input sequence length. With these memory requirements and the ever-increasing size of large language models, such as the GPT series (Brown et al., 2020;OpenAI, 2023) and LLaMA (Touvron et al., 2023), a need for more efficient attention mechanisms has emerged (Dao et al., 2022). Current implementations of more efficient self-attention mechanisms can be roughly grouped into the following categories: (1) apply self-attention on subsets of the input sequences(sparsification) (Ainslie et al., 2020;Daras et al., 2020;Kitaev et al., 2020;Ma et al., 2023;Tay et al., 2020b;Zaheer et al., 2021), (2) approximate the self-attention mechanism with a lower complexity (Choromanski et al., 2020;Liu et al., 2021a;Wang et al., 2020), and (3) remove self-attention in favor of a lower complexity similar operation (Gu et al., 2022;Lee-Thorp et al., 2021;Smith et al., 2023;Tolstikhin et al., 2021).",
            "score": 0.5547164853847371,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "235959867"
                },
                {
                    "start": 635,
                    "end": 667,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 924,
                    "end": 944,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1898193359375
        },
        {
            "corpus_id": "268385087",
            "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
            "text": "Attention Speedup: Prior work focus on improving inference speed for transformer (Vaswani et al., 2017) based models.PoWER-BERT (Goyal et al., 2020) utilizes wordvector elimination by exploiting redundancy for encoderbased models.Linformer (Wang et al., 2020) tries to reduce the attention mechanism from quadratic to linear.Reformer (Kitaev, 2020) reduces attention complexity by locality-sensitive hash (LSH).Linear transformers (Katharopoulos et al., 2020) store accumulated states rather than preserving every representation.FLAT (Kao et al., 2023) suggests optimized dataflow, while other research (Wang et al., 2022) overlaps communication with dependent computation to enhance attention execution.In contrast, Keyformer aims to reduce KV cache, speeds up attention by reducing the tokens.\n\nSparse Attention: One line of work sparsifies the attention mechanism to reduce the computational and memory capacity of the attention block.BigBird (Zaheer et al., 2020) combines random, windowed, and global attention to maintain the accuracy for transformers while sparsifying the attention block.LongFormer (Beltagy et al., 2020) also utilizes windowed attention with task-based local attention to achieve sparse attention.Spatten (Wang et al., 2021) introduces sparsity at both the head and token levels.However, it needs a dedicated architecture to exploit sparsity.Furthermore, these works do not address inference optimizations.\n\nKV Cache Reduction: El-Attention (Yan et al., 2021) modifies the multi-head attention module to reduce the KV cache size, leveraging key and value stability during incremental decoding for reuse across layers.In contrast, H 2 O (Zhang et al., 2023) identifies heavy-hitters and keeps them in the KV cache to reduce its size, neglecting the attention score distribution shift that occurs post elimination of previous tokens from the KV cache, leading to accuracy trade-offs.Other approaches (Liu et al., 2023;Anagnostidis et al., 2023) introduce sparsity at both coarse and finegrained levels, targeting the elimination of specific heads and tokens during inference.",
            "score": 0.5544588086646642,
            "section_title": "RELATED WORK",
            "char_start_offset": 31980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 795
                },
                {
                    "start": 797,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1432
                },
                {
                    "start": 1434,
                    "end": 1643
                },
                {
                    "start": 1643,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 148,
                    "matchedPaperCorpusId": "219792793"
                },
                {
                    "start": 431,
                    "end": 459,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 534,
                    "end": 552,
                    "matchedPaperCorpusId": "248227781"
                },
                {
                    "start": 603,
                    "end": 622,
                    "matchedPaperCorpusId": "254927634"
                },
                {
                    "start": 946,
                    "end": 967,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1231,
                    "end": 1250,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 1467,
                    "end": 1485,
                    "matchedPaperCorpusId": "234358001"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4306640625
        },
        {
            "corpus_id": "267770311",
            "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
            "text": "Long-range self-attention. Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling. \n\nMemory-augmented LLMs. Memory-augmented language models have emerged as a promising way to model extended context window sizes (Packer et al., 2023;Dai et al., 2019;Wu et al., 2022;Tworkowski et al., 2023;Weston et al., 2014). In particular, Wu et al. (2022) show that a kNN lookup into a memory cache bank containing (key, value) pairs of past inputs can improve language modeling. Tworkowski et al. (2023) further improved this approach using contrastive learning. In the same vein, Wang et al. (2023) addressed the memory staleness limitation of these works by training a side network model, while keeping the LLM frozen. Unlike these methods, our approach relies on consolidated representations of past tokens which are dynamically updated, enabling the context window to be arbitrarily large, without being limited by the number of memory slots. Moreover, different from these approaches, our method is training-free (memory updates occur solely at runtime), making it easier to integrate our memory module into any existing LLM architecture. (Ge et al., 2023;Mu et al., 2023;Chevalier et al., 2023) have been recently explored for extending the context length in transformer models.",
            "score": 0.5544207504741259,
            "section_title": "Related Work",
            "char_start_offset": 5301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 325,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "270379556",
            "title": "An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding",
            "text": "Efficient Transformers and Extra Memory FoT [Tworkowski et al., 2024] addresses the limitations of local attention in transformers by integrating memory attention layers, which enable large models to learn from a wide context while reducing interference. Infini-attention [Munkhdalai et al., 2024] incorporates compressed memory into the standard attention mechanism and integrates masked local attention and long-term linear attention mechanisms within a single Transformer block. \n\nLLoCO [Tan et al., 2024] employs LoRA in conjunction with context compression, retrieval, and parameter-efficient fine-tuning to learn context offline. Although these methods can successfully extend the long context window of LLMs, they either require modifications to the attention mechanism or the addition of extra modules for assistance. In contrast, CREAM does not require these operations and can be directly applied to a pre-trained model. \n\nPositional Interpolation Chen et al. [2023] first proposed extending the context window through positional interpolation, which linearly reduces the input position indices to match the original context window size, thereby preventing catastrophic high attention scores from completely disrupting the self-attention mechanism. Subsequently, various methods (such as NTK [Peng and Quesnelle, 2023], ABF [Xiong et al., 2023], and EABF [Zhang et al., 2024]) emerged that modify the base frequency of rotary positional encoding to achieve positional interpolation. YaRN [Peng et al., 2023] introduced a segmented interpolation method, applying different positional interpolations to different dimensions. LongRoPE [Ding et al., 2024] identifies and utilizes two forms of non-uniformity in positional interpolation through search, and introduces a progressive expansion strategy for positiona interpolation. Moreover, CREAM can be combined with any positional interpolation method. \n\nPositional Encoding RandPos [Ruoss et al., 2023] first modified position indices so that the model leverages the relativity of positions, enabling it to extend to the target length with fine-tuning over shorter lengths. PoSE [Zhu et al., 2023] then emphasized the importance of continuous segments, dividing the training length into two parts to further enhance the interpolation effect.",
            "score": 0.5544116293878432,
            "section_title": "Related Works",
            "char_start_offset": 20756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 69,
                    "matchedPaperCorpusId": "259360592"
                },
                {
                    "start": 1939,
                    "end": 1959,
                    "matchedPaperCorpusId": "258947457"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1385498046875
        },
        {
            "corpus_id": "277634298",
            "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers",
            "text": "The quadratic complexity of standard self-attention has motivated numerous efforts to reduce computational costs when handling long sequences. One prominent line of re- search involves linear attention methods that approximate or replace the softmax operation with kernel-based transformations [3,11]. Although these methods achieve O(N) complexity, the departure from the exact softmax structure can lead to training instabilities and performance drops, limiting their effectiveness in practice [23]. \n\nIn parallel, a growing body of work explores fundamentally different architectures that sidestep attention altogether. State-space models such as S4 [10] or Mamba [8,9] adopt distinct sequence-processing mechanisms, potentially improving scalability at the cost of diverging significantly from the original Transformer design. Their reliance on alternative parameterizations and radically different update rules often entails increased complexity in other dimensions and may not serve as direct drop-in replacements for attention [20]. \n\nBeyond kernel-based and state-space approaches, there exist architectures that simplify or remove attention-like operations entirely. For instance, PoolFormer [21] and MLPlike models [18] reduce complexity by aggregating information through fixed or simple mixing operations, rather than relying on pairwise token interactions. While these strategies can be computationally appealing, they often sacrifice the flexible, data-dependent modeling of long-range relationships that attention provides [1,22]. In summary, existing solutions present a clear trade-off: \u2022 Linear attention variants achieve O(N ) scaling but often struggle with softmax fidelity and training stability. \u2022 State-space and other non-attention models offer alternative inductive biases but diverge from the core Transformer framework, potentially increasing complexity elsewhere. \u2022 Pooling-based or token-mixing architectures reduce complexity at the cost of losing attention's nuanced representational capabilities. This landscape underscores the challenge of achieving scalability without relinquishing the essential properties that made attention mechanisms so powerful-particularly their ability to capture complex, context-dependent relationships between tokens. In the following sections, we build upon these insights to guide the development of a novel approach that retains the conceptual core of attention while improving efficiency and stability.",
            "score": 0.5539838329909107,
            "section_title": "Related Work",
            "char_start_offset": 2708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2469
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "267523164"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 670,
                    "end": 672,
                    "matchedPaperCorpusId": "270199762"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "244478080"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "233714958"
                },
                {
                    "start": 1541,
                    "end": 1544,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1033935546875
        },
        {
            "corpus_id": "258740790",
            "title": "CageViT: Convolutional Activation Guided Efficient Vision Transformer",
            "text": "Transformer [37] and its variations have achieved great success in both the visual and linguistic areas. The Transformer, for instance, is the backbone architecture of several advanced pre-trained language models in natural language processing, including BERT [9] and GPT [25]. In vision-related tasks, such as classification [10], object detection [3], semantic segmentation [50], pose estimation [17] and video captioning [52], Transformer also shows significant potential. Core to a Transformer model is the self-attention mechanism, which allows the Transformer to represent the contexts within an input sequence [21]. Due to the fact that self-attention computes the dot-product between each pair of input representations, its complexity is quadratic to the length of the input sequence [37]. Therefore, it is challenging for standard Transformer models to effectively process lengthy input sequences [35]. In computer vision, however, many tasks [10,42,41] demand high-resolution images that are transformed into lengthy series of image patches ahead of being processed using the Transformer model. Consequently, it is essential to construct an effective attention mechanism capable of modeling lengthy sequences. The overview of activation guided attention in our model. After partitioning the input image into patches of fixed size, the heat map produced by Grad-CAM++ via the auxiliary convolutional model M conv is utilized to choose the major tokens and the minor tokens. Then, for each patch, we perform a linear embedding. The minor tokens (grey tokens) are combined into several representation tokens (N f < N m ) via a fusion network in order to decrease computation while preserving background knowledge. After incorporating positional embedding and fusion embedding into tokens, the output is fed into an efficient transformer encoder. \n\nUtilizing more efficient attention processes, many approaches have been proposed to accelerate the vanilla Transformer [8,15,22]. In one direction, the approaches lower the length of input tokens, such as [39,32] by leveraging low-rank projections. These methods employ complex projection techniques to reduce computing effort while retaining feature representation.",
            "score": 0.5539778667341507,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "229924195"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "229297707"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "4564155"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "232417787"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "235652212"
                },
                {
                    "start": 1980,
                    "end": 1983,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 2064,
                    "end": 2067,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "248427085",
            "title": "Attention mechanism in neural networks: where it comes and where it goes",
            "text": "All these aforementioned studies undoubtedly demonstrate significant success. But success not make one great. The Transformer also brings a very high computational complexity and memory cost. The necessity of storing attention matrix to compute the gradients with respect to queries, keys and values causes a non-negligible quadratic computation and memory requirements. Training the Transformer is a slow process for very long sequences because of its quadratic complexity. There is also time complexity which is quadratic with respect to the sequence length. In order to improve the Transformer in this respect, recent studies have been conducted to improve this issue. One of them is Linear Transformer which expresses the self-attention as a linear dot-product of kernel feature maps [170]. Linear Transformer reduces both memory and time complexity by changing the self-attention from the softmax function in Eq. (8) to a feature map-based dot-product attention. Its performance is competitive with the vanilla Transformer architecture on image generation and automatic speech recognition tasks while being faster during inference. On the other side, FMMformers which use the idea of the fast multipole method (FMM) [171] outperform the linear Transformer by decomposing the attention matrix into near-field and far-field attention with linear time and memory complexity [172].\n\nAnother suggestion made in response to the Transformer's quadratic nature is The Reformer that replaces dot-product attention by one that uses locality-sensitive hashing [173]. It reduces the complexity but one limitation of the Reformer is its requirement for the queries and keys to be identical. Set Transformer aims to reduce computation time of self-attention from quadratic to linear by using an attention mechanism based on sparse Gaussian process literature [174]. Routing Transformer aims to reduce the overall complexity of attention by learning dynamic sparse attention patterns by using routing attention with clustering [175]. It applies k-means clustering to model sparse attention matrices. At first, queries and keys are assigned to clusters. The attention scheme is determined by considering only queries and keys from the same cluster. Thus, queries are routed to keys belonging to the same cluster [175].\n\nSparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176",
            "score": 0.5539491509545894,
            "section_title": "What about complexity?",
            "char_start_offset": 32405,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1221,
                    "end": 1226,
                    "matchedPaperCorpusId": "16415369"
                },
                {
                    "start": 1376,
                    "end": 1381,
                    "matchedPaperCorpusId": "236924765"
                },
                {
                    "start": 2017,
                    "end": 2022,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "273345829",
            "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes",
            "text": "Therefore, we provide the optimal bounds and algorithms for backward passes for all cache sizes. This fully characterizes the I/O complexity of attention forward/backward when combined with existing results on forward passes [SY24]. Notably, we confirm that FlashAttention is optimal for both the forward and backward passes when the cache size is large enough M = \u2126(d 2 ). \n\nMoreover, in recent years, sparse attention has become another mainstream method for speeding up the training process of transformer-based models [CGRS19, Table 1: Summary of our contributions. We categorize the cache size M into two cases: (1) Large cache M = \u2126(d 2 ); (2) Small cache M = o(d 2 ). Assume n \u2265 d. We list our contributions for general and sparse attention below. Z input and Z QK denote the number of nonzero entries of the input matrix and the key-query matrix, respectively.",
            "score": 0.5537063150780246,
            "section_title": "Cache Size",
            "char_start_offset": 5580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 868
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 231,
                    "matchedPaperCorpusId": "272330254"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345947265625
        },
        {
            "corpus_id": "258887482",
            "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
            "text": "During training, our method has only a negligible overhead due to the computation of GroupedSoftmax. In particular, our method does not require maintaining a cache of previous values at training time. Furthermore, we decouple the training context length from the inference context length since it is possible to perform inference at any context length using the method described in Section 3.2 regardless of the train context length. As such, when comparing training time in terms of inference context length, we offer constant training time (O(1)) whereas training time for a standard transformer scales quadratically with the operational (inference) context length. \n\nFurthermore, in comparison with standard attention over the whole input, our method reduces inference time by computing the attention score over a smaller set of token pairs. For instance, during auto-regressive generation, where tokens are generated sequentially by using the previously generated token as input for obtaining the next one, the traditional approach involves computing attention across all preceding tokens when generating the next token. In contrast, our method allows computing attention solely over landmark tokens, followed by attention computation over the retrieved blocks. Given the constant size of the blocks, the cost of computing the attention over the retrieved blocks remains constant regardless of the total context length. While the cost of finding the most relevant landmark tokens increases linearly with the context length, the rate of increase is 1 every \u2113 block + 1 tokens. This immediately reduces the number of operations by a factor of block length \u2113 block . For example, when using \u2113 block = 50 as in our experiments, this can lead to a 50x boost. Importantly, the same reduction can be obtained in terms of memory. In the standard Transformer, a cache of all previous key and values (KV-cache) is needed to perform the generation efficiently. In contrast, we only need immediate access to the landmark tokens and can offload the blocks to slow memory (e.g. CPU), loading only the retrieved blocks. The reduction in memory and compute can be further improved if the search for retrieving blocks is performed using more advanced data structures such as FAISS [15].",
            "score": 0.5532911250675611,
            "section_title": "Memory & Computation",
            "char_start_offset": 23314,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.131103515625
        },
        {
            "corpus_id": "214612335",
            "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection",
            "text": "Many recent researches have focused on modifying the structure of self-attention to relieve the computation and memory burden. Transformer-XL (Dai et al., 2019) enables learning long-term dependency by introducing a segment-level recurrence mechanism and a novel relative position encoding scheme. (Guo et al., 2019) proposed Star Transformer, a variant of Transformer which replaces the fully-connected structure in self-attention with a star-shaped topology, where all tokens are connected with a relay node. (Child et al., 2019;Kitaev et al., 2020) suggest sparsifying Transformer by focusing only on a fraction of attention connections. (Child et al., 2019) introduced sparse factorizations of the attention matrix, which scale as O(n p \u221a n) with the sequence length, and a set of sparse attention kernels which efficiently compute subsets of the attention matrix. (Kitaev et al., 2020) proposed Reformer, a more efficient yet complicated method for computing self-attention, where the dot-product attention is replaced by one that uses a locality-sensitive hashing, reducing the complexity from O(n 2 ) to O(n log n). (Ye et al., 2019) partitioned the sequence to different multi-scale spans and attended the context information from fine-grain to coarse-grain as the relative distance increases. All above works rely on manually designed structures. Our work is inspired by (Sukhbaatar et al., 2019), which takes a step forward and uses a novel adaptive self-attention mechanism that can learn its optimal attention span for each head.",
            "score": 0.5532804180994595,
            "section_title": "Modifying Self-Attention in Transformer",
            "char_start_offset": 2539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1541
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 160,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 298,
                    "end": 316,
                    "matchedPaperCorpusId": "209009596"
                },
                {
                    "start": 531,
                    "end": 551,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 869,
                    "end": 890,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1380,
                    "end": 1405,
                    "matchedPaperCorpusId": "159041867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44091796875
        },
        {
            "corpus_id": "248780136",
            "title": "Long-range Sequence Modeling with Predictable Sparse Attention",
            "text": "Models based on the Transformer architecture (Vaswani et al., 2017) have been firmly established as state of the art approaches across a range of domains like language (Brown et al., 2020;Clark et al., 2020;Devlin et al., 2018), and vision (Carion et al., 2020;Dosovitskiy et al., 2020). The Transformer architecture perceiving long-range context heavily relies on the multi-head self-attention mechanism, in which the relevance of every token pairs is computed to decide the attention scores and token's representations are the weighted average of all tokens using the attention scores. \n\nDespite its effectiveness, self-attention mechanism's quadratic time and memory complexity about the sequence length is an obstacle to extend Transformer for very long sequences, such as document-level text tasks, high-resolution images, videos, etc. Shen et al. (2021Shen et al. ( , 2018) ) elaborate the issue of high computational complexity. For instance, more than 68GB GPU memory and 1.6T multiply-accumulation operations are required for a 64 \u00d7 64 \u00d7 32 3D feature volume. \n\nGreat efforts have been made to develop Transformer's variants for long-range sequence modeling tasks. Tay et al. (2020c) categorize the researches of efficient Transformers: (a) Fixed patterns or combination of patterns (Beltagy et al., 2020;Zaheer et al., 2020), in which the field to be attended is pre-defined by fixed pattern. (b) Learnable patterns (Kitaev et al., 2020;Tay et al., 2020a), in which tokens are sorted or clustered in a data-driven fashion. (c) Memory (Ma et al., 2021;Lee et al., 2019), in which spacial tokens with global view are introduced to compress the input sequence. (d) Low-rank methods (Tay et al., 2021;Wang et al., 2020), which adopt low-rank approximations of the self-attention matrix. (e) Kernels (Katharopoulos et al., 2020;Choromanski et al., 2020a,b), which view the attention mechanism through kernelization.",
            "score": 0.5529137207473428,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 67,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 240,
                    "end": 261,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 841,
                    "end": 858,
                    "matchedPaperCorpusId": "215999966"
                },
                {
                    "start": 1314,
                    "end": 1334,
                    "matchedPaperCorpusId": "28972310"
                },
                {
                    "start": 1447,
                    "end": 1465,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1689,
                    "end": 1707,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36572265625
        },
        {
            "corpus_id": "248498407",
            "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation",
            "text": "Transformer models [63] have been adapted in a wide variety of applications, including natural language processing [7,26,50], image processing [10,47], and speech recognition [42]. Training large Transformers requires extensive computational and memory resources, especially when modeling long sequences, mainly due to the quadratic complexity (w.r.t. sequence length) in attention layers. Recent advances in efficient transformers [17,22,35,36,65] leverage attention approximation to overcome the bottleneck by approximating the attention matrices. However, it is challenging to find a robust approximation method that balances the efficiency-accuracy trade-off on a wide variety of tasks [57,58]. \n\nWe categorize most of the existing approaches for efficient attention matrix computation into two major groups: exploiting either the sparsity, e.g., Reformer [36], SMYRF [22], or low-rank properties of the attention matrices, e.g., Linformer [65], Linear Transformer [35], and Performer [17]. However, these techniques usually have different strengths and focus on the performance of specific tasks, so their approximations still cause accuracy degradation on many other tasks. For instance, according to a recent benchmark paper [57] and our experiments, low-rank-based attention might be less effective on hierarchically structured data or language modeling tasks, while sparse-based variants do not perform well on classification tasks. We observe that sparse and low-rank approximations are complementary for many attention matrices in practice, and sparse+low-rank could outperform each individually (Figure 1 left). We empirically categorize the regimes in which sparse or low-rank approximation achieves better error based on the softmax temperature of attention (of which the entropy of softmax distribution can be used as a proxy). We expect that sparse methods perform well if the attention depends on a few entries (low entropy softmax). In contrast, low-rank methods do better if the attention depends on a mixture of many components (high entropy softmax). This explains the phenomenon that current sparse and low-rank-based approaches excel on different kinds of tasks. A natural question is whether one could understand and unify the strength of both approaches.",
            "score": 0.5519565475007308,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "218763647"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436767578125
        },
        {
            "corpus_id": "276107726",
            "title": "Hamming Attention Distillation: Binarizing Keys and Queries for Efficient Long-Context Transformers",
            "text": "Pre-trained transformer models with extended context windows are notoriously expensive to run at scale, often limiting real-world deployment due to their high computational and memory requirements. In this paper, we introduce Hamming Attention Distillation (HAD), a novel framework that binarizes keys and queries in the attention mechanism to achieve significant efficiency gains. By converting keys and queries into {-1, +1} vectors and replacing dot-product operations with efficient Hamming distance computations, our method drastically reduces computational overhead. Additionally, we incorporate attention matrix sparsification to prune low-impact activations, which further reduces the cost of processing long-context sequences. \\par Despite these aggressive compression strategies, our distilled approach preserves a high degree of representational power, leading to substantially improved accuracy compared to prior transformer binarization methods. We evaluate HAD on a range of tasks and models, including the GLUE benchmark, ImageNet, and QuALITY, demonstrating state-of-the-art performance among binarized Transformers while drastically reducing the computational costs of long-context inference. \\par We implement HAD in custom hardware simulations, demonstrating superior performance characteristics compared to a custom hardware implementation of standard attention. HAD achieves just $\\mathbf{1.78}\\%$ performance losses on GLUE compared to $9.08\\%$ in state-of-the-art binarization work, and $\\mathbf{2.5}\\%$ performance losses on ImageNet compared to $12.14\\%$, all while targeting custom hardware with a $\\mathbf{79}\\%$ area reduction and $\\mathbf{87}\\%$ power reduction compared to its standard attention counterpart.",
            "score": 0.5518251427078557,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33154296875
        },
        {
            "corpus_id": "259858862",
            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
            "text": "Transformer-based models (Vaswani et al., 2017) have achieved state-of-the-art performance on a wide variety of natural language processing tasks (Devlin et al., 2019;Liu et al., 2019;Yang et al., 2019). It is also gradually applied to other research fields such as speech and computer vision (Dong et al., 2018;Li et al., 2019;Zhang et al., 2020;Dosovitskiy et al., 2021;Zhu et al., 2021;Touvron et al., 2021). Although self-attention module, the core component in Transformer, can capture global contexts from the whole sequence, the time and memory complexity are both quadratic to the sequence length. Especially when facing longer sequences, Transformer becomes more difficult to process them efficiently and effectively. \n\nRecently, a wide spectrum of efficient Transformers (Child et al., 2019;Ho et al., 2019;Rae et al., 2020;Zhao et al., 2019;Kitaev et al., 2020;Tay et al., 2020;Beltagy et al., 2020;Choromanski et al., 2020;Wang et al., 2020;Zaheer et al., 2020;Roy et al., 2021;Xiong et al., 2021;Tay et al., 2021a;Ma et al., 2021;Chen, 2021;Zhu and Soricut, 2021;Liu et al., 2022) have been proposed to tackle the problem of efficiency, which can be roughly divided into the following directions: sparse attention, low-rank and kernel methods. Because sparse-based attention is intuitive and interpretable in addition to efficiency, we focus on this method in this paper. It usually utilizes some strategies or patterns to limit the number of tokens involved in the attention calculation. Different from traditional sparse Transformer (Martins and Astudillo, 2016;Correia et al., 2019;Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity.",
            "score": 0.5514348701270131,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 146,
                    "end": 167,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 184,
                    "end": 202,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 312,
                    "end": 328,
                    "matchedPaperCorpusId": "59413863"
                },
                {
                    "start": 347,
                    "end": 372,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 389,
                    "end": 410,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 817,
                    "end": 834,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 852,
                    "end": 872,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 872,
                    "end": 889,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 973,
                    "end": 990,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 990,
                    "end": 1009,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1009,
                    "end": 1027,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1027,
                    "end": 1043,
                    "matchedPaperCorpusId": "235313355"
                },
                {
                    "start": 1043,
                    "end": 1054,
                    "matchedPaperCorpusId": "237421288"
                },
                {
                    "start": 1054,
                    "end": 1076,
                    "matchedPaperCorpusId": "236428421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385498046875
        },
        {
            "corpus_id": "268680782",
            "title": "Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement",
            "text": "Despite the promising performance of the transformer, its high calculation complexity and memory cost hinder further applications.As a representative of attention lightweightness, deformable attention [37] attends sparse spatial samplings to reduce the computational and memory complexity.Efficient-DETR [31] optimizes the structure to reduce encoder and decoder layers while maintaining comparable performance.Recent works focus on reducing the number of queries participating in self-attention in the encoder.In particular, Lite DETR [15] prioritizes high-level feature updates to reduce the number of queries.PnP-DETR [29] compresses entire feature maps by abstracting them into a fine-grained vector and a coarse background vector.Sparse DETR [26] refines only the top-\u03c1% tokens for all encoder layers based on DAM results.Focus DETR [35] further introduces the foreground token selector integrated with a  cascade set to allocate attention to more informative tokens.However, most of the above methods directly integrate sparsity designs for all tokens while neglecting their multi-scale characteristics.",
            "score": 0.5511438393612716,
            "section_title": "Lightweight Detection Transformer",
            "char_start_offset": 4985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1109
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "222208633"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "257495882"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "237513604"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "244714783"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "260125992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14892578125
        },
        {
            "corpus_id": "258887482",
            "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
            "text": "While Transformers have shown remarkable success in natural language processing, their attention mechanism's large memory requirements have limited their ability to handle longer contexts. Prior approaches, such as recurrent memory or retrieval-based augmentation, have either compromised the random-access flexibility of attention (i.e., the capability to select any token in the entire context) or relied on separate mechanisms for relevant context retrieval, which may not be compatible with the model's attention. In this paper, we present a novel approach that allows access to the complete context while retaining random-access flexibility, closely resembling running attention on the entire context. Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism. Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths. We demonstrate that our method can obtain comparable performance with Transformer-XL while significantly reducing the number of retrieved tokens in each step. Finally, we show that fine-tuning LLaMA 7B with our method successfully extends its context length capacity to over 32k tokens, allowing for inference at the context lengths of GPT-4. We release the implementation of landmark attention and the code to reproduce our experiments at https://github.com/epfml/landmark-attention/.",
            "score": 0.5511002581439413,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "277150882",
            "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism",
            "text": "Transformer-based models [34] have shown tremendous success in showcasing exceptional performance across wide range of AI applications and have emerged as the architecture of choice in applications such as natural language processing and image classification. Long-context Transformers are essential for tackling a diverse array of AI challenges, including processing books and highresolution images, analyzing long videos and complex codebases, and developing multi-agent AI solutions. The use of vastly enlarged context has become a growing trend. For example, GPT-4o [27] has a context length of 128,000 tokens, while Anthropic's Claude 3 model [2] features a context length of 200,000 tokens. \n\nSelf-attention operation is a key building block of Transformer models. However, self-attention is slow and memory-intensive, especially for long contexts. This has motivated a surge of research on cutting down the memory and computation needs of Transformer models. Memory-efficient Transformers [9,8] have emerged that reduce the memory cost from quadratic to linear in terms of the context length, while maintaining accuracy. However, little progress has been made in improving the communication cost. Despite numerous efforts [32,17,19,20,18,13,12], the communication cost remains linear with respect to context length, regardless of the number of processing units used. Specifically, for a Transformer model with L layers, M self-attention heads, and a head dimension of H, these prior methods incur a training cost of O(BLNMH ) per training step for a batch size of B and a context length of N . The communication cost of these methods is independent of the number Table 1: Communication costs of various parallelization strategies. Here, B is batch size, N is context length, H is head dimension, M is number of heads, and P is number of processors.",
            "score": 0.5509883755553402,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 999,
                    "end": 1001,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 1236,
                    "end": 1239,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 1242,
                    "end": 1245,
                    "matchedPaperCorpusId": "270973564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.240478515625
        },
        {
            "corpus_id": "259833842",
            "title": "Can we Pretrain a SotA Legal Language Model on a Budget From Scratch?",
            "text": "In the past few years, a vast amount of research has been devoted to addressing the problem of quadratic time and memory complexity associated with the dense attention mechanism (Vaswani et al., 2017), practically limiting the maximum sequence length severely (often to 512 tokens) (Tay et al., 2020b;Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2021;Roy et al., 2021;Kitaev et al., 2020;Tay et al., 2021;Lee-Thorp et al., 2021). These research works have given rise to a new class of transformers, referred to as sparse transformers or efficient transformers (Tay et al., 2020b). Reducing the cost associated with the computation of the dense attention matrix while maintaining the same performance is the core idea behind efficient transformers. This is often achieved by introducing sparsity in the attention matrix in a variety of ways that may be fixed pattern such as local (windowed) attention (Child et al., 2019;Beltagy et al., 2020), global attention (Zaheer et al., 2021) or learnable patterns such as routing attention (Roy et al., 2021) and LSH attention (Kitaev et al., 2020) or a random pattern (Zaheer et al., 2021;Tay et al., 2021). Recently, Lee-Thorp et al. (2021) proposed to use Fourier transforms instead of the attention layer. Tay et al. (2020b) provide a comprehensive list of efficient transformers and the detailed description of their attention mechanism. (Tay et al., 2020a) proposed a series of tasks designed for testing the capabilities of these different models suitable for longer inputs. However, this so-called \"Long Range Arena\" considers mostly artificial tasks, with the goal of evaluating the models independent of any pretraining.",
            "score": 0.5508253818507769,
            "section_title": "Long Document Processing",
            "char_start_offset": 6825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 378,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 378,
                    "end": 398,
                    "matchedPaperCorpusId": "203610181"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1078,
                    "end": 1099,
                    "matchedPaperCorpusId": "203610181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3564453125
        },
        {
            "corpus_id": "256697616",
            "title": "Efficient Attention via Control Variates",
            "text": "For the simulation experiment conducted in \u00a75.3, we adopt the same transformer architecture across all attention variants. In particular, it uses 8 transformer layers, 192 embedding dimensions, and 2 attention heads so that longer sequences can fit into our devices. The batch size is set to 64 across  8 V100 GPUs, and the statistics are computed by averaging the results of 30 runs. Besides, in our ablation study, the efficiency metrics reported in Table 6 and Table 7 are evaluated under the same setup used during training. \n\nRemark on Modeling Short Sequences. Unfortunately, similar to most previous efficient attention baselines, EVA also runs slower than softmax attention under shorter sequences (e.g., length of 128 or 256), but it soon catches up in running speed, and the reduction of memory consumption is still significant. Besides, in short-sequence settings (such as the case of DeiT-Tiny/Small with sequences of 196 tokens), EVA often performs on par with or better than conventional softmax attention (see Table 1), whereas most previous attention variants usually perform much worse. This implies EVA can achieve a better trade-off between efficiency and quality: for short sequences, EVA is possible to achieve stronger performance competitive with softmax attention (despite in longer running time); while for long sequences, EVA can be run much faster with less memory. \n\nComparison to Memory-efficient Attention Mechanisms. In this section, we conduct an empirical efficiency comparison between efficient approximate attention methods and FlashAttention, one of the memory-efficient attention mechanisms (Rabe & Staats, 2021;Dao et al., 2022) with optimized memory accesses. FlashAttention computes the exact softmax attention in an online manner without materializing the full attention matrix, achieving linear memory complexity with respect to sequence lengths; besides, both runtime and memory usage are further improved by minimizing IO accesses. \n\nWe benchmark different attention modules on one NVIDIA GeForce RTX 3090 GPU, where we measure the memory usage and runtime of running a single attention block, consisting of 8 attention heads with 512 embedding dimension size, for both a forward and backward pass.",
            "score": 0.5507874738075724,
            "section_title": "E.4 EXPERIMENTAL SETTINGS OF EFFICIENCY COMPARISON",
            "char_start_offset": 44307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "259584220",
            "title": "Analysis of different transformer variants",
            "text": "Each Transformer variant has proposed different improvement methods for the purpose of improving Transformer efficiency. Longformer improves Transformer's traditional self-attention mechanism. Specifically, each token only gives local attention to tokens near a fixed window size. Moreover, Longformer adds a kind of global attention based on the former local attention for specific tasks. Transformer-XL proposes a segment-level recurrence mechanism and a Relative positional encoding mechanism. BigBird uses Random Attention, Window Attention, and Global Attention to get a Sparse attention mechanism. Star-transformer simplifies the architecture by moving a fully connected topology into a star structure. Through these changes, four transformer variants all improves efficiency. Longformer achieves SOTA on two character-level language modeling tasks. In addition, when using Longformer's attention method to continue pretraining RoBERTa, the performance of the trained language model is better than RoBERTa in multiple long document tasks. Transformer-XL improves the ability to capture long-term dependence, solves the context segmentation problem, and improves the prediction speed and accuracy of the model. BigBird can accommodate longer sequences with 16 GB of RAM (8 times the size of the Transformer, or 4096) and get better performance. It has achieved the SOTA effect on many tasks with long text sequences, such as long text summaries, long text questions, and answers. Star-transformer can reduce the parameters significantly with efficiency, and also reduce the complexity. At the same time, it performs better than the traditional transformer on medium-size datasets.",
            "score": 0.5505041244279278,
            "section_title": "Discussion",
            "char_start_offset": 19231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1685
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49951171875
        },
        {
            "corpus_id": "264146030",
            "title": "OAAFormer: Robust and Efficient Point Cloud Registration Through Overlapping-Aware Attention in Transformer",
            "text": "In the standard Transformer model [48], the memory cost exhibits a quadratic increase due to matrix multiplication, which has become a bottleneck when handling long sequences. Recently, several efficient Transformer variants have been introduced [25,40,53,65,56]. For example, the Linear Transformer [25] reformulates self-attention as a linear dot product of kernel feature maps and exploits the associativity property of matrix products to reduce computational complexity. BigBird [65] combines local and global attention mechanisms at specific positions and introduces random attention for selected token pairs. FastFormer [56] employs an additive attention mechanism to model global contexts, achieving effective context modeling with linear complexity. Inspired by these advancements, we propose a region-wise attention module with linear complexity during the fine-level matching phase, meticulously designed to enhance the discriminative capabilities of the extracted features for points within overlapping areas.",
            "score": 0.549252893521681,
            "section_title": "Efficient Transformer",
            "char_start_offset": 8640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "215999966"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "270440710",
            "title": "DiTFastAttn: Attention Compression for Diffusion Transformer Models",
            "text": "Various studies have delved into the utilization of local attention patterns, where each token attends to a set of neighboring tokens within a fixed window size, aiming to mitigate the computational burden associated with processing long sequences. The concept of local windowed attention was initially introduced by Beltagy et al. in Longformer, presenting an attention mechanism that scales linearly with sequence length. Bigbird (Zaheer et al., 2020) extends this idea by incorporating window attention, random attention, and global attention mechanisms, enabling the retention of long-range dependencies while mitigating computational costs. In the realm of computer vision, Swin Transformer (Liu et al., 2021) adopts a similar approach by confining attention computation to nonoverlapping local windows, utilizing shifted windows across different layers to capture global context efficiently. Twins Transformer (Chu et al., 2021), FasterViT(Vasu et al., 2023), and Neighborhood attention transformer (Hassani et al., 2023) employ window-based attention to enhance computational efficiency, leveraging different module designs such as global sub-sampled attention and hierarchical attention to exploit global context effectively. In our work, we employ fixed-sized window attention to accelerate pretrained Diffusion Transformer models and introduce a novel technique named Window Attention with Residual Sharing to preserve long-range dependencies for image tokens.",
            "score": 0.5491073137501308,
            "section_title": "Local Attention",
            "char_start_offset": 5813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1470
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 453,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 696,
                    "end": 714,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 916,
                    "end": 934,
                    "matchedPaperCorpusId": "234364557"
                },
                {
                    "start": 1005,
                    "end": 1027,
                    "matchedPaperCorpusId": "233296711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309814453125
        },
        {
            "corpus_id": "259858862",
            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
            "text": "Different from traditional sparse Transformer (Martins and Astudillo, 2016;Correia et al., 2019;Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered. \n\nTo address these issues, we propose A 2 -Former with adaptive attention to model longer sequences in this paper. It can select useful tokens automatically in sparse attention by learnable position vectors, which consist of meta position and offset position vectors. Because each element in the learnable offset position vector is not an integer, we utilize linear interpolation to gather discrete vectors from original the input embedding matrix. Position visualization further shows that traditional attention patterns are not enough to cover the valuable positions automatically selected by models. Experiments on Long Range Arena, a systematic and unified benchmark with different tasks, show that our model has achieved further improvement in performance compared with other sparse-based Transformers. \n\nOverall, the main contributions are as follows: \n\n\u2022 We propose a novel efficient Transformer, A 2 -Former, which replaces hand-crafted attention patterns with learnable adaptive attention in sparse attention. Besides, position visualization (Figure 3) further shows that traditional attention patterns are not enough to cover the useful positions automatically selected by models. \n\n\u2022 We adopt an interpolation technique to help the model gather discrete positions with a continuous weight matrix. By combining the meta position and generated offset position, the position of tokens can be selected dynamically according to the context. \n\n\u2022 Experiments on different long sequence tasks validate the effectiveness of our model.",
            "score": 0.5486559572993824,
            "section_title": "Introduction",
            "char_start_offset": 1517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2347
                },
                {
                    "start": 2350,
                    "end": 2437
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 75,
                    "matchedPaperCorpusId": "16432551"
                },
                {
                    "start": 75,
                    "end": 96,
                    "matchedPaperCorpusId": "202538495"
                },
                {
                    "start": 96,
                    "end": 116,
                    "matchedPaperCorpusId": "153313159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "269982604",
            "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
            "text": "It is worth lightly elaborating upon the throughput observations provided in Table 2, as some of them may be surprising to readers.For example, it is initially unclear as to why truly linear attention mechanisms (e.g.Linear Attention from Katharopoulos et al. 2020) are exceedingly fast in comparison to some alternatives, like BigBird (Zaheer et al., 2020), that rely on sparsity-focused approaches.While sparsity-focused approaches can be theoretically fast, inefficiencies in memory accesses can result in difficult to avoid slowdowns, especially on general purpose hardware.Indeed, the original publication of BigBird acknowledges this and recommends a block-based approach to its various components to avoid memory access bottlenecks, which has been implemented for the version of BigBird that we test in this paper and is implemented for every version of BigBird that we are aware of (i.e.present in the original LRA publication).\n\nDespite employing a block-based accessing approach for BigBird, findings across efficient attention mechanism literature continue to find that, generally, this particular efficient attention scheme scales poorly with respect to throughput for extremely long sequences (Tay et al., 2021;Chen et al., 2021;Qin et al., 2022b), although this depends in part on hyperparameters that are employed to determine how such sparsity-focused methods behaved (i.e.how often they actually compute information).Other efficient attention mechanisms, like Reformer (Kitaev et al., 2020) and Linformer (Wang et al., 2020b), can also be considered somewhat tuning-heavy to find a desirable performance-to-throughput trade-off versus truly linear attention mechanisms.Given that we do not explore tweaking hyperparameters in this paper to boost the performance of any architecture, we consider it out of the scope of this work to search for anything beyond the hyperparameters employed by Skyformer (Chen et al., 2021), whose PyTorch implementation of the LRA is what our implementation is based upon.A.9.1.",
            "score": 0.5486027307274369,
            "section_title": "A.9. Elaboration on Throughput Observations for Efficient Attention Mechanisms",
            "char_start_offset": 42624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 936
                },
                {
                    "start": 938,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1686
                },
                {
                    "start": 1686,
                    "end": 2019
                },
                {
                    "start": 2019,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 1206,
                    "end": 1224,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1224,
                    "end": 1242,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 1242,
                    "end": 1260,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 1917,
                    "end": 1936,
                    "matchedPaperCorpusId": "240354799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427490234375
        },
        {
            "corpus_id": "272310408",
            "title": "HLogformer: A Hierarchical Transformer for Representing Log Data",
            "text": "Global Memory Tokens in Transformers. Models like Longformer (Beltagy, Peters, and Cohan 2020), ETC (Extended Transformer Construction) (Ainslie et al. 2020), and Big Bird (Zaheer et al. 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms. Sparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al. 2021) and (Kitaev, Kaiser, and Levskaya 2020), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure. Segment-based Recurrence. Segment-based recurrence methods, such as Transformer-XL (Dai et al. 2019) and Compressive Transformer (Rae et al. 2019), introduce mechanisms to maintain and leverage contextual information across segments, significantly reducing memory and computational costs. \n\nDespite their effectiveness, these approaches are not specifically tailored to the unique characteristics of log data, which often exhibit a hierarchical, dictionary-like structure. This gap underscores the need for models designed to capture and leverage the intrinsic structure of log data.",
            "score": 0.5484572016564052,
            "section_title": "Efficient Transformers",
            "char_start_offset": 6293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1605
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 94,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 136,
                    "end": 157,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 172,
                    "end": 192,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 800,
                    "end": 816,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 822,
                    "end": 857,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1151,
                    "end": 1168,
                    "matchedPaperCorpusId": "5987139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "275458476",
            "title": "ELFATT: Efficient Linear Fast Attention for Vision Transformers",
            "text": "FlashAttention (Dao et al., 2022;Dao, 2024;Shah et al., 2024) and FlashSigmoid (Ramapuram et al., 2024) are representatives of memory-efficient methods. Computationefficient methods focus on minimizing the computation bound of the attention computation by using linear approximations based on different kernel methods (Han et al., 2023;Choromanski et al., 2021;Wortsman et al., 2023;Katharopoulos et al., 2020;Bolya et al., 2023;Lu et al., 2021;Peng et al., 2021;Qin et al., 2022;Shen et al., 2021;Han et al., 2024), low-rank decomposition (Han et al., 2022;Xiong et al., 2021;Lu et al., 2021;Wu et al., 2024), and sparse computation (Liu et al., 2021;Dong et al., 2022;Wang et al., 2020b;Zhao et al., 2019;Beltagy et al., 2020;Child et al., 2019;Tay et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020). However, memory-efficient methods still have quadratic computational complexity, and computationefficient methods usually have lower performance compared to VaniATT. \n\nIn this paper, we propose a novel efficient linear fast attention (ELFATT) mechanism that has low memory I/O operations and linear computational complexity and maintains noninferior performance compared to VaniATT. The core idea of ELFATT is the combination of sparse computation with a linear approximation. Each ELFATT block has two parallel attention heads. One head is used to compute sparse blockify attention to introduce inductive biases, and the other one head is used to compute global linear atten-1 arXiv:2501.06098v2 [eess.IV] 29 Jan 2025 tion to capture long-range dependencies.",
            "score": 0.548310129265951,
            "section_title": "Introduction",
            "char_start_offset": 1096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 43,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "260351423"
                },
                {
                    "start": 336,
                    "end": 361,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 383,
                    "end": 410,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 429,
                    "end": 445,
                    "matchedPaperCorpusId": "239616022"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 463,
                    "end": 480,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 480,
                    "end": 498,
                    "matchedPaperCorpusId": "215999966"
                },
                {
                    "start": 540,
                    "end": 558,
                    "matchedPaperCorpusId": "250301956"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 577,
                    "end": 593,
                    "matchedPaperCorpusId": "239616022"
                },
                {
                    "start": 634,
                    "end": 652,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 652,
                    "end": 670,
                    "matchedPaperCorpusId": "235694312"
                },
                {
                    "start": 747,
                    "end": 764,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 764,
                    "end": 784,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 784,
                    "end": 804,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.411865234375
        },
        {
            "corpus_id": "232318583",
            "title": "Linear-Time Self Attention with Codeword Histogram for Efficient Recommendation",
            "text": "Considerable efforts have been made trying to scale Transformers to long sequences. Transformer-XL in [9] captures longer-term dependency by employing a segment-level recurrent mechanism, which splits the inputs into segments to perform attention. Sukhbaatar et al. [30] limited the self-attention context to the closest samples. However, these techniques do not improve the  ( 2 ) asymptotic complexity of self-attention. \n\nIn another line of work, attempts in reducing the asymptotic complexity are made. Child et al. [6] proposed to factorize the attention computation into local and strided ones. Tay et al. [33], on the other hand, improved local attention by introducing a differentiable sorting network to re-sort the buckets. Reformer [19] hashes the query-keys into buckets via hashing functions based on random projection, and attention is computed within each bucket. In a similar manner, Roy et al. [28] assign tokens to buckets through clustering. Built on top of ETC [1], Big Bird [44] considers a mixture of various sparse patterns, including sliding window attention and random attention. Clustered Attention, introduced in [36], however, groups queries into clusters and perform attention on centroids. Linformer [37] resorts to a low-rank projection on the length dimension. However, it can only operate in a bidirectional mode without casual masking. \n\nMost of the aforementioned approaches rely on sparse attention patterns, while our method performs full contextual attention over the whole sequence. Besides, Linformer and Sinkhorn Transformer assume a fixed sequence length due to the use of sorting network and projection, while our method poses no such constraint. Our method is also notably faster than the existing approaches, enjoying an asymptotic complexity of  (), while inner product can be stored in a table.",
            "score": 0.5482095247498137,
            "section_title": "Improving Efficiency of Attention",
            "char_start_offset": 7237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2294921875
        },
        {
            "corpus_id": "276421479",
            "title": "Linear Diffusion Networks",
            "text": "Transformers Vaswani et al. [2017] revolutionized sequential modeling by leveraging self-attention to capture global dependencies. However, the quadratic complexity of full self-attention has spurred the development of more efficient variants. Linear attention methods Katharopoulos et al. [2020], Choromanski et al. [2021] approximate the attention mechanism to achieve linear complexity, while approaches such as Linformer Wang et al. [2020] and Reformer Kitaev et al. [2020] use low-rank or reversible mechanisms to further reduce computational overhead. Recent methods like Big Bird Zaheer et al. [2020] and Sparse Transformers Child et al. [2019] introduce sparsity to scale to longer sequences, and the Nystr\u00f6mformer Xiong et al. [2021] employs a sampling-based approach for efficiency. Furthermore, state-of-the-art innovations such as the Routing Transformer Roy et al. [2021], FlashAttention Dao et al. [2022], andCosFormer Zhou et al. [2022] continue to push the envelope on both performance and computational speed. Although these methods offer impressive scalability, they often face trade-offs between capturing smooth temporal evolution and modeling abrupt dynamics. Additionally, Fein-Ashley [2025] introduced a fast-fourier transform (FFT) based token mixing method that allows for an efficient and effective alternative to costly self-attention.",
            "score": 0.5481203692220427,
            "section_title": "Transformer Linear Methods and Efficient Attention",
            "char_start_offset": 3210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1362
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 269,
                    "end": 296,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 298,
                    "end": 323,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "129945531"
                },
                {
                    "start": 867,
                    "end": 884,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 901,
                    "end": 923,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "276580352",
            "title": "SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference",
            "text": "SpargeAttn contains a two-stage online filter to implement sparse FlashAttention. First, as shown in Step1 and Step2 in Fig. 3, we design a fast and accurate method to predict the sparse block in the attention map, thereby skipping the corresponding products of Q i K \u22a4 j and P ij V j . Second, as shown in Step3 in Fig. 3, we design a sparse online softmax method to further skip the products of P ij V j . \n\nwhere m ij and l ij are b q \u00d7 1 vectors, which are initialized to \u2212\u221e and 0 respectively. The \u03c3() is an operator similar to softmax.: \n\nImplementing sparse FlashAttention is intuitive. By skipping certain block matrix multiplications of Q i K \u22a4 j and P ij V j , we can accelerate the attention computation. We formulate sparse attention based on FlashAttention in the following definitions. Definition 1 (Block Masks). Let M g and M pv be binary masks of dimensions \u2308N/b q \u2309 \u00d7 \u2308N/b k \u2309, where each value is either 0 or 1. These masks determine which computations are skipped in the sparse attention mechanism. Definition 2 (Sparse FlashAttention). The computation rules for sparse FlashAttention based on the masks are defined as follows: \n\n(3)",
            "score": 0.5477842324101969,
            "section_title": "SpargeAttn",
            "char_start_offset": 8824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4189453125
        },
        {
            "corpus_id": "247058768",
            "title": "FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks",
            "text": "Transformer architecture (Vaswani et al., 2017) originally proposed for machine translation tasks has shown impressive results in a wide range of domains, including natural language processing, image recognition, audio captioning, graph analysis, and bioinformatics (Lin et al., 2021). However, in applications that require processing long sequences, the benefits of transformers are often accompanied by high consumption of computational and memory resources. The main bottleneck is the transformer's core component, the self-attention mechanism. Self-attention computes similarity scores for all pairs of tokens in the input sequence, and therefore, it has a quadratic complexity O(N 2 ) in computations and memory relative to the length of the input sequence N 2 . \n\nRecently, several approaches have been introduced to reduce the computational complexity and memory footprint of self-attention. Some works utilize the sparsity of the attention map (Beltagy et al., 2020), others express self-attention as a linear dot-product of kernel feature maps \u03c6(\u2022) (Katharopoulos et al., 2020), or utilize random feature vectors (Choromanski et al., 2020). Pro- posed approaches reduce the computational complexity to O(N )3 . One of the promising variants of a transformer is the Linear Transformer (Katharopoulos et al., 2020) since, along with linear complexity, it requires constant O(1) memory in auto-regressive language modeling. Experiments with the long sequence benchmark Long Range Arena (LRA) (Tay et al., 2020) 4 have indeed shown that the Linear Transformer is 5x times faster than the vanilla Transformer in training speed. However, the drawback of this architecture is lower performance compared to the original Transformer. \n\nOne way to reduce the performance gap between the Linear Transformer and the original one is to select a more suitable kernel function \u03c6(\u2022) in linear attention (Choromanski et al., 2020;Schlag et al., 2021). The poor performance of efficient transformers on LRA can also be attributed to the model's ability to capture positional information.",
            "score": 0.5477370099154553,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1058,
                    "end": 1086,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1293,
                    "end": 1321,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193359375
        },
        {
            "corpus_id": "276421279",
            "title": "AdaSplash: Adaptive Sparse Flash Attention",
            "text": "The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.",
            "score": 0.5474943345319095,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "248965487",
            "title": "Linearizing Transformer with Key-Value Memory",
            "text": "Transformers with Memory Mechanism Previous work investigated injecting a memory mechanism into transformers. Burtsev et al. (2020) augmented Transformer by adding memory tokens to store non-local representation. Lample et al. (2019) used a product key memory layer to substitute the feed-forward layer in Transformer. Fan et al. (2021) used a KNN-based information fetching module to enable Transformer access to external knowledge. Our approach is fundamentally different from them as we replace the standard attention (SA) with a key-value memory layer, which leads to linear complexity and recurrent computation.\n\nRecurrent Transformers Previous work proposed several recurrent transformers focusing on approximating the softmax attention kernel between q and k by projecting them via feature map function \u03c6(\u22c5). These recurrent variants scale at the linear time and constant space complexity in sequence length. Katharopoulos et al. (2020) proposed \u03c6 (x) = elu (x) + 1 and applied it to image generation. In language modeling and machine translation tasks, RFA  and Performer (Choromanski et al., 2021) used random features that approximate the softmax attention via Monte Carlo sampling (Rahimi and Recht, 2007;Yu et al., 2016). T2R (Kasai et al., 2021) used trainable feature mapping which allows smaller feature size thus further improving the efficiency. Schlag et al. (2021) connects kernel-based transformers with previous fast weight systems. However, approximating softmax typically needs additional steps to obtain intermediate feature mapping results. Instead of approximating the self-attention softmax kernel, MemSizer employs a key-value memory module, which suppresses these intermediate steps. The output projection step in SA is also omitted in this key-value memory module, yielding further computation and memory savings.\n\nOther Efficient Transformers One family of efficient transformers limited the receptive fields that are attended to by sparsifying the attention patterns. Some works introduced fixed patterns of blockwise attention (Qiu et al., 2020) and strided attention (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020). (Sukhbaatar et al., 2019) learned sparse attention patterns in a data-driven manner. These",
            "score": 0.547350827158222,
            "section_title": "Related Work",
            "char_start_offset": 23190,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 233,
                    "matchedPaperCorpusId": "195886317"
                },
                {
                    "start": 319,
                    "end": 336,
                    "matchedPaperCorpusId": "216553133"
                },
                {
                    "start": 1192,
                    "end": 1216,
                    "matchedPaperCorpusId": "877929"
                },
                {
                    "start": 1216,
                    "end": 1232,
                    "matchedPaperCorpusId": "5354603"
                },
                {
                    "start": 1238,
                    "end": 1258,
                    "matchedPaperCorpusId": "232335426"
                },
                {
                    "start": 1363,
                    "end": 1383,
                    "matchedPaperCorpusId": "235377069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "235313355",
            "title": "Luna: Linear Unified Nested Attention",
            "text": "Transformers (Vaswani et al., 2017) are surprisingly versatile models that preform well on a wide range of language and vision tasks, including machine translation (Vaswani et al., 2017;Ott et al., 2018), language understanding (Devlin et al., 2019), image recognition (Dosovitskiy et al., 2020) and bioinformatics (Madani et al., 2020). Attention (Bahdanau et al., 2015) provides the key mechanism that captures contextual information from the entire sequence by modeling pairwise interactions between the inputs at every timestep. However, a common weakness of Transformers is their quadratic time and memory complexity within the attention mechanism w.r.t the length of the input sequence, which prohibitively restricts their potential application to tasks requiring longer input sequences. \n\nA number of techniques have been recently introduced to improve the time and memory efficiency of Transformer models ('xformers') (Tay et al., 2020b(Tay et al., , 2021)). One popular technique is using sparsity to restrict the attention field range, such as local attention (Parmar et al., 2018), blockwise attention (Qiu et al., 2019), strided attention patterns (Child et al., 2019;Beltagy et al., 2020), compressed attention (Liu et al., 2018), and attention with learnable patterns (Kitaev et al., 2020;Tay et al., 2020a;Roy et al., 2021). Another emerging approach is to improve efficiency by leveraging low-rank approximations of the attention matrix. Linformer (Wang et al., 2020), for example, projects the length dimension of key and value matrices to a fixed-dimensional representation by assuming low-rank structure in the full-rank attention matrix. Recently, some kernel-based methods, such as Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020) and Random Feature Attention (Peng et al., 2021), attempt to efficiently approximate regular (softmax) full-rank attention through kernelization.",
            "score": 0.5473260482125347,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 164,
                    "end": 186,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 186,
                    "end": 203,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 228,
                    "end": 249,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 315,
                    "end": 336,
                    "matchedPaperCorpusId": "214725226"
                },
                {
                    "start": 348,
                    "end": 371,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 944,
                    "end": 965,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1070,
                    "end": 1091,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1224,
                    "end": 1242,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1303,
                    "end": 1321,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1321,
                    "end": 1338,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187744140625
        },
        {
            "corpus_id": "276574945",
            "title": "Compression Barriers for Autoregressive Transformers",
            "text": "Generating long sequences with sparse transformers. arXiv preprint arXiv: . , . \n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv: . , . \n\nGonc \u00b8alo M Correia, Vlad Niculae, and Andr\u00e9 FT Martins. Adaptively sparse transformers. arXiv preprint arXiv: . , . \n\nTri Dao. Flashattention-: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv: . , . \n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, : -, . \n\nAlexey Dosovitskiy. An image is worth x words: Transformers for image recognition at scale. arXiv preprint arXiv: . , . \n\nHaojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. Skvq: Slidingwindow key and value cache quantization for large language models. In First Conference on Language Modeling, . \n\nQuentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, ( s): -, . \n\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In International Conference for Learning Representations, . \n\nInsu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. arXiv preprint arXiv: . , . \n\nThemistoklis Haris.",
            "score": 0.5466117958565773,
            "section_title": "Conclusion",
            "char_start_offset": 28110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 79
                },
                {
                    "start": 82,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1659
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043609619140625
        },
        {
            "corpus_id": "270199793",
            "title": "\"Forgetting\" in Machine Learning and Beyond: A Survey",
            "text": "The aforementioned methods primarily aim to make attention more efficient without minimizing memory usage.To address this, Expire-span [203] was introduced, calculating a specific lifespan for each memory (Figure 9).Once a memory surpasses its lifespan, it becomes inaccessible.This strategy prunes distant tokens deemed \"unimportant\", allowing transformers to handle memories tens of thousands in size.This temporal forgetting means transformers retain only recent and pertinent data, thereby cutting memory demands and computational costs.\n\nThe principle of lossless compression resonates with the transformer's goal of maximizing information retention with minimal resources.Compression techniques, similar to the act of forgetting, aspire to condense data without significant information loss.In transformers, the attention mechanism, optimised via methods like Adaptive-Span or Expire-span, is akin to data compression, retaining only vital information.\n\nThe principle behind the compressor serves various applications, with its primary use in language model training.\n\nRECOMP [250] is proposed for optical language model performance and operates by first retrieving and then compressing documents into summaries before assimilation.The model uses two compressor types: an extractive one that selects key sentences and an abstractive one that generates summaries from multiple sources.These compressors, designed to enhance the language model's task performance, can also opt for \"selective augmentation\" by returning an empty string if the documents are irrelevant.This selective approach mirrors the \"forgetting\" concept in transformers, streamlining information input to ensure optimal trade-off between model accuracy and efficiency.Meanwhile, TRIME [268] emphasizes memory type classification, introducing methods for memory construction and data batching for different memory types during testing.The attentive Item2Vec++ (AI2V++) model proposed a neural attentive collaborative filtering approach in which the user representation adapts dynamically in the presence of the recommended item [67].\n\nInspired by the dynamic nature of the human memory, in which different memories become more accessible in different contexts, the model uses attention mechanisms to dynamically adjust the importance of the historical items based on the item being considered for recommendation.\n\nManuscript submitted to ACM Fig. 10.SISA proposed by [28].",
            "score": 0.5463088095112996,
            "section_title": "Lossless compression.",
            "char_start_offset": 56864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 541
                },
                {
                    "start": 543,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 797
                },
                {
                    "start": 797,
                    "end": 958
                },
                {
                    "start": 960,
                    "end": 1073
                },
                {
                    "start": 1075,
                    "end": 1238
                },
                {
                    "start": 1238,
                    "end": 1390
                },
                {
                    "start": 1390,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1742
                },
                {
                    "start": 1742,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 2106
                },
                {
                    "start": 2108,
                    "end": 2385
                },
                {
                    "start": 2387,
                    "end": 2423
                },
                {
                    "start": 2423,
                    "end": 2445
                }
            ],
            "ref_mentions": [
                {
                    "start": 1759,
                    "end": 1764,
                    "matchedPaperCorpusId": "249062699"
                },
                {
                    "start": 2101,
                    "end": 2105,
                    "matchedPaperCorpusId": "257941960"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07147216796875
        },
        {
            "corpus_id": "274023420",
            "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
            "text": "Once the important keys are identified through our centroid lookup, the second stage of our system implementation leverages a sparse FlashAttention kernel to attain speedups during both prefill and generation stages. This stage also uses similar parallelization strategies as FlashAttention-2 by splitting work across heads and along the sequence length dimension [10]. Our kernel implementation builds on top of prior work on Triton implementations for FlashAttention-2 [34] and for dynamic sparse FlashAttention [36]. The kernel first loads in query vectors, and then iterates over a tensor of key indices that need to be selectively loaded. These indices are then used to load the corresponding keys from memory and compute exact attention. \n\nAn additional challenge when computing attention to the fixed context is the imbalanced distribution of important key tokens across different heads, which is highlighted in Figure A.2 in Appendix A.2. When using the default parallelization strategy in FlashAttention-2, if one head contains more important keys than the other heads, it will have significantly longer runtime, hindering speedups. In order to obtain latency benefits in these scenarios, we split keys and values along the sequence-length dimension as in Flash-Decoding [11], based on a fixed number of desired keys and values to be computed for a single Streaming Multiprocessor (SM). This means that if there are more keys and values that need to be computed for a particular head (due to unbalanced sparsity for different heads), the work for this head will be parallelized across a greater number of SMs in the GPU. The kernel is designed in two phases, as in Flash-Decoding. The first phase computes the partial attention outputs for each block of valid keys and values. The second stage merges the partial attention outputs, while correcting the outputs using the partial Softmax denominators and max values.",
            "score": 0.5457193090832212,
            "section_title": "Sparse Attention with Retrieved Keys",
            "char_start_offset": 26762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "268064867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2122802734375
        },
        {
            "corpus_id": "215828216",
            "title": "ETC: Encoding Long and Structured Data in Transformers",
            "text": "Since the publication of the original Transformer model (Vaswani et al., 2017), and especially after the great success of BERT (Devlin et al., 2018), a number of variations of the model have been proposed in the literature. For example, work exists on scaling up the training process (RoBERTa (Liu et al., 2019)), scaling the internal representation of the model (ALBERT (Lan et al., 2019)), or both (T5 (Raffel et al., 2019)), outperforming the original BERT model in tasks such as GLUE (Wang et al., 2018), SQuAD (Rajpurkar et al., 2016) or RACE (Lai et al., 2017). However, these models use input sequences of length up to 512 tokens due to computational and memory constraints: namely, the O(n 2 ) computational and memory cost of attention in Transformers, where n is the length of the input sequences. This work builds on prior efforts to scale up the attention mechanism and network architecture to accommodate longer input sequences. We classify these approaches into four broad categories: sparse attention, recurrence, hierarchical mechanisms, and compressed attention, which we elaborate on below. \n\nSparse Attention involves limiting each token to attend only to a subset of the other tokens in the input. For example, the Sparse Transformer (Child et al., 2019) used predefined attention patterns with applications to both natural language tasks and image generation. For example, they showed that attending only to previous pixels in the same row or column was enough for generating high quality images. This allows attention to go from \n\nAnother idea is that of Adaptive Attention Span Transformer (Sukhbaatar et al., 2019), where each attention head is associated with a decaying masking function, which limits the number of tokens it can attend to. Making those masking functions learnable, they show that lower layers tend to learn to use short attention spans, and it is not until the higher layers of the model, that attention spans are longer.",
            "score": 0.545669997392386,
            "section_title": "Background",
            "char_start_offset": 3068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 78,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 293,
                    "end": 311,
                    "matchedPaperCorpusId": "52044834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Section 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.",
            "score": 0.5456431600311742,
            "section_title": "Introduction",
            "char_start_offset": 2176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2374
                },
                {
                    "start": 2375,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 2333,
                    "end": 2337,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "268231045",
            "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention",
            "text": "Efficient and Approximate Attention Since the introduction of attention in transformers (Vaswani et al., 2017), there has been a body of work on approximating the attention mechanism for efficient training and inference of transformers. For example, dynamically sparse attention has been achieved using LSH (Kitaev et al., 2020), Nystr\u00f6m method (Xiong et al., 2021), and random sampling (Zaheer et al., 2020). Furthermore, low-rank attention has been extensively explored (Wang et al., 2020;Choromanski et al., 2020;Chen et al., 2021) and shown to have compute-and memory-efficiency advantages over regular transformers. Attention mechanisms with hardware-aware designs such as FlashAttention (Dao et al., 2022) have been proposed to mitigate the IO bottleneck in GPUs. In large language models, multiple approaches (Zhang et al., 2023b;Liu et al., 2023) have been proposed to reduce the high memory overhead of the KV cache. For CPU-only environments, Shen et al. (2023) proposes to speed up LLM inference through weight quantization.",
            "score": 0.5453578255607301,
            "section_title": "Related Works",
            "char_start_offset": 26315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1035
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 365,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 387,
                    "end": 408,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "248498407"
                },
                {
                    "start": 693,
                    "end": 711,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Long-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context.Numerous efficient approaches have emerged, spanning state-space models [30,62], recurrent neural networks [45,52,49], linear attention [55,38] and low-rank approximations of self-attention [75,14,53], which replace the self-attention with novel linear blocks for long-context modeling.Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29,77].Besides, a few studies combine the Transformer with block-wise recurrence [17,35,36,12] or key-value compression [60,59,18].In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix.This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15,27,42].\n\nSparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56,51], dilated sliding windows [4,22], combination of patterns [34,13], or domain-specific patterns [31].Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81,42,27].However, these static methods often prove suboptimal in various scenarios [66,2].Alternatively, sparse patterns can be learned in a data-driven manner.For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens.Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs.Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference.",
            "score": 0.5452731339862182,
            "section_title": "Related Work",
            "char_start_offset": 5218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1069
                },
                {
                    "start": 1071,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "259262301"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "211505992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "237260051",
            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
            "text": "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms. In addition, on the MIND datasets, we compare two additional SOTA news recommendation methods, i.e., NRMS (Wu et al., 2019) and FIM (Wang et al., 2020a), to provide benchmarks for the comparison. The performance of different methods on different datasets are compared in Tables 2, 3 and 4. We have several observations from the results. First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens. Third, Smart Bird can achieve better performance than other compared methods on all datasets in different tasks. This is because Smart Bird incorporates learnable sparse attention to better capture token interactions that may be important for context modeling. These results demonstrate the effectiveness and generality of Smart Bird. \n\nFurthermore, we compare the theoretical computational complexity of different methods in   with other sparse attention based methods if the sequence length is not extremely long. 10 These results show that Smart Bird is also efficient.",
            "score": 0.545263715845755,
            "section_title": "Performance Evaluation",
            "char_start_offset": 15625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 695,
                    "matchedPaperCorpusId": "202774468"
                },
                {
                    "start": 704,
                    "end": 724,
                    "matchedPaperCorpusId": "220045915"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "237412971",
            "title": "\\infty-former: Infinite Memory Transformer",
            "text": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \\infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\infty-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \\infty-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \\infty-former\u2019s ability to retain information from long sequences.",
            "score": 0.5444779959551166,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04443359375
        },
        {
            "corpus_id": "276094438",
            "title": "Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers",
            "text": "Transformer architecture and layer normalization. The original transformer architecture [50], referred to as Post-LN, applies layer normalization after the residual connection. In contrast, the Pre-LN architecture places layer normalization before the residual connection. Wang et al. [53] demonstrated that Post-LN transformers are difficult to train when the number of layers is large, a finding later theoretically confirmed by Xiong et al. [59] using mean field theory. Other architectures such as Reformer [21] were also introduced. Shi et al. [44] showed that a large standard deviation in layer normalization leads to rank collapse in Post-LN transformers. Furthermore, Wu et al. [57] observed that sparse masked attention mitigates rank collapse in the absence of layer normalization and that layer normalization induces equilibria ranging from rank one to full rank. \n\nAttention sparsity. Sparse attention mechanisms have been proposed to reduce the computational costs of transformers. For example, ETC [2] introduces efficient sparse attention, and Zaheer et al. [61] proposed BigBird, which they theoretically demonstrated to be as expressive as full attention. These sparse attention mechanisms are widely used in language models with large context windows, such as Longformer [5] and Mistral 7B [23]. In NLP, Clark [13] found that attention of pre-trained BERT focuses on specific tokens. In vision, Hyeon-Woo et al. [22] showed that while uniform attention is challenging to learn with the softmax function, ViT successfully learns uniform attention, which is key to its success. Additionally, Zhai et al. [62] suggested that low attention entropy contributes to training instability in transformers, a phenomenon they termed entropy collapse. Furthermore, Bao et al. [4] demonstrated that a small eigenspectrum variance of query and key matrices leads to localized attention and mitigates both rank and entropy collapse.",
            "score": 0.5443754258452865,
            "section_title": "A Additional related work",
            "char_start_offset": 20234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 92,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "174799399"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "211082816"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "229376913"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "246904522"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "270094757"
                },
                {
                    "start": 1013,
                    "end": 1016,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1431,
                    "end": 1435,
                    "matchedPaperCorpusId": "252918794"
                },
                {
                    "start": 1621,
                    "end": 1625,
                    "matchedPaperCorpusId": "257496258"
                },
                {
                    "start": 1783,
                    "end": 1786,
                    "matchedPaperCorpusId": "267412993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53076171875
        },
        {
            "corpus_id": "270764930",
            "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
            "text": "We introduce Fibottention, an efficient, robust, and sparse attention mechanism that diversifies attention computation across heads while maintaining structured sparsity, with two variants -one for image classification tasks (Wythoff), and the other for action classification and robot imitation learning in video domains (modified Wythoff). We implement Fibottention in conjunction with multiple state-of-the-art Transformer architectures curated for visual representation learning. Empirically, Fibottention outperforms the baselines on small-scale and mid-scale datasets and performs on par on large-scale datasets utilizing only 2-5% token interactions in the MHSA across three diverse visual tasks. We envision the next generation of Transformers [50] with billions of tokens that may benefit from such optimized architectures. We hope this inspires investigation to recover marginally compromised performance on large-scale datasets and the sparse implementation as CUDA kernels for enhanced efficiency. \n\nwhere we used the characteristic equation ( 6) for solutions \u03d5 and \u03c8. For this, we obtain the system of equations \n\nThis implies equation ( 4), from which (5) follows.",
            "score": 0.5441251868633797,
            "section_title": "Conclusion",
            "char_start_offset": 32559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226318359375
        },
        {
            "corpus_id": "276106883",
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "text": "Transformers have emerged as one of the most successful AI/ML techniques for modeling sequence data. They have drastically impacted a variety of scientific and engineering applications such as language modeling [1], molecular design [2], cancer pathology classification [3], genomic sequence modeling [4], and others. These models derive their power from an operation called the attention mechanism. Given a sequence of tokens of length L, the attention mechanism extracts pairwise similarities between the tokens [5]. \n\nHowever, the time and memory complexity of the attention mechanism is quadratic in L as it captures interactions between each pair of the sequence. This has severely limited the context length, i.e., the length of a sequence over which a transformer model can capture these interactions (also know as sequence length in the literature). For applications such as genomics, at least 4-5 orders of magnitude of increase in context length is needed [4]. \n\nA popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism [6]- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach. \n\nHowever, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.",
            "score": 0.5439241977355027,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "268423149"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "268040015"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "259274952"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "259274952"
                },
                {
                    "start": 1086,
                    "end": 1089,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "276558398",
            "title": "AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms",
            "text": "The attention mechanism takes large proportion in LLM computation. Table 1 shows the attention proportion in LLAMA-3B inference. Efficient implementations of various attention mechanisms hinge on reducing memory access and maximizing the utilization of compute units. Many libraries with handcrafted kernels achieve this by fusing memoryintensive operations, including element-wise calculations and reductions. \n\nFlashAttention [19] exemplifies this approach by integrating softmax computation, memory-efficient pipelining, and kernel fusion, thereby reducing computational overhead and improving performance. However, these libraries impose strict constraints on the attention patterns they support. Even minor deviations, such as the atypical input dimensions used in DeepSeek V2 and RetNet, can invalidate these optimizations. Figure 2 illustrates the performance disparity across different attention variants. For standard Softmax-Attention, the handcrafted library FlashAttention3 [19] significantly outperforms the native PyTorch implementation, achieving over 60% FLOPS utilization. In contrast, for less common variants like Gated-RetNet and ReLU-Attention, these libraries exhibit Softmax-Attention Gated-RetNet ReLU-Attention 0% 20% 40% 60% 80% 100%",
            "score": 0.5433764294888228,
            "section_title": "Efficient Implementation of Attention",
            "char_start_offset": 9272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1259
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.218994140625
        },
        {
            "corpus_id": "268599416",
            "title": "Automatic text summarization of scientific articles using transformers\u2014A brief review",
            "text": "The model has adopted three different attention mechanisms in the architecture: Random Attention, Sliding Window Attention and Global attention. \n\n1) Random Attention: A (i, \u2022 ) =1 for r randomly selected keys indicates a sparse attention system in which each query attends over r random numbers of keys. 2) Sliding Window Attention: A window of width W that restricts the query node's ability to attend to only its peers inside the key nodes and the key node's immediate neighbors inside the window. This is known as a sliding window attention. 3) Global Attention: This attention mechanism incorporates the importance of global tokens. A token attends every other token in an input sequence. \n\nApplying global, random, and sparse attention has been demonstrated to be computationally more efficient for longer sequences while roughly achieving the same results as complete attention. Big Bird has demonstrated enhanced performance on a variety of long document natural language processing tasks, including question answering and summarization, because of its capacity to handle longer contexts. To summarize all the different transformers-based models Table 2 is provided below. It compares the pretraining objective, pretraining corpora, total number of parameters and supported token length.",
            "score": 0.5432666867204952,
            "section_title": "\u2022",
            "char_start_offset": 28029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1295
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "The attention mechanism models interactions between words and is a significant component of the Transformer architecture. Enhancements to the attention module have predominantly focused on two aspects: modeling long texts and optimizing the Key-Value (KV) cache. ( 1) Modeling Long Texts: The vanilla attention mechanism has a complexity of O(n 2 ), which significantly increases computational costs for long texts. To address this, Sparse Transformer [13] employs sparse attention, utilizing predesigned attention patterns to avoid the computation of attention over long sequences. Another approach, Reformer [14], uses Locality-Sensitive Hashing (LSH) to reduce computational complexity. Additionally, Munkhdalai et al. [15] compressed context information to shorten sequences, thereby reducing overhead. Others have explored retrieval-based methods [16,17]. This area of research continues to hold considerable potential for future advancements. (2) Optimizing KV Cache: classic Transformers use multi-head attention (MHA), which requires storing extensive key-value caches during inference, slowing down model generation. To mitigate this, Shazeer [18] proposed multi-query attention, which employs multiple key heads but only a single value head, substantially reducing the key-value cache and enhancing computational speed. However, Ainslie et al. [19] found that this could degrade model performance, leading to the development of grouped query attention. This method allows multiple key heads to share a single value head, effectively serving as a hybrid between MQA and MHA, balancing computational complexity and performance more effectively. Recently, DeepSeek-AI [20] introduced multi-head latent attention, which compresses keys and values into a single latent space, thereby reducing the key-value cache while maintaining robust representational capacity.",
            "score": 0.5428844304184856,
            "section_title": "Attention",
            "char_start_offset": 7423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1869
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "266690973",
            "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation",
            "text": "While Transformer architectures have gained significant prominence in LLMs recently, there continues to be a surge of interest in their effective utilization in diverse domains, including computer vision tasks. In light of this, we review classical works dedicated to enhancing Transformer structures, with a particular focus on augmenting model nonlinearity and efficiency. \n\nNatural language processing domain. The conventional selfattention mechanism, with quadratic computational complexity, poses challenges for handling long input sequences during training and inference. To mitigate this, various structural priors on attention, including sparsity [33], [34], [35], [36], [37] and linear attention [15], [38], have been proposed. Notably, Reformer [39] employs locality-sensitive hashing to approximate full attention. Longformer [40] integrates local windowed attention with taskmotivated global attention. Models such as GPT-3 [41] incorporate locally banded sparse attention methods, such as Factorized Attention [34]. There are also works focusing on replacing the attention module by incorporating recurrent models [42], [43], [44]. Hyena [45] trained a recurrence of gating units and implicitly parametrized long convolutions, which serves as an attention-free drop-in replacement for the traditional Transformer architecture. RWKV [46] replaced the quadratic QK attention with a scalar formulation that has linear cost. RetNet [44] theoretically derived the connection between recurrence and attention and proposed the retention mechanism for sequence modeling. There are also efficient enhancements focused on the Feed-Forward Network (FFN). Mixture-of-Experts (MoE) [47], [17], [48], [49], [50] has demonstrated effectiveness in the pre-training of LLMs. In addition to MoE, PaLM [51] and LLaMA [23] leverage the SwiGLU activation for original FFN intermediate activations. This choice is grounded in the observation that SwiGLU activations, as demonstrated in compute-equivalent experiments [52], substantially enhance quality compared to standard activation functions like ReLU, GeLU, or Swish. \n\nComputer vision domain.",
            "score": 0.5428440746646135,
            "section_title": "Enhanced Transformer Architectures",
            "char_start_offset": 11409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1682,
                    "end": 1686,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1688,
                    "end": 1692,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "235367626"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "248266346"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "matchedPaperCorpusId": "232428341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271728515625
        },
        {
            "corpus_id": "259360659",
            "title": "Scaling In-Context Demonstrations with Structured Attention",
            "text": "Therefore, a variety of efficient Transformer models have been proposed to mitigate this issue (Child et al., 2019;Beltagy et al., 2020;Wang et al., 2020;Kitaev et al., 2019;Katharopoulos et al., 2020;Zaheer et al., 2020;Qiu et al., 2020;Tay et al., 2020;Peng et al., 2021;Qin et al., 2021;Zhou et al., 2021). We refer the readers to Tay et al. (2022a) for a comprehensive survey. In this paper, we draw inspiration from the design of sparse attention mechanisms in efficient Transformers (Beltagy et al., 2020;Zaheer et al., 2020). However, instead of using sparse attention to approximate the ability of full attention to process any sequences, we exploit the inherent structure of in-context learning (test input follows a number of demonstrations), and design a structured attention mechanism tailored for the problem. \n\nComparison with concurrent works. Concurrent with our work, there are several works (Hao et al., 2022;Ratner et al., 2022;Ye et al., 2022a) exploring architectural designs to improve the in-context learning ability of LLMs. Among them, Hao et al. (2022); Ratner et al. (2022) focus on tackling the limitation of context length by independently processing several prompts within the max length constraint and then combining them. This idea is similar to our adaptation of FiD while Hao et al. (2022); Ratner et al. (2022) use fixed schemes (e.g., average) for fusing parallel prompts so that their methods can be used directly on top of pretrained models without further fine-tuning. Compared to their methods, we, in addition, seek to address the efficiency and instability issues while exploring better encoding to retain the dependencies between demonstrations. These goals cannot be directly achieved by processing several grouped prompts in parallel.",
            "score": 0.5425715770389885,
            "section_title": "Related Work",
            "char_start_offset": 8050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1779
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 174,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 174,
                    "end": 201,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 201,
                    "end": 221,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 221,
                    "end": 238,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 255,
                    "end": 273,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 273,
                    "end": 290,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "229156802"
                },
                {
                    "start": 334,
                    "end": 352,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 511,
                    "end": 531,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364990234375
        },
        {
            "corpus_id": "220831004",
            "title": "Big Bird: Transformers for Longer Sequences",
            "text": "We propose BigBird: a sparse attention mechanism that is linear in the number of tokens.\n\nBigBird satisfies a number of theoretical results: it is a universal approximator of sequence to sequence functions and is also Turing complete. Theoretically, we use the power of extra global tokens preserve the expressive powers of the model. We complement these results by showing that moving to sparse attention mechanism do incur a cost. Empirically, BigBird gives state-of-the-art performance on a number of NLP tasks such as question answering and long document summarization. We further introduce attention based contextual language model for DNA and fine-tune it for down stream tasks such as promoter region prediction and predicting effects of non-coding variants.\n\n[ We begin by setting up some notations following P\u00e9rez et al. [73] to formally describe the complete architecture of Transformers. A single layer of Transformer encoder is a parametric function Enc receiving a sequence X = (x 1 , ..., x n ) of vectors in R d and returning a sequence Z = (z 1 , ..., z n ) of the same length. Each z i is a d dimensional vector as well.\n\nWe interchangeably treat the sequence X as a matrix in R n\u00d7d . Enc has two components:\n\n1. An attention mechanism Attn that takes in the sequence X and returns sequence (a 1 , ..., a n ) of the same length and dimensionality; and 2. A two layer fully connected network O that takes in a vector in R d and returns a vector in R d .\n\nThen i-th output vector of Enc(X) is computed as follows:\n\nNow it remains to define Attn and O which we do next.\n\nAs described in Sec. 2, an attention mechanism is parameterized by three functions: Q, K, V : R d \u2192 R m . In this paper, we assume that they are simply matrix products: \n\nwhere N (i) denote the out-neighbors set of node i in D. In other words, the set of arcs (directed edges) in D represents the set of inner products that our attention mechanism will consider. Also recall that \u03c3 is a scoring function such as softmax or hardmax.\n\nLastly, we define the output fully connected network as follows:\n\nHere",
            "score": 0.5425073637685082,
            "section_title": "Conclusion",
            "char_start_offset": 33819,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496826171875
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "There might be multiple explanations from multiple angles for this phenomena. Firstly, linear attention (e.g., Performer) models struggle to be competitive on common benchmarks, as noted from multiple sources (Xiong et al., 2021a;Anonymous, 2021b). \n\nIt is good to note that, apart from toy setups or specific domains and problems, they have never been battle-tested against common paradigms like pretrain-and-finetuning only up till recently. Meanwhile, local attention models based on fixed and/or learned patterns such as Sparse Transformers (Child et al., 2019), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020) or BigBird (Zaheer et al., 2020) have seen more reasonable usage, especially within the areas of long context question answering. However, the high intrinsic implementation complexity of methods such as in ETC (Ainslie et al., 2020) (substantially increases code complexity by having so many different directions of attention), Swin Transformer (Liu et al., 2021b) or Longformer (Beltagy et al., 2020) (requiring custom CUDA kernels and thus making it prohibitive on hardware such as TPUs) might be reasons why these models have yet to found themselves serving as a good, simple-to-use drop-in Transformer replacement. \n\nAs noted by (Rabe and Staats, 2021), for applications that require to flex on sequence length and memory needs time to time, it might be suffice to 'just sequentially process it' even if that might not be inherently as satisfying as finding a theoretical approximate. In parallel, (Xiong et al., 2021a) suggests that local attention, when done right, can be a really tough baseline to beat. \n\nA notable fact about the barrage of efficient attention models is the overloading of the term efficient. It is commonly misunderstood that efficient attention models always imply that the Transformer is fast. The truth is that many of these efficient attention models, owing to their innovation constraints, may make the model much slower. Moreover, many linear attention models do not observe any speed or memory gain at all if the sequence length is short.",
            "score": 0.5422625487678085,
            "section_title": "A Retrospective on the Past Year and Future Research Directions",
            "char_start_offset": 73282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 247,
                    "matchedPaperCorpusId": "249120570"
                },
                {
                    "start": 578,
                    "end": 600,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 606,
                    "end": 628,
                    "matchedPaperCorpusId": "215828216"
                },
                {
                    "start": 640,
                    "end": 661,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 839,
                    "end": 861,
                    "matchedPaperCorpusId": "215828216"
                },
                {
                    "start": 1008,
                    "end": 1030,
                    "matchedPaperCorpusId": "215737171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.180419921875
        },
        {
            "corpus_id": "220250819",
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
            "text": "Existing works seek to improve memory efficiency in transformers through weight pruning (Michel et al., 2019), weight factorization (Lan et al., 2020), weight quantization (Zafrir et al., 2019) or knowledge distillation. Clark et al. (2020) proposed a new pretraining objective called replaced token detection that is more sample efficient and reduces the overall computation. Lample et al. (2019) used product-key attention to increase the capacity of any layer with negligible computational overhead. \n\nReducing the memory or computational requirements with these methods leads to training or inference time speedups, but, fundamentally, the time complexity is still quadratic with respect to the sequence length which hinders scaling to long sequences. In contrast, we show that our method reduces both memory and time complexity of transformers both theoretically ( \u00a7 3.2) and empirically ( \u00a7 4.1). \n\nAnother line of research aims at increasing the \"context\" of self-attention in transformers. Context refers to the maximum part of the sequence that is used for computing selfattention. Dai et al. (2019) introduced Transformer-XL which achieves state-of-the-art in language modeling by learning dependencies beyond a fixed length context without disrupting the temporal coherence. However, maintaining previous contexts in memory introduces significant additional computational cost. In contrast, Sukhbaatar et al. (2019) extended the context length significantly by learning the optimal attention span per attention head, while maintaining control over the memory footprint and computation time. Note that both approaches have the same asymptotic complexity as the vanilla model. In contrast, we improve the asymptotic complexity of the self-attention, which allows us to use significantly larger context. \n\nMore related to our model are the works of Child et al. (2019) and Kitaev et al. (2020). The former (Child et al., 2019) introduced sparse factorizations of the attention matrix reducing the overall complexity from quadratic to O N \u221a N for generative modeling of long sequences. More recently, Kitaev et al. (2020) proposed Reformer. This method further reduces complexity to O (N log N ) by using locality-sensitive hashing (LSH) to perform fewer dot products.",
            "score": 0.5419930487711212,
            "section_title": "Efficient Transformers",
            "char_start_offset": 3393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 109,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 132,
                    "end": 150,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 377,
                    "end": 397,
                    "matchedPaperCorpusId": "195886317"
                },
                {
                    "start": 1091,
                    "end": 1108,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1402,
                    "end": 1426,
                    "matchedPaperCorpusId": "159041867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "250118028",
            "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics",
            "text": "The vanilla Transformer models that utilize full attention have a memory complexity  ( 2 ). This attribute limits its wider usage across many domains, including long document summarization. For example, to circumvent the input tokens limits of PEGASUS, DANCER [37] summarizes each section of the long document separately and concatenates each of them to form the final summary. \n\nAs not all benchmark datasets contain discourse information such as section structures, this limits the model usage in many long document summarization settings. To this end, researchers have proposed various ingenious ideas to reduce the memory and time complexity of Transformer models. The variants of Transformer models that require less memory are often known as efficient Transformers [109,110] and the mechanism is referred to as efficient attentions [49]. \n\nLongformer [2] combines local attention, stride patterns and global memory for fine-tuning pre-trained BART to effectively summarize long documents with a maximum input length of 16,384 tokens as opposed to the 1,024 token limit of the original BART model. The model achieved state-of-the-art results in the long document summarization along with other NLP tasks when the model was introduced. BigBird [127] also implemented the efficient attention mechanism on Transformer-based abstractive summarizer by utilizing the same attention modifications as Longformer with an additional random pattern to achieve matching performance results in terms of ROUGE score. An important work by Huang et al. [49] explores and compares the performance of different variants of efficient transformers in the context of long document summarization.",
            "score": 0.5419724665917605,
            "section_title": "3) Efficient Attentions",
            "char_start_offset": 47390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 377
                },
                {
                    "start": 380,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1679
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "219425367"
                },
                {
                    "start": 771,
                    "end": 776,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1542,
                    "end": 1546,
                    "matchedPaperCorpusId": "233033613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2156982421875
        },
        {
            "corpus_id": "263765214",
            "title": "Challenges in Domain-Specific Abstractive Summarization and How to Overcome Them",
            "text": "The quadratic complexity of the Transformer block is a well-known issue and several approaches to counter this have been proposed in the past years. All of these approaches focusing on adapting the selfattention mechanism of the Transformer block to reduce the quadratic complexity are categorized as Efficient Transformers. The survey by Tay et al. provides a detailed taxonomy of all available Efficient Transformers (Tay et al., 2020). Some state-of-the-art Efficient Transformers suitable for domain-specific text summarization are discussed below: BigBird. BigBird is a long sequence Transformer that was introduced by Zaheer et al. and can process up to 4,096 tokens at a time. The attention mechanism of BigBird essentially consists of three parts in which all tokens attend to 1) a set of global tokens, 2) a set of randomly chosen tokens, and 3) all tokens in direct adjacency (Zaheer et al., 2020). The set of global  (Tay et al., 2020).\n\nLongformer Encoder-Decoder. The Longformer Encoder-Decoder (LED) model is a variant of the Longformer for sequence-to-sequence tasks such as summarization or translation (Beltagy et al., 2020). Similar to the BigBird model, the original Longformer relies on a sliding window attention of width w with each token attending to the w/2 preceding and following tokens in the sequence. Stacking multiple layers, each using sliding window attention, ensures that a large amount of contextual information is embedded in each token's encoding. Apart from sliding window attention, the authors also use dilated sliding window attention. This in effect reduces the resolution of the sequence and allows the model to include more contextual information with fixed computational costs. The Longformer model also incorporates global attention. Similar to BigBird's global attention, a set of predefined positions in the input sequence attend to the entire sequence and all tokens in the sequence attend to the same global tokens. LED has an encoder that uses the local+global attention pattern of the original Longformer and a decoder that uses the full self-attention on the encoding provided by the encoder. The LED model scales linear",
            "score": 0.5416551633552812,
            "section_title": "Efficient Transformers",
            "char_start_offset": 10958,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1876220703125
        },
        {
            "corpus_id": "263831088",
            "title": "Humans and language models diverge when predicting repeating text",
            "text": "Human performance on recall tasks, like the experiment we propose here, is primarily limited by shortterm memory (Baddeley, 1992). In these tasks, humans show both recency biases (i.e. better recall for the most recent items) and primacy biases (better for the first items) (Tzeng, 1973;Jefferies et al., 2004). Recall tasks often show repetition effects; presenting a stimulus multiple times successively decreases the recall error rate (Kintsch, 1965;Baddeley and Ecob, 1973;Amlund et al., 1986). Some have suggested a link between language deficits and the number of presentations needed to reach perfect verbatim sentence recall (Miles et al., 2006). Many studies have also shown that human memory decay follows a power law (Donkin and Nosofsky, 2012), where, for example, the number of items accurately recalled from a list will decrease over time t proportional to t \u2212d for some constant decay rate d. \n\nTransformers neural networks, in contrast with humans, can attend to exact token identities hundreds or thousands of tokens in the past at no additional cost, subject only to the context length. One limitation of the standard attention implementation is that memory and runtime scale quadratically with the number of tokens, making longer inputs prohibitively expensive. Recently, significant work has gone into extending the maximum context length for transformers while avoiding these computational issues. Transformer-XL caches hidden states to allow attention to tokens beyond the immediate input (Dai et al., 2019). FlashAttention is an optimized attention algorithm that exploits the hardware architecture to train models with context lengths up to 64K tokens (Dao et al., 2022). The ALiBi method (Press et al., 2022) replaces sinusoidal positional embeddings with a recency bias on the attention scores, such that closer query-key pairs are weighted higher than more distant pairs. Using ALiBi necessitates retraining a model with the new attention mechanism, though once trained it can generalize to longer lengths.",
            "score": 0.541537071596593,
            "section_title": "Related works",
            "char_start_offset": 2982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 129,
                    "matchedPaperCorpusId": "1916803"
                },
                {
                    "start": 274,
                    "end": 287,
                    "matchedPaperCorpusId": "144714640"
                },
                {
                    "start": 287,
                    "end": 310,
                    "matchedPaperCorpusId": "15970720"
                },
                {
                    "start": 438,
                    "end": 453,
                    "matchedPaperCorpusId": "147192184"
                },
                {
                    "start": 453,
                    "end": 477,
                    "matchedPaperCorpusId": "145166503"
                },
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "143909332"
                },
                {
                    "start": 633,
                    "end": 653,
                    "matchedPaperCorpusId": "36267825"
                },
                {
                    "start": 728,
                    "end": 755,
                    "matchedPaperCorpusId": "6812055"
                },
                {
                    "start": 1713,
                    "end": 1733,
                    "matchedPaperCorpusId": "237347130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251220703125
        },
        {
            "corpus_id": "254096202",
            "title": "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?",
            "text": "In the past few years, a vast amount of research has been devoted to addressing the problem of quadratic time and memory complexity associated with the dense attention mechanism (Vaswani et al., 2017), practically limiting the maximum sequence length severely (often to 512 tokens) (Tay et al., 2020b;Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2021;Roy et al., 2021;Kitaev et al., 2020;Tay et al., 2021;Lee-Thorp et al., 2021). These research works have given rise to a new class of transformers, referred to as sparse transformers or efficient transformers (Tay et al., 2020b). Reducing the cost associated with the computation of the dense attention matrix while maintaining the same performance is the core idea behind efficient transformers. This is often achieved by introducing sparsity in the attention matrix in a variety of ways that may be fixed pattern such as local (windowed) attention (Child et al., 2019;Beltagy et al., 2020), global attention (Zaheer et al., 2021) or learnable patterns such as routing attention (Roy et al., 2021) and LSH attention (Kitaev et al., 2020) or a random pattern (Zaheer et al., 2021;Tay et al., 2021). Recently, Lee-Thorp et al. (2021) proposed to use Fourier transforms instead of the attention layer. A comprehensive list of efficient transformers and the detailed description of their attention mechanism can be found in the survey by Tay et al. (2020b). (Tay et al., 2020a) proposed a series of tasks designed for testing the capabilities of these different models suitable for longer inputs. However, this so-called \"Long Range Arena\" considers mostly artificial tasks, with the goal of evaluating the models independently of any pretraining.",
            "score": 0.5414540929572605,
            "section_title": "Long Document Processing",
            "char_start_offset": 8074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1705
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 378,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390380859375
        },
        {
            "corpus_id": "247218606",
            "title": "DCT-Former: Efficient Self-Attention with Discrete Cosine Transform",
            "text": "Reducing memory footprint and computational cost is the main objective of our work, therefore hereafter we provide detailed results on the requirements of our model and compare them against the main competitors in the literature. We compare our attention head against the original (Vanilla [1]) transformer implementation as well as Linformer, Nystr\u00f6mformer and Performer.\n\nModel Inference results For a fair comparison we used for all the tests our transformer model defined in Sect. 5.1, replacing only the attention head. We tested with randomly generated sequences of length n \u2208 {128, 512, 1024, 4096} adapting the batch size accordingly to fit the model in memory: to adjust for the non fixed batch size we normalize both the inference time and the memory occupation for the current batch size. All the measurements are taken accounting only for the forward propagation.\n\nIn Table 1 we adopt the notation {Model} \u2212 {scale} where scale indicates the (fixed) ratio of the input sequence length used to instantiate the efficient attention: for our method it defines the number of DCT coefficients, for Linformer the dimension of the learnable projection , for Nystr\u00f6mformer the number of selected landmarks and for Performer the  , we argue that it would be mathematically unfounded to assume that is possible to obtain a constant complexity for an arbitrary input length, whatever efficient attention head is used. From the reported results it is clear that the transformer model, equipped with our DCT based efficient attention, outperforms all the competitors. As expected and discussed in Sect. 2.2 the savings in memory and inference times, from the usage of our attention head, are directly proportional to the sequence length. In the next paragraph we discuss this important aspect in more details.",
            "score": 0.5413838488735603,
            "section_title": "Evaluation",
            "char_start_offset": 31769,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "250113485",
            "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
            "text": "Nowadays, models based on Transformer [13] have achieved an extremely high performance in deep learning research. In the area of natural language processing, Transformer and its variants, like BERT [5] and GPT-3 [3], have outperformed other models based on RNN and CNN in most tasks. Inspired by the great ability that transformers have shown in NLP tasks, researchers have started to migrate the network to other fields like computer vision and recommender system recently. Vision Transformer (ViT) [6] is one of the representative works which apply the transformer directly to sequences of image patches. \n\nTransformer and its variants achieve state-of-the-art results by capturing contextual information from the entire sequence using the self-attention mechanism. Each self-attention block takes three matrices, namely  (query),  (key), and  (value), as its inputs. These three matrices will be used to capture the relation between tokens in a given sequence. The computation of attention includes two matrix multiplications, a scaling operation and a softmax operation. The two matrix multiplications introduce the quadratic computational complexity with respect to the sequence length. As transformers are being extensively applied in many areas, the model size increases greatly, together with the lengths of input sequences. For example, BERT-base [5] and GPT-3 [3] have 110 million and 175 billion parameters, respectively. Due to the quadratic complexity of self-attention w.r.t the sequence length, the model complexity is further exacerbated in scenarios with long input sequences. Such considerable workload brings about heavy computational and memory burdens, making it difficult to train and deploy these models, especially for tasks with long input sequences. \n\nTo remedy this issue, several models with hybrid sparse attention mechanisms are proposed to handle long sequences, such as Longformer [2] (up to 16384 tokens in a sequence) and ViL [16] (up to 96 \u00d7 96 patches in an image). They hybridize two variants of the attention mechanism and perform the computation of a local window attention and a task-motivated global attention. Such a hybrid sparse attention mechanism successfully reduce the complexity of attention to a linear level. This linear complexity significantly alleviates the memory burden, making it possible to train models with long sequences.",
            "score": 0.5411986639128534,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2382
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "239049478",
            "title": "Transformer Acceleration with Dynamic Sparse Attention",
            "text": "Transformers with the use of self-attention mechanism are difficult to scale with sequence length because of the quadratic time and memory complexity. Our paper focuses on the exploration of sparse attention patterns in Transformers. Other orthogonal approaches such as parameters sharing (Gong et al., 2019) can mitigate the issue. We refer readers to a survey paper for a more comprehensive view of efficient Transformers (Tay et al., 2020c). \n\nStatic Sparse Patterns. A straightforward way to exploit attention sparsity is to set static or fixed sparse patterns, such as local windows, block-wise, dilated patterns, or a combination of static patterns (Zaheer et al., 2020;Child et al., 2019;Qiu et al., 2020). However, as the sparse attention patterns are inherently dynamic depending on input sequences, those work lack the capability of capturing dynamic sparse patterns. As shown in our evaluation, the sparsity-saving trade-offs of representative methods using static sparse patterns are worse than our dynamic sparse attention approach. \n\nClustering-based methods. Building upon static blocksparse patterns, another line of research is to group similar tokens into chunks and perform local attention within chunks (Kitaev et al., 2020;Roy et al., 2021;Tay et al., 2020a). The similarity function used to group tokens can be hashing, clustering, or learned sorting. However, those methods are designed for training memory reduction and impractical at inference time when operating on each sequence. The quality of grouping, e.g., convergence of clustering, is not guaranteed at long sequences, and the overhead of onthe-fly clustering is not acceptable. \n\nApproximation methods. Recent work proposes to replace standard attention with forms of approximation of the at-tention weights (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021). While we provide a comparison in our evaluation, we regard those work out the scope of our discussion for exploring sparsity in (standard) attention. Whether using a form of approximation to replace standard attention or as we suggest to predict sparse patterns explicitly is a design choice leaving up to practitioners.",
            "score": 0.5410555928500973,
            "section_title": "RELATED WORK",
            "char_start_offset": 37247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "174799716"
                },
                {
                    "start": 1223,
                    "end": 1244,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1244,
                    "end": 1261,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1261,
                    "end": 1279,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1811,
                    "end": 1838,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1863,
                    "end": 1881,
                    "matchedPaperCorpusId": "232105052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332275390625
        },
        {
            "corpus_id": "271431900",
            "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
            "text": "Transformer-based LLMs have opened up fresh opportunities to both research and applications (Ope-nAI, 2023;Touvron et al., 2023). Their quadratic overhead imposes prohibitive overhead in training and serving these models. For example, training Llama 2 (Touvron et al., 2023) 70B with a 4K context length on 2 trillion tokens takes 23 days on 2048 A100 GPUs Rucinski (2024). When serving the model, the model's KV cache consumes 343 GB GPU memory with a batch size 32 and 4K context length. There is an urgent need to train LLMs efficiently and serve them cost-effectively. \n\nMany works aim to improve efficiency of attention through various sparse attention techniques (Tay et al., 2023;Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), where only a subset of the   tokens in the context are attended to. Despite their promising on-paper FLOP savings compared to full-context dense attention, these methods often fail to deliver real-world efficiency gains. As pointed out by the seminal work FlashAttention (Dao et al., 2022;Dao, 2023), GPU memory access, rather than computation, is the primary bottleneck for attention. Dense attention has benefited from CUDA-level implementations specifically optimized for more efficient memory IO, an significant optimization that sparse attention methods have yet to receive. The absence of a flexible, efficient, and easy-to-use library for optimized implementations for sparse attention has become a major roadblock for research in this area, slowing the progress in improving LLMs' training and serving efficiency. \n\nWith Sparsely-Sharded(S2) Attention, we aim to bridge this gap. S2-Attention is a Triton library that provides kernel optimization for sparse attention. It is highly flexible, allowing practitioners to explore various sparse attention strategies and customize different attention patterns across attention heads and context ranges. The main challenge in implementing a fused kernel for general sparse attention arises from the sparsity itself.",
            "score": 0.5410240490916385,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 747,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1020,
                    "end": 1038,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60498046875
        },
        {
            "corpus_id": "218487799",
            "title": "Multi-scale Transformer Language Models",
            "text": "Transformer-based language models (Vaswani et al., 2017;Radford et al., 2018;Al-Rfou et al., 2019) have become the model of choice for most large-scale language modeling benchmarks. Kaplan et al. (2020) report power-law scaling of transformer language models with model capacity and data size. As models get bigger, the amount of computational resources, especially memory, grows quickly. In the next paragraph we review some recent efforts attempting to address this issue. \n\nMemory-Efficient Transformers. Sukhbaatar et al. (2019) present an adaptive attention mechanism that learns how far back into the past each head in a transformer should look, and if implemented efficiently with sparse matrix operations, can help save memory. Liu et al. (2018); Rae et al. (2019) present approaches that compress the transformer's memory with strided convolutions. Specifically, Liu et al. (2018) compress the keys and values in the multi-headed attention by a factor of 3 for long-text abstractive summarization. Child et al. (2019) present sparse transformers along with efficient CUDA kernels for sparse attention demonstrating the ability to generate very long sequences. Rae et al. (2019) compress the recurrent memory for a transformer-XL (Dai et al., 2019), but find that the best performing variant is one that does not learn the compression function end-to-end. Liu & Lapata (2019) proposes a hierarchical extension of the architecture proposed in Liu et al. (2018) that attends over very long sequences, with the aim to better model paragraph-and document-level contexts. Kitaev et al. (2019) describe the different factors that contribute to large memory footprints in vanilla transformers. They use reversible layers to remove the need to store activations at every layer in the forward pass, LSH attention to decrease memory requirements from O(N 2 ) to O(N log N ) where N is the sequence length, and they split activations in the feedforward layers.",
            "score": 0.5408471184277913,
            "section_title": "Related Work",
            "char_start_offset": 3186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 56,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 77,
                    "end": 98,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1575,
                    "end": 1595,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36767578125
        },
        {
            "corpus_id": "269982604",
            "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
            "text": "Efficient and linear transformers have emerged over the past few years as an active area of research for particularly resource or latency-constrained environments, exhibiting notable inference speedups and smaller memory footprints.These transformer variants focus on alternative attention mechanisms that reduce the quadratic complexity of typical softmax attention.A plethora of efficient transformer options exist that can be classified into a few groups: slidingwindow or localized attention mechanisms (Parmar et al., 2018;Dai et al., 2019;Wu et al., 2020;Beltagy et al., 2020), pattern or sparsity-based attention mechanisms (Child et al., 2019;Zaheer et al., 2020), kernel-based and truly linear attention mechanisms with no priors (Katharopoulos et al., 2020;Choromanski et al., 2020;Peng et al., 2021;Chen et al., 2021;Qin et al., 2022b), and some unique outliers (Wang et al., 2020b;Kitaev et al., 2020).\n\nWhile many approaches linearize the computations, trulylinear transformers, such as the kernel-based substitutions for the softmax mechanism, do not make any prior assumptions of the environments (e.g., no assumed sparsity or local dependencies).This can be described via row-wise outputs (represented by a h,i (x)) for each attention head in Equations 3, 4, and 5, with S corresponding to any similarity function that transforms the product of the query and key matrices.If S becomes exp, Equation 3 is an accurate representation of softmax attention.If we decompose S into S q and S k , as shown in Equation 4, computation can be reordered such that the attention complexity reduces from 5.\n\nN 1 corresponds to the sequence length of the query matrix and N 2 corresponds to those of the key and value matrices (a generalization for encoder-decoder cross-attention).When N 1 or N 2 are significantly larger than d, this rearrangement of the attention calculation leads to linear complexity with respect to the sequence length.",
            "score": 0.5400543682242605,
            "section_title": "Efficient and Linear Transformers",
            "char_start_offset": 6031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 914
                },
                {
                    "start": 916,
                    "end": 1162
                },
                {
                    "start": 1162,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1608
                },
                {
                    "start": 1610,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 828,
                    "end": 846,
                    "matchedPaperCorpusId": "246904340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "268230534",
            "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
            "text": "Large language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019;Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT-3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. \n\nThe rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022;Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result-ing in both faster execution and a lower memory footprint compared to standard attention implementations.",
            "score": 0.5396766299767337,
            "section_title": "Large Language Models",
            "char_start_offset": 4332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1822509765625
        },
        {
            "corpus_id": "268793982",
            "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
            "text": "The Transformer architecture (Vaswani et al., 2017) has revolutionized the field of Natural Language Processing (NLP), achieving outstanding results in various tasks such as speech recognition (Dong et al., 2018), machine translation (Wang et al., 2019), and document generation/summarization (Kim et al., 2022).This success has led to an era dominated by large language models (LLMs), where the Transformer structure is scaled up to handle increasingly complex tasks.However, this scaling brings with it substantial computational demands, especially due to the attention mechanism which requires cross-correlation calculations between each token.These computational requirements, coupled with the significant inference costs and energy consumption, present considerable obstacles to deploying these models in resource-constrained environments like mobile devices and robotics.\n\nIn response to the pressing need for more efficient Transformer models, the research community has directed its efforts towards optimizing the Transformer architecture.A myriad of strategies has been put forward, encompassing methods such as model pruning, quantization, and the development of more efficient attention mechanisms.Among these initiatives, simplifying the attention mechanism has emerged as a particularly promising avenue.This approach focuses on transforming the traditionally quadratic complexity of attention mechanisms into a more manageable linear scale.(Katharopoulos et al., 2020) introduces Linear Transformers, which leverage kernel feature maps to transform self-attention, reducing complexity from quadratic to linear while maintaining comparable results to traditional Transformers.(Kitaev et al., 2020) proposes replacies dotproduct attention with locality-sensitive hashing and using reversible residual layers to minimize memory usage in training.Performer (Choromanski et al., 2020) utilize positive orthogonal random features to approximate softmax-based self-attention in Transformers, achieving a transformative leap to linear complexity.\n\nHowever, the majority of existing methods for optimizing Transformers, particularly in relation to their attention mechanisms, necessitate comprehensive retraining.This retraining process presents a formidable challenge, especially for models with an immense array of parameters.It requires a significant investment in terms of computational resources and time.",
            "score": 0.5395137862195041,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1209
                },
                {
                    "start": 1209,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1454
                },
                {
                    "start": 1454,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1857
                },
                {
                    "start": 1857,
                    "end": 2052
                },
                {
                    "start": 2054,
                    "end": 2218
                },
                {
                    "start": 2218,
                    "end": 2333
                },
                {
                    "start": 2333,
                    "end": 2415
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 311,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1454,
                    "end": 1482,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "251979721",
            "title": "Efficient Methods for Natural Language Processing: A Survey",
            "text": "The transformer's self-attention mechanism has a quadratic dependency on sequence length which is not fully utilized by existing models (Hassid et al., 2022). To reduce computational costs, efficient attention mechanisms for long sequences have been proposed (Tay et al., 2022). Existing strategies include better using already processed segments via recurrence to connect multiple segments (Dai et al., 2019), learning a network to compress a longer-term memory (Rae et al., 2020), separately modeling global and local attention (Ainslie et al., 2020), and modeling long inputs as a continuoustime signal (Martins et al., 2022b). Another line of research uses fixed attention patterns, where tokens attend to their immediate context (local attention) \n\nand possibly to a few global positions (global attention; Beltagy et al., 2020;Zaheer et al., 2020;Child et al., 2019). Compared to using the full selfattention matrix, such approaches can scale linearly with the input length. Some methods learn attention sparsity patterns directly from data, e.g. by grouping tokens into buckets, leading to a more accurate yet more expensive approximation of the full attention matrix (Kitaev et al., 2020;Daras et al., 2020;Roy et al., 2021). Instead of seeking better attention patterns, some strategies modify the attention mechanism and derive low-rank approximations to the querykey matrices via reverse application of the kernel trick, resulting in linear time attention (Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2020;Zhai et al., 2021). Recently, IO-aware attention mechanisms have been proposed, decreasing reads and writes to the attention matrix to GPU high-bandwidth memory (Dao et al., 2022b). \n\nDespite various improvements in attention mechanisms, most of them struggle with very long sequences (Tay et al., 2021).",
            "score": 0.539256166555204,
            "section_title": "Improving Attention in Transformers",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 157,
                    "matchedPaperCorpusId": "253384631"
                },
                {
                    "start": 463,
                    "end": 481,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 530,
                    "end": 552,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 833,
                    "end": 853,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 853,
                    "end": 872,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1215,
                    "end": 1232,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1467,
                    "end": 1495,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1520,
                    "end": 1538,
                    "matchedPaperCorpusId": "232105052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1419677734375
        },
        {
            "corpus_id": "250113485",
            "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
            "text": "The attention mechanisms of transformers effectively extract pertinent information from the input sequence. However, the quadratic complexity of self-attention w.r.t the sequence length incurs heavy computational and memory burdens, especially for tasks with long sequences. Existing accelerators face performance degradation in these tasks. To this end, we propose SALO to enable hybrid sparse attention mechanisms for long sequences. SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation. We show that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads, i.e., Longformer and ViL.",
            "score": 0.5391588336443913,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3876953125
        },
        {
            "corpus_id": "278339364",
            "title": "IAFormer: Interaction-Aware Transformer network for collider data analysis",
            "text": "Sparse attention is a technique used in Transformer models to reduce the computational complexity of the self-attention mechanism by limiting the number of key-query pairs attend to the classification. Instead of computing attention over all tokens in a sequence, sparse attention selectively attends to a subset of tokens based on predefined patterns, learned structures, or efficient approximations. This significantly lowers the memory and computational cost from quadratic to linear or sub-quadratic complexity, making it possible to process longer sequences efficiently. \n\nDifferent types of sparse attention have been introduced such as fixed pattern and learnable sparse attention. Fixed pattern sparse attention is a type of sparse attention mechanism where each token attends only to a predefined subset of tokens based on a fixed rule. [58][59][60]. On the other hand, Learnable sparse attention dynamically determines which tokens to attend to based on learned patterns. Learnable sparse attention allows the model to adaptively focus on the most relevant tokens in different contexts, leading to improved efficiency. In this paper we utilize a dynamic sparse attention mechanism incorporated in transformer called \"differential attention\" which was first introduced in [51]. \n\nwith \u03b2 is a learnable vector and W 1 , W 2 are two learnable matrices. During the training process, the value of \u03b2 vector is optimized to demote the attention score assigned to less relevant tokens, resulting in the attention score matrix to be sparse. This results in an implicit sparsity, where tokens gradually focus only on meaningful interactions while ignoring less relevant ones. The self-attention mechanism is guided by this dynamic sparsity, reducing computational overhead while preserving essential long-range dependencies. The IAFormer shows better performance compared with ParT with notably smaller number of parameters. \n\nDespite the advantages of dynamic sparse attention, a challenge remains in controlling the value of the \u03b2 parameter. In the current setup, we clip \u03b2 to lie within the range [0, 1]. Additionally, the attention score \u03b1 i,j can sometimes take negative values. In such cases, it becomes unclear how to interpret it as an attention probability.",
            "score": 0.538612488723589,
            "section_title": "Dynamic sparse attention via differential attention",
            "char_start_offset": 17352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "258048654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "232404731",
            "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "text": "The third is the low-rank based mechanism. For example the Linformer [46] projects the input key-value pairs into a smaller chunk, and performs cross-attention between the queries and the projected key-value pairs. The fourth is the (generalized) kernel-based mechanism, including Performer [10] and Linear Transformers [17]. Many models utilize hybrid attention mechanisms. For example, Longformer [3], BigBird [51] and ETC [1] combine the sparsity and memory mechanisms; Synthesizers [39] combines the sparsity and low-rank mechanisms. Readers may refer to [42] and [41] for a comprehensive survey and benchmarks, respectively. \n\nIn this paper, we developed a 2-D version of Longformer [3], called Vision Longformer, which utilizes both the sparsity and memory mechanisms. Its conv-like sparsity mechanism is conceptually similar to the sparsity mechanism used in the Image Transformer [30]. \n\nThe multi-scale vision Transformer architecture is another technique we use in our proposed high-resolution Vision Longformer. The hierarchical Transformers [29] for NLP contain two stages, with the first stage processing overlapping segments and the second stage using the embeddings of the CLS tokens from all segments as input. In our proposed Vision Longformer, size reduction is performed by the patch embedding at the beginning of each stage, by merging all tokens in a patch from previous stage into a single token at the current stage. We typically use 4 stages for our model since we have empirically verified that using 4 stages is better than using 2 or 3 stages, especially for object detection tasks. Informer [55] takes a similar stacked multistage approach to encoding long sequences, where the size reduction between stages is achieved by max-pooling. \n\nPyramid Vision Transformer (PVT) [47], Swin Transformer [26] and HanoNet [44] are concurrent works of ours. All these works use a multi-scale architecture where multiple (slightly modified) ViTs are stacked. The authors of PVT propose the spatial-reduction attention (SRA) to alleviate the cost increase in self-attention layers.",
            "score": 0.5384711209052243,
            "section_title": "Related Work",
            "char_start_offset": 5118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "204852089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "274598006",
            "title": "Flex Attention: A Programming Model for Generating Optimized Attention Kernels",
            "text": "Many kernels have been manually implemented to efficiently support attention. Among these, the IO-aware FlashAttention (Dao, 2024) has become one of the most widely adopted solutions. FlashAttention significantly reduces memory access and achieves substantial speedups. Specifically, it avoids materializing the large score matrix S and computes it on the fly. Recently, Flash Attention v3 has been released to further accelerate attention by leveraging advanced hardware features and manually tuning the performance. While flash attention delivers state-of-the-art performance, it is specifically tailored towards a limited selection of attention variants (Table 1). One recent work FlashMask (Wang et al., 2024) extends Flash Attention with a column-wise sparse representation to support more mask designs. However, it still lacks of flexibility in terms of score modifications and adds large overhead for complex masks. Given that attention variants exhibit diverse computational characteristics such as sparsity and locality, significant manual effort is required to adapt existing attention implementations to support numerous attention variants. This lack of flexibility and efficient kernels has become a barrier for machine learning researchers seeking to explore novel attention variants.",
            "score": 0.5384367799218259,
            "section_title": "State-of-the-art Attention Implementations",
            "char_start_offset": 8146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1297
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 130,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 684,
                    "end": 713,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376708984375
        },
        {
            "corpus_id": "231698395",
            "title": "Does Dialog Length matter for Next Response Selection task? An Empirical Study",
            "text": "Recently, many new architectures have been proposed to address the challenge of scaling input length in the standard Transformer. Transformer-XL (Dai et al., 2019) introduce segment-level recurrence mechanism and relative positional encoding scheme. They reuse the hidden states for previous segments as memory for the current segment, to build a recurrent connection between the segments. Longformer (Beltagy et al., 2020) replaces the standard quadratic self-attention with an attention mechanism that scales linearly with sequence length. They propose a combination of a windowed local-context self-attention and an end-task motivated global attention to encode inductive bias about the task. Reformer (Kitaev et al., 2020) propose locality-sensitive hashing to reduce the sequence-length complexity and approximate the costly softmax computation in the full dot-product attention computation. Similar to Reformer, Performer Choromanski et al. (2020) estimate the softmax attention by using a Fast Attention Via positive Orthogonal Random features approach. \n\nBig Bird (Zaheer et al., 2020) propose a sparse attention mechanism that reduces the quadratic self-attention dependency to linear. Ainslie et al. (2020) introduce global-local attention mechanism between global tokens and regular input tokens. They combine the idea of Longformers and Randomized Attention which reduces quadratic dependency on the sequence length to linear, but requires additional layers for training.",
            "score": 0.5383284197269312,
            "section_title": "Group #2: New architectures",
            "char_start_offset": 6186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1483
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 163,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 705,
                    "end": 726,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1195,
                    "end": 1216,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47412109375
        },
        {
            "corpus_id": "249152218",
            "title": "Understanding Long Programming Languages with Structure-Aware Sparse Attention",
            "text": "There have been several studies [2,4,5,18,24] that process long sequences in NLP efficiently. BlockBERT [18] divides the attention matrix into  blocks and defines attention on each block, reducing the computational and memory cost to  ( 2 /). Sparse Transformer [4] and Longformer [2] employ sliding windows and global tokens to combine local and global information of input sequences. BigBird [24] extends random attention on top of Sparse Transformer. These models achieve time and memory savings without significant performance degradation but are not designed for code-related tasks. In this paper, we study processing long codes efficiently and making full use of code structures.",
            "score": 0.5382997556373605,
            "section_title": "Sparse Transformer",
            "char_start_offset": 5583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 685
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 39,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 39,
                    "end": 42,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 42,
                    "end": 45,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428466796875
        },
        {
            "corpus_id": "252873279",
            "title": "CPSAA: Accelerating Sparse Attention Using Crossbar-Based Processing-In-Memory Architecture",
            "text": "Attention-based neural network shows accuracy leaps in machine learning applications, e. g., natural language processing (NLP) [10] and computer vision [8]. Different from the commonly used Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN) models, Transformer [30] adopts a pure attention-based neural network to better identify the dependencies between tokens of the input sequence. Following this design, Transformer and its variants achieve great accuracy improvement in NLP tasks [10], such as machine translation [30] and question answering [6], etc. Attention is also widely used in computer vision tasks [8] including image classification [1] and object detection [18], etc. \n\nThe vanilla attention mechanism [30] is usually implemented as DDMM and softmax operations. By computing an attention score matrix, the attention mechanism can pay attention to these relevant token pairs. There is overwhelming computation pressure in processing these irrelevant token pairs, leading to intolerable execution time [7]. Researchers propose sparse attention by adding a sparsity pruning phase before the attention calculation to reduce irrelevant calculations [7], [19], since most tokens in the input sequence are unrelated to the current query. There are two types of sparse attention designs, i.e., software-based and software-hardware co-design methods [19]. Software-based methods [29], [38] aim to propose various optimization algorithms to reduce computational overhead by increasing sparsity. Software-hardware co-design solutions accelerate sparse attention by taking advantage of high-parallelism hardware, such as Field Programmable Gate Array (FPGA) [16], [40] and Application Specific Integrated Circuit (ASIC) [7], [19], [32]. \n\nThe above solutions only achieve limited speedups since both the sparsity pruning and attention calculation phases involve many off-chip data transfers. Emerging crossbar-based architectures are promising to solve the off-chip data transmission problem, such as Resistive Random Access Memory (ReRAM) and ReRAM-based content addressable memory (ReCAM) [12]. ReCAM is suitable for high parallel comparison, the core operation of content-based similarity search in the attention mechanism.",
            "score": 0.5379320086479349,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "236986986"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "236986986"
                },
                {
                    "start": 665,
                    "end": 668,
                    "matchedPaperCorpusId": "232404237"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 1182,
                    "end": 1186,
                    "matchedPaperCorpusId": "239012114"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "239012114"
                },
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "220633179"
                },
                {
                    "start": 1685,
                    "end": 1689,
                    "matchedPaperCorpusId": "237596895"
                },
                {
                    "start": 1741,
                    "end": 1744,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "239012114"
                },
                {
                    "start": 1752,
                    "end": 1756,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 2112,
                    "end": 2116,
                    "matchedPaperCorpusId": "2998769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498779296875
        },
        {
            "corpus_id": "236975917",
            "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
            "text": "The recent emergence of the Transformer has drastically reshaped the landscape of natural language processing research. Transformers have demonstrated superior performance in a wide variety of tasks, such as machine translation [1], text classification [2], question answering [3], automatic speech recognition [4], image generation [5], and image captioning [6]. The key innovation in Transformers is the introduction of a multi-head self-attention mechanism, which models the pairwise interactions between all positions in the sequence, regardless of their distance from each other. This operation has been shown quite effective.\n\nNonetheless, despite several notable successes of Transformers, computing the attention matrix, which is their key component, also turns out to be a major efficiency bottleneck due to its quadratic time and space complexity with respect to the sequence length. Therefore, the maximum sequence length is restricted by the amount of memory available. This inherent limitation of Transformers has prevented them from being successfully applied to domains requiring longer sequence lengths, like document classification. Furthermore, building large Transformer-based models in practice takes an enormous amount of time. Although fine-tuning pre-trained Transformers \u00a7 Equal contribution  Table III for more details. is relatively inexpensive, the memory issue still restricts the scenarios in which these models can be used. Besides the computational cost, qualitative analysis of attention heads [1] suggests that heads tend to favor flatter or more peaked distributions, depending on what phenomena they capture. Thus, using an extremely long sequence may limit the power of the model. To this end, a wide spectrum of efficient, fast Transformers has been proposed to tackle these limitations. For instance, [7]- [12] addresses the problematic complexity by limiting the number of keys that each query attends to. However, these methods either break long-term dependency or hurt time efficiency. There is also a long line of research on using dense attention matrix but defined by low-rank kernels substituting softmax [13]- [16]. Although these approaches have achieved better speed-memory-accuracy trade-off, they still suffer from the aforementioned limitations of the selfattention mechanism. Another prominent line of work is to increase the memory capacity [17]- [19]. However, these works still suffer from problematic complexity issues.\n\nBesides the computational cost, the self-attention mechanism of all models mentioned",
            "score": 0.5377882728677256,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 1526,
                    "end": 1529,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 2150,
                    "end": 2154,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 2156,
                    "end": 2160,
                    "matchedPaperCorpusId": "232105052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1263427734375
        },
        {
            "corpus_id": "254685525",
            "title": "Full Contextual Attention for Multi-resolution Transformers in Semantic Segmentation",
            "text": "Swin [24] and its variant [2,48] proposed to used shifted windows, Twins [9] uses interleaved fine and coarse resolu-Figure 2. The GLAM module for modeling full-range interaction in multi-resolution transformers. GLAM is included at each resolution level of any multi-resolution transformer architecture, e.g. Swin-Unet [24] or Swin-UperNet [24]. GLAM includes learnable global tokens, which are leveraged into a succession of two attention steps. We show that this design can indirectly represent long-range interactions between all image regions at all scales, and also external information useful for segmentation while retaining efficiency. We also introduce a non-local upsampling scheme (NLU) to extend the global context modeling in full transformer U-shape architectures such as [2,48]. tion transformers, and CvT [39] replaces linear embedding with convolutions. Efficient Self-Attention. Long sequences have been a challenge for transformers because the original self-attention mechanism has a quadratic complexity in the sequence length. To tackle this, many approaches focus on designing efficient self-attention mechanisms. \n\nMost of them are developed for NLP tasks and can be grouped into four categories. The first category uses a sparse approximation of the attention matrix [30,20,28]. Among these approaches, window-based patch extraction vision transformers recently provided a simple yet efficient approach to compute attention [24,37,13]. The second category is composed of methods based on a low-rank approximation of the attention matrix, such as Linformer [36]. The third category (memory-based transformers) construct buffers of extra tokens used as static memory [31,22]. The fourth category (kernel-based methods) provides a linear approximation of the softmax kernel [8,29,19]. Some vision transformers have combined multiple efficient attention mechanisms. The recent ViT-inspired backbone PvT [38] is based on windowed self-attention and attention approximation close to Linformer. ViL [43] balances sparse attention by using a reduced set of global tokens (usually a single one) to extract global representations of the input image.",
            "score": 0.5375830783363751,
            "section_title": "Related work",
            "char_start_offset": 5482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 9,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1296,
                    "end": 1299,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1799,
                    "end": 1802,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 1924,
                    "end": 1928,
                    "matchedPaperCorpusId": "232035922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21435546875
        },
        {
            "corpus_id": "273849947",
            "title": "LASER: Attention with Exponential Transformation",
            "text": "Routing Transformers (Roy et al., 2021) take a different approach by introducing a mechanism that sparsifies attention through data-dependent sparsity patterns with subquadratic computational complexity in sequence length. Similarly, Longformer (Beltagy et al., 2020) modifies the standard self-attention mechanism to handle long documents by combining local windowed attention with global attention. FlashAttention (Dao et al., 2022;2024) is a recent advancement that reduces HBM memory accesses by using GPU SRAM during attention computation, improving the speed of attention computation. LASER Attention can be thought of as complementing these approaches, as it conducts attention using the exponential transformation of inputs, without any change to the underlying attention function.",
            "score": 0.5366216121997562,
            "section_title": "RELATED WORK",
            "char_start_offset": 7684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 789
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 434,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "249151871",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "text": "We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention, whose IO complexity is smaller than FlashAttention by a factor proportional to the sparsity. \n\nGiven inputs Q, K, V \u2208 R  \u00d7 and a mask matrix M \u2208 {0, 1}  \u00d7 , we want to compute: \n\nwhere (S  M)  = S  if M = 1 and \u2212\u221e if M  = 0. We require M to have block form: for some block sizes   ,   , for all , , M, = M   with  = /  ,  = /  for some M \u2208 {0, 1}  /  \u00d7 /  . \n\nGiven a predefined block sparsity mask M \u2208 {0, 1}  /  \u00d7 /  we can easily adapt Algorithm 1 to only compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. \n\nWe also analyze the IO complexity of block-sparse FlashAttention. \n\nProposition 4. Let  be the sequence length,  be the head dimension, and  be size of SRAM with  \u2264  \u2264  . Block-sparse FlashAttention (Algorithm 5) requires \u0398(  +  2  2  \u22121 ) HBM accesses where  is the fraction of nonzero blocks in the block-sparsity mask. \n\nWe see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the IO complexity. For large sequence lengths ,  is often set to  \u22121/2 [11] or  \u22121 log  [3,17,92], resulting in \u0398( \u221a ) or \u0398( log ) IO complexity. For downstream experiments, we use the fixed butterfly sparsity pattern [17], which has been shown to be able to approximate arbitrary sparsity [16]. \n\nIn Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves 2.8\u00d7 speedup, while performing on par with standard attention (Section 4).",
            "score": 0.5363554720014734,
            "section_title": "Extension: Block-Sparse FlashAttention",
            "char_start_offset": 17041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1732
                }
            ],
            "ref_mentions": [
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "244773609"
                },
                {
                    "start": 1264,
                    "end": 1267,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "244773609"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "213704197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426513671875
        },
        {
            "corpus_id": "252239868",
            "title": "A Survey of Text Representation Methods and Their Genealogy",
            "text": "BigBird. Zaheer et al. [77] emphasize the low theoretical 1236 understanding of the self-attention operation of the original 1237 Transformer. Hence, they question the necessity of full self-1238 attention, which scales quadratically with sequence length, 1239 for good NLP performance. BigBird uses sparse matrix cal-1240 culations in three distinct attention patterns that retain the 1241 expressiveness and flexibility of the model while reducing 1242 computational complexity to be linear. The first pattern is ran-1243 dom attention. Here, each query attends to a random number 1244 of randomly chosen keys. The goal is to approximate some 1245 characteristics of the full self-attention. The second pattern is 1246 called window attention. It aims at capturing local relations 1247 between tokens as each token attends to a certain number of 1248 preceding and succeeding tokens. The third pattern is global 1249 attention, in which a specific added or a chosen existing token 1250 attends to and is attended to by every other token in the input. 1251 This captures sequence-level information in the embedding 1252 of global tokens.",
            "score": 0.5363333465545782,
            "section_title": "1235",
            "char_start_offset": 34430,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258544921875
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "In the field of multimodal large models, the Transformer architecture is widely used for its excellent performance and flexibility.However, as model size and application demands increase, the Transformer architecture faces challenges in computational complexity and memory bottlenecks.To address these challenges, researchers have proposed various optimization strategies and alternative architectures to enhance model efficiency and scalability.Besides Transformers, most other challenger architectures originate from recurrent neural networks (RNNs), including Gated Convolution, Temporal Convolutional Networks (TCN), RWKV, Mamba, and S4, which replace attention with recurrent structures.This approach uses fixed memory to remember previous information, although it can remember a certain length, achieving longer lengths is challenging.Another approach is improving Transformers, such as linear attention improvements mentioned earlier.Representative models include Mega, Yan, and JEPA.We will introduce some representative approaches among them.\n\nThe RWKV model [22] uses linear attention mechanisms, allowing the model to parallelize computations during training and maintain constant computational and memory complexity during inference.The RWKV model consists of stacked residual blocks, each containing time-mixing and channelmixing sub-blocks, using a recurrent structure to leverage past information.The authors trained RWKV models with sizes ranging from 169 million to 14 billion parameters, making it the largest dense RNN trained to date.Experimental results show that RWKV performs comparably to Transformers of similar size, indicating future work can utilize this architecture to create more efficient models.However, RWKV models have some limitations, such as linear attention potentially limiting performance on tasks requiring long-term dependencies.\n\nThe Mega model [23] introduces sparse attention mechanisms, zeroing out most elements in the attention matrix and retaining only a few important attention values.This method significantly reduces computational load and memory usage while maintaining predictive performance.Similar to Longformer and Sparse Transformer, the Mega model has unique optimizations in sparse strategies and implementations.By using sparse attention mechanisms, the Mega model greatly reduces computational complexity and memory usage, making it more efficient in handling long-sequence tasks.\n\nJEPA (Joint Embedding Predictive Architecture) [24] is a novel machine learning model designed to optimize complex tasks and large-scale problem handling through hierarchical decision-making and control methods.",
            "score": 0.5362771063966858,
            "section_title": "H. Other Challengers to Model Architectures",
            "char_start_offset": 34423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1051
                },
                {
                    "start": 1053,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1412
                },
                {
                    "start": 1412,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1872
                },
                {
                    "start": 1874,
                    "end": 2036
                },
                {
                    "start": 2036,
                    "end": 2147
                },
                {
                    "start": 2147,
                    "end": 2274
                },
                {
                    "start": 2274,
                    "end": 2443
                },
                {
                    "start": 2445,
                    "end": 2656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36376953125
        },
        {
            "corpus_id": "236924765",
            "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
            "text": "The self-attention mechanism is used to learn long-range dependencies while enabling parallel processing of the input sequence. For a given input sequence X := [x 1 , x 2 , \u2022 \u2022 \u2022 , x N ] \u2208 R N \u00d7Dx of N feature vectors that have been encoded in a D x -dimensional vector space, self-attention transforms X into an output sequence V in the following two steps: \n\nStep 1. Project the input sequence X into three matrices via the following linear transformations \n\nwhere W Q , W K \u2208 R D\u00d7Dx , and W V \u2208 R Dv\u00d7Dx are the weight matrices. We denote \n\nwhere the vectors q i , k i , v i for i = 1, \u2022 \u2022 \u2022 , N are the query, key, and value vectors, respectively. \n\nStep 2. For each query vector q i for i = 1, \u2022 \u2022 \u2022 , N , we compute the output vector vi as follows \n\nwhere the softmax function is applied to each row of the matrix (QK )/ \u221a D. \n\nFor long sequences, the computational time and memory footprint of transformers are dominated by (1). It is evident that the memory cost is O(N 2 ) to store the attention matrix A. Also, the computational complexities of computing the matrix-matrix products QK and AV are both O(N 2 ). These limitations impede the application of transformers to many important settings that involve very long sequences [38,28,46]. When applying self-attention for long sequence modeling, we have to limit the context window to a reasonable size to make it computationally feasible, limiting the effectiveness of learning long-term dependencies. Efficient transformer models have been proposed, including leveraging sparse and low-rank attention. Many of the existing efficient transformers gain computational and memory efficiency at the cost of significant accuracy degradation.",
            "score": 0.536200933690633,
            "section_title": "Self-attention",
            "char_start_offset": 824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "matchedPaperCorpusId": "3353110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1973876953125
        },
        {
            "corpus_id": "258854013",
            "title": "Classifying European Court of Human Rights Cases Using Transformer-Based Techniques",
            "text": "RoBERTa builds upon BERT by effectively fine-tuning BERT with further training. RoBERTa is essentially BERT retrained over 160GB of additional text. Adding to the baseline of Wikipedia and the corpus of books that BERT was trained on RoBERTa was further trained on CommonCrawl News Data, stories, and text from OpenAI GPT [32]. \n\nBigBird: was proposed by [33] and is a sparse-attentionbased transformer that extends the usual Transformer based models like BERT for longer sequences. \n\nBigBird uses sparse attention but also global attention and random attention on the input sequence. This is due to the theory of using all three attention types: sparse, global, and random attention, which equals full attention while being much more computationally efficient on longer sequences. This makes BigBird an excellent fit for modeling a network on long NLP tasks.",
            "score": 0.5361284129936367,
            "section_title": "2) TRANSFORMER-BASED NEURAL NETWORKS",
            "char_start_offset": 23748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "277780735",
            "title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
            "text": "The advent of the transformer architecture [14] marked a paradigm shift in natural language processing (NLP). Its core innovation, the self-attention mechanism, enables models to weigh the importance of different tokens within an input sequence when computing the representation for each token. This allows for the direct modeling of long-range dependencies, overcoming limitations of previous recurrent and convolutional architectures and leading to groundbreaking performance in tasks like machine translation, text summarization, and question answering. \n\nDespite its success, the standard dot-product self-attention mechanism suffers from a critical drawback: its computational and memory complexity scales quadratically, O(T 2 ), with the sequence length T . This quadratic scaling poses a significant bottleneck when dealing with long documents, highresolution images (when adapted for vision), or applications requiring real-time processing. \n\nRecognizing this limitation, the research community has actively explored more efficient alternatives. Notable approaches include: \n\n\u2022 Sparse Attention: Methods like Longformer [1] and BigBird [16] use predefined sparsity patterns (e.g., sliding window, global tokens, random patterns) to reduce the number of computed attention scores. \n\n\u2022 Linearized Attention: Techniques such as Linformer [15] project the key and value matrices to lower dimensions before the attention computation, achieving linear complexity. Performers [2] use random feature maps to approximate the softmax kernel, also resulting in linear complexity. \n\n\u2022 Fourier Transforms: FNet [11] replaces self-attention entirely with unparameterized Fourier Transforms, mixing tokens with O(T log T ) complexity. Recent work like SFDLM [9] utilizes state Fourier diffusion models, demonstrating competitive results without attention's quadratic cost for long sequences. \n\n\u2022 Hybrid Approaches: Some methods combine different techniques, like the hybrid Wavelet-Fourier approach for image generation [8]. \n\n\u2022 Other Mechanisms: State Space Models (SSMs) and specialized convolutional or recurrent structures have also emerged as alternatives [5,4,3].",
            "score": 0.5360935263687797,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1144,
                    "end": 1148,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1477,
                    "end": 1480,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1606,
                    "end": 1610,
                    "matchedPaperCorpusId": "234336004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "270391996",
            "title": "Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences",
            "text": "Efficient transformer models A variety of efforts have been made to decrease the quadratic time and space complexity of standard attention mechanisms.One method is to utilize \"sparse attention,\" where each token only attends to a subset of all the tokens based on predefined patterns, such as neighboring tokens within a fixed-size window.(Child et al., 2019) started the attempt to sparse the attention, and then there were a lot more followers, such as ETC (Ainslie et al., 2020), Longformer (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), Poolingformer (Zhang et al., 2021), and HEPOS (Huang et al., 2021) are some examples of this approach.Another option is to utilize \"lowrank projection,\" as mentioned in the work by (Wang et al., 2020).Similar techniques include Nystr\u00f6mformer (Xiong et al., 2021), Synthesizer (Tay et al., 2021), and Luna (Ma et al., 2021).However, these methods encounter challenges when dealing with causal tasks, such as auto-regressive language modeling.Another approach uses \"clustering method,\" where we partition Q or K into multiple clusters and perform inter-cluster attention.Examples of such methods include Sinkhorn Transformer (Tay et al., 2020a), Reformer (Kitaev et al., 2020), Routing Transformer (Roy et al., 2021), and simplified FLASH (Hua et al., 2022), etc. \"Methods based on kernels\" can be utilized to approximate the complete attention Attn(X).These methods replace the quadratic-time softmax attention with fast linear-time kernel approximations (such as Gaussian and arc-cosine kernels).Some instances of this approach include Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020), and FMMformer (Nguyen et al., 2021), etc.Both low-dimensional projection and methods based on kernels are employed to estimate full attention and, as a result, are vulnerable to significant approximation errors.",
            "score": 0.5359994612933922,
            "section_title": "Related Works",
            "char_start_offset": 22127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1118
                },
                {
                    "start": 1118,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 481,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 526,
                    "end": 547,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 563,
                    "end": 583,
                    "matchedPaperCorpusId": "234339280"
                },
                {
                    "start": 595,
                    "end": 615,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 791,
                    "end": 811,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1172,
                    "end": 1191,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1245,
                    "end": 1263,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1286,
                    "end": 1304,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 1604,
                    "end": 1632,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1686,
                    "end": 1707,
                    "matchedPaperCorpusId": "236924765"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.395751953125
        },
        {
            "corpus_id": "234336004",
            "title": "FNet: Mixing Tokens with Fourier Transforms",
            "text": "The standard self-attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in text tasks involving long range dependencies, character-level modelling, speech processing, image and video processing.\n\nMost efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020d) survey many of the recent efficient attention works, which they broadly categorize as: (1) Data independent sparsity approaches that use fixed attention patterns (Child et al., 2019;Qiu et al., 2019;Parmar et al., 2018;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020); (2) Data dependent sparsity approaches that dynamically compress the attention matrix Tay et al., 2020b,a;Kitaev et al., 2020;Vyas et al., 2020;; and (3) Linearization of the attention matrix using kernel decompositions (Katharopoulos et al., 2020;Choromanski et al., 2020b;Peng et al., 2021). Several of these works achieve O(N \u221a N ) or even O(N ) theoretical complexity. However, the scaling constants hidden by this notation can be large. For example, in models such as Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), andBigBird (Zaheer et al., 2020), attention is O(N ) as a function of the input length, but quadratic in the number of \"global tokens\"; the latter must be sufficiently large to ensure good performance.\n\nThe Long-Range Arena benchmark (Tay et al., 2020c) attempts to compare many of the above \"efficient\" Transformers in a series of tasks requiring long range dependencies, finding that the Performer (Choromanski et al., 2020b), Linear Transformer (Katharopoulos et al., 2020), Linformer , and Image Transformer (Local Attention) (Parmar et al., 2018) were the fastest on 4 \u00d7 4 TPU v3",
            "score": 0.53590726928359,
            "section_title": "Efficient Transformers and long sequence models",
            "char_start_offset": 9304,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 621,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 642,
                    "end": 663,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1197,
                    "end": 1224,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "273186571",
            "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective",
            "text": "Transformer-XL [83] adopts a segment-level recurrence mechanism and a novel positional encoding scheme to learn dependencies beyond a fixed length without disrupting temporal coherence. Linear Transformer [84] represents selfattention as a linear dot product of kernel feature maps and alters the computation order by leveraging the associativity of matrix multiplication. This modification reduces the complexity from O(L 2 ) to O(L), where L is the context length, significantly accelerating the computation of autoregressive Transformers. Another efficient structure is the Attention-Free Transformer (AFT) [85]. Unlike vanilla transformers, which first compute the query-key product, AFT combines the key and value with a set of learned positional biases before performing element-wise multiplication with the query. As a result, the memory complexity of AFT is linear with respect to both the context size and feature dimensions, enabling support for larger input lengths and model sizes. Based on AFT, the Receptance Weighted Key Value (RWKV) [86] combines the efficient parallel training capabilities of Transformers with the efficient inference of RNNs. It leverages linear attention mechanisms and allows the model to be expressed as either a transformer or an RNN. It also enables parallel computation during training while maintaining constant computational and memory complexity during inference. DiJiang [87] introduces a novel frequency-domain kernelization method based on the Discrete Cosine Transform (DCT). It points out that improving attention mechanisms often requires extensive retraining, which is impractical for large language models with vast numbers of parameters. This approach enables the conversion of a pre-trained standard Transformer into a model with linear complexity and low training costs, utilizing a weighted quasi-Monte Carlo method for sampling. Extensive experiments demonstrate that this method achieves performance comparable to the vanilla transformer while significantly reducing training costs and substantially increasing inference speed.",
            "score": 0.535795515791754,
            "section_title": "Transformer-based LLM",
            "char_start_offset": 10176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1573486328125
        },
        {
            "corpus_id": "270063820",
            "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",
            "text": "A gating mechanism is utilized to smooth training, and a new tensor normalization scheme is proposed to accelerate the model while preserving its accuracy.We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models.As shown in Fig. 1, TNL achieves the lowest training loss among the existing efficient transformer structures (Qin et al., 2023a;c) as well as SOTA transformer models (Touvron et al., 2023b).\n\nWe perform a comprehensive evaluation of Lightning Attention across a diverse range of sequence lengths to assess its accuracy and compare its computational speed and memory utilization with FlashAttention-2 (Dao, 2023).Lightning Attention exhibits a notable advantage in computational speed and memory consumption compared to its counterparts without compromising performance.We also validate our model design through a series of ablations and train models with sizes of 44M, 385M, 1B, 7B, and 15B on standard or our self-collected datasets.Benchmark results demonstrate that TNL not only matches the performance of SOTA LLMs with Transformer but is also significantly faster.Although its theoretical complexity is O(nd 2 ), the actual computational efficiency of linear attention becomes low when used in causal attention due to the need for cumsum operations (Hua et al., 2022).Moreover, most linear attention still exhibits a certain performance gap compared to traditional Transformers (Katharopoulos et al., 2020;Liu et al., 2022).",
            "score": 0.5353888935571984,
            "section_title": "Introduction",
            "char_start_offset": 3674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 338
                },
                {
                    "start": 338,
                    "end": 529
                },
                {
                    "start": 531,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1412
                },
                {
                    "start": 1412,
                    "end": 1568
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 467,
                    "matchedPaperCorpusId": "258557266"
                },
                {
                    "start": 1522,
                    "end": 1550,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "268437153",
            "title": "SWattention: designing fast and memory-efficient attention for a new Sunway Supercomputer",
            "text": "In the past few years, Transformer-based large language models (LLM) have become the dominant technology in a series of applications. To scale up the sequence length of the Transformer, FlashAttention is proposed to compute exact attention with reduced memory requirements and faster execution. However, implementing the FlashAttention algorithm on the new generation Sunway Supercomputer faces many constraints such as the unique heterogeneous architecture and the limited memory bandwidth. This work proposes SWattention, a highly efficient method for computing the exact attention on the SW26010pro processor. To fully utilize the 6 core groups (CG) and 64 cores per CG on the processor, we design a two-level parallel task partition strategy. Asynchronous memory access is employed to ensure that memory access overlaps with computation. Additionally, a tiling strategy is introduced to determine optimal SRAM block sizes. Compared with the standard attention, SWattention achieves around 2.0x speedup for FP32 training and 2.5x speedup for mixed-precision training. The sequence lengths range from 1k to 8k and scale up to 16k without being out of memory. As for the end-to-end performance, SWattention achieves up to 1.26x speedup for training GPT-style models, which demonstrates that SWattention enables longer sequence length for LLM training.",
            "score": 0.5345724789695423,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2042236328125
        },
        {
            "corpus_id": "239885427",
            "title": "Hierarchical Transformers Are More Efficient Language Models",
            "text": "relevant because the method is compatible with any attention type, including efficient attention, and can be combined with our model (Section 3.4.2).\n\nSparse Attention A well-known approach addressing the memory bottleneck is utilizing sparsity patterns in the attention matrix -Routing (Roy et al., 2020) and Sparse Transformer (Child et al., 2019) are examples of such methods. Our solution is different in the sense that it uses full attention -just with shortened sequence length. Combiner (Ren et al., 2021) makes a step further and provides full attention capabilities with similar computational complexity to Routing and Sparse transformers by leveraging structured factorization. This work, similarly to papers mentioned above on efficient transformers, concentrates on speeding up the attention component, while the most important feature of the Hourglass architecture is that it can use any attention module as a drop-in.\n\nImage generation on downsampled ImageNet VDM (Kingma et al., 2021) and DenseFlow (Grci\u0107 et al., 2021) are recently proposed state-of-the-art methods for density estimation on this dataset. The difference between these methods and Transformerbased methods (Parmar et al., 2018;Ho et al., 2019) including this work is that the former, unlike Transformers, are non-autoregressive.",
            "score": 0.5343643158905176,
            "section_title": "Related Work",
            "char_start_offset": 21626,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308837890625
        },
        {
            "paperId": "c3c469b8d3392aa1117e1d82bd3357d2c12d87ce",
            "corpusId": 269208179,
            "title": "Raptor-T: A Fused and Memory-Efficient Sparse Transformer for Long and Variable-Length Sequences",
            "venue": "IEEE transactions on computers",
            "year": 2024,
            "referenceCount": 40,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TC.2024.3389507?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TC.2024.3389507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295845990",
                    "name": "Hulin Wang"
                },
                {
                    "authorId": "2007708782",
                    "name": "Donglin Yang"
                },
                {
                    "authorId": "2111131052",
                    "name": "Yaqi Xia"
                },
                {
                    "authorId": "2274734490",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2297058601",
                    "name": "Qigang Wang"
                },
                {
                    "authorId": "2260856326",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2278825938",
                    "name": "Xiaobo Zhou"
                },
                {
                    "authorId": "1778706",
                    "name": "Dazhao Cheng"
                }
            ],
            "abstract": "Transformer-based models have made significant advancements across various domains, largely due to the self-attention mechanism's ability to capture contextual relationships in input sequences. However, processing long sequences remains computationally expensive for Transformer models, primarily due to the <inline-formula><tex-math notation=\"LaTeX\">$O(n^{2})$</tex-math><alternatives><mml:math><mml:mi>O</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3389507.gif\"/></alternatives></inline-formula> complexity associated with self-attention. To address this, sparse attention has been proposed to reduce the quadratic dependency to linear. Nevertheless, deploying the sparse transformer efficiently encounters two major obstacles: 1) Existing system optimizations are less effective for the sparse transformer due to the algorithm's approximation properties leading to fragmented attention, and 2) the variability of input sequences results in computation and memory access inefficiencies. We present Raptor-T, a cutting-edge transformer framework designed for handling long and variable-length sequences. Raptor-T harnesses the power of the sparse transformer to reduce resource requirements for processing long sequences while also implementing system-level optimizations to accelerate inference performance. To address the fragmented attention issue, Raptor-T employs fused and memory-efficient Multi-Head Attention. Additionally, we introduce an asynchronous data processing method to mitigate GPU-blocking operations caused by sparse attention. Furthermore, Raptor-T minimizes padding for variable-length inputs, effectively reducing the overhead associated with padding and achieving balanced computation on GPUs. In evaluation, we compare Raptor-T's performance against state-of-the-art frameworks on an NVIDIA A100 GPU. The experimental results demonstrate that Raptor-T outperforms FlashAttention-2 and FasterTransformer, achieving an impressive average end-to-end performance improvement of 3.41X and 3.71X, respectively.",
            "corpus_id": "269208179",
            "text": "Transformer-based models have made significant advancements across various domains, largely due to the self-attention mechanism's ability to capture contextual relationships in input sequences. However, processing long sequences remains computationally expensive for Transformer models, primarily due to the <inline-formula><tex-math notation=\"LaTeX\">$O(n^{2})$</tex-math><alternatives><mml:math><mml:mi>O</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3389507.gif\"/></alternatives></inline-formula> complexity associated with self-attention. To address this, sparse attention has been proposed to reduce the quadratic dependency to linear. Nevertheless, deploying the sparse transformer efficiently encounters two major obstacles: 1) Existing system optimizations are less effective for the sparse transformer due to the algorithm's approximation properties leading to fragmented attention, and 2) the variability of input sequences results in computation and memory access inefficiencies. We present Raptor-T, a cutting-edge transformer framework designed for handling long and variable-length sequences. Raptor-T harnesses the power of the sparse transformer to reduce resource requirements for processing long sequences while also implementing system-level optimizations to accelerate inference performance. To address the fragmented attention issue, Raptor-T employs fused and memory-efficient Multi-Head Attention. Additionally, we introduce an asynchronous data processing method to mitigate GPU-blocking operations caused by sparse attention. Furthermore, Raptor-T minimizes padding for variable-length inputs, effectively reducing the overhead associated with padding and achieving balanced computation on GPUs. In evaluation, we compare Raptor-T's performance against state-of-the-art frameworks on an NVIDIA A100 GPU. The experimental results demonstrate that Raptor-T outperforms FlashAttention-2 and FasterTransformer, achieving an impressive average end-to-end performance improvement of 3.41X and 3.71X, respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.40234375
        },
        {
            "paperId": "e125b9231ed304e7cbae4e264216b7d6930cb8e2",
            "corpusId": 271693511,
            "title": "An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding",
            "venue": "European Conference on Computer Vision",
            "year": 2024,
            "referenceCount": 53,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.01120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314839401",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2306951870",
                    "name": "Long Chen"
                },
                {
                    "authorId": "2307035320",
                    "name": "Yu Wu"
                }
            ],
            "abstract": "Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual features as queries. This allows fusion to scale linearly with language expression length. In the visual aspect, we introduce a parameter-free approach to reduce computation by eliminating background visual tokens based on attention scores. We then design a light mask head to directly predict segmentation masks from the remaining sparse feature maps. Extensive results and ablation studies on benchmarks demonstrate the efficiency and effectiveness of our approach. Code is available in https://github.com/chenwei746/EEVG.",
            "corpus_id": "271693511",
            "text": "Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual features as queries. This allows fusion to scale linearly with language expression length. In the visual aspect, we introduce a parameter-free approach to reduce computation by eliminating background visual tokens based on attention scores. We then design a light mask head to directly predict segmentation masks from the remaining sparse feature maps. Extensive results and ablation studies on benchmarks demonstrate the efficiency and effectiveness of our approach. Code is available in https://github.com/chenwei746/EEVG.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0728759765625
        },
        {
            "paperId": "896d5a81f2861b79776a57379a115da85485338f",
            "corpusId": 278756182,
            "title": "A 52.03TOPS/W DCIM-Based Accelerator with FlashAttention and Sparsity-Aware Alignment for LLMs",
            "venue": "IEEE Custom Integrated Circuits Conference",
            "year": 2025,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CICC63670.2025.10982870?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CICC63670.2025.10982870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342976863",
                    "name": "Bo Liu"
                },
                {
                    "authorId": "2310949430",
                    "name": "Xingyuan Xu"
                },
                {
                    "authorId": "2266840598",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2348646880",
                    "name": "Xilong Kang"
                },
                {
                    "authorId": "2267008828",
                    "name": "Qingwen Wei"
                },
                {
                    "authorId": "2298463999",
                    "name": "Zihan Zou"
                },
                {
                    "authorId": "2283159995",
                    "name": "Jun Yang"
                },
                {
                    "authorId": "2276968370",
                    "name": "Hao Cai"
                },
                {
                    "authorId": "2283151092",
                    "name": "Xin Si"
                }
            ],
            "abstract": "Decoder-based large language models (LLMs) are designed specifically for text generation, consisting primarily of multiple decoder layers that utilize unidirectional attention mechanism [1], as shown in Fig. 1. Due to the enormous number of parameters and the complexity of the attention mechanism, LLMs require substantial computational and memory resources in deployment for resource-constrained edge devices. Algorithms and model compression methods such as KV Cache and pruning have been proposed to reduce the workload, achieving remarkable success [2], [3]. However, high-efficient hardware implementation such as sparsity exploitation [14] still remains unexplored. Meanwhile, Digital Computing-in-Memory (DCIM) can reduce the computation and memory demands by minimizing data movement and enabling more efficient parallel processing directly within the memory, making it suitable for memory-bound LLMs [6\u20139, 11\u201313]. Designing DCIM-based LLM accelerators faces the following challenges: 1) The KV Cache size scales linearly with the context length, which induces a mismatch between high storage demand and on-chip memory size, as well as frequent external memory access. 2) Scaling the CIM Macro size can reduce computation complexity from the FlashAttention (FA) mechanism [4], [5], but tiling size in FA introduces imbalanced workload distribution in CIM Macro computation stages, exacerbating inefficiencies in resource utilization. 3) Unstructured sparsity brought by model pruning causes low spatial and temporal utilization of CIM Macro, which compromises hardware efficiency.",
            "corpus_id": "278756182",
            "text": "Decoder-based large language models (LLMs) are designed specifically for text generation, consisting primarily of multiple decoder layers that utilize unidirectional attention mechanism [1], as shown in Fig. 1. Due to the enormous number of parameters and the complexity of the attention mechanism, LLMs require substantial computational and memory resources in deployment for resource-constrained edge devices. Algorithms and model compression methods such as KV Cache and pruning have been proposed to reduce the workload, achieving remarkable success [2], [3]. However, high-efficient hardware implementation such as sparsity exploitation [14] still remains unexplored. Meanwhile, Digital Computing-in-Memory (DCIM) can reduce the computation and memory demands by minimizing data movement and enabling more efficient parallel processing directly within the memory, making it suitable for memory-bound LLMs [6\u20139, 11\u201313]. Designing DCIM-based LLM accelerators faces the following challenges: 1) The KV Cache size scales linearly with the context length, which induces a mismatch between high storage demand and on-chip memory size, as well as frequent external memory access. 2) Scaling the CIM Macro size can reduce computation complexity from the FlashAttention (FA) mechanism [4], [5], but tiling size in FA introduces imbalanced workload distribution in CIM Macro computation stages, exacerbating inefficiencies in resource utilization. 3) Unstructured sparsity brought by model pruning causes low spatial and temporal utilization of CIM Macro, which compromises hardware efficiency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.178955078125
        }
    ],
    "quotes": {
        "cost": 0.28158299999999997,
        "quotes": [
            {
                "idx": 0,
                "key": "[220831004 | Zaheer et al. | 2020 | Citations: 2103]",
                "snippets": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 876,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[221140187 | Yoshida et al. | 2020 | Citations: 7]",
                "snippets": "Other methods modify the attention function to reduce the quadratic memory footprint down to a manageable amount. Child et al. (2019) modify the transformer architecture to replace the standard attention with a sparse one. Qiu et al. (2019) enforce a block-sparse structure on the attention matrix. (Kitaev et al., 2020) also introduce sparsity, but instead do so by using locality sensitive hashing to select positions over which a full attention is computed, reducing the memory cost from quadratic to O(T log T ) for an input of size T . Rae et al. ( 2019) introduce a memory compression technique that allows much longer contexts to be attended to in memory. Beltagy et al. (2020) replace the standard attention with a combination of dilated sliding windows, and global attention from selected tokens that. (Sukhbaatar et al., 2019) learn a masking function such that not all tokens attend to every previous position. Tay et al. (2020) learn synthetic attention weights, removing the need for token-token interactions. Wu et al. (2019) replace the full self-attention with a dynamic convolution depending only on the current timestep, yielding a linear dependence on length instead of a quadratic dependence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[159041867 | Sukhbaatar et al. | 2019 | Citations: 286]": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",
                    "[209315300 | Kitaev et al. | 2020 | Citations: 2333]": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 588,
                        "end": 1798,
                        "sentence_offsets": [
                            {
                                "start": 588,
                                "end": 701
                            },
                            {
                                "start": 702,
                                "end": 810
                            },
                            {
                                "start": 811,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 1127
                            },
                            {
                                "start": 1128,
                                "end": 1249
                            },
                            {
                                "start": 1250,
                                "end": 1397
                            },
                            {
                                "start": 1398,
                                "end": 1507
                            },
                            {
                                "start": 1508,
                                "end": 1608
                            },
                            {
                                "start": 1609,
                                "end": 1798
                            }
                        ],
                        "ref_mentions": [
                            "209315300",
                            "159041867"
                        ],
                        "quote": "Other methods modify the attention function to reduce the quadratic memory footprint down to a manageable amount. Child et al. (2019) modify the transformer architecture to replace the standard attention with a sparse one. Qiu et al. (2019) enforce a block-sparse structure on the attention matrix. (Kitaev et al., 2020) also introduce sparsity, but instead do so by using locality sensitive hashing to select positions over which a full attention is computed, reducing the memory cost from quadratic to O(T log T ) for an input of size T . Rae et al. ( 2019) introduce a memory compression technique that allows much longer contexts to be attended to in memory. Beltagy et al. (2020) replace the standard attention with a combination of dilated sliding windows, and global attention from selected tokens that. (Sukhbaatar et al., 2019) learn a masking function such that not all tokens attend to every previous position. Tay et al. (2020) learn synthetic attention weights, removing the need for token-token interactions. Wu et al. (2019) replace the full self-attention with a dynamic convolution depending only on the current timestep, yielding a linear dependence on length instead of a quadratic dependence."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[229923177 | Ding et al. | 2021 | Citations: 55]",
                "snippets": "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209315300 | Kitaev et al. | 2020 | Citations: 2333]": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1405,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "209315300"
                        ],
                        "quote": "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[233307400 | Novotny et al. | 2021 | Citations: 4]",
                "snippets": "Since the dense attention mechanism learns weights for all pairs of source and target words, its space complexity is O(n 2 ) in the source sequence length. Several sparse attention architectures have been proposed in literature to enable the translation of longer source sequences by making the space complexity O(n). \n\nChild et al. [Chi+[19] proposed the Sparse Transformer architecture, which factorized the dense attention using p separate attention heads to learn only O(n\u2022 p \u221a n) weights. They showed that the resulting model could use larger context sizes and achieved significantly better results than Transformers on density modeling tasks. \n\nFollowing the success of Sparse Transformers, Beltagy, Peters, Cohan [BPC[20] proposed the Longformer architecture, which reduced the number of attention weights to O(n) and achieved significantly better results than Transformers on multiple long document tasks including question answering, coreference resolution, and classification. \n\nFinally, Zaheer et al. [Zah+[20] proposed the BigBird architecture. Like Longformers, BigBird also used O(n) weights. Unlike Longformers, BigBird has been shown to be a Turing-complete universal approximator. Although attention enabled the recollection of long-range memories, sparse attention made it computationally tractable to do so.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Sparse attention",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1323,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 155
                            },
                            {
                                "start": 156,
                                "end": 317
                            },
                            {
                                "start": 320,
                                "end": 492
                            },
                            {
                                "start": 493,
                                "end": 647
                            },
                            {
                                "start": 650,
                                "end": 984
                            },
                            {
                                "start": 987,
                                "end": 1053
                            },
                            {
                                "start": 1054,
                                "end": 1103
                            },
                            {
                                "start": 1104,
                                "end": 1194
                            },
                            {
                                "start": 1195,
                                "end": 1323
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Since the dense attention mechanism learns weights for all pairs of source and target words, its space complexity is O(n 2 ) in the source sequence length. Several sparse attention architectures have been proposed in literature to enable the translation of longer source sequences by making the space complexity O(n). \n\nChild et al. [Chi+[19] proposed the Sparse Transformer architecture, which factorized the dense attention using p separate attention heads to learn only O(n\u2022 p \u221a n) weights. They showed that the resulting model could use larger context sizes and achieved significantly better results than Transformers on density modeling tasks. \n\nFollowing the success of Sparse Transformers, Beltagy, Peters, Cohan [BPC[20] proposed the Longformer architecture, which reduced the number of attention weights to O(n) and achieved significantly better results than Transformers on multiple long document tasks including question answering, coreference resolution, and classification. \n\nFinally, Zaheer et al. [Zah+[20] proposed the BigBird architecture. Like Longformers, BigBird also used O(n) weights. Unlike Longformers, BigBird has been shown to be a Turing-complete universal approximator. Although attention enabled the recollection of long-range memories, sparse attention made it computationally tractable to do so."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[237260051 | Wu et al. | 2021 | Citations: 3]",
                "snippets": "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms...First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Performance Evaluation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 570,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 139
                            },
                            {
                                "start": 142,
                                "end": 571
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms"
                    },
                    {
                        "section_title": "Performance Evaluation",
                        "pdf_hash": "",
                        "start": 909,
                        "end": 1623,
                        "sentence_offsets": [
                            {
                                "start": 909,
                                "end": 1136
                            },
                            {
                                "start": 1137,
                                "end": 1339
                            },
                            {
                                "start": 1340,
                                "end": 1518
                            },
                            {
                                "start": 1519,
                                "end": 1622
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[248427085 | Soydaner | 2022 | Citations: 166]",
                "snippets": "Sparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight (Correia et al., 2019). By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods (Peng et al., 2021). Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method (Chen et al., 2021). A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear (Zaheer et al., 2020). Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[202538495 | Correia et al. | 2019 | Citations: 256]": "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.",
                    "[232105052 | Peng et al. | 2021 | Citations: 362]": "Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
                    "[240354799 | Chen et al. | 2021 | Citations: 51]": "Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nystr\\\"om method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources."
                },
                "metadata": [
                    {
                        "section_title": "What about complexity?",
                        "pdf_hash": "",
                        "start": 166,
                        "end": 1303,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "202538495",
                            "232105052",
                            "240354799",
                            "220831004"
                        ],
                        "quote": "Sparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight (Correia et al., 2019). By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods (Peng et al., 2021). Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method (Chen et al., 2021). A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear (Zaheer et al., 2020). Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[249151871 | Dao et al. | 2022 | Citations: 2285]",
                "snippets": "FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[252216464 | Chen et al. | 2022 | Citations: 50]",
                "snippets": "Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,(Guo et al., 2019)29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer (Guo et al., 2019) replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209009596 | Guo et al. | 2019 | Citations: 263]": "Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data. In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification. To reduce model complexity, we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. Thus, complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency. The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets."
                },
                "metadata": [
                    {
                        "section_title": "Sparse Transformer",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 924,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 176
                            },
                            {
                                "start": 177,
                                "end": 298
                            },
                            {
                                "start": 299,
                                "end": 405
                            },
                            {
                                "start": 406,
                                "end": 592
                            },
                            {
                                "start": 593,
                                "end": 668
                            },
                            {
                                "start": 669,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 924
                            }
                        ],
                        "ref_mentions": [
                            "209009596",
                            "209009596"
                        ],
                        "quote": "Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,(Guo et al., 2019)29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer (Guo et al., 2019) replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[254725259 | Bae et al. | 2022 | Citations: 0]",
                "snippets": "In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                },
                "metadata": [
                    {
                        "section_title": "B. SPARSE ATTENTION",
                        "pdf_hash": "",
                        "start": 92,
                        "end": 837,
                        "sentence_offsets": [
                            {
                                "start": 92,
                                "end": 278
                            },
                            {
                                "start": 279,
                                "end": 331
                            },
                            {
                                "start": 332,
                                "end": 387
                            },
                            {
                                "start": 388,
                                "end": 489
                            },
                            {
                                "start": 490,
                                "end": 634
                            },
                            {
                                "start": 635,
                                "end": 837
                            }
                        ],
                        "ref_mentions": [
                            "220831004"
                        ],
                        "quote": "In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[258762176 | KABENAMUALU et al. | 2023 | Citations: 20]",
                "snippets": "BigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle (Zaheer et al., 2020). BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) (Zaheer et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                },
                "metadata": [
                    {
                        "section_title": "BigBird",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 792,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 126
                            },
                            {
                                "start": 127,
                                "end": 274
                            },
                            {
                                "start": 275,
                                "end": 414
                            },
                            {
                                "start": 415,
                                "end": 634
                            },
                            {
                                "start": 635,
                                "end": 792
                            }
                        ],
                        "ref_mentions": [
                            "220831004",
                            "220831004"
                        ],
                        "quote": "BigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle (Zaheer et al., 2020). BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) (Zaheer et al., 2020)."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258840932 | Zhuang et al. | 2022 | Citations: 3]",
                "snippets": "Sparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention (Parmar et al., 2018) and Multi-passage BERT (Wang et al., 2019) use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird (Zaheer et al., 2020) further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[201307832 | Wang et al. | 2019 | Citations: 244]": "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models.",
                    "[3353110 | Parmar et al. | 2018 | Citations: 1686]": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. We propose another extension of self-attention allowing it to efficiently take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or significantly outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art."
                },
                "metadata": [
                    {
                        "section_title": "Attention Methods",
                        "pdf_hash": "",
                        "start": 577,
                        "end": 1168,
                        "sentence_offsets": [
                            {
                                "start": 577,
                                "end": 594
                            },
                            {
                                "start": 595,
                                "end": 740
                            },
                            {
                                "start": 741,
                                "end": 880
                            },
                            {
                                "start": 881,
                                "end": 1168
                            }
                        ],
                        "ref_mentions": [
                            "3353110",
                            "201307832",
                            "220831004"
                        ],
                        "quote": "Sparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention (Parmar et al., 2018) and Multi-passage BERT (Wang et al., 2019) use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird (Zaheer et al., 2020) further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[259063695 | Pagliardini et al. | 2023 | Citations: 25]",
                "snippets": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1342,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[259858862 | Zhang et al. | 2023 | Citations: 7]",
                "snippets": "Different from traditional sparse Transformer (Martins et al., 2016)(Correia et al., 2019)(Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[153313159 | Peters et al. | 2019 | Citations: 214]": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of \\alpha-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any \\alpha > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.",
                    "[16432551 | Martins et al. | 2016 | Citations: 726]": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.",
                    "[202538495 | Correia et al. | 2019 | Citations: 256]": "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 900,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 254
                            },
                            {
                                "start": 255,
                                "end": 460
                            },
                            {
                                "start": 461,
                                "end": 601
                            },
                            {
                                "start": 602,
                                "end": 710
                            },
                            {
                                "start": 711,
                                "end": 767
                            },
                            {
                                "start": 768,
                                "end": 900
                            }
                        ],
                        "ref_mentions": [
                            "16432551",
                            "202538495",
                            "153313159"
                        ],
                        "quote": "Different from traditional sparse Transformer (Martins et al., 2016)(Correia et al., 2019)(Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[264426102 | Madani et al. | 2023 | Citations: 4]",
                "snippets": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, (Zaheer et al., 2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                },
                "metadata": [
                    {
                        "section_title": "A Model Detail",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 901,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 107
                            },
                            {
                                "start": 108,
                                "end": 270
                            },
                            {
                                "start": 271,
                                "end": 404
                            },
                            {
                                "start": 405,
                                "end": 577
                            },
                            {
                                "start": 578,
                                "end": 769
                            },
                            {
                                "start": 770,
                                "end": 901
                            }
                        ],
                        "ref_mentions": [
                            "220831004"
                        ],
                        "quote": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, (Zaheer et al., 2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[264451707 | Chen et al. | 2023 | Citations: 32]",
                "snippets": "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, (Dai et al., 2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[250526424 | Bulatov et al. | 2022 | Citations: 110]": "Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",
                    "[57759363 | Dai et al. | 2019 | Citations: 3746]": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 956,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 45
                            },
                            {
                                "start": 46,
                                "end": 270
                            },
                            {
                                "start": 271,
                                "end": 496
                            },
                            {
                                "start": 497,
                                "end": 678
                            },
                            {
                                "start": 679,
                                "end": 802
                            },
                            {
                                "start": 803,
                                "end": 956
                            }
                        ],
                        "ref_mentions": [
                            "57759363",
                            "250526424"
                        ],
                        "quote": "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, (Dai et al., 2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[266110855 | Zhou | 2023 | Citations: 0]",
                "snippets": "As the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention (Ainslie et al., 2020) if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird (Zaheer et al., 2020), LongT5 (Guo et al., 2021) have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[221845203 | Ainslie et al. | 2020 | Citations: 354]": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
                    "[245144820 | Guo et al. | 2021 | Citations: 316]": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks."
                },
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 815,
                        "end": 2080,
                        "sentence_offsets": [
                            {
                                "start": 815,
                                "end": 1110
                            },
                            {
                                "start": 1111,
                                "end": 1348
                            },
                            {
                                "start": 1349,
                                "end": 1665
                            },
                            {
                                "start": 1666,
                                "end": 1893
                            },
                            {
                                "start": 1894,
                                "end": 2080
                            }
                        ],
                        "ref_mentions": [
                            "221845203",
                            "220831004",
                            "245144820"
                        ],
                        "quote": "As the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention (Ainslie et al., 2020) if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird (Zaheer et al., 2020), LongT5 (Guo et al., 2021) have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[266210450 | Song et al. | 2023 | Citations: 7]",
                "snippets": "Attention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[211505992 | Tay et al. | 2020 | Citations: 342]": "We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.",
                    "[221845203 | Ainslie et al. | 2020 | Citations: 354]": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 924,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 10
                            },
                            {
                                "start": 11,
                                "end": 107
                            },
                            {
                                "start": 108,
                                "end": 208
                            },
                            {
                                "start": 209,
                                "end": 374
                            },
                            {
                                "start": 375,
                                "end": 456
                            },
                            {
                                "start": 457,
                                "end": 678
                            },
                            {
                                "start": 679,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 924
                            }
                        ],
                        "ref_mentions": [
                            "211505992",
                            "221845203",
                            "220831004"
                        ],
                        "quote": "Attention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021)."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[266999285 | Zhang et al. | 2024 | Citations: 12]",
                "snippets": "Sparse transformers (Child et al., 2019;(Zaheer et al., 2020)Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;(Zaheer et al., 2020)Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020)(Lewis et al., 2020)Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022)Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                },
                "metadata": [
                    {
                        "section_title": "More Efficient Transformers",
                        "pdf_hash": "",
                        "start": 319,
                        "end": 1986,
                        "sentence_offsets": [
                            {
                                "start": 319,
                                "end": 587
                            },
                            {
                                "start": 588,
                                "end": 956
                            },
                            {
                                "start": 957,
                                "end": 1129
                            },
                            {
                                "start": 1130,
                                "end": 1292
                            },
                            {
                                "start": 1293,
                                "end": 1472
                            },
                            {
                                "start": 1473,
                                "end": 1777
                            },
                            {
                                "start": 1778,
                                "end": 1837
                            },
                            {
                                "start": 1838,
                                "end": 1986
                            }
                        ],
                        "ref_mentions": [
                            "220831004",
                            "220831004",
                            "211204736",
                            "218869575",
                            "249151871"
                        ],
                        "quote": "Sparse transformers (Child et al., 2019;(Zaheer et al., 2020)Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;(Zaheer et al., 2020)Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020)(Lewis et al., 2020)Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022)Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[270063477 | Yan et al. | 2024 | Citations: 1]",
                "snippets": "BigBird is designed to handle long sequences while maintaining a manageable computational complexity. It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer. The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.\n2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.\n3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "E.3 Sparse Autoencoders",
                        "pdf_hash": "",
                        "start": 182,
                        "end": 1009,
                        "sentence_offsets": [
                            {
                                "start": 171,
                                "end": 267
                            },
                            {
                                "start": 269,
                                "end": 370
                            },
                            {
                                "start": 370,
                                "end": 535
                            },
                            {
                                "start": 535,
                                "end": 604
                            },
                            {
                                "start": 606,
                                "end": 690
                            },
                            {
                                "start": 690,
                                "end": 805
                            },
                            {
                                "start": 805,
                                "end": 895
                            },
                            {
                                "start": 897,
                                "end": 1092
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "BigBird is designed to handle long sequences while maintaining a manageable computational complexity. It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer. The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.\n2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.\n3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[270094757 | Wu et al. | 2024 | Citations: 12]",
                "snippets": "Sparse and local Attention While many existing transformer models and LLMs do not use sparse attention, sparse and local attention is gaining popularity due to the demand for efficiency, particular for long-context tasks. For example, sparse attention was populated by Longformer [4] and OpenAI [10] and nowadays popular LLMs like Mistral 7B [22] use sliding window attention by default. Other popular sparse attention models include, but are not limited to BigBird [42], Recurrent Memory Transformers (RMTs) (Bulatov et al., 2022), and Streaming Attention [38]. Besides language tasks, sparse attention is also common in vision transformers (Hassani et al., 2022)(Liu et al., 2021)(Pan et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[232352874 | Liu et al. | 2021 | Citations: 21580]": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
                    "[248178045 | Hassani et al. | 2022 | Citations: 274]": "We present Neighborhood Attention (NA), the first efficient and scalable sliding window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MSCOCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding window attention, we open source our project and release our checkpoints.",
                    "[250526424 | Bulatov et al. | 2022 | Citations: 110]": "Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",
                    "[258048654 | Pan et al. | 2023 | Citations: 60]": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 634,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 221
                            },
                            {
                                "start": 222,
                                "end": 387
                            },
                            {
                                "start": 388,
                                "end": 543
                            },
                            {
                                "start": 544,
                                "end": 634
                            }
                        ],
                        "ref_mentions": [
                            "250526424",
                            "248178045",
                            "232352874",
                            "258048654"
                        ],
                        "quote": "Sparse and local Attention While many existing transformer models and LLMs do not use sparse attention, sparse and local attention is gaining popularity due to the demand for efficiency, particular for long-context tasks. For example, sparse attention was populated by Longformer [4] and OpenAI [10] and nowadays popular LLMs like Mistral 7B [22] use sliding window attention by default. Other popular sparse attention models include, but are not limited to BigBird [42], Recurrent Memory Transformers (RMTs) (Bulatov et al., 2022), and Streaming Attention [38]. Besides language tasks, sparse attention is also common in vision transformers (Hassani et al., 2022)(Liu et al., 2021)(Pan et al., 2023)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[270370979 | Zhang | 2024 | Citations: 2]",
                "snippets": "Notable examples include Longformer [4] and BigBird (Zaheer et al., 2020), which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to O(n).These models achieve a balance between maintaining sufficient context for understanding while managing computational load.For achieving complexity of O(n log n), several approaches have been proposed.Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window.Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity.LSG Attention (Condevaux et al., 2022), adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[253157377 | Condevaux et al. | 2022 | Citations: 24]": "Transformer models achieve state-of-the-art performance on a wide range of NLP tasks. They however suffer from a prohibitive limitation due to the self-attention mechanism, inducing $O(n^2)$ complexity with regard to sequence length. To answer this limitation we introduce the LSG architecture which relies on Local, Sparse and Global attention. We show that LSG attention is fast, efficient and competitive in classification and summarization tasks on long documents. Interestingly, it can also be used to adapt existing pretrained models to efficiently extrapolate to longer sequences with no additional training. Along with the introduction of the LSG attention mechanism, we propose tools to train new models and adapt existing ones based on this mechanism."
                },
                "metadata": [
                    {
                        "section_title": "Long-context Transformers",
                        "pdf_hash": "",
                        "start": 365,
                        "end": 1242,
                        "sentence_offsets": [
                            {
                                "start": 365,
                                "end": 566
                            },
                            {
                                "start": 566,
                                "end": 688
                            },
                            {
                                "start": 688,
                                "end": 766
                            },
                            {
                                "start": 766,
                                "end": 905
                            },
                            {
                                "start": 905,
                                "end": 1080
                            },
                            {
                                "start": 1080,
                                "end": 1242
                            }
                        ],
                        "ref_mentions": [
                            "220831004",
                            "253157377"
                        ],
                        "quote": "Notable examples include Longformer [4] and BigBird (Zaheer et al., 2020), which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to O(n).These models achieve a balance between maintaining sufficient context for understanding while managing computational load.For achieving complexity of O(n log n), several approaches have been proposed.Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window.Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity.LSG Attention (Condevaux et al., 2022), adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[270703226 | Lou et al. | 2024 | Citations: 26]",
                "snippets": "Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,(Roy et al., 2020) allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42](Yu et al., 2023).(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212718077 | Roy et al. | 2020 | Citations: 603]": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
                    "[264439578 | Yu et al. | 2023 | Citations: 6]": "The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 644,
                        "end": 1491,
                        "sentence_offsets": [
                            {
                                "start": 644,
                                "end": 777
                            },
                            {
                                "start": 777,
                                "end": 1008
                            },
                            {
                                "start": 1008,
                                "end": 1095
                            },
                            {
                                "start": 1095,
                                "end": 1305
                            },
                            {
                                "start": 1305,
                                "end": 1408
                            },
                            {
                                "start": 1408,
                                "end": 1491
                            }
                        ],
                        "ref_mentions": [
                            "212718077",
                            "264439578"
                        ],
                        "quote": "Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,(Roy et al., 2020) allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42](Yu et al., 2023).(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[271329267 | Hagos et al. | 2024 | Citations: 26]",
                "snippets": "Sparse Transformers. The standard transformer's attention mechanism calculates attention scores for all pairs of positions in a sequence, leading to quadratic time complexity (Vaswani et al., 2017). Sparse Transformers address this issue by considering only a subset of positions during attention computation [117]. This introduces sparsity, significantly reducing memory requirements and computational load, making them suitable for longer sequences [117].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132443]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                "metadata": [
                    {
                        "section_title": "F. Long Sequence Language Models",
                        "pdf_hash": "",
                        "start": 416,
                        "end": 855,
                        "sentence_offsets": [
                            {
                                "start": 416,
                                "end": 436
                            },
                            {
                                "start": 437,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 713
                            },
                            {
                                "start": 714,
                                "end": 855
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "Sparse Transformers. The standard transformer's attention mechanism calculates attention scores for all pairs of positions in a sequence, leading to quadratic time complexity (Vaswani et al., 2017). Sparse Transformers address this issue by considering only a subset of positions during attention computation [117]. This introduces sparsity, significantly reducing memory requirements and computational load, making them suitable for longer sequences [117]."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[272310078 | Yao et al. | 2024 | Citations: 2]",
                "snippets": "FlashAttention (Dao et al., 2022) utilizes an online softmax computation technique (Milakov & Gimelshein, 2018) and reduces memory requirements of self-attention from O(n 2 ) to O(n) (Rabe & Staats, 2021) while preserving the accuracy. Other notable strategies include lowrank approximations (Katharopoulos et al., 2020)Wang et al., 2020), kernel-based methods (Kitaev et al., 2020;(Lu et al., 2021)(Xiong et al., 2021), and sparse attention mechanisms (Child et al., 2019), which approximate or selectively compute attention to minimize memory consumption. Furthermore, techniques that combine local and global contexts (Ainslie et al., 2020;Beltagy et al., 2020;(Liu et al., 2021)(Zaheer et al., 2020) enable superior performance on tasks involving long sequences or large-scale inputs while maintaining computational efficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                    "[220250819 | Katharopoulos et al. | 2020 | Citations: 1790]": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
                    "[231847231 | Xiong et al. | 2021 | Citations: 526]": "Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.",
                    "[232352874 | Liu et al. | 2021 | Citations: 21580]": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
                    "[239616022 | Lu et al. | 2021 | Citations: 166]": "Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity."
                },
                "metadata": [
                    {
                        "section_title": "Memory-efficient Transformer",
                        "pdf_hash": "",
                        "start": 175,
                        "end": 1002,
                        "sentence_offsets": [
                            {
                                "start": 175,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 730
                            },
                            {
                                "start": 731,
                                "end": 1002
                            }
                        ],
                        "ref_mentions": [
                            "249151871",
                            "220250819",
                            "239616022",
                            "231847231",
                            "232352874",
                            "220831004"
                        ],
                        "quote": "FlashAttention (Dao et al., 2022) utilizes an online softmax computation technique (Milakov & Gimelshein, 2018) and reduces memory requirements of self-attention from O(n 2 ) to O(n) (Rabe & Staats, 2021) while preserving the accuracy. Other notable strategies include lowrank approximations (Katharopoulos et al., 2020)Wang et al., 2020), kernel-based methods (Kitaev et al., 2020;(Lu et al., 2021)(Xiong et al., 2021), and sparse attention mechanisms (Child et al., 2019), which approximate or selectively compute attention to minimize memory consumption. Furthermore, techniques that combine local and global contexts (Ainslie et al., 2020;Beltagy et al., 2020;(Liu et al., 2021)(Zaheer et al., 2020) enable superior performance on tasks involving long sequences or large-scale inputs while maintaining computational efficiency."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[272310408 | Hou et al. | 2024 | Citations: 0]",
                "snippets": "Sparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al., 2020)) and (Liu et al., 2021), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure...Models like Longformer (Beltagy et al., 2020), ETC (Extended Transformer Construction) (Zaheer et al., 2020), and Big Bird (Zaheer et al., 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[212718077 | Roy et al. | 2020 | Citations: 603]": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
                    "[215737171 | Beltagy et al. | 2020 | Citations: 4100]": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
                    "[232352874 | Liu et al. | 2021 | Citations: 21580]": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer."
                },
                "metadata": [
                    {
                        "section_title": "Efficient Transformers",
                        "pdf_hash": "",
                        "start": 547,
                        "end": 1020,
                        "sentence_offsets": [
                            {
                                "start": 547,
                                "end": 575
                            },
                            {
                                "start": 576,
                                "end": 758
                            },
                            {
                                "start": 759,
                                "end": 921
                            },
                            {
                                "start": 922,
                                "end": 1021
                            }
                        ],
                        "ref_mentions": [
                            "212718077",
                            "232352874"
                        ],
                        "quote": "Sparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al., 2020)) and (Liu et al., 2021), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure"
                    },
                    {
                        "section_title": "Efficient Transformers",
                        "pdf_hash": "",
                        "start": 38,
                        "end": 547,
                        "sentence_offsets": [
                            {
                                "start": 38,
                                "end": 299
                            },
                            {
                                "start": 300,
                                "end": 546
                            }
                        ],
                        "ref_mentions": [
                            "215737171",
                            "220831004",
                            "220831004"
                        ],
                        "quote": "Models like Longformer (Beltagy et al., 2020), ETC (Extended Transformer Construction) (Zaheer et al., 2020), and Big Bird (Zaheer et al., 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273026224 | Wang et al. | 2024 | Citations: 3]",
                "snippets": "FlashAttention achieves a memory overhead of O(), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process...Sparse Causal Flash Attention (SCFA) (Pagliardini et al., 2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                    "[259063695 | Pagliardini et al. | 2023 | Citations: 25]": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens."
                },
                "metadata": [
                    {
                        "section_title": "ATTENTION OPTIMIZATION TECHNIQUES",
                        "pdf_hash": "",
                        "start": 799,
                        "end": 1198,
                        "sentence_offsets": [
                            {
                                "start": 792,
                                "end": 955
                            },
                            {
                                "start": 956,
                                "end": 1131
                            },
                            {
                                "start": 1132,
                                "end": 1296
                            }
                        ],
                        "ref_mentions": [
                            "249151871"
                        ],
                        "quote": "FlashAttention achieves a memory overhead of O(), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process"
                    },
                    {
                        "section_title": "ATTENTION OPTIMIZATION TECHNIQUES",
                        "pdf_hash": "",
                        "start": 1499,
                        "end": 2063,
                        "sentence_offsets": [
                            {
                                "start": 1499,
                                "end": 1664
                            },
                            {
                                "start": 1665,
                                "end": 1857
                            },
                            {
                                "start": 1858,
                                "end": 2062
                            }
                        ],
                        "ref_mentions": [
                            "259063695"
                        ],
                        "quote": "Sparse Causal Flash Attention (SCFA) (Pagliardini et al., 2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273228328 | Eisner | 2024 | Citations: 0]",
                "snippets": "These Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques:...For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks...while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background and Introduction",
                        "pdf_hash": "",
                        "start": 404,
                        "end": 897,
                        "sentence_offsets": [
                            {
                                "start": 404,
                                "end": 652
                            },
                            {
                                "start": 653,
                                "end": 808
                            },
                            {
                                "start": 811,
                                "end": 1122
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "These Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques:"
                    },
                    {
                        "section_title": "Background and Introduction",
                        "pdf_hash": "",
                        "start": 900,
                        "end": 1758,
                        "sentence_offsets": [
                            {
                                "start": 811,
                                "end": 1122
                            },
                            {
                                "start": 1123,
                                "end": 1280
                            },
                            {
                                "start": 1281,
                                "end": 1438
                            },
                            {
                                "start": 1439,
                                "end": 1594
                            },
                            {
                                "start": 1595,
                                "end": 1966
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks"
                    },
                    {
                        "section_title": "Background and Introduction",
                        "pdf_hash": "",
                        "start": 1762,
                        "end": 1967,
                        "sentence_offsets": [
                            {
                                "start": 1595,
                                "end": 1966
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[273821735 | Datta | 2024 | Citations: 1]",
                "snippets": "Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers (Katharopoulos et al., 2020) reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird (Zaheer et al., 2020) combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[220250819 | Katharopoulos et al. | 2020 | Citations: 1790]": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."
                },
                "metadata": [
                    {
                        "section_title": "Efficient Attention Mechanisms",
                        "pdf_hash": "",
                        "start": 299,
                        "end": 1732,
                        "sentence_offsets": [
                            {
                                "start": 297,
                                "end": 499
                            },
                            {
                                "start": 500,
                                "end": 628
                            },
                            {
                                "start": 629,
                                "end": 807
                            },
                            {
                                "start": 808,
                                "end": 925
                            },
                            {
                                "start": 926,
                                "end": 1041
                            },
                            {
                                "start": 1042,
                                "end": 1244
                            },
                            {
                                "start": 1245,
                                "end": 1344
                            },
                            {
                                "start": 1345,
                                "end": 1587
                            },
                            {
                                "start": 1588,
                                "end": 1732
                            }
                        ],
                        "ref_mentions": [
                            "220250819",
                            "220831004"
                        ],
                        "quote": "Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers (Katharopoulos et al., 2020) reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird (Zaheer et al., 2020) combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273850602 | Haris | 2024 | Citations: 0]",
                "snippets": "Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention...Efficient computation of self-attention has been a focal point of research in recent years (Fournier et al., 2021). Flash Attention [DFE + 22] and related work [SY[24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC[21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC[20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF[24], or adaptive techniques [CNM[19].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[232380042 | Fournier et al. | 2021 | Citations: 101]": "Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models\u2019 efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer\u2019s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methods\u2019 strengths, limitations, and underlying assumptions."
                },
                "metadata": [
                    {
                        "quote": "Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 736,
                        "end": 1379,
                        "sentence_offsets": [
                            {
                                "start": 736,
                                "end": 835
                            },
                            {
                                "start": 836,
                                "end": 990
                            },
                            {
                                "start": 991,
                                "end": 1062
                            },
                            {
                                "start": 1063,
                                "end": 1177
                            },
                            {
                                "start": 1178,
                                "end": 1378
                            }
                        ],
                        "ref_mentions": [
                            "232380042"
                        ],
                        "quote": "Efficient computation of self-attention has been a focal point of research in recent years (Fournier et al., 2021). Flash Attention [DFE + 22] and related work [SY[24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC[21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC[20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF[24], or adaptive techniques [CNM[19]."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[273856640 | Pham et al. | 2024 | Citations: 3]",
                "snippets": "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)...While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[57825721 | Perez et al. | 2019 | Citations: 146]": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results."
                },
                "metadata": [
                    {
                        "section_title": "F. BIGBIRD",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 370,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 255
                            },
                            {
                                "start": 256,
                                "end": 371
                            }
                        ],
                        "ref_mentions": [
                            "220831004"
                        ],
                        "quote": "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)"
                    },
                    {
                        "section_title": "F. BIGBIRD",
                        "pdf_hash": "",
                        "start": 945,
                        "end": 1511,
                        "sentence_offsets": [
                            {
                                "start": 890,
                                "end": 1184
                            },
                            {
                                "start": 1185,
                                "end": 1310
                            },
                            {
                                "start": 1311,
                                "end": 1510
                            }
                        ],
                        "ref_mentions": [
                            "57825721",
                            "220831004",
                            "220831004"
                        ],
                        "quote": "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276106883 | Tomczak et al. | 2025 | Citations: 0]",
                "snippets": "A popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism (Zaheer et al., 2020)- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach...However, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                },
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 973,
                        "end": 1815,
                        "sentence_offsets": [
                            {
                                "start": 973,
                                "end": 1095
                            },
                            {
                                "start": 1096,
                                "end": 1192
                            },
                            {
                                "start": 1193,
                                "end": 1321
                            },
                            {
                                "start": 1322,
                                "end": 1517
                            },
                            {
                                "start": 1518,
                                "end": 1710
                            },
                            {
                                "start": 1711,
                                "end": 1816
                            }
                        ],
                        "ref_mentions": [
                            "220831004"
                        ],
                        "quote": "A popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism (Zaheer et al., 2020)- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach"
                    },
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1819,
                        "end": 2408,
                        "sentence_offsets": [
                            {
                                "start": 1819,
                                "end": 1909
                            },
                            {
                                "start": 1910,
                                "end": 2030
                            },
                            {
                                "start": 2031,
                                "end": 2293
                            },
                            {
                                "start": 2294,
                                "end": 2407
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276421279 | Goncalves et al. | 2025 | Citations: 0]",
                "snippets": "The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 344,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[276575796 | Navardi et al. | 2025 | Citations: 2]",
                "snippets": "FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations...Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Attention Optimization",
                        "pdf_hash": "",
                        "start": 280,
                        "end": 1328,
                        "sentence_offsets": [
                            {
                                "start": 280,
                                "end": 610
                            },
                            {
                                "start": 611,
                                "end": 825
                            },
                            {
                                "start": 826,
                                "end": 1110
                            },
                            {
                                "start": 1111,
                                "end": 1329
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations"
                    },
                    {
                        "section_title": "Attention Optimization",
                        "pdf_hash": "",
                        "start": 1731,
                        "end": 2167,
                        "sentence_offsets": [
                            {
                                "start": 1731,
                                "end": 2166
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[276618265 | Fu et al. | 2025 | Citations: 2]",
                "snippets": "While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Efficient Transformers",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 777,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 198
                            },
                            {
                                "start": 201,
                                "end": 270
                            },
                            {
                                "start": 271,
                                "end": 441
                            },
                            {
                                "start": 442,
                                "end": 680
                            },
                            {
                                "start": 681,
                                "end": 777
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[276725385 | Xia et al. | 2025 | Citations: 11]",
                "snippets": "FlashAttention [12,(Dao et al., 2022)(Shah et al., 2024) addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                    "[271098045 | Shah et al. | 2024 | Citations: 157]": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention."
                },
                "metadata": [
                    {
                        "section_title": "FlashAttention",
                        "pdf_hash": "",
                        "start": 866,
                        "end": 1858,
                        "sentence_offsets": [
                            {
                                "start": 866,
                                "end": 959
                            },
                            {
                                "start": 960,
                                "end": 1059
                            },
                            {
                                "start": 1060,
                                "end": 1245
                            },
                            {
                                "start": 1246,
                                "end": 1305
                            },
                            {
                                "start": 1308,
                                "end": 1467
                            },
                            {
                                "start": 1468,
                                "end": 1576
                            },
                            {
                                "start": 1579,
                                "end": 1737
                            },
                            {
                                "start": 1738,
                                "end": 1858
                            }
                        ],
                        "ref_mentions": [
                            "249151871",
                            "271098045"
                        ],
                        "quote": "FlashAttention [12,(Dao et al., 2022)(Shah et al., 2024) addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[276775748 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence...Flash Attention (Dao et al., 2022) reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                },
                "metadata": [
                    {
                        "section_title": "A. Variants of Transformer",
                        "pdf_hash": "",
                        "start": 426,
                        "end": 546,
                        "sentence_offsets": [
                            {
                                "start": 426,
                                "end": 547
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence"
                    },
                    {
                        "section_title": "A. Variants of Transformer",
                        "pdf_hash": "",
                        "start": 816,
                        "end": 996,
                        "sentence_offsets": [
                            {
                                "start": 816,
                                "end": 995
                            }
                        ],
                        "ref_mentions": [
                            "249151871"
                        ],
                        "quote": "Flash Attention (Dao et al., 2022) reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[276906703 | Zhang et al. | 2025 | Citations: 1]",
                "snippets": "A plethora of transformer derivatives, termed \"x-formers\" have been developed, employing tactics such as sparse attention [36][37][38], full attention matrix approximation 19,39,40, and chunked attention with gating 41. Notably, recent innovations like FlashAttention 17 improve memory efficiency, but time complexity remains an issue, either remaining quadratic or influenced by hidden factors like chunk size. Conversely, RWKV 20 exemplifies superior space-time complexity trade-offs during inference by ingeniously casting linear attention as an RNN.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245144820 | Guo et al. | 2021 | Citations: 316]": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks."
                },
                "metadata": [
                    {
                        "section_title": "Optimizing attention mechanism",
                        "pdf_hash": "",
                        "start": 189,
                        "end": 742,
                        "sentence_offsets": [
                            {
                                "start": 158,
                                "end": 225
                            },
                            {
                                "start": 226,
                                "end": 448
                            },
                            {
                                "start": 449,
                                "end": 640
                            },
                            {
                                "start": 641,
                                "end": 782
                            }
                        ],
                        "ref_mentions": [
                            "245144820"
                        ],
                        "quote": "A plethora of transformer derivatives, termed \"x-formers\" have been developed, employing tactics such as sparse attention [36][37][38], full attention matrix approximation 19,39,40, and chunked attention with gating 41. Notably, recent innovations like FlashAttention 17 improve memory efficiency, but time complexity remains an issue, either remaining quadratic or influenced by hidden factors like chunk size. Conversely, RWKV 20 exemplifies superior space-time complexity trade-offs during inference by ingeniously casting linear attention as an RNN."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[277151262 | Xu et al. | 2025 | Citations: 15]",
                "snippets": "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[278000642 | Ahn | 2025 | Citations: 0]",
                "snippets": "Sparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Long-Context Transformers",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 700,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 137
                            },
                            {
                                "start": 138,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 575
                            },
                            {
                                "start": 576,
                                "end": 700
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[278339364 | Esmail et al. | 2025 | Citations: 0]",
                "snippets": "Sparse attention is a technique used in Transformer models to reduce the computational complexity of the self-attention mechanism by limiting the number of key-query pairs attend to the classification. Instead of computing attention over all tokens in a sequence, sparse attention selectively attends to a subset of tokens based on predefined patterns, learned structures, or efficient approximations. This significantly lowers the memory and computational cost from quadratic to linear or sub-quadratic complexity, making it possible to process longer sequences efficiently. \n\nDifferent types of sparse attention have been introduced such as fixed pattern and learnable sparse attention. Fixed pattern sparse attention is a type of sparse attention mechanism where each token attends only to a predefined subset of tokens based on a fixed rule. [58](Pan et al., 2023)[60]. On the other hand, Learnable sparse attention dynamically determines which tokens to attend to based on learned patterns. Learnable sparse attention allows the model to adaptively focus on the most relevant tokens in different contexts, leading to improved efficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258048654 | Pan et al. | 2023 | Citations: 60]": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Dynamic sparse attention via differential attention",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1128,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 401
                            },
                            {
                                "start": 402,
                                "end": 575
                            },
                            {
                                "start": 578,
                                "end": 688
                            },
                            {
                                "start": 689,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 859
                            },
                            {
                                "start": 860,
                                "end": 981
                            },
                            {
                                "start": 982,
                                "end": 1128
                            }
                        ],
                        "ref_mentions": [
                            "258048654"
                        ],
                        "quote": "Sparse attention is a technique used in Transformer models to reduce the computational complexity of the self-attention mechanism by limiting the number of key-query pairs attend to the classification. Instead of computing attention over all tokens in a sequence, sparse attention selectively attends to a subset of tokens based on predefined patterns, learned structures, or efficient approximations. This significantly lowers the memory and computational cost from quadratic to linear or sub-quadratic complexity, making it possible to process longer sequences efficiently. \n\nDifferent types of sparse attention have been introduced such as fixed pattern and learnable sparse attention. Fixed pattern sparse attention is a type of sparse attention mechanism where each token attends only to a predefined subset of tokens based on a fixed rule. [58](Pan et al., 2023)[60]. On the other hand, Learnable sparse attention dynamically determines which tokens to attend to based on learned patterns. Learnable sparse attention allows the model to adaptively focus on the most relevant tokens in different contexts, leading to improved efficiency."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.041709,
        "cot": "I need to organize these quotes into meaningful dimensions that will help answer the query about how sparse attention mechanisms and memory-efficient implementations like FlashAttention and BigBird compare in extending transformer context length while minimizing computational costs.\n\nAfter analyzing the query and quotes, I'll organize this into the following dimensions:\n\n1. \"Introduction and Background of Transformer Attention Limitations\" - This will be a synthesis section to introduce the problem of quadratic complexity in transformers and why solutions are needed. This provides essential context for understanding the other sections.\n\n2. \"Types and Approaches of Sparse Attention Mechanisms\" - This should be a synthesis section describing the various sparse attention approaches (BigBird, Longformer, etc.) and how they work conceptually.\n\n3. \"FlashAttention and Memory-Efficient Implementations\" - This should be a synthesis section focusing specifically on FlashAttention and other memory-efficient approaches that are distinct from sparse attention patterns.\n\n4. \"Computational Complexity Comparison\" - This should be a synthesis section comparing the computational and space complexity of the different approaches (O(n\u00b2) vs O(n) vs O(n log n)).\n\n5. \"Performance and Context Length Capabilities\" - This should be a synthesis section examining how these approaches perform in practice, particularly regarding their ability to handle longer sequences.\n\n6. \"Limitations and Trade-offs\" - This should be a synthesis section discussing the limitations and trade-offs of sparse attention vs. memory-efficient implementations.\n\nThis organization allows for a comprehensive comparison of the approaches while maintaining a logical flow from background to specific implementations to practical considerations.",
        "plan": {
            "Introduction and Background of Transformer Attention Limitations (synthesis)": [],
            "Types and Approaches of Sparse Attention Mechanisms (synthesis)": [
                0,
                1,
                2,
                3,
                4,
                5,
                7,
                8,
                9,
                10,
                12,
                13,
                14,
                15,
                16,
                18,
                19,
                20,
                22,
                24,
                26,
                27,
                29,
                30,
                32,
                33,
                36,
                38,
                39
            ],
            "FlashAttention and Memory-Efficient Implementations (synthesis)": [
                6,
                11,
                17,
                21,
                23,
                25,
                28,
                31,
                32,
                34,
                35,
                36
            ],
            "Computational Complexity Comparison (synthesis)": [
                0,
                1,
                2,
                3,
                5,
                8,
                10,
                14,
                15,
                16,
                20,
                27,
                28,
                29,
                30,
                36,
                37
            ],
            "Performance and Context Length Capabilities (synthesis)": [
                0,
                2,
                4,
                6,
                8,
                9,
                13,
                15,
                20,
                29,
                38
            ],
            "Limitations and Trade-offs (synthesis)": [
                2,
                4,
                7,
                8,
                12,
                14,
                21,
                26,
                30,
                33,
                37
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background of Transformer Attention Limitations",
                "tldr": "Transformers revolutionized NLP with their self-attention mechanism, but standard attention has quadratic complexity that limits context length. This fundamental bottleneck creates memory and computational challenges when processing long sequences. (LLM Memory)",
                "text": "\nTransformer models have fundamentally changed the landscape of natural language processing since their introduction in 2017. At their core, these models rely on a self-attention mechanism that allows them to process input sequences in parallel while capturing relationships between all tokens in the sequence. This mechanism enables transformers to model long-range dependencies more effectively than previous architectures like RNNs. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nHowever, the standard self-attention mechanism in transformers has a critical limitation: it scales quadratically with sequence length in both computational complexity and memory usage. This is because for each token in a sequence of length n, the model must compute attention scores with every other token, resulting in an n\u00d7n attention matrix. As sequence length grows, this quadratic scaling becomes prohibitively expensive. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFor example, doubling the context length from 2,048 to 4,096 tokens increases memory requirements by 4x and computational costs by 4x. This quadratic bottleneck severely restricts the maximum context length that transformer models can effectively process, typically limiting earlier models to sequences of only a few thousand tokens. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThis limitation has practical implications for many applications requiring long-context understanding, such as document analysis, conversation modeling, and code generation. As a result, researchers have been actively developing alternative attention mechanisms and optimizations to overcome these constraints while preserving the powerful capabilities of transformer architectures. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types and Approaches of Sparse Attention Mechanisms",
                "tldr": "Sparse attention mechanisms reduce transformer complexity by selectively computing attention for subsets of tokens rather than all pairs. Key approaches include pattern-based methods like BigBird and Longformer that combine local, global, and random attention patterns to achieve linear complexity while maintaining model effectiveness. (12 sources)",
                "text": "\nSparse attention mechanisms fundamentally work by computing attention scores for only a subset of token pairs rather than the full n\u00d7n attention matrix used in standard transformers. This selective approach significantly reduces the computational and memory requirements from quadratic O(n\u00b2) to linear O(n) or sub-quadratic O(n log n) complexity, enabling the processing of much longer sequences.\n\nSeveral distinct categories of sparse attention have emerged in the research literature:\n\n## Local Window Attention\nOne of the most intuitive approaches is to restrict attention to a fixed window of neighboring tokens. This pattern assumes that nearby tokens are most relevant for understanding context. Local attention appears in models like Longformer, which uses sliding window patterns where each token attends only to tokens within a fixed distance <Paper corpusId=\"215737171\" paperTitle=\"(Beltagy et al., 2020)\" isShortName></Paper>. Some implementations extend this approach with dilated sliding windows to increase the effective receptive field without adding computational cost <Paper corpusId=\"258840932\" paperTitle=\"(Zhuang et al., 2022)\" isShortName></Paper>.\n\n## Global Attention\nTo maintain the ability to capture long-range dependencies, many sparse attention models incorporate global attention mechanisms. These allow certain tokens (like classification tokens) to attend to all other tokens in the sequence. BigBird notably includes a set of global tokens that attend to the entire sequence, which theoretical analysis showed to be particularly beneficial <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\n## Random Attention\nSome models incorporate randomized attention patterns to capture unexpected long-range dependencies. BigBird combines its global and local attention with random attention, where each token attends to a fixed number of randomly selected tokens throughout the sequence <Paper corpusId=\"270063477\" paperTitle=\"(Yan et al., 2024)\" isShortName></Paper>. This random component helps ensure that important connections aren't systematically overlooked.\n\n## Combined Pattern Approaches\nMost successful sparse attention implementations combine multiple patterns. BigBird integrates local window attention, global attention, and random attention <Paper corpusId=\"258762176\" paperTitle=\"(KABENAMUALU et al., 2023)\" isShortName></Paper>. Longformer combines local windowed attention with task-specific global attention <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>. These hybrid approaches aim to balance computational efficiency with the ability to capture diverse types of dependencies.\n\n## Locality-Sensitive Hashing\nRather than using fixed patterns, some models dynamically determine which tokens should attend to each other. Reformer uses locality-sensitive hashing (LSH) to identify similar tokens that are likely to have high attention scores, reducing complexity to O(n log n) <Paper corpusId=\"209315300\" paperTitle=\"(Kitaev et al., 2020)\" isShortName></Paper> <Paper corpusId=\"273821735\" paperTitle=\"(Datta, 2024)\" isShortName></Paper>.\n\n## Adaptive and Learnable Sparsity\nMore sophisticated approaches learn the sparsity pattern during training. The Adaptively Sparse Transformer replaces the standard softmax with \u03b1-entmax, allowing the model to learn which tokens should receive zero attention weight <Paper corpusId=\"202538495\" paperTitle=\"(Correia et al., 2019)\" isShortName></Paper>. This creates flexible, context-dependent sparsity patterns that can adapt to different types of inputs and tasks.\n\nDespite their efficiency advantages, sparse attention mechanisms face certain limitations. Research has shown that sparse attention cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. Some problems require dense attention to be solved efficiently, with theoretical analysis demonstrating that certain tasks solvable by dense attention in O(1) layers would require \u03a9(n) layers with sparse attention <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\nAnother practical challenge is implementation complexity\u2014many sparse attention patterns require custom CUDA kernels or specialized programming, making them difficult to maintain and use <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper> <Paper corpusId=\"259858862\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nThe trade-offs between sparse and dense attention are context-dependent. Studies show that sparse attention models like BigBird, Longformer, and Sparse Transformer outperform standard transformers on long document tasks but may underperform on shorter sequences where full attention is more beneficial <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Beltagy et al., 2020)",
                        "snippets": [
                            "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset."
                        ],
                        "paper": {
                            "corpus_id": 215737171,
                            "title": "Longformer: The Long-Document Transformer",
                            "authors": [
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "39139825",
                                    "name": "Matthew E. Peters"
                                },
                                {
                                    "authorId": "2527954",
                                    "name": "Arman Cohan"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 4100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhuang et al., 2022)",
                        "snippets": [
                            "Sparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention (Parmar et al., 2018) and Multi-passage BERT (Wang et al., 2019) use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird (Zaheer et al., 2020) further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions."
                        ],
                        "paper": {
                            "corpus_id": 258840932,
                            "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability",
                            "authors": [
                                {
                                    "authorId": "1505801820",
                                    "name": "Yufan Zhuang"
                                },
                                {
                                    "authorId": "2240689",
                                    "name": "Zihan Wang"
                                },
                                {
                                    "authorId": "3180064",
                                    "name": "Fangbo Tao"
                                },
                                {
                                    "authorId": "2163679367",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "UniReps",
                            "n_citations": 3
                        },
                        "score": 0.79541015625
                    },
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.82763671875
                    },
                    {
                        "id": "(Yan et al., 2024)",
                        "snippets": [
                            "BigBird is designed to handle long sequences while maintaining a manageable computational complexity. It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer. The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.\n2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.\n3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer."
                        ],
                        "paper": {
                            "corpus_id": 270063477,
                            "title": "Scorch: A Library for Sparse Deep Learning",
                            "authors": [
                                {
                                    "authorId": "2303621454",
                                    "name": "Bobby Yan"
                                },
                                {
                                    "authorId": "2303402046",
                                    "name": "Alexander J Root"
                                },
                                {
                                    "authorId": "2303401237",
                                    "name": "Trevor Gale"
                                },
                                {
                                    "authorId": "2303401552",
                                    "name": "David Broman"
                                },
                                {
                                    "authorId": "2303400919",
                                    "name": "Fredrik Kjolstad"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.681640625
                    },
                    {
                        "id": "(KABENAMUALU et al., 2023)",
                        "snippets": [
                            "BigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle (Zaheer et al., 2020). BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) (Zaheer et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 258762176,
                            "title": "ORKG-Leaderboards: a systematic workflow for mining leaderboards as a knowledge graph",
                            "authors": [
                                {
                                    "authorId": "1591123106",
                                    "name": "Salomon Kabongo KABENAMUALU"
                                },
                                {
                                    "authorId": "1789682566",
                                    "name": "J. D\u2019Souza"
                                },
                                {
                                    "authorId": "145044578",
                                    "name": "S. Auer"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Journal on Digital Libraries",
                            "n_citations": 20
                        },
                        "score": 0.6875
                    },
                    {
                        "id": "(Pham et al., 2024)",
                        "snippets": [
                            "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)",
                            "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 273856640,
                            "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels",
                            "authors": [
                                {
                                    "authorId": "2292320440",
                                    "name": "Linh Manh Pham"
                                },
                                {
                                    "authorId": "2330416501",
                                    "name": "Hoang Cao the"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.615234375
                    },
                    {
                        "id": "(Kitaev et al., 2020)",
                        "snippets": [
                            "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
                        ],
                        "paper": {
                            "corpus_id": 209315300,
                            "title": "Reformer: The Efficient Transformer",
                            "authors": [
                                {
                                    "authorId": "143808231",
                                    "name": "Nikita Kitaev"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2333
                        },
                        "score": 0
                    },
                    {
                        "id": "(Datta, 2024)",
                        "snippets": [
                            "Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers (Katharopoulos et al., 2020) reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird (Zaheer et al., 2020) combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices."
                        ],
                        "paper": {
                            "corpus_id": 273821735,
                            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2345187428",
                                    "name": "Akul Datta"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7080078125
                    },
                    {
                        "id": "(Correia et al., 2019)",
                        "snippets": [
                            "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."
                        ],
                        "paper": {
                            "corpus_id": 202538495,
                            "title": "Adaptively Sparse Transformers",
                            "authors": [
                                {
                                    "authorId": "146783606",
                                    "name": "Gon\u00e7alo M. Correia"
                                },
                                {
                                    "authorId": "2114966",
                                    "name": "Vlad Niculae"
                                },
                                {
                                    "authorId": "145644643",
                                    "name": "Andr\u00e9 F. T. Martins"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 256
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2021)",
                        "snippets": [
                            "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."
                        ],
                        "paper": {
                            "corpus_id": 229923177,
                            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                            "authors": [
                                {
                                    "authorId": "2092641069",
                                    "name": "Siyu Ding"
                                },
                                {
                                    "authorId": "40861754",
                                    "name": "Junyuan Shang"
                                },
                                {
                                    "authorId": "104463827",
                                    "name": "Shuohuan Wang"
                                },
                                {
                                    "authorId": "2117103617",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "50007795",
                                    "name": "Hao Tian"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 55
                        },
                        "score": 0.6767578125
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Different from traditional sparse Transformer (Martins et al., 2016)(Correia et al., 2019)(Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered."
                        ],
                        "paper": {
                            "corpus_id": 259858862,
                            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
                            "authors": [
                                {
                                    "authorId": "4874730",
                                    "name": "Xuanyu Zhang"
                                },
                                {
                                    "authorId": "2068974",
                                    "name": "Zhepeng Lv"
                                },
                                {
                                    "authorId": "2149535351",
                                    "name": "Qing Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 7
                        },
                        "score": 0.60986328125
                    },
                    {
                        "id": "(Wu et al., 2021)",
                        "snippets": [
                            "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms",
                            "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."
                        ],
                        "paper": {
                            "corpus_id": 237260051,
                            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
                            "authors": [
                                {
                                    "authorId": "15161448",
                                    "name": "Chuhan Wu"
                                },
                                {
                                    "authorId": "2397264",
                                    "name": "Fangzhao Wu"
                                },
                                {
                                    "authorId": "50329599",
                                    "name": "Tao Qi"
                                },
                                {
                                    "authorId": "1731776",
                                    "name": "Yongfeng Huang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.88623046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "FlashAttention and Memory-Efficient Implementations",
                "tldr": "FlashAttention introduces an I/O-aware approach that reduces memory usage from O(n\u00b2) to O(n) through tiling and block-wise computation without sacrificing accuracy. This innovation has enabled longer context lengths and spawned extensions like sparse FlashAttention variants that combine efficiency optimizations with sparsity patterns for even greater computational gains. (12 sources)",
                "text": "\nWhile sparse attention mechanisms like BigBird focus on reducing complexity by limiting which tokens attend to each other, FlashAttention takes a fundamentally different approach. It addresses the memory bottleneck in transformer attention by optimizing how data moves between different levels of GPU memory. FlashAttention uses tiling to process smaller chunks of the attention matrix sequentially, minimizing the number of memory transfers between high-bandwidth memory (HBM) and on-chip SRAM <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"276725385\" paperTitle=\"(Xia et al., 2025)\" isShortName></Paper>.\n\nThe key insight of FlashAttention is that it maintains mathematical equivalence to standard attention while achieving O(n) memory complexity through block-wise computation and the use of \"online softmax\" techniques <Paper corpusId=\"276775748\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. This I/O-aware approach resulted in significant practical speed improvements: 15% faster training for BERT-large, 3\u00d7 speedup for GPT-2, and 2.4\u00d7 speedup on long-range arena benchmarks <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\nMore importantly, FlashAttention enabled transformers to handle much longer sequences than previously possible. Models built with FlashAttention achieved better-than-chance performance on extremely long sequences of 16K tokens (Path-X challenge) and 64K tokens (Path-256), opening new capabilities for transformer models <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\n## Extensions and Hybrid Approaches\n\nFlashAttention has evolved through several iterations. FlashAttention-2 improved parallelism and work distribution for better GPU utilization, while FlashAttention-3 introduced asynchronous computation and FP8 precision support to reach 75% GPU utilization on H100 hardware <Paper corpusId=\"276575796\" paperTitle=\"(Navardi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276725385\" paperTitle=\"(Xia et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271098045\" paperTitle=\"(Shah et al., 2024)\" isShortName></Paper>.\n\nResearchers have also created hybrid approaches that combine FlashAttention's memory efficiency with sparse attention patterns:\n\n1. **Block-Sparse FlashAttention** extends the original algorithm with a two-dimensional block mask matrix to skip computations for masked blocks, maintaining efficiency while incorporating sparsity <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"273026224\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n2. **Sparse Causal Flash Attention (SCFA)** optimizes for causal attention in autoregressive models, adding support for key/query dropping and hashing-based attention patterns. This approach achieved training speedups of 2.0\u00d7 and 3.3\u00d7 for sequences of 8K and 16K tokens, respectively, without sacrificing model quality <Paper corpusId=\"259063695\" paperTitle=\"(Pagliardini et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273026224\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Alternative Memory-Efficient Approaches\n\nBeyond FlashAttention, other memory-efficient implementations have emerged:\n\n1. **Multi-query attention (MQA)** and **Grouped-query attention (GQA)** reduce memory bandwidth by using fewer \"key\" and \"value\" heads, addressing the I/O bottleneck from a different angle. GQA is notably used in LLaMA 2 <Paper corpusId=\"266999285\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\n2. **Retrieval-augmented models** avoid storing the full attention context by retrieving information from external memory, offering another approach to process lengthy inputs <Paper corpusId=\"266999285\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\n3. **Clustering-based approaches** allow queries to attend to different sets of key-value (KV) pairs, though these often face challenges with memory redundancy during long-range inference <Paper corpusId=\"270703226\" paperTitle=\"(Lou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212718077\" paperTitle=\"(Roy et al., 2020)\" isShortName></Paper>.\n\nThe trend in memory-efficient implementations reflects a shift in focus from reducing raw computational complexity (FLOPs) to optimizing memory access patterns, as the bottleneck in modern hardware has increasingly become I/O overhead rather than computation itself <Paper corpusId=\"266999285\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This change in perspective has been crucial for enabling transformers to handle the increasingly long contexts required by modern applications.",
                "citations": [
                    {
                        "id": "(Dao et al., 2022)",
                        "snippets": [
                            "FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                        ],
                        "paper": {
                            "corpus_id": 249151871,
                            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                            "authors": [
                                {
                                    "authorId": "24593911",
                                    "name": "Tri Dao"
                                },
                                {
                                    "authorId": "49577833",
                                    "name": "Daniel Y. Fu"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "1755572",
                                    "name": "A. Rudra"
                                },
                                {
                                    "authorId": "2061444681",
                                    "name": "Christopher R'e"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2285
                        },
                        "score": 0.7119140625
                    },
                    {
                        "id": "(Xia et al., 2025)",
                        "snippets": [
                            "FlashAttention [12,(Dao et al., 2022)(Shah et al., 2024) addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention."
                        ],
                        "paper": {
                            "corpus_id": 276725385,
                            "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
                            "authors": [
                                {
                                    "authorId": "2344790336",
                                    "name": "Yifei Xia"
                                },
                                {
                                    "authorId": "2348282342",
                                    "name": "Suhan Ling"
                                },
                                {
                                    "authorId": "46182701",
                                    "name": "Fangcheng Fu"
                                },
                                {
                                    "authorId": "2167500394",
                                    "name": "Yujie Wang"
                                },
                                {
                                    "authorId": "2108525422",
                                    "name": "Huixia Li"
                                },
                                {
                                    "authorId": "2319391688",
                                    "name": "Xuefeng Xiao"
                                },
                                {
                                    "authorId": "2313408987",
                                    "name": "Bin Cui"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.603515625
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence",
                            "Flash Attention (Dao et al., 2022) reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion."
                        ],
                        "paper": {
                            "corpus_id": 276775748,
                            "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
                            "authors": [
                                {
                                    "authorId": "2348389336",
                                    "name": "Yujiao Yang"
                                },
                                {
                                    "authorId": "2282559167",
                                    "name": "Jing Lian"
                                },
                                {
                                    "authorId": "2244250357",
                                    "name": "Linhui Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6806640625
                    },
                    {
                        "id": "(Navardi et al., 2025)",
                        "snippets": [
                            "FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations",
                            "Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns."
                        ],
                        "paper": {
                            "corpus_id": 276575796,
                            "title": "GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices",
                            "authors": [
                                {
                                    "authorId": "2180067662",
                                    "name": "Mozhgan Navardi"
                                },
                                {
                                    "authorId": "2346327200",
                                    "name": "Romina Aalishah"
                                },
                                {
                                    "authorId": "2335995540",
                                    "name": "Yuzhe Fu"
                                },
                                {
                                    "authorId": "2223973348",
                                    "name": "Yueqian Lin"
                                },
                                {
                                    "authorId": "2346061748",
                                    "name": "Hai Li"
                                },
                                {
                                    "authorId": "2247106559",
                                    "name": "Yiran Chen"
                                },
                                {
                                    "authorId": "2393902",
                                    "name": "T. Mohsenin"
                                }
                            ],
                            "year": 2025,
                            "venue": "Proceedings of the AAAI Symposium Series",
                            "n_citations": 2
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Shah et al., 2024)",
                        "snippets": [
                            "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention."
                        ],
                        "paper": {
                            "corpus_id": 271098045,
                            "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
                            "authors": [
                                {
                                    "authorId": "2275225725",
                                    "name": "Jay Shah"
                                },
                                {
                                    "authorId": "3206527",
                                    "name": "Ganesh Bikshandi"
                                },
                                {
                                    "authorId": "2310719512",
                                    "name": "Ying Zhang"
                                },
                                {
                                    "authorId": "2310700174",
                                    "name": "Vijay Thakkar"
                                },
                                {
                                    "authorId": "2310700428",
                                    "name": "Pradeep Ramani"
                                },
                                {
                                    "authorId": "2310701039",
                                    "name": "Tri Dao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 157
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "FlashAttention achieves a memory overhead of O(), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process",
                            "Sparse Causal Flash Attention (SCFA) (Pagliardini et al., 2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed."
                        ],
                        "paper": {
                            "corpus_id": 273026224,
                            "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
                            "authors": [
                                {
                                    "authorId": "2315166927",
                                    "name": "Guoxia Wang"
                                },
                                {
                                    "authorId": "2072984835",
                                    "name": "Jinle Zeng"
                                },
                                {
                                    "authorId": "2325510350",
                                    "name": "Xiyuan Xiao"
                                },
                                {
                                    "authorId": "2323811334",
                                    "name": "Siming Wu"
                                },
                                {
                                    "authorId": "2323896653",
                                    "name": "Jiabin Yang"
                                },
                                {
                                    "authorId": "2324060184",
                                    "name": "Lujing Zheng"
                                },
                                {
                                    "authorId": "2323781886",
                                    "name": "Zeyu Chen"
                                },
                                {
                                    "authorId": "2324064223",
                                    "name": "Jiang Bian"
                                },
                                {
                                    "authorId": "2315250077",
                                    "name": "Dianhai Yu"
                                },
                                {
                                    "authorId": "2323851061",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3
                        },
                        "score": 0.701171875
                    },
                    {
                        "id": "(Pagliardini et al., 2023)",
                        "snippets": [
                            "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens."
                        ],
                        "paper": {
                            "corpus_id": 259063695,
                            "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
                            "authors": [
                                {
                                    "authorId": "2435537",
                                    "name": "Matteo Pagliardini"
                                },
                                {
                                    "authorId": "50552613",
                                    "name": "Daniele Paliotta"
                                },
                                {
                                    "authorId": "2456863",
                                    "name": "Martin Jaggi"
                                },
                                {
                                    "authorId": "116272138",
                                    "name": "Franccois Fleuret"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 25
                        },
                        "score": 0.732421875
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Sparse transformers (Child et al., 2019;(Zaheer et al., 2020)Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;(Zaheer et al., 2020)Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020)(Lewis et al., 2020)Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022)Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs."
                        ],
                        "paper": {
                            "corpus_id": 266999285,
                            "title": "Extending LLMs' Context Window with 100 Samples",
                            "authors": [
                                {
                                    "authorId": "2279654115",
                                    "name": "Yikai Zhang"
                                },
                                {
                                    "authorId": "2278801242",
                                    "name": "Junlong Li"
                                },
                                {
                                    "authorId": "2256991660",
                                    "name": "Pengfei Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.67529296875
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lou et al., 2024)",
                        "snippets": [
                            "Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,(Roy et al., 2020) allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42](Yu et al., 2023).(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters."
                        ],
                        "paper": {
                            "corpus_id": 270703226,
                            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
                            "authors": [
                                {
                                    "authorId": "2061806821",
                                    "name": "Chao Lou"
                                },
                                {
                                    "authorId": "1453587987",
                                    "name": "Zixia Jia"
                                },
                                {
                                    "authorId": "2266032392",
                                    "name": "Zilong Zheng"
                                },
                                {
                                    "authorId": "40341553",
                                    "name": "Kewei Tu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.74609375
                    },
                    {
                        "id": "(Roy et al., 2020)",
                        "snippets": [
                            "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1"
                        ],
                        "paper": {
                            "corpus_id": 212718077,
                            "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
                            "authors": [
                                {
                                    "authorId": "39788470",
                                    "name": "Aurko Roy"
                                },
                                {
                                    "authorId": "2814161",
                                    "name": "M. Saffar"
                                },
                                {
                                    "authorId": "40348417",
                                    "name": "Ashish Vaswani"
                                },
                                {
                                    "authorId": "2529182",
                                    "name": "David Grangier"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 603
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Computational Complexity Comparison",
                "tldr": "Different sparse attention mechanisms reduce transformer complexity from O(n\u00b2) to O(n) or O(n log n), with methods like BigBird and Longformer achieving linear scaling through complementary attention patterns. FlashAttention takes a different approach by optimizing memory usage while maintaining full attention computation. (12 sources)",
                "text": "\nTransformers with the standard attention mechanism have a computational and memory complexity of O(n\u00b2) with respect to sequence length, creating a fundamental bottleneck for processing long documents. Sparse attention mechanisms address this challenge by reducing this quadratic complexity to more manageable levels:\n\n## Linear Complexity O(n) Approaches\n\nSeveral sparse attention methods achieve linear complexity by limiting attention to a subset of token pairs. BigBird combines local window attention, global tokens, and random attention patterns to reduce complexity to O(n) while maintaining theoretical properties like Turing completeness and universal approximation <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. Similarly, Longformer uses a combination of sliding window patterns and global attention to achieve O(n) complexity <Paper corpusId=\"266110855\" paperTitle=\"(Zhou, 2023)\" isShortName></Paper> <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\nThese pattern-based approaches provide a significant advantage: for a sequence of length n, they require only O(n) memory and computation rather than O(n\u00b2), enabling them to process sequences up to 8 times longer than was previously possible on the same hardware <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\n## Sub-quadratic O(n log n) Approaches\n\nSome methods achieve a complexity of O(n log n), striking a middle ground between full quadratic attention and strictly linear approaches:\n\n- Reformer uses locality-sensitive hashing (LSH) to identify similar tokens likely to have high attention scores, reducing complexity to O(n log n) <Paper corpusId=\"209315300\" paperTitle=\"(Kitaev et al., 2020)\" isShortName></Paper> <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>.\n- Sparse Transformer uses factorized self-attention patterns that reduce complexity to O(n\u221an) <Paper corpusId=\"248427085\" paperTitle=\"(Soydaner, 2022)\" isShortName></Paper>.\n\n## Theoretical Limitations\n\nDespite their efficiency gains, sparse attention mechanisms face theoretical limitations. Research has shown that sparse attention cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. Some problems that can be solved by dense attention in O(1) layers would require \u03a9(n) layers with sparse attention <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>. This theoretical limitation has practical implications, as noted by researchers who caution that models like BigBird and Longformer may not be universal replacements for dense attention but rather task-specific solutions for long sequences <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\n## Implementation Challenges\n\nImplementation of sparse attention presents technical challenges. Most current implementations are not fully \"hardware-aware\" and may perform redundant computations, such as dense-dense matrix multiplication followed by masking based on the attention pattern <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>. Many sparse attention patterns require custom CUDA kernels or specialized programming, making them difficult to maintain and use in practice <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>.\n\n## Alternative Approaches\n\nWhile sparse attention reduces computational complexity by computing fewer attention scores, other approaches tackle the efficiency problem differently:\n\n- Linear attention methods like Performers and Linear Transformers reformulate the attention function to achieve linear complexity without explicit computation of the full attention matrix <Paper corpusId=\"273821735\" paperTitle=\"(Datta, 2024)\" isShortName></Paper> <Paper corpusId=\"220250819\" paperTitle=\"(Katharopoulos et al., 2020)\" isShortName></Paper>.\n- Low-rank approximation methods like Linformer reduce complexity by approximating the attention matrix <Paper corpusId=\"266210450\" paperTitle=\"(Song et al., 2023)\" isShortName></Paper>.\n- Memory-efficient implementations like FlashAttention maintain the standard attention computation but optimize memory access patterns, addressing I/O bottlenecks rather than reducing computational complexity <Paper corpusId=\"273850602\" paperTitle=\"(Haris, 2024)\" isShortName></Paper>.\n\nThe diversity of approaches reflects the complex trade-offs between computational efficiency, memory usage, and model effectiveness, with the optimal choice depending on the specific application requirements and hardware constraints <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.82763671875
                    },
                    {
                        "id": "(Zhou, 2023)",
                        "snippets": [
                            "As the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention (Ainslie et al., 2020) if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird (Zaheer et al., 2020), LongT5 (Guo et al., 2021) have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy."
                        ],
                        "paper": {
                            "corpus_id": 266110855,
                            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
                            "authors": [
                                {
                                    "authorId": "2189279577",
                                    "name": "Le Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.669921875
                    },
                    {
                        "id": "(Bae et al., 2022)",
                        "snippets": [
                            "In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."
                        ],
                        "paper": {
                            "corpus_id": 254725259,
                            "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion",
                            "authors": [
                                {
                                    "authorId": "2195944251",
                                    "name": "Jongseong Bae"
                                },
                                {
                                    "authorId": "2196700504",
                                    "name": "Byung Do Cheon"
                                },
                                {
                                    "authorId": "2129420267",
                                    "name": "Ha Young Kim"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.81591796875
                    },
                    {
                        "id": "(Kitaev et al., 2020)",
                        "snippets": [
                            "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
                        ],
                        "paper": {
                            "corpus_id": 209315300,
                            "title": "Reformer: The Efficient Transformer",
                            "authors": [
                                {
                                    "authorId": "143808231",
                                    "name": "Nikita Kitaev"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2333
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2021)",
                        "snippets": [
                            "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."
                        ],
                        "paper": {
                            "corpus_id": 229923177,
                            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                            "authors": [
                                {
                                    "authorId": "2092641069",
                                    "name": "Siyu Ding"
                                },
                                {
                                    "authorId": "40861754",
                                    "name": "Junyuan Shang"
                                },
                                {
                                    "authorId": "104463827",
                                    "name": "Shuohuan Wang"
                                },
                                {
                                    "authorId": "2117103617",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "50007795",
                                    "name": "Hao Tian"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 55
                        },
                        "score": 0.6767578125
                    },
                    {
                        "id": "(Soydaner, 2022)",
                        "snippets": [
                            "Sparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight (Correia et al., 2019). By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods (Peng et al., 2021). Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method (Chen et al., 2021). A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear (Zaheer et al., 2020). Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering."
                        ],
                        "paper": {
                            "corpus_id": 248427085,
                            "title": "Attention mechanism in neural networks: where it comes and where it goes",
                            "authors": [
                                {
                                    "authorId": "1409173278",
                                    "name": "Derya Soydaner"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural computing & applications (Print)",
                            "n_citations": 166
                        },
                        "score": 0.6787109375
                    },
                    {
                        "id": "(Pham et al., 2024)",
                        "snippets": [
                            "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)",
                            "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 273856640,
                            "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels",
                            "authors": [
                                {
                                    "authorId": "2292320440",
                                    "name": "Linh Manh Pham"
                                },
                                {
                                    "authorId": "2330416501",
                                    "name": "Hoang Cao the"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.615234375
                    },
                    {
                        "id": "(Tomczak et al., 2025)",
                        "snippets": [
                            "A popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism (Zaheer et al., 2020)- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach",
                            "However, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements."
                        ],
                        "paper": {
                            "corpus_id": 276106883,
                            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                            "authors": [
                                {
                                    "authorId": "2276535724",
                                    "name": "Nathaniel Tomczak"
                                },
                                {
                                    "authorId": "2873546",
                                    "name": "S. Kuppannagari"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.82958984375
                    },
                    {
                        "id": "(Datta, 2024)",
                        "snippets": [
                            "Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers (Katharopoulos et al., 2020) reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird (Zaheer et al., 2020) combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices."
                        ],
                        "paper": {
                            "corpus_id": 273821735,
                            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2345187428",
                                    "name": "Akul Datta"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7080078125
                    },
                    {
                        "id": "(Katharopoulos et al., 2020)",
                        "snippets": [
                            "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."
                        ],
                        "paper": {
                            "corpus_id": 220250819,
                            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
                            "authors": [
                                {
                                    "authorId": "3493855",
                                    "name": "Angelos Katharopoulos"
                                },
                                {
                                    "authorId": "2992087",
                                    "name": "Apoorv Vyas"
                                },
                                {
                                    "authorId": "143958923",
                                    "name": "Nikolaos Pappas"
                                },
                                {
                                    "authorId": "116272138",
                                    "name": "Franccois Fleuret"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1790
                        },
                        "score": 0
                    },
                    {
                        "id": "(Song et al., 2023)",
                        "snippets": [
                            "Attention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 266210450,
                            "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
                            "authors": [
                                {
                                    "authorId": "50982080",
                                    "name": "Kaiqiang Song"
                                },
                                {
                                    "authorId": "2250363276",
                                    "name": "Xiaoyang Wang"
                                },
                                {
                                    "authorId": "2173531",
                                    "name": "Sangwoo Cho"
                                },
                                {
                                    "authorId": "2243367575",
                                    "name": "Xiaoman Pan"
                                },
                                {
                                    "authorId": "2256336899",
                                    "name": "Dong Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.59375
                    },
                    {
                        "id": "(Haris, 2024)",
                        "snippets": [
                            "Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention",
                            "Efficient computation of self-attention has been a focal point of research in recent years (Fournier et al., 2021). Flash Attention [DFE + 22] and related work [SY[24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC[21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC[20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF[24], or adaptive techniques [CNM[19]."
                        ],
                        "paper": {
                            "corpus_id": 273850602,
                            "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
                            "authors": [
                                {
                                    "authorId": "2329374652",
                                    "name": "Themistoklis Haris"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.61474609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance and Context Length Capabilities",
                "tldr": "Sparse attention mechanisms like BigBird and Longformer enable processing sequences up to 8 times longer than standard transformers while maintaining theoretical properties like Turing completeness. FlashAttention achieves even greater capabilities, handling sequences of 16K to 64K tokens while improving model quality. (9 sources)",
                "text": "\nSparse attention mechanisms have significantly extended the maximum context length that transformer models can effectively process. BigBird, with its combination of global, local, and random attention patterns, can handle sequences up to 8 times longer than was previously possible using similar hardware <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper> <Paper corpusId=\"264426102\" paperTitle=\"(Madani et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>. This dramatic increase in context length capability has practical implications for long-document tasks such as document classification, question answering, and summarization.\n\nModels like Longformer and BigBird have demonstrated particularly strong performance on tasks requiring long-context understanding. Research comparing these sparse attention models with standard transformers found that they outperform vanilla transformers on long document modeling tasks such as classification on Amazon, IMDB, and MIND datasets <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>. The performance advantage stems from their ability to process more of the input text, as standard transformers must truncate long documents due to computational constraints.\n\nHowever, sparse attention models show a performance trade-off based on sequence length. For shorter sequences, standard transformers with full attention can outperform sparse attention models. Studies have shown that sparse attention approaches are inferior to vanilla transformers in short sequence modeling tasks like AG classification and recommendation on MIND <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>. This suggests that sparse attention models cannot fully capture important interactions between tokens when working with shorter inputs, highlighting that they are not universal replacements for dense attention but rather specialized solutions for long-sequence tasks <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\nThe architectural choices in sparse attention models involve significant trade-offs. While models with global attention elements can capture long-range dependencies, they still retain some quadratic computational elements that limit maximum context length. Conversely, models using only local attention can process much longer documents but may struggle to effectively model long-range dependencies, resulting in reduced accuracy <Paper corpusId=\"266110855\" paperTitle=\"(Zhou, 2023)\" isShortName></Paper>.\n\nMemory-efficient implementations like FlashAttention take a different approach to extending context length. Rather than sparsifying attention, FlashAttention optimizes memory usage while maintaining full attention computation. This approach has yielded impressive results, enabling models to achieve better-than-chance performance on extremely long sequences: 61.4% accuracy on the Path-X challenge with 16K tokens and 63.1% accuracy on Path-256 with 64K tokens <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. Beyond just handling longer sequences, FlashAttention also improves model quality, delivering 0.7 better perplexity on GPT-2 and 6.4 points of improvement on long-document classification tasks <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\nThe evolution of context length capabilities has continued to advance rapidly. Recent commercial models have pushed boundaries even further, with OpenAI's GPT-4o and Google's Gemini 2.0 Flash reportedly supporting context windows of 1 million tokens for both text-only and multimodal inputs by combining blockwise attention with disk-paged key-value caches <Paper corpusId=\"278000642\" paperTitle=\"(Ahn, 2025)\" isShortName></Paper>. This represents a dramatic expansion beyond the 16-32K token limits that earlier sparse attention families like Longformer, BigBird, and Reformer helped establish.\n\nDespite these advances, theoretical analysis reveals that sparse attention mechanisms still face fundamental limitations. Research has shown that certain problems solvable by dense attention in O(1) layers would require \u03a9(n) layers with any sparse attention mechanism <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. This theoretical constraint helps explain why sparse attention models, despite their efficiency advantages, cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.82763671875
                    },
                    {
                        "id": "(Madani et al., 2023)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, (Zaheer et al., 2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware."
                        ],
                        "paper": {
                            "corpus_id": 264426102,
                            "title": "REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization",
                            "authors": [
                                {
                                    "authorId": "2261279246",
                                    "name": "Mohammad Reza Ghasemi Madani"
                                },
                                {
                                    "authorId": "2294362638",
                                    "name": "Pasquale Minervini"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Computational Natural Language Learning",
                            "n_citations": 4
                        },
                        "score": 0.857421875
                    },
                    {
                        "id": "(Pham et al., 2024)",
                        "snippets": [
                            "The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)",
                            "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 273856640,
                            "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels",
                            "authors": [
                                {
                                    "authorId": "2292320440",
                                    "name": "Linh Manh Pham"
                                },
                                {
                                    "authorId": "2330416501",
                                    "name": "Hoang Cao the"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.615234375
                    },
                    {
                        "id": "(Wu et al., 2021)",
                        "snippets": [
                            "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms",
                            "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."
                        ],
                        "paper": {
                            "corpus_id": 237260051,
                            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
                            "authors": [
                                {
                                    "authorId": "15161448",
                                    "name": "Chuhan Wu"
                                },
                                {
                                    "authorId": "2397264",
                                    "name": "Fangzhao Wu"
                                },
                                {
                                    "authorId": "50329599",
                                    "name": "Tao Qi"
                                },
                                {
                                    "authorId": "1731776",
                                    "name": "Yongfeng Huang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Bae et al., 2022)",
                        "snippets": [
                            "In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."
                        ],
                        "paper": {
                            "corpus_id": 254725259,
                            "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion",
                            "authors": [
                                {
                                    "authorId": "2195944251",
                                    "name": "Jongseong Bae"
                                },
                                {
                                    "authorId": "2196700504",
                                    "name": "Byung Do Cheon"
                                },
                                {
                                    "authorId": "2129420267",
                                    "name": "Ha Young Kim"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.81591796875
                    },
                    {
                        "id": "(Zhou, 2023)",
                        "snippets": [
                            "As the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention (Ainslie et al., 2020) if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird (Zaheer et al., 2020), LongT5 (Guo et al., 2021) have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy."
                        ],
                        "paper": {
                            "corpus_id": 266110855,
                            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
                            "authors": [
                                {
                                    "authorId": "2189279577",
                                    "name": "Le Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.669921875
                    },
                    {
                        "id": "(Dao et al., 2022)",
                        "snippets": [
                            "FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                        ],
                        "paper": {
                            "corpus_id": 249151871,
                            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                            "authors": [
                                {
                                    "authorId": "24593911",
                                    "name": "Tri Dao"
                                },
                                {
                                    "authorId": "49577833",
                                    "name": "Daniel Y. Fu"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "1755572",
                                    "name": "A. Rudra"
                                },
                                {
                                    "authorId": "2061444681",
                                    "name": "Christopher R'e"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2285
                        },
                        "score": 0.7119140625
                    },
                    {
                        "id": "(Ahn, 2025)",
                        "snippets": [
                            "Sparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression."
                        ],
                        "paper": {
                            "corpus_id": 278000642,
                            "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations",
                            "authors": [
                                {
                                    "authorId": "2356854731",
                                    "name": "Kwangseob Ahn"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.693359375
                    },
                    {
                        "id": "(Ding et al., 2021)",
                        "snippets": [
                            "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."
                        ],
                        "paper": {
                            "corpus_id": 229923177,
                            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                            "authors": [
                                {
                                    "authorId": "2092641069",
                                    "name": "Siyu Ding"
                                },
                                {
                                    "authorId": "40861754",
                                    "name": "Junyuan Shang"
                                },
                                {
                                    "authorId": "104463827",
                                    "name": "Shuohuan Wang"
                                },
                                {
                                    "authorId": "2117103617",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "50007795",
                                    "name": "Hao Tian"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 55
                        },
                        "score": 0.6767578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations and Trade-offs",
                "tldr": "While sparse attention mechanisms reduce computational complexity from O(n\u00b2) to O(n) or O(n log n), they face fundamental theoretical limitations, implementation challenges, and context-dependent performance trade-offs that prevent them from universally replacing dense attention for all tasks. (14 sources)",
                "text": "\nDespite their impressive efficiency gains, sparse attention mechanisms and memory-efficient implementations face several critical limitations and trade-offs that impact their practical utility across different applications.\n\n## Theoretical Limitations\n\nResearch has demonstrated that sparse attention mechanisms cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. This theoretical constraint has practical implications: certain problems that can be efficiently solved by dense attention in O(1) layers would require \u03a9(n) layers with any sparse attention mechanism <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. This fundamental limitation helps explain why sparse attention models, while efficient, cannot serve as universal replacements for standard transformers across all applications <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\n## Implementation Challenges\n\nMost sparse attention mechanisms require specialized programming and custom CUDA kernels, making them difficult to implement, maintain, and use in practice <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. Additionally, current implementations of sparse attention are often not fully \"hardware-aware\" \u2013 they typically perform dense-dense matrix multiplication followed by masking based on the attention pattern, potentially resulting in redundant computations <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>.\n\nMany implementations face efficiency challenges at both training and inference times. For example, clustering-based methods that allow queries to attend to different sets of key-value pairs generally cost O(n log n) to maintain clusters <Paper corpusId=\"270703226\" paperTitle=\"(Lou et al., 2024)\" isShortName></Paper>. During long-range inference, these methods can lead to significant memory redundancy as key-value embeddings must be fully stored to avoid repetitive computation <Paper corpusId=\"270703226\" paperTitle=\"(Lou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264439578\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\n## Context-Dependent Performance\n\nThe performance of sparse attention models shows a strong dependency on sequence length and task requirements. Empirical studies demonstrate that while sparse attention models like BigBird and Longformer outperform standard transformers on long document tasks (e.g., classification on Amazon, IMDB, and MIND datasets), they underperform compared to vanilla transformers on short sequence modeling tasks (e.g., AG classification) <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>. This performance differential suggests that sparse attention models cannot fully capture important interactions between tokens in shorter contexts <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>.\n\n## Pattern Design Limitations\n\nMost early sparse attention approaches rely on hand-crafted patterns that are selected empirically or randomly. Models like BigBird combine random attention, global attention, and local sliding windows, while Longformer employs task-specific global attention and local windowed attention <Paper corpusId=\"259858862\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. These predefined patterns, while effective, inherently limit flexibility <Paper corpusId=\"276618265\" paperTitle=\"(Fu et al., 2025)\" isShortName></Paper>. The challenge of adaptively selecting useful tokens for sparse attention according to context remains an important research problem <Paper corpusId=\"259858862\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nMore recent approaches have attempted to address this limitation through learnable sparsity patterns. The Adaptively Sparse Transformer replaces the standard softmax with \u03b1-entmax, allowing attention heads to choose between focused or spread-out behavior <Paper corpusId=\"252216464\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"202538495\" paperTitle=\"(Correia et al., 2019)\" isShortName></Paper>. However, these adaptive approaches often introduce additional computational overhead that can offset some of the efficiency gains from sparsity.\n\n## Information Loss\n\nBy design, sparse attention mechanisms sacrifice full access to the long sequence during attention, resulting in an inevitable loss of some contextual information <Paper corpusId=\"264451707\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This trade-off is particularly challenging for block-sparse attention methods, which struggle to balance accuracy and efficiency due to the computational cost of measuring block importance <Paper corpusId=\"277151262\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n## Computational vs. Memory Optimization\n\nAn important distinction exists between approaches that optimize computational complexity (FLOPs) versus those that target memory efficiency. Many sparse attention mechanisms focus primarily on reducing computational complexity but may still face memory bottlenecks. Conversely, memory-efficient implementations like FlashAttention optimize memory access patterns while maintaining the standard attention computation, addressing I/O bottlenecks rather than reducing computational complexity directly <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe diversity of approaches\u2014from pattern-based sparse attention to memory-efficient implementations\u2014reflects the complex, multi-dimensional trade-offs between computational efficiency, memory usage, and model effectiveness. The optimal choice among these approaches depends heavily on specific application requirements, hardware constraints, and whether the priority is processing extremely long sequences or maintaining full attention capabilities <Paper corpusId=\"273228328\" paperTitle=\"(Eisner, 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ding et al., 2021)",
                        "snippets": [
                            "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."
                        ],
                        "paper": {
                            "corpus_id": 229923177,
                            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                            "authors": [
                                {
                                    "authorId": "2092641069",
                                    "name": "Siyu Ding"
                                },
                                {
                                    "authorId": "40861754",
                                    "name": "Junyuan Shang"
                                },
                                {
                                    "authorId": "104463827",
                                    "name": "Shuohuan Wang"
                                },
                                {
                                    "authorId": "2117103617",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "50007795",
                                    "name": "Hao Tian"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 55
                        },
                        "score": 0.6767578125
                    },
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.82763671875
                    },
                    {
                        "id": "(Bae et al., 2022)",
                        "snippets": [
                            "In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."
                        ],
                        "paper": {
                            "corpus_id": 254725259,
                            "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion",
                            "authors": [
                                {
                                    "authorId": "2195944251",
                                    "name": "Jongseong Bae"
                                },
                                {
                                    "authorId": "2196700504",
                                    "name": "Byung Do Cheon"
                                },
                                {
                                    "authorId": "2129420267",
                                    "name": "Ha Young Kim"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.81591796875
                    },
                    {
                        "id": "(Tomczak et al., 2025)",
                        "snippets": [
                            "A popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism (Zaheer et al., 2020)- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach",
                            "However, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements."
                        ],
                        "paper": {
                            "corpus_id": 276106883,
                            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                            "authors": [
                                {
                                    "authorId": "2276535724",
                                    "name": "Nathaniel Tomczak"
                                },
                                {
                                    "authorId": "2873546",
                                    "name": "S. Kuppannagari"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.82958984375
                    },
                    {
                        "id": "(Lou et al., 2024)",
                        "snippets": [
                            "Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,(Roy et al., 2020) allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42](Yu et al., 2023).(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters."
                        ],
                        "paper": {
                            "corpus_id": 270703226,
                            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
                            "authors": [
                                {
                                    "authorId": "2061806821",
                                    "name": "Chao Lou"
                                },
                                {
                                    "authorId": "1453587987",
                                    "name": "Zixia Jia"
                                },
                                {
                                    "authorId": "2266032392",
                                    "name": "Zilong Zheng"
                                },
                                {
                                    "authorId": "40341553",
                                    "name": "Kewei Tu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.74609375
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters."
                        ],
                        "paper": {
                            "corpus_id": 264439578,
                            "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2261476365",
                                    "name": "Haofei Yu"
                                },
                                {
                                    "authorId": "35504092",
                                    "name": "Cunxiang Wang"
                                },
                                {
                                    "authorId": "2261496744",
                                    "name": "Yue Zhang"
                                },
                                {
                                    "authorId": "2237804371",
                                    "name": "Wei Bi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2021)",
                        "snippets": [
                            "We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms",
                            "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."
                        ],
                        "paper": {
                            "corpus_id": 237260051,
                            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
                            "authors": [
                                {
                                    "authorId": "15161448",
                                    "name": "Chuhan Wu"
                                },
                                {
                                    "authorId": "2397264",
                                    "name": "Fangzhao Wu"
                                },
                                {
                                    "authorId": "50329599",
                                    "name": "Tao Qi"
                                },
                                {
                                    "authorId": "1731776",
                                    "name": "Yongfeng Huang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Different from traditional sparse Transformer (Martins et al., 2016)(Correia et al., 2019)(Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered."
                        ],
                        "paper": {
                            "corpus_id": 259858862,
                            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
                            "authors": [
                                {
                                    "authorId": "4874730",
                                    "name": "Xuanyu Zhang"
                                },
                                {
                                    "authorId": "2068974",
                                    "name": "Zhepeng Lv"
                                },
                                {
                                    "authorId": "2149535351",
                                    "name": "Qing Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 7
                        },
                        "score": 0.60986328125
                    },
                    {
                        "id": "(Fu et al., 2025)",
                        "snippets": [
                            "While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility."
                        ],
                        "paper": {
                            "corpus_id": 276618265,
                            "title": "Sliding Window Attention Training for Efficient Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2275537250",
                                    "name": "Zichuan Fu"
                                },
                                {
                                    "authorId": "2347893589",
                                    "name": "Wentao Song"
                                },
                                {
                                    "authorId": "2162455919",
                                    "name": "Yejing Wang"
                                },
                                {
                                    "authorId": "2277462592",
                                    "name": "Xian Wu"
                                },
                                {
                                    "authorId": "2237585282",
                                    "name": "Yefeng Zheng"
                                },
                                {
                                    "authorId": "2344958511",
                                    "name": "Yingying Zhang"
                                },
                                {
                                    "authorId": "2262514619",
                                    "name": "Derong Xu"
                                },
                                {
                                    "authorId": "2298206411",
                                    "name": "Xuetao Wei"
                                },
                                {
                                    "authorId": "2151647484",
                                    "name": "Tong Xu"
                                },
                                {
                                    "authorId": "2281902096",
                                    "name": "Xiangyu Zhao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.63427734375
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,(Guo et al., 2019)29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer (Guo et al., 2019) replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts."
                        ],
                        "paper": {
                            "corpus_id": 252216464,
                            "title": "Denoising Self-Attentive Sequential Recommendation",
                            "authors": [
                                {
                                    "authorId": "1504511015",
                                    "name": "Huiyuan Chen"
                                },
                                {
                                    "authorId": "2410838",
                                    "name": "Yusan Lin"
                                },
                                {
                                    "authorId": "29913565",
                                    "name": "Menghai Pan"
                                },
                                {
                                    "authorId": "2127181454",
                                    "name": "Lan Wang"
                                },
                                {
                                    "authorId": "3056465",
                                    "name": "Chin-Chia Michael Yeh"
                                },
                                {
                                    "authorId": "2185014510",
                                    "name": "Xiaoting Li"
                                },
                                {
                                    "authorId": "2185013996",
                                    "name": "Yan Zheng"
                                },
                                {
                                    "authorId": "2148956204",
                                    "name": "Fei Wang"
                                },
                                {
                                    "authorId": "2145058012",
                                    "name": "Hao Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM Conference on Recommender Systems",
                            "n_citations": 50
                        },
                        "score": 0.71435546875
                    },
                    {
                        "id": "(Correia et al., 2019)",
                        "snippets": [
                            "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."
                        ],
                        "paper": {
                            "corpus_id": 202538495,
                            "title": "Adaptively Sparse Transformers",
                            "authors": [
                                {
                                    "authorId": "146783606",
                                    "name": "Gon\u00e7alo M. Correia"
                                },
                                {
                                    "authorId": "2114966",
                                    "name": "Vlad Niculae"
                                },
                                {
                                    "authorId": "145644643",
                                    "name": "Andr\u00e9 F. T. Martins"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 256
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, (Dai et al., 2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information."
                        ],
                        "paper": {
                            "corpus_id": 264451707,
                            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2155315840",
                                    "name": "Guanzheng Chen"
                                },
                                {
                                    "authorId": "40613621",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": "3451645",
                                    "name": "Zaiqiao Meng"
                                },
                                {
                                    "authorId": "3279808",
                                    "name": "Shangsong Liang"
                                },
                                {
                                    "authorId": "2211459675",
                                    "name": "Li Bing"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 32
                        },
                        "score": 0.72216796875
                    },
                    {
                        "id": "(Xu et al., 2025)",
                        "snippets": [
                            "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements."
                        ],
                        "paper": {
                            "corpus_id": 277151262,
                            "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
                            "authors": [
                                {
                                    "authorId": "2351045310",
                                    "name": "Ruyi Xu"
                                },
                                {
                                    "authorId": "2046958974",
                                    "name": "Guangxuan Xiao"
                                },
                                {
                                    "authorId": "2351511467",
                                    "name": "Haofeng Huang"
                                },
                                {
                                    "authorId": "2325916768",
                                    "name": "Junxian Guo"
                                },
                                {
                                    "authorId": "2249530374",
                                    "name": "Song Han"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 15
                        },
                        "score": 0.65234375
                    },
                    {
                        "id": "(Eisner, 2024)",
                        "snippets": [
                            "These Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques:",
                            "For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks",
                            "while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token."
                        ],
                        "paper": {
                            "corpus_id": 273228328,
                            "title": "InAttention: Linear Context Scaling for Transformers",
                            "authors": [
                                {
                                    "authorId": "2325097804",
                                    "name": "Joseph Eisner"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6142578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.290562
    }
}