{
    "query": "was anyone work LLM fine tuning when done the model worse some things For example lots of anecdotes fine LLMs specific tasks hurts generation ability but I papers with systematic exploration",
    "user_id": "lib_user",
    "task_id": "0b8fac41-1f22-408b-835f-82ef92f935b6",
    "timestamp": "2025-06-23T21:19:35.620728",
    "n_retrieval": 256,
    "n_retrieved": 274,
    "n_candidates": 4,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.075042,
    "decomposed_query": {
        "rewritten_query": "Studies exploring when LLM fine-tuning makes the model worse at some tasks, particularly when fine-tuning for specific tasks hurts general generation ability.",
        "keyword_query": "LLM fine-tuning worse model performance generation ability systematic exploration",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010053,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 18,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "122045993",
                    "name": "Sheng-Chieh Lin"
                },
                {
                    "authorId": "2299485255",
                    "name": "Luyu Gao"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "2266752758",
                    "name": "Wenhan Xiong"
                },
                {
                    "authorId": "2273564585",
                    "name": "Jimmy Lin"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "2292024725",
                    "name": "Xilun Chen"
                }
            ],
            "abstract": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
            "corpus_id": 269502676,
            "sentences": [
                {
                    "corpus_id": "269502676",
                    "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
                    "text": "In contrast, our work provides a comprehensive examination of improving LLMs' factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO.Concurrent to our work, Kang et al. (2024) find that LLMs tend to hallucinate when facing unfamiliar queries.They consider improving LLMs' factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with FLAME (see case studies in Section 5.5).It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation).In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions.",
                    "score": 0.5908954811578881,
                    "section_title": "Related Work",
                    "char_start_offset": 6250,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 184,
                            "end": 293
                        },
                        {
                            "start": 293,
                            "end": 525
                        },
                        {
                            "start": 525,
                            "end": 751
                        },
                        {
                            "start": 751,
                            "end": 858
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5400390625
                }
            ],
            "relevance_judgement": 0.5400390625,
            "relevance_judgment_input_expanded": "# Title: FLAME: Factuality-Aware Alignment for Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Sheng-Chieh Lin, Luyu Gao, Barlas O\u011fuz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, Xilun Chen\n## Abstract\nAlignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.\n## Related Work\nIn contrast, our work provides a comprehensive examination of improving LLMs' factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO.Concurrent to our work, Kang et al. (2024) find that LLMs tend to hallucinate when facing unfamiliar queries.They consider improving LLMs' factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with FLAME (see case studies in Section 5.5).It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation).In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions.",
            "reference_string": "[269502676 | Lin et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 12,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2193250617",
                    "name": "Shivam Ratnakar"
                },
                {
                    "authorId": "2114584681",
                    "name": "Abhiroop Talasila"
                },
                {
                    "authorId": "2348250516",
                    "name": "Raghav Chamadiya"
                },
                {
                    "authorId": "2348252708",
                    "name": "Nikhil Agarwal"
                },
                {
                    "authorId": "2348247241",
                    "name": "Vinayak K Doifode"
                }
            ],
            "abstract": "This paper presents an extensive examination of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain specific facts into Large Language Models (LLMs), focusing on improving the fine-tuning process by categorizing question-answer (QA) pairs into Factual and Conceptual classes using a BERT-based classifier. Two distinct Llama-2 models are fine-tuned based on these classifications and evaluated using larger models like GPT-3.5 Turbo and Gemini. Our results indicate that models trained on conceptual datasets outperform those trained on factual datasets. Additionally, we compare the efficiency of two synthetic fine-tuning dataset generation techniques, D-RAG and D-Naive, with D-Naive demonstrating superior performance. Although PEFT has shown effectiveness, our research indicates that it may not be the most optimal method for embedding facts into LLMs. However, it has demonstrated exceptional performance in instruction-based tasks. Our findings are reinforced by a 1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B model significantly outperforms the baseline model in generating product recommendations. Our study highlights the importance of QA pair categorization and synthetic dataset generation techniques in enhancing the performance of LLMs in specific domains.",
            "corpus_id": 276741193,
            "sentences": [
                {
                    "corpus_id": "276741193",
                    "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
                    "text": "Fine-tuning for domainspecific tasks has been extensively explored. The RAFT approach (Zhang et al. 2024) combines Retrieval-Augmented Generation (RAG) with fine-tuning to enhance LLM performance in specific domains by training models to disregard irrelevant retrieved documents, improving focus and accuracy. Similarly, \"RAG vs Fine-tuning\" (Gupta et al. 2024) compares both approaches across various LLMs, demonstrating how each method can be effectively employed for domain-specific applications, particularly in underexplored sectors like agriculture. Additionally, \"Fine-tuning Language Models for Factuality\" (Tian et al. 2023) leverages recent innovations in factuality judgment and preference optimization algorithms to improve the factual accuracy of LLMs, offering a novel approach to mitigating misinformation. \n\nInstruction Tuning and Data Selection Efficient data selection for instruction tuning is crucial for optimizing LLM performance. \"From Quantity to Quality\" (Li et al. 2023) introduces a self-guided methodology that employs the Instruction-Following Difficulty metric to identify highquality instruction data, enhancing training efficiency. Additionally, \"Rethinking Data Selection for Supervised Fine-Tuning\" (Shen 2024) argues that selecting data reflecting human-like interactions, rather than purely based on quality and diversity, yields better results in aligning models with human expectations. The MoDS approach (Du, Zong, and Zhang 2023) further refines data selection by focusing on quality, coverage, and necessity, demonstrating improved performance with a significantly reduced dataset. Addressing LLM limitations such as hallucinations and weak numerical reasoning, ToolQA (Zhuang et al. 2024) introduces a dataset to evaluate LLMs' ability to use external tools for question answering, providing insights into their strengths and weaknesses.",
                    "score": 0.5776064642077654,
                    "section_title": "Domain-Specific Adaptation",
                    "char_start_offset": 7054,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 821
                        },
                        {
                            "start": 824,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1424
                        },
                        {
                            "start": 1425,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1879
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.509765625
                }
            ],
            "relevance_judgement": 0.509765625,
            "relevance_judgment_input_expanded": "# Title: Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs\n# Venue: arXiv.org\n# Authors: Shivam Ratnakar, Abhiroop Talasila, Raghav Chamadiya, Nikhil Agarwal, Vinayak K Doifode\n## Abstract\nThis paper presents an extensive examination of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain specific facts into Large Language Models (LLMs), focusing on improving the fine-tuning process by categorizing question-answer (QA) pairs into Factual and Conceptual classes using a BERT-based classifier. Two distinct Llama-2 models are fine-tuned based on these classifications and evaluated using larger models like GPT-3.5 Turbo and Gemini. Our results indicate that models trained on conceptual datasets outperform those trained on factual datasets. Additionally, we compare the efficiency of two synthetic fine-tuning dataset generation techniques, D-RAG and D-Naive, with D-Naive demonstrating superior performance. Although PEFT has shown effectiveness, our research indicates that it may not be the most optimal method for embedding facts into LLMs. However, it has demonstrated exceptional performance in instruction-based tasks. Our findings are reinforced by a 1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B model significantly outperforms the baseline model in generating product recommendations. Our study highlights the importance of QA pair categorization and synthetic dataset generation techniques in enhancing the performance of LLMs in specific domains.\n## Domain-Specific Adaptation\nFine-tuning for domainspecific tasks has been extensively explored. The RAFT approach (Zhang et al. 2024) combines Retrieval-Augmented Generation (RAG) with fine-tuning to enhance LLM performance in specific domains by training models to disregard irrelevant retrieved documents, improving focus and accuracy. Similarly, \"RAG vs Fine-tuning\" (Gupta et al. 2024) compares both approaches across various LLMs, demonstrating how each method can be effectively employed for domain-specific applications, particularly in underexplored sectors like agriculture. Additionally, \"Fine-tuning Language Models for Factuality\" (Tian et al. 2023) leverages recent innovations in factuality judgment and preference optimization algorithms to improve the factual accuracy of LLMs, offering a novel approach to mitigating misinformation. \n\nInstruction Tuning and Data Selection Efficient data selection for instruction tuning is crucial for optimizing LLM performance. \"From Quantity to Quality\" (Li et al. 2023) introduces a self-guided methodology that employs the Instruction-Following Difficulty metric to identify highquality instruction data, enhancing training efficiency. Additionally, \"Rethinking Data Selection for Supervised Fine-Tuning\" (Shen 2024) argues that selecting data reflecting human-like interactions, rather than purely based on quality and diversity, yields better results in aligning models with human expectations. The MoDS approach (Du, Zong, and Zhang 2023) further refines data selection by focusing on quality, coverage, and necessity, demonstrating improved performance with a significantly reduced dataset. Addressing LLM limitations such as hallucinations and weak numerical reasoning, ToolQA (Zhuang et al. 2024) introduces a dataset to evaluate LLMs' ability to use external tools for question answering, providing insights into their strengths and weaknesses.",
            "reference_string": "[276741193 | Ratnakar et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 172,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.19744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284061879",
                    "name": "Qi Wang"
                },
                {
                    "authorId": "2243469243",
                    "name": "Jindong Li"
                },
                {
                    "authorId": "2283983135",
                    "name": "Shiqi Wang"
                },
                {
                    "authorId": "9123083",
                    "name": "Qianli Xing"
                },
                {
                    "authorId": "2174434599",
                    "name": "Runliang Niu"
                },
                {
                    "authorId": "2283935174",
                    "name": "He Kong"
                },
                {
                    "authorId": "2328073820",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2293722046",
                    "name": "Guodong Long"
                },
                {
                    "authorId": "2243466364",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2283189862",
                    "name": "Chengqi Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) have not only revolutionized the field of natural language processing (NLP) but also have the potential to bring a paradigm shift in many other fields due to their remarkable abilities of language understanding, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to improve recommender systems, and it is imperative to thoroughly review the recent advances and challenges of LLM-based recommender systems. Unlike existing work, this survey does not merely analyze the classifications of LLM-based recommendation systems according to the technical framework of LLMs. Instead, it investigates how LLMs can better serve recommendation tasks from the perspective of the recommender system community, thus enhancing the integration of large language models into the research of recommender system and its practical application. In addition, the long-standing gap between academic research and industrial applications related to recommender systems has not been well discussed, especially in the era of large language models. In this review, we introduce a novel taxonomy that originates from the intrinsic essence of recommendation, delving into the application of large language model-based recommendation systems and their industrial implementation. Specifically, we propose a three-tier structure that more accurately reflects the developmental progression of recommendation systems from research to practical implementation, including representing and understanding, scheming and utilizing, and industrial deployment. Furthermore, we discuss critical challenges and opportunities in this emerging field. A more up-to-date version of the papers is maintained at: https://github.com/jindongli-Ai/Next-Generation-LLM-based-Recommender-Systems-Survey.",
            "corpus_id": 273653859,
            "sentences": [
                {
                    "corpus_id": "273653859",
                    "title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond",
                    "text": "Naive Fine-Tuning. [118] finds that after training, further fine-tuning the context-aware embeddings and the recommendation LLM would result in better performance. [77] investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. [8] finds that fine-tuning an LLM for recommendation tasks enables it to learn not only the tasks but also concepts of a domain to some extent. It also shows that fine-tuning OpenAI GPT led to considerably better performance than fine-tuning Google PaLM 2 [32]. [31] uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation. [45] fine-tunes a 7-billion-parameter model by designing an imitation dataset derived from GPT-4, which can improve the ability of interactive recommendations. [36] fine-tunes an LLM with dataset-specific information in the form of prompt-completion pairs and asks the model to produce next item recommendations for test prompts. [25] proposes to fine-tune LLM with reinforcement learning from conversational recommender systems performance feedback for improving the performance of recommendation. [63] explores fine-tuning ChatGPT by formulating the news recommendation as direct ranking and rating tasks. [9] proposes to align LLMs with the recommendation domain by fine-tuning with data samples that encode recommendation knowledge and also proposed auxiliary-task data samples that encode item correlations contained in users' preferences. In the e-commerce domain, [83] investigates the effectiveness of combining LLM and conversational recommendation systems and fine-tunes large language models including ChatGLM and Chinese-Alpaca-7B using pre-sales dialogues. [72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms. [20] conducts supervised fine-tuning (SFT) for the LLM to activate its power in the task-related domain.",
                    "score": 0.511068560315316,
                    "section_title": "Non-Generative LLM-based Approaches",
                    "char_start_offset": 42352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 18
                        },
                        {
                            "start": 19,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2221
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 164,
                            "end": 168,
                            "matchedPaperCorpusId": "261076297"
                        },
                        {
                            "start": 682,
                            "end": 686,
                            "matchedPaperCorpusId": "262045057"
                        },
                        {
                            "start": 972,
                            "end": 976,
                            "matchedPaperCorpusId": "261823711"
                        },
                        {
                            "start": 1420,
                            "end": 1423,
                            "matchedPaperCorpusId": "268819967"
                        },
                        {
                            "start": 1683,
                            "end": 1687,
                            "matchedPaperCorpusId": "264426498"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50146484375
                }
            ],
            "relevance_judgement": 0.50146484375,
            "relevance_judgment_input_expanded": "# Title: Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond\n# Venue: arXiv.org\n# Authors: Qi Wang, Jindong Li, Shiqi Wang, Qianli Xing, Runliang Niu, He Kong, Rui Li, Guodong Long, Yi Chang, Chengqi Zhang\n## Abstract\nLarge language models (LLMs) have not only revolutionized the field of natural language processing (NLP) but also have the potential to bring a paradigm shift in many other fields due to their remarkable abilities of language understanding, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to improve recommender systems, and it is imperative to thoroughly review the recent advances and challenges of LLM-based recommender systems. Unlike existing work, this survey does not merely analyze the classifications of LLM-based recommendation systems according to the technical framework of LLMs. Instead, it investigates how LLMs can better serve recommendation tasks from the perspective of the recommender system community, thus enhancing the integration of large language models into the research of recommender system and its practical application. In addition, the long-standing gap between academic research and industrial applications related to recommender systems has not been well discussed, especially in the era of large language models. In this review, we introduce a novel taxonomy that originates from the intrinsic essence of recommendation, delving into the application of large language model-based recommendation systems and their industrial implementation. Specifically, we propose a three-tier structure that more accurately reflects the developmental progression of recommendation systems from research to practical implementation, including representing and understanding, scheming and utilizing, and industrial deployment. Furthermore, we discuss critical challenges and opportunities in this emerging field. A more up-to-date version of the papers is maintained at: https://github.com/jindongli-Ai/Next-Generation-LLM-based-Recommender-Systems-Survey.\n## Non-Generative LLM-based Approaches\nNaive Fine-Tuning. [118] finds that after training, further fine-tuning the context-aware embeddings and the recommendation LLM would result in better performance. [77] investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. [8] finds that fine-tuning an LLM for recommendation tasks enables it to learn not only the tasks but also concepts of a domain to some extent. It also shows that fine-tuning OpenAI GPT led to considerably better performance than fine-tuning Google PaLM 2 [32]. [31] uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation. [45] fine-tunes a 7-billion-parameter model by designing an imitation dataset derived from GPT-4, which can improve the ability of interactive recommendations. [36] fine-tunes an LLM with dataset-specific information in the form of prompt-completion pairs and asks the model to produce next item recommendations for test prompts. [25] proposes to fine-tune LLM with reinforcement learning from conversational recommender systems performance feedback for improving the performance of recommendation. [63] explores fine-tuning ChatGPT by formulating the news recommendation as direct ranking and rating tasks. [9] proposes to align LLMs with the recommendation domain by fine-tuning with data samples that encode recommendation knowledge and also proposed auxiliary-task data samples that encode item correlations contained in users' preferences. In the e-commerce domain, [83] investigates the effectiveness of combining LLM and conversational recommendation systems and fine-tunes large language models including ChatGLM and Chinese-Alpaca-7B using pre-sales dialogues. [72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms. [20] conducts supervised fine-tuning (SFT) for the LLM to activate its power in the task-related domain.",
            "reference_string": "[273653859 | Wang et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 29,
            "citation_count": 49,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10047, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108176413",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2258779676",
                    "name": "Avi Singh"
                },
                {
                    "authorId": "2258721696",
                    "name": "C. D. Freeman"
                },
                {
                    "authorId": "1388383230",
                    "name": "John D. Co-Reyes"
                },
                {
                    "authorId": "2258936205",
                    "name": "Peter J. Liu"
                }
            ],
            "abstract": "Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting.",
            "corpus_id": 264146593,
            "sentences": [
                {
                    "corpus_id": "264146593",
                    "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
                    "text": "In this work we investigated different fine-tuning methods to improve the LLMs' performance on math problem solving. Starting with supervised step-by-step fine-tuning, we first demonstrated the importance of step-by-step solutions for improving fine-tuned LLM performance. We then studied re-ranking methods for fine-tuning the LLMs as solution evaluators, and proposed a new re-ranking method which combines the benefit of majority voting and re-ranking together, simultaneously achieving better solution accuracy and computational efficiency. Lastly, we introduced a multi-task sequential fine-tuning method, aiming at improving the model's solution generation ability with the training objective of the solution evaluation. Our method outperforms the baseline fine-tuning method based on the solution generation training objective only, demonstrating its ability of improving a generation task using the supervision signal of the corresponding evaluation task.     1) the ratio of the number of correct solutions v.s. all solutions, (2) the ratio of number of correct solutions v.s. Maj1@64 solutions when the correct solution is not the Maj1@64 solution.",
                    "score": 0.5330189582681992,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 24098,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1158
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50048828125
                }
            ],
            "relevance_judgement": 0.50048828125,
            "relevance_judgment_input_expanded": "# Title: Improving Large Language Model Fine-tuning for Solving Math Problems\n# Venue: arXiv.org\n# Authors: Yixin Liu, Avi Singh, C. D. Freeman, John D. Co-Reyes, Peter J. Liu\n## Abstract\nDespite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting.\n## CONCLUSION\nIn this work we investigated different fine-tuning methods to improve the LLMs' performance on math problem solving. Starting with supervised step-by-step fine-tuning, we first demonstrated the importance of step-by-step solutions for improving fine-tuned LLM performance. We then studied re-ranking methods for fine-tuning the LLMs as solution evaluators, and proposed a new re-ranking method which combines the benefit of majority voting and re-ranking together, simultaneously achieving better solution accuracy and computational efficiency. Lastly, we introduced a multi-task sequential fine-tuning method, aiming at improving the model's solution generation ability with the training objective of the solution evaluation. Our method outperforms the baseline fine-tuning method based on the solution generation training objective only, demonstrating its ability of improving a generation task using the supervision signal of the corresponding evaluation task.     1) the ratio of the number of correct solutions v.s. all solutions, (2) the ratio of number of correct solutions v.s. Maj1@64 solutions when the correct solution is not the Maj1@64 solution.",
            "reference_string": "[264146593 | Liu et al. | 2023 | Citations: 49]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273636798",
            "title": "Large language models and synthetic health data: progress and prospects",
            "text": "Choices related to the specific LLM model to be deployed as well as the generation approach are critical to understand. In the context of synthetic data, both prompt engineering and fine-tuning have already been applied. 51,52 Prompt engineering can involve a variety of prompt templates and include either few or zero examples as part of the prompt 69 -an example of a zero-example prompt is shown in Figure 2. Similarly, one can explore or develop multiple different finetuning approaches. 70 Furthermore, there are many existing LLMs, each of which may be more appropriate for certain approaches-fine-tuning may be easier with \"smaller\" LLMs such as Llama 71 whereas prompt engineering is better suited for \"larger\" LLMs such as GPT-4. These choices can also be framed in the tradeoff between \"buy\" versus \"build\"-do the benefits of fine-tuning LLM models over direct application of out-of-the-box models outweigh the fine-tuning development cost? Extensive exploration of such strategic choices in adoption is necessary to make informed decisions for a specific context and task.",
            "score": 0.6673499189087666,
            "section_title": "LLM adoption",
            "char_start_offset": 12228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1083
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 494,
                    "matchedPaperCorpusId": "153312532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs\u2019 generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model\u2019s generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.",
            "score": 0.6634887012591991,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "For each task, we designate a specific dataset as the training set.The remaining selected datasets are subsequently divided into two groups: in-domain datasets, closely aligned with the training set in terms of content and structure, and out-of-domain datasets, which possess significant differences.With these datasets, our research investigates two critical questions: i) the ability of fine-tuned LLMs to adapt to both in-domain and out-of-domain test sets, and ii) the impact of finetuning on the ICL ability of LLMs across different types of tasks.\n\nWe find that models fine-tuned on text generation and classification tasks exhibit different behaviors when evaluated on test sets.Specifically, we observe that models fine-tuned for classification tasks tend to exhibit positive transfer when applied to out-of-domain datasets of the same fine-tuning/test task type.In contrast, models fine-tuned on generation tasks frequently experience negative transfer under similar conditions.Interestingly, while fine-tuning the LLMs on generation tasks generally does not detrimentally affect their performance on classification tasks, the reverse is not true; models fine-tuned on classification tasks typically fail to work on generation tasks.Moreover, we experimentally observe that integrating the ICL strategy during fine-tuning on generation tasks can enhance an LLM's generalization ability.We also investigate other factors, such as training data size and the number of in-context examples.We hope this study offers comprehensive insights into finetuning strategies for LLMs, not only in enhancing task-specific performance but also in fostering broader generalization abilities.",
            "score": 0.6553790657822745,
            "section_title": "Introduction",
            "char_start_offset": 1781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 67,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 553
                },
                {
                    "start": 555,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1684
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2440185546875
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "The transformative impact of in-context learning (ICL) (Brown et al., 2020;Wei et al., 2022b;Rubin et al., 2022;Liu et al., 2023;Chowdhery et al., 2022;Wang et al., 2023b) in LLMs, as demonstrated by models like Llama-2 (tou, 2023) and GPT-3 (Brown et al., 2020), marks a significant advancement in the field of artificial intelligence.This learning paradigm allows LLMs to adapt to various tasks by leveraging multiple demonstration examples presented within a prompt, without training the LLMs.However, when it comes to a specific task, fine-tuning often achieves better performance than ICL, which has been substantiated by recent studies (Shi et al., 2023;Jiao et al., 2023;Wang et al., 2023a;Zhang et al., 2023).\n\nThere are some works studying the properties of fine-tuning and ICL for LLMs.For example, Wei et al. (2022a) reveal that multi-task fine-tuning can enhance an LLM's zero-shot and ICL capabilities.It indicates that fine-tuning, when applied across multiple tasks, does not merely improve performance on those seen tasks but also augments the model's inherent learning abilities.The work of Mosbach et al. (2023) highlights that in terms of the out-of-domain generalization in classification tasks, few-shot fine-tuning and ICL exhibit similar levels of generalization.Wang et al. (2023c) find that fine-tuning may overly tailor the model to task-specific formats, potentially compromising its adaptability to other new tasks.\n\nIn this paper, we conduct a comprehensive study on how task-specific (not multi-task or few-shot) fine-tuning affects the generalization ability of LLMs.To provide a thorough analysis, we design a series of experiments encompassing a diverse range of datasets and tasks, covering both classification and generation tasks.For each task, we designate a specific dataset as the training set.",
            "score": 0.6547013506304508,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 717
                },
                {
                    "start": 719,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1443
                },
                {
                    "start": 1445,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1766
                },
                {
                    "start": 1766,
                    "end": 1833
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 93,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 93,
                    "end": 112,
                    "matchedPaperCorpusId": "245218561"
                },
                {
                    "start": 1108,
                    "end": 1129,
                    "matchedPaperCorpusId": "258947047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "273026376",
            "title": "Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on LLM Performance - A Case Study in Finance",
            "text": "Recently, the application of large language models (LLMs) in domain-specific contexts has seen rapid growth, particularly in fields such as medicine (Singhal et al., 2023;Wu et al., 2024), law (Huang et al., 2023), and finance (Cheng et al., 2023;Wu et al., 2023). As LLMs are increasingly adopted across various domains, accurate evaluation of their domain-specific capabilities has become more necessary. While many benchmarks exist to evaluate LLM performance, they are typically designed for general purposes and not specifically for domainspecific evaluations. \n\nA common method for assessing LLM performance within a domain is through downstream tasks (Yang et al., 2024;Gu et al., 2021;Xie et al., 2024b). Such benchmarks emphasize well-defined, highly specific tasks that seek to reflect real-world applications within the target domain. These tasks are frequently framed as standard natural language processing (NLP) problems, such as text classification, summarization, causal reasoning, arithmetic reasoning, and more. While each test individually provides limited insight into domain-specific capabilities, when combined, they offer a broader representation, facilitating a more comprehensive evaluation. \n\nLLMs possess zero-shot capabilities (Kojima et al., 2022), allowing them to perform downstream tasks without prior task-specific training. However, they sometimes struggle with these tasks due to issues such as formatting, problem understanding, or reasoning failures. A common approach to improve their performance is to fine-tune the models directly on the downstream task, improving performance on it directly (Zhou et al., 2023). Consequently, many benchmarks provide both training and test splits to facilitate fine-tuning and evaluation. Still, fine-tuning on a single task may not fully optimize the model's performance. \n\nIn this work, we investigated the impact of multitask fine-tuning. Instead of fine-tuning the model solely on the target downstream task, we fine-tune it on multiple related downstream tasks simultaneously. We conduct a massive ablation study to explore the interactions between various financial tasks and datasets. In total, we conduct 220 train- ing experiments to provide an in-depth evaluation of different financial benchmarks and LLMs.",
            "score": 0.6512867286015359,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 171,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 171,
                    "end": 187,
                    "matchedPaperCorpusId": "269136910"
                },
                {
                    "start": 227,
                    "end": 247,
                    "matchedPaperCorpusId": "271745635"
                },
                {
                    "start": 658,
                    "end": 677,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 693,
                    "end": 711,
                    "matchedPaperCorpusId": "268042106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "258291494",
            "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "text": "At the end of this section, we also want to provide more analyses about the impacts of the fine-tuning on LLM's language generation abilities. In Table 8, we illustrate some examples about the generation results by the LLM in Graph-ToolFormer before and after the fine-tuning with the graph reasoning prompt data. Specifically, we select the inputs from two different sources, i.e., two instances from the Pile testing set (Pile was the data used for GPT-J pre-training) and two instances from the recent news articles on the web. \n\nBy comparing the generation results, we can observe very large (negative) impacts of the fine-tuning with the prompt datasets on LLM's language generation ability. For these four input payloads, after the fine-tuning, the outputs generated by the LLM in Graph-ToolFormer are either some random tokens or contains the unexpected API calls, and only for the third input, the output by the LLM in Graph-ToolFormer is till closely related to the inputs. \n\nTherefore, if we plan to make Graph-ToolFormer a very general language interface that can not only handle graph reasoning tasks but also still possess the language generation ability for the inputs not related to graph reasoning, some new continual learning techniques will be needed in model fine-tuning, so the LLM in Graph-ToolFormer will not suffer from the catastrophic forgetting problem after fine-tuning.",
            "score": 0.6338446491680119,
            "section_title": "Language Generation Ability Revisit",
            "char_start_offset": 118374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "262054014",
            "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
            "text": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
            "score": 0.622331263484937,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158935546875
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "Since LLMs have shown surprisingly emergent abilities in handling various NLP tasks, multitask fine-tuning appears to be promising to improve LLMs' generalization performance on unseen tasks. The benefits of increasing the number of tasks in SFT have been experimentally proven on models with different sizes ranging from 3B to 540B parameters (Wang et al., 2022;Sanh et al., 2022;Wei et al., 2021;Chung et al., 2022). With the scaling of tasks, the mixture ratio of data targeting different tasks is also found to be critical and usually decided by experiments and intuitions (Iyer et al., 2022;Longpre et al., 2023a). To enable LLMs to solve targeted tasks with specific skills, representation similarity (Ivison et al., 2023;Lee et al., 2024) and gradient similarity (Xia et al., 2024) is proposed to select relevant multitask subsets. \n\nHowever, conflicts might exist among the many tasks. Dong et al. (2023) focus on task composition among mathematical reasoning, code generation, and general human-aligning abilities. They find that model abilities are improved when the mixed data amount is small but decreased otherwise. The negative impact of large amount mixing data might lie in the similarity degree of data format and data distribution among different SFT tasks. Wang et al. (2023b) also experimentally show that different instruction datasets may correspond to different specific abilities. And winning across all evaluations using a single dataset or combination seems to be challenging. \n\nDivergent from compositing multiple tasks, some works claim that integration of LLMs tuned on single task data can outperform one LLM tuned on multiple tasks (Jang et al., 2023;Chen et al., 2023b). But fine-tuning more task-specific LLMs also means more resource consumption. How to efficiently equip LLMs with the ability to solve multiple tasks still demands more exploration.",
            "score": 0.6216613010184996,
            "section_title": "Task Composition",
            "char_start_offset": 17280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 381,
                    "end": 398,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 596,
                    "end": 618,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 1663,
                    "end": 1682,
                    "matchedPaperCorpusId": "256627673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "In the world of language models like ChatGPT, a significant challenge is figuring out if these models' responses are correct or if the models are hallucinating-providing random or incorrect answers. Large Language Models (LLMs) represent a significant leap in deep learning, characterized by their expansive size and pre-training on diverse datasets. Built on the transformer architecture, LLMs such as GPT-3 and ChatGPT, possess encoder and decoder components with self-attention capabilities, enabling them to extract meanings and comprehend relationships within text sequences. The unsupervised training of transformer LLMs facilitates self-learning, encompassing basic grammar, languages, and knowledge. The scalability of the transformer architecture accommodates very large models, often containing billions of parameters, capable of ingesting extensive datasets from diverse sources. The versatility of LLMs is showcased in their ability to perform tasks ranging from question answering to code generation. Once LLMs complete their initial training, they possess the remarkable capability of adapting to various tasks through a process known as fine-tuning. Fine-tuning involves adjusting the model's parameters using additional, more specific data, allowing it to specialize in a particular application. For example, with a singular LLM such as GPT-3, it can be fine-tuned for various use cases, such as answering questions in different industries such as finance or healthcare, or answering questions in simple terms for elementary school-level students. There are three common learning models that showcase the versatility of LLMs: zero-shot learning, where the base models can respond broadly to various requests without specific training; few-shot learning, where providing a small set of relevant examples significantly improves performance in a specific area; and fine-tuning, which extends few-shot learning by allowing data scientists to train the base model with additional data tailored to the specific application, refining its parameters for optimal performance in diverse tasks. This adaptability makes LLMs powerful tools that can be fine-tuned to excel in specific domains and meet varied requirements effectively. \n\nThe current concern with the employment of LLMs is the notion of hallucinations and their implications. LLMs tend to distort information or offer inaccurate answers.",
            "score": 0.6026669969649796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2237
                },
                {
                    "start": 2240,
                    "end": 2343
                },
                {
                    "start": 2344,
                    "end": 2405
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177490234375
        },
        {
            "corpus_id": "274286927",
            "title": "Application of Large Language Models in Cybersecurity: A Systematic Literature Review",
            "text": "Large Language Models are sophisticated language models encompassing billions or even trillions of parameters. These models are trained on vast datasets, often amounting to terabytes of data. Notable examples such as T5 [26], GPT, and LLaMA [27] have demonstrated remarkable proficiency in tasks such as text generation, chatbot interactions, and programming. Furthermore, it has been established that LLMs can effectively conduct penetration testing [16]. There are different types of LLMs, including general-use models and domain-specific models tailored to particular tasks. For instance, BloombergGPT [28] is designed to perform financial tasks. \n\nThe fundamental concept underpinning LLMs involves the tokenization of text, which breaks text down into smaller units known as tokens. The core principle of LLMs is to predict the next token (word) based on the context and input data. Utilizing NLP technology, LLMs can generate human-like language tailored to specific tasks, making the interpretation of their output exceedingly straightforward. One of the most prominent LLMs today is ChatGPT, which was trained using Reinforcement Learning from Human Feedback (RLHF) [29]. \n\nTraditional Reinforcement Learning (RL) involves a model learning to make decisions by interacting with its environment, receiving penalties and rewards based on its actions. These values are determined by a reward function created by engineers, making the design of this function critical. Poorly designed reward functions can lead to unintended consequences, such as agents exploiting loopholes to achieve high rewards without genuinely fulfilling the intended objectives. To address these issues, RLHF incorporates a human-in-the-loop approach. Rather than relying on a static reward function, the agent's learning objective is iteratively refined through human feedback [30]. \n\nAlso, terms like fine-tuning, zero-shot and few-shot learning should be introduced. Fine-tuning is the process of adapting an LLM to perform better in certain domains by training it on task-oriented datasets. It helps the model to adjust its parameters, improving its ability to respond to domain-specific queries with higher accuracy and relevance. \n\nFor this, techniques like Low-Rank Adaptation (LoRA) and Parameter Efficient Fine-Tuning (PEFT) are used [31].",
            "score": 0.6022807163535404,
            "section_title": "B. LARGE LANGUAGE MODELS",
            "char_start_offset": 22225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2326
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "260351250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072509765625
        },
        {
            "corpus_id": "274166126",
            "title": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI",
            "text": "Fine-tuning LLMs is widely used to adapt models to specific downstream tasks [3], [4]. The model leverages knowledge acquired from pre-training on large unsupervised language learning tasks, allowing its weights to be adjusted using a smaller, task-specific dataset. This enables the model to capture novel patterns and improve task-specific reasoning without training from scratch. Fine-tuning LLMs offers several key advantages: (i) Reduced training time and computational costs by leveraging pre-trained foundational knowledge, (ii) Enhanced performance on downstream tasks, particularly beneficial when data is scarce, and (iii) Improved transfer learning across subtasks within a domain, increasing efficiency in model deployment. \n\nBenchmarking LLMs: We evaluated the capabilities of LLMs in analog circuit design, using both open-source (CodeLlama-70B [39], DeepSeek-V2 [40]) and proprietary (GPT-3.5,4o-mini [41]) models. We fine-tuned all baseline LLMs using the corpus collected through the MASALA-CHAI framework. Open-source models were fine-tuned on a Nvidia A100 80GB GPU utilizing LoRA [42], while GPT models were fine-tuned using their respective fine-tuning playground [43]. \n\nMetrics: We adopt the Pass@k metric, widely used in codegeneration tasks [3], [4], [14], as our main evaluation measure.",
            "score": 0.6021947075688692,
            "section_title": "B. Finetuning",
            "char_start_offset": 15780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1313
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 80,
                    "matchedPaperCorpusId": "260775786"
                },
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "260379192"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "260775786"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "260379192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "We explore the relationship between fine-tuning and pre-training LLMs through fine-tuning multiple pre-training checkpoints of large language models. Our results on 18 datasets and two models provide insights into LLM training. We identify the aspects that LLM learns and forgets during supervised fine-tuning; By analyzing pre-training history, we find that pre-training improves the model in a latent way that is only observable after finetuning. The model may excel at some tasks without fine-tuning. However, the model can rapidly learn datasets that it does not demonstrate capabilities during pre-training with a small amount of supervision. Overall, our study highlights the value of analyzing language model training dynamics. We encourage model developers to release pre-training checkpoints to facilitate research on LLM training.",
            "score": 0.6018546232127857,
            "section_title": "Conclusion",
            "char_start_offset": 26766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 840
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.300048828125
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "The alignment stage usually involves either supervised fine-tuning for specific tasks or instruction finetuning for general-purpose usage. Regardless, finetuning (almost always) comes at the end of pretraining and yields remarkable improvements on downstream tasks (Touvron et al., 2023;Groeneveld et al., 2024). Consequently, the benefits of each stage are largely explored independently, with improvements to pretraining being orthogonal to benefits from model alignment. \n\nRather than exploring these two training regimes independently, we ask: What does the model learn and forget during pre-training and fine-tuning? Specifically, how do pretraining and fine-tuning interact to produce the resulting model? Does more pre-training hinder better fine-tuning results? Answering these questions requires us to examine how models learn during pre-training and how this affects fine-tuning. Therefore, we begin by fine-tuning two language models under a variety of conditions to determine how fine-tuning affects model behavior. We explore both supervised and instruction fine-tuning, testing the models' memorization and forgetting when learning specific tasks and serving as general-purpose language-AI tools. We then explore the affect of pre-training on these behaviors by fine-tuning multiple pre-training checkpoints of a large language model (Figure 1), evaluating each checkpoint and its fine-tuned variant on downstream evaluation sets. We track model abilities during pre-training and compare them to improvements achieved after fine-tuning at the corresponding pre-training step. 2  Our experiments yield the following insights into LLM training: (1) although supervised fine-tuning can improve performance on in-distribution tasks, it can also cause the model to forget domain knowledge or tasks that it was previously capable of solving ( \u00a74); (2) fine-tuned models show high sensitivity to evaluation prompts, but this sensitivity can be alleviated by more pre-training ( \u00a74); (3) continued pre-training can improve a model in ways that are only revealed after fine-tuning ( \u00a76); (4) tasks for which the model already performs well during pre-training benefit much less from fine-tuning than those where the model does not demonstrate capabilities ( \u00a75, \u00a76);",
            "score": 0.5995794597390707,
            "section_title": "Introduction",
            "char_start_offset": 1678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1539306640625
        },
        {
            "corpus_id": "267500209",
            "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
            "text": "Reasoning ability, as a basic ability of LLMs, has received great attention recently due to its great importance. Despite the notable improvements made by CoT (Wei et al., 2022), LLMs are still struggling with the tasks that require complex or high-order multi-step reasoning, such as logical reasoning and mathematic proof. Therefore, intensive research efforts have been dedicated to addressing the aforementioned issues. Generally, they can be categorized as follows. \n\nFine-tuning-based methods. These methods aim to improve the reasoning ability of LLMs through supervised fine-tuning. Usually, LLMs are fine-tuned by the samples which require manual labeling of reasoning processes, such as (Ouyang et al., 2022;Wang et al., 2022b). However, it can be labor-intensive due to the costly labeling of complex reasoning processes. The works of (Shridhar et al., 2022;Zelikman et al., 2022) first used LLMs to generate reasoning processes, but only the samples with correct results are selected for fine-tuning LLMs to reduce the labeling cost. Additionally, fine-tuned LLMs on specific tasks can suffer from the problem of \"catastrophic forgetting\", which means that the original knowledge inherited by the pre-trained LLMs will be lost and thus the ability to generalize to downstream tasks will be weakened. To this end, Cheng et al. (2023) trained a prompt retriever using the output scores of LLMs. When fine-tuning, LLMs are frozen just as a data labeler which effectively reduces the impact on LLMs. \n\nTool-based methods. Tool-based methods propose to utilize external tools to augment the capabilities of LLMs in accomplishing complex tasks (Qin et al., 2023;Schick et al., 2024). More-over, Jin et al. (2024); Yang et al. (2023) augment LLMs with external real-time knowledge or domain-specific information through specific tools. Additionally, Retrieval-Augmented Generation (RAG) related methods (Gao et al., 2023;Ma et al., 2023;Peng et al., 2024) have received a lot of attention recently, and these methods improve the reasoning ability of LLMs by incorporating external knowledge.",
            "score": 0.5995452806483752,
            "section_title": "Related Work",
            "char_start_offset": 25306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 718,
                    "end": 737,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 869,
                    "end": 891,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 1325,
                    "end": 1344,
                    "matchedPaperCorpusId": "257532394"
                },
                {
                    "start": 1668,
                    "end": 1688,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 1701,
                    "end": 1718,
                    "matchedPaperCorpusId": "258298113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "259145356",
            "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
            "text": "We see that across models and different numbers of in-context examples, the reasoning gap \u2206 reas accounts for up to 79.11% of the performance gap between in-context learning and fine-tuning. This indicates that the LLM representations have sufficient information but lack the ability to reason over them. \n\nFine-tuning: Improves task-specific reasoning. We next investigate how fine-tuning for a specific task affects the performance of the base model. In Figure 3b, we show a scatter plot of the gains that can be attributed to improved representations against the reasoning gains. We see that, across models, reasoning improvements accounts for 73.06% of the improvements. This indicates that while fine-tuning improves both reasoning and representations of the LLM, the gains are predominantly due to improvements in task-specific reasoning. Furthermore, this task-specific fine-tuning of the LLM hurts its performance on other tasks. In Figure 3c, we show that the accuracy of a model fine-tuned on the AGNews dataset [ZZL15], leads to an average decrease of 25.77% on other tasks. Furthermore, this drop in accuracy can be attributed to the drop in task-specific reasoning capabilities-these account for 72.58% of the drop (see Appendix B for more details). \n\nAdapters: Impairs task-agnosticity via reasoning. Task-specific adapters do not change the underlying representation ability of the model. To study their ability to generalize across tasks, we train an adapter for the AGNews dataset and evaluate it on other tasks. In Appendix B, we show that the performance drops across tasks by an average of 19.8%, indicating that adapters only learn task-specific reasoning abilities. \n\n4 Tart: Task-Agnostic Reasoning Transformers \n\nThe above analysis showed how it is the effective reasoning capabilities of the LLMs which limits its performance when compared with task-specific adaptation approaches. Building on this insight, we propose Tart, which learns a general-purpose reasoning module completely agnostic to the underlying base LLM and when composed with any LLM via its embeddings, generically improves upon its reasoning abilities. Tart is a completely task-agnostic method which works across a suite of tasks without any task-specific training.",
            "score": 0.5951495379657064,
            "section_title": "Understanding performance via Representation-Reasoning decomposition",
            "char_start_offset": 12481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 1022,
                    "end": 1029,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.131103515625
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "In contrast, our work provides a comprehensive examination of improving LLMs' factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO.Concurrent to our work, Kang et al. (2024) find that LLMs tend to hallucinate when facing unfamiliar queries.They consider improving LLMs' factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with FLAME (see case studies in Section 5.5).It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation).In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions.",
            "score": 0.5908954811578881,
            "section_title": "Related Work",
            "char_start_offset": 6250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "271516210",
            "title": "AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs",
            "text": "RTLFixer [11] and AutoChip [12] improves syntactic accuracy by utilizing compiler-generated error messages as feedback to LLM across multiple rounds of queries. While prompt engineering is convenient and requires less preparation than fine-tuning, it does not fundamentally improve the underlying ability of LLMs for RTL code generation, making supervised fine-tuning a necessary step. Thakur et al. [13] and Dehaerne et al. [14] adopt full fine-tuning which adjusts parameters of the entire LLM model with their dataset collected from GitHub. However, the lack of adequate data cleaning and task-specific training influences their functional accuracy. ChipNeMo [15] from Nvidia deploys a two-round fine-tuning process with their in-house data, while only the first round can benefit RTL code generation. The dataset and model are not released to the public. Meanwhile, its functional accuracy is not satisfying due to the same issue as [13,14]. To improve the model performance, VerilogEval [16] and the most recent work, RTLCoder [17], utilize ChatGPT-3.5 to generate high-quality problem-code pairs as the dataset for the singleround task-specific fine-tuning, demonstrating relatively good results on existing benchmarks. However, their small synthetic dataset lacks diversity, influencing the generality and making it hard to achieve higher accuracy. BetterV [18] simultaneously fine-tunes the model and trains a generative discriminator to assist in Verilog code generation, which increases the difficulty of deployment. \n\nTo summarize, previous methods fail to balance the diversity and quality of dataset. Additionally, the single-round domainadapted fine-tuning limits the ability of LLM to maintain high code quality while generating diverse outputs. Meanwhile, LLM faces the hallucination problem and may incorporate software coding habits into Verilog code generation, potentially causing syntactic and functional errors. However, few of prior methods notices this issue.",
            "score": 0.5904911162919122,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "254926675"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "254926675"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "261822682"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "267500201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4345703125
        },
        {
            "corpus_id": "271534687",
            "title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications",
            "text": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance by adapting them to specific domains and tasks (Shi et al., 2023). This approach is particularly valuable in real-world enterprise scenarios, where there is often a need to address specific downstream tasks using available data, such as the company's proprietary data. \n\nRecently released base-models, such as Gemma (Team et al., 2024), Llama (Touvron et al., 2023a,b), and Mistral (Jiang et al., 2023), claim ease of fine-tuning across various tasks (Zhao et al.,   * These authors equally contributed to this work. 2024). However, comprehensive studies of these models in the context of fine-tuning are still limited, leaving several important questions less explored. In this paper, we focus on the role of hyperparameter optimization (HPO) in the fine-tuning process of LLMs, and provide detailed and concrete recommendations for HP values, aiming to save practitioners time and computational resources. We present Coverage-based Search (CBS), which leverages an extensive grid search for highlighting an effective HP recommendation, as well as the ability to expand to a few promising HP configurations that collectively suggest high performance across diverse datasets and tasks. \n\nFor the purpose of providing these recommendations we conduct a comprehensive systematic study, focusing on practical scenarios where relatively small training data are available for tuning. We examine prominent tasks such as classification, summarization, and contextual question-answering (CQA) across various domains. Our study considers two leading LLMs, Llama-3-8B (AI@Meta, 2024) and Mistral-7B-v0.3 (Jiang et al., 2023), as well as two commonly used fine-tuning methods: full fine-tuning (FFT) and LoRA (Hu et al., 2021). \n\nOur main contributions are as follows: \n\n1. Recommended HP configurations for tuning, optimized per model and tuning method. \n\n2. Analysis of the differences between Llama-3-8B and Mistral-7B-v0.3, as well as between LoRA and FFT, across 3 real-world tasks in practical scenarios.",
            "score": 0.5903040070741223,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338134765625
        },
        {
            "corpus_id": "272366724",
            "title": "Enhancing Event Reasoning in Large Language Models through Instruction Fine-Tuning with Semantic Causal Graphs",
            "text": "Improving LLM performance on specific tasks faces challenges when pretraining hasn't provided the necessary skills or domain-specific knowledge. Various approaches have been proposed to address this issue, each targeting different aspects of model capabilities. Few-shot learning (Brown et al. 2020) provides the LLM with task examples, leveraging the model's ability to adapt to new tasks with minimal guidance. Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) augments the model with external knowledge, enabling access to information beyond its training data. Chain-of-Thought (CoT) prompting (Wei et al. 2022) guides LLMs to break down complex problems into intermediate steps, enhancing their reasoning abilities. \n\nMore fundamental model training techniques have also been developed to align LLMs with specific tasks or desired behaviors. These include Instruction Tuning (Ouyang et al. 2022), which fine-tunes models on diverse task instructions, Reinforcement Learning with Human Feedback (RLHF) (Stiennon et al. 2020), which optimizes model outputs based on human preferences, and Direct Preference Optimization (DPO) (Rafailov et al. 2024), which efficiently adapts model parameters using positive and negative examples. Despite these advancements, there remains a significant gap in explicitly enhancing LLMs' understanding of causal relationships, which could be crucial for tasks requiring such capabilities.",
            "score": 0.5887457092397549,
            "section_title": "Enhancing LLM Task Performance",
            "char_start_offset": 5932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 298,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 450,
                    "end": 468,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 886,
                    "end": 906,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "268417191"
                },
                {
                    "start": 1135,
                    "end": 1157,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271240234375
        },
        {
            "corpus_id": "271218609",
            "title": "Evaluating the Efficacy of Foundational Models: Advancing Benchmarking Practices to Enhance Fine-Tuning Decision-Making",
            "text": "There exist various categories of fine-tuning techniques that are worth mentioning. First, fine-tuning the pre-trained parameters can be performed in either a full [40] or partial [41] manner, aiming to update the pre-trained parameters to suit a new task. Although this approach has demonstrated remarkable performance, particularly in domain-specific tasks, it is computationally expensive. Second, parameter-efficient finetuning (PEFT) involves adding a small trainable parameter for fine-tuning. PEFT utilizes only a small percentage of existing fine-tuned parameters, referred to as low-rank, to adapt to a downstream task and incorporates them into the pre-trained model [42]- [44]. While this strategy balances performance and resource efficiency better than full finetuning, it increases model size. Finally, prompt-based finetuning [45], [46] is a method to construct prompts in a more insightful manner to optimize the model's performance without altering its parameters. In addition, advanced prompt tuning techniques, such as retrieval augmented generation (RAG), have been introduced and demonstrated to effectively mitigate LLM hallucinations [47]. However, a drawback of prompt tuning is that it requires users to have more experience in creating prompts or crafting RAGs that align with their objectives. \n\nIn general, the perspective on LLM fine-tuning may vary depending on the researcher's objectives. A large organization with abundant computing resources may prioritize highaccuracy downstream tasks or specific tasks. Conversely, for a small organization, institution, or individual researcher with limited resources, the objectives may include reducing fine-tuning computational overhead while enhancing overall performance.",
            "score": 0.5884056393780668,
            "section_title": "A. Large Language Foundation Model (LLFM)",
            "char_start_offset": 9185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "258947572"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "262084134"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2119140625
        },
        {
            "corpus_id": "269605525",
            "title": "A scoping review of using Large Language Models (LLMs) to investigate Electronic Health Records (EHRs)",
            "text": "LLMs possess robust capabilities in representing general domain knowledge, yet they often require customization for specific downstream tasks or domains, usually achieved through fine-tuning.In conventional full-parameter fine-tuning, all model weights are modified, presenting inefficiencies, especially for LLMs.Consequently, several parameter-efficient fine-tuning techniques have been developed to improve efficiency, as briefly introduced below.\n\nAdapter Tunning -Adapter tuning introduces small and trainable modules called adapters into the layers of a pre-trained model, allowing for fine-tuning to be performed exclusively on these added modules.This approach can significantly reduce the computational cost and preserve the general knowledge of the original pre-trained model.The idea of adopting an adapter for fine-tuning LLM was first proposed by Houlsby et al. 89 .In its design, adapter modules (each module consists of two feedforward layers) are added between each feed-forward layer and the Layernorm layer.During fine-tuning, the pre-trained model remains frozen, and the adapter modules learn the knowledge specific to the downstream task.The addition of adapter modules can mitigate the efficiency issue of fine-tuning the entire model and prevent catastrophic forgetting.R\u00fcckl\u00e9 et al. 90 observed that while the adapter presents greater efficiency in training, it slows down at inference time, compared with full-model finetuning.Motivated by this, they proposed AdapterDrop, a technique that dynamically removes adapters from lower Transformer layers.This can enable the shared representations at adapters added at low Transformer layers to improve inference efficiency, especially when conducting multiple task inferences simultaneously.\n\nPrompt Tuning -Another parameter-efficient fine-tuning method 91 involves reformulating downstream tasks into a conditional generation task by designing prompts, followed by fine-tuning the model under the condition of these prompts.We focus exclusively on approaches that treat prompts as trainable parameters.Within the trainable prompt framework, prompt vectors are not constrained by the language model's vocabulary embeddings.Instead, they are optimized as continuous variables and concatenated with word type embeddings before being input into the language model.\n\nNumerous models implement this concept in diverse ways.",
            "score": 0.5856823003111049,
            "section_title": "Fine-tuning",
            "char_start_offset": 13296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 450
                },
                {
                    "start": 452,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1574
                },
                {
                    "start": 1574,
                    "end": 1761
                },
                {
                    "start": 1763,
                    "end": 1996
                },
                {
                    "start": 1996,
                    "end": 2074
                },
                {
                    "start": 2074,
                    "end": 2194
                },
                {
                    "start": 2194,
                    "end": 2332
                },
                {
                    "start": 2334,
                    "end": 2389
                }
            ],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 877,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247802734375
        },
        {
            "corpus_id": "253244132",
            "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
            "text": "Natural language processing (NLP) has recently been revolutionized by scaling up transformer based large language models (LLMs) together with large-scale pretraining (Vaswani et al., 2017;Devlin et al., 2019;Raffel et al., 2020a;Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Smith et al., 2022;Touvron et al., 2023). In addition to improved downstream performances, these pretrained LLMs can perform a broad array of unforeseen tasks when provided with a prompt. This in-context learning capability allows users to flexibly re-purpose LLMs for specific tasks with a minimum amount of supervised data, making it extremely convenient for fast prototyping and experimentation, especially in the low data regime. \n\nHowever, even the largest and most advanced LLMs leave a lot to be improved. Grounding and eliminating hallucinations (Maynez et al., 2020), reasoning and logical clarity (Creswell & Shanahan, 2022), mathematics (Brown et al., 2020;Noorbakhsh et al., 2021) are just a few examples where LLMs still lag behind the best human performances, or in some cases, the fine-tuned performances of the same model. \n\nThe most common practice to improve a pretrained model is to fine-tune it on a specialized task or several tasks. However, fine-tuning on LLM usually causes over-specialization to the fine-tuning tasks, and harm the model's pre-existing generalization ability on unseen tasks via in-context learning. As we show later, an mT5 model finetuned on a single task loses its few-shot performance on unseen tasks within one thousand steps of fine-tuning. When faced with hundreds of downstream tasks and even unknown tasks, we expect to have a single fine-tuned model that is both superior on supervised fine-tuned tasks and general unseen tasks. Thus, it becomes very important to develop new techniques for finetuning that prevent over-specialization of these fine-tuned models only to a few tasks.",
            "score": 0.5829577717764928,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 188,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 188,
                    "end": 208,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 208,
                    "end": 229,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 229,
                    "end": 248,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 842,
                    "end": 863,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 936,
                    "end": 956,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25634765625
        },
        {
            "corpus_id": "272770124",
            "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical Tasks in Diabetes Care and Management",
            "text": "Given that data quality is a key determinant of model performance, we further conducted data refinement to construct a high-quality dataset. Note that although RLHF (Ouyang et al., 2022) is frequently used to improve the LLM alignment with human preference after conducting vanilla finetuning, it always requires expensive preference-labeling process for reward modeling in open-ended scenarios (Xu et al., 2024). To address this, motivated by previous research (Yang et al., 2024c) that designs a self-distillation method to enhance model performance during the continual fine-tuning, we apply an improved self-distillation pipeline. This approach is effective in our case for reducing the data distribution shift relative to the knowledge contained in the LLM. \n\nPreliminary. Though LLMs showcase outstanding performance in various language tasks, they often face limitations with downstream tasks that require continual fine-tuning. Specifically, we refer to an LLM in need of fine-tuning as a seed LLM, denoted as f and parameterized by \u03b8. The seed LLM typically undergoes vanilla fine-tuning to map any natural language instruction x to its corresponding output y by updating the model parameters. This update aims at minimizing the disparity between the data distribution and the LLM distribution: \n\nwhich seeks to minimize the negative log likelihood P of the target output y given the input x with the model parameters \u03b8. L converges when the fine-tuned LLM's generated response matches y, i.e., the distribution of fine-tuned LLM aligns with the task data distribution. This process can inject the knowledge contained in the data into the LLM. \n\nMethod. Note that vanilla fine-tuning an LLM on a collected dataset, whose distribution is far from the LLM's, can be harmful to the LLM's original alignment with human preference and lead to catastrophic forgetting in general instruction-following capabilities, which consequently results to the decrease of LLM's response quality (Ren et al., 2024a). To address these issues in vanilla fine-tuning, we propose a modified self-distillation (SD) pipeline to make the LLM better align the distribution of the collected dialogue dataset.",
            "score": 0.5826403524392813,
            "section_title": "DATA REFINEMENT",
            "char_start_offset": 4056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 186,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "We then show the results of the fine-tuned Llama-2 on out-of-domain testing datasets of the same task type (Setting 2).We derive the following observations based on Figure 2:\n\nFine-tuned models underperform compared to the baseline model on generation tasks, yet outperform on classification tasks.For the out-ofdomain testing results, a clear distinction emerges based on the task categories, i.e., between generation and classification tasks.In generation testing datasets ((a) PeerRead, (b) CNN/DailyMail, (c) Tweetqa, and (d) Sciqa in Figure 2), fine-tuned models perform worse than baseline models, and this gap persists regardless of the number of incontext examples provided.Notably, there lacks a performance growth with more training examples in datasets like CNN/DailyMail, Tweetqa and Sciqa, suggesting that the generalization ability of fine-tuned LLMs is impaired.\n\nOn the other hand, in the context of sentiment classification tasks, the best performance of the fine-tuned models is on par with the baseline Llama-2, which means the fine-tuned LLMs' sen- The diverging effects of fine-tuning on generation and classification tasks for out-of-domain testing may originate from the difference in task output space constraints.The output space of classification tasks is inherently predefined and limited, enabling fine-tuned LLMs to apply their inherited and adapted knowledge relatively easily to new domains.In contrast, the output space of out-ofdomain generation datasets largely deviates from that of the training set.Despite being given a few in-context examples, fine-tuned models may still find it challenging to reason about the expansive range of possible outputs in new domains.",
            "score": 0.5805297766027044,
            "section_title": "Same Task, Out-of-domain Datasets",
            "char_start_offset": 15292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 174
                },
                {
                    "start": 176,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 1238
                },
                {
                    "start": 1238,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1754150390625
        },
        {
            "corpus_id": "273655060",
            "title": "Geo-FuB: A method for constructing an Operator-Function knowledge base for geospatial code generation with large language models",
            "text": "By incorporating domain-specific knowledge from the knowledge base through fine-tuning approaches, general LLMs can retain their general language processing capabilities while acquiring the specialized knowledge required for specific tasks. Compared to RAG technology, the process of fine-tuning a model demands a more substantial investment of computational resources. However, this method yields a higher degree of domain adaptability, which leads to improved accuracy and efficiency when tackling domain-specific tasks. \n\nFine-tuning is particularly suitable for scenarios that call for profound domain expertise. Therefore, by fine-tuning LLMs with the Geo-FuB knowledge base, these LLMs can be specialized to precisely grasp the intricacies of terminology, logic, and operations of the geospatial modeling domain , thereby enhancing their proficiency in geospatial code genenration tasks.",
            "score": 0.5803270314914168,
            "section_title": "Fine-Tuning Large Language Models",
            "char_start_offset": 46969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 893
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1468505859375
        },
        {
            "corpus_id": "277999732",
            "title": "GENCNIPPET: Automated Generation of Code Snippets for Supporting Programming Questions",
            "text": "The foundational LLMs are general-purpose models and often lack the necessary contextual focus. As a result, they may struggle to generate code examples that are relevant and appropriate to the specific context of a question. However, existing studies suggest that fine-tuned models with highquality task-specific data help models better understand the context to perform specific tasks [23,24,25,26,27,28]. Therefore, we plan to fine-tune LLM base models, specifically the open-source Llama-3 (e.g., Llama-3-8B), due to its accessibility and cost-effectiveness. We will randomly select 80% of Java-and Python-related questions from our dataset for fine-tuning, with 10% used for validation and 10% for testing. We plan to use LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning approach that significantly reduces memory requirements to ensure practical feasibility. For example, when fine-tuning Llama-3-8B, we may require a GPU with at least 16 GB VRAM (e.g., RTX 3090) for basic training, while 24 GB VRAM (e.g., RTX 4090) provides better flexibility and performance. If the fine-tuned Llama model demonstrates satisfactory performance, we will not consider any paid models (e.g., GPT-4). However, if the results indicate limitations, we will explore alternative models such as GPT-4 and CodeLlama for comparison. We plan to conduct instructional fine-tuning by providing structured input-output pairs to enhance alignment with SO-style question structures. \n\nTo fine-tune the LLM model, we will follow these steps: \n\n\u2022 Input/Output Data Preparation: We will extract problem descriptions and their corresponding code snippets from SO questions. Programming language specifications will be extracted from question tags. While our current approach plans to fine-tune LLMs on static data, we also plan to explore RAG technology or API search integration to enhance snippet accuracy and adaptability to evolving frameworks.",
            "score": 0.5785478333119526,
            "section_title": "B. Model Fine-Tuning",
            "char_start_offset": 7015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "257495997"
                },
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "270371731"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "222080996"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "232076119"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "271234568"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "272668518"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2359619140625
        },
        {
            "corpus_id": "276741193",
            "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
            "text": "Fine-tuning for domainspecific tasks has been extensively explored. The RAFT approach (Zhang et al. 2024) combines Retrieval-Augmented Generation (RAG) with fine-tuning to enhance LLM performance in specific domains by training models to disregard irrelevant retrieved documents, improving focus and accuracy. Similarly, \"RAG vs Fine-tuning\" (Gupta et al. 2024) compares both approaches across various LLMs, demonstrating how each method can be effectively employed for domain-specific applications, particularly in underexplored sectors like agriculture. Additionally, \"Fine-tuning Language Models for Factuality\" (Tian et al. 2023) leverages recent innovations in factuality judgment and preference optimization algorithms to improve the factual accuracy of LLMs, offering a novel approach to mitigating misinformation. \n\nInstruction Tuning and Data Selection Efficient data selection for instruction tuning is crucial for optimizing LLM performance. \"From Quantity to Quality\" (Li et al. 2023) introduces a self-guided methodology that employs the Instruction-Following Difficulty metric to identify highquality instruction data, enhancing training efficiency. Additionally, \"Rethinking Data Selection for Supervised Fine-Tuning\" (Shen 2024) argues that selecting data reflecting human-like interactions, rather than purely based on quality and diversity, yields better results in aligning models with human expectations. The MoDS approach (Du, Zong, and Zhang 2023) further refines data selection by focusing on quality, coverage, and necessity, demonstrating improved performance with a significantly reduced dataset. Addressing LLM limitations such as hallucinations and weak numerical reasoning, ToolQA (Zhuang et al. 2024) introduces a dataset to evaluate LLMs' ability to use external tools for question answering, providing insights into their strengths and weaknesses.",
            "score": 0.5776064642077654,
            "section_title": "Domain-Specific Adaptation",
            "char_start_offset": 7054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1879
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.509765625
        },
        {
            "corpus_id": "272922197",
            "title": "The Impact of Prompting Techniques on the Security of the LLMs and the Systems to Which They Belong",
            "text": "The general-purpose ability to generate text or perform different kinds of NLP tasks makes the LLMs a perfect starting point for a lot of use cases we might be interested in. But once we have our task defined, we would like perhaps to increase the performance of the model on that specific job. The traditional way of doing so is usually to fine tune the model for our use case. We adapt our general-purpose model to a specific task by updating the parameters on a new dataset specific to our task. Most of the time, this approach yields the best results, but there are a few problems with it. One of these problems is the fact that the amount of NLP tasks and different variations for these jobs are too big to have for every one of them a dataset that fits them, especially if we want something custom-made for our system. \n\nThe \"Language Models are Few-Shot Learners\" [1] paper presents a different method of tweaking an LLM to increase its performance on an NLP job. This paper explores the ability of the LLMs to conduct in-context learning and to adapt to new kinds of NLP tasks more or less on the fly. Inspired by human behavior, the authors observe that most of the time, humans need a few examples or instructions to understand how to perform an NLP task. This type of approach was not very effective on the smaller language models, but as the size of the model scales, they notice that this method \"improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches\" [1]. The model used to test this hypothesis is GPT-3, which is an auto-regressive language model with 175 billion parameters without any kind of fine tuning or gradient updates. They test three different settings for it-zero-shot (the model is only given a natural language instruction describing the task, but no demonstration is provided), one-shot (the model is given a natural language instruction describing the task alongside a single demonstration of how should the task be performed) and few-shot (the model is given a few demonstrations of the task at inference time as conditioning together with the natural language instruction describing the task)-and compare them with the traditional fine-tuned models specific for each task.",
            "score": 0.577452350471657,
            "section_title": "Language Models Are Few-Shot Learners",
            "char_start_offset": 8764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "266362350",
            "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
            "text": "LLMs are pretrained on a large general-purpose corpus and then plugged into user modeling systems. A lot of works use LLMs as a general-purpose reasoner with frozen pertaining parameters, while others fine-tune LLMs to help them adapt to the specific task in user modeling. The latter paradigm allows the model to leverage the general language understanding capabilities learned during the pretraining stage while specializing the knowledge to the task at hand and shows superior task performance. However, fine-tuning and deployment can be highly computationally expensive, especially when deploying on millions and even billions of scale user data. Therefore, developing efficient domain adaptation methods to reduce the cost for LLMs to fix in user modeling systems and efficiently help LLM-UM be personalized and adapt to the user's dynamic needs. Moreover, user-specific data is often sparse and noisy [38,107], which can lead to overfitting during the fine-tuning process, which can limit the model's ability to generalize to different users and contexts. Therefore, future research to combat these challenges can focus on 1) efficient fine-tuning for LLM-UM: There is a need to develop and refine techniques for more efficient fine-tuning of LLMs specific for user modeling context. Techniques such as sparsity induction, model pruning, and knowledge distillation could be explored to reduce the computational and memory footprints. 2) online learning: To cope with the dynamics of user interests, research could explore online learning strategies for LLM-UM systems. These strategies would continuously update the model based on the latest user-generated content and interactions.",
            "score": 0.5765740199806115,
            "section_title": "Efficient and Effective Domain Adaptation",
            "char_start_offset": 92247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "52984533"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "58370896"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1766357421875
        },
        {
            "corpus_id": "273248016",
            "title": "Text processing using LLM for automatic creation of agricultural crops knowledge bases",
            "text": "The use of large language models (LLMs) for processing text documents is a classical approach based on prompts, known as Retrieval-Augmented Generation (RAG). Before the era of LLMs, models were often supplemented with new data simply by fine-tuning them. However, now that the models being used have become much larger, training them on significantly larger datasets is suitable for only a few usage scenarios. Fine-tuning is particularly effective in cases where the model needs to interact with users using a style and tone of speech different from the original. However, fine-tuning is not as effective when new data needs to be added to the model, which I have found to be a much more common business scenario. Additionally, fine-tuning LLMs requires large volumes of high-quality data, substantial computational resources, and significant time. For most LLM users, these are considered limited resources (Fig. 1).",
            "score": 0.5761033261144847,
            "section_title": "Formulation of the problem",
            "char_start_offset": 4308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159912109375
        },
        {
            "corpus_id": "260379087",
            "title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation",
            "text": "Finding 5: In the one-shot setting, BM25-based selection strategy is often the best on generation problems such as assertion generation and code summarization, but has no significant difference than basic strategies (random or fixed) on classification problems such as defect detection and clone detection. \n\nInstructed LLMs v.s. SOTA models. After being fine-tuned on the same training dataset of each task, most instructiontuned LLMs outperform the small SOTA models. As mentioned in Section III-D, for the resource limits, we finetune each instruction-tuned LLM with the parameter-efficient tuning strategy LoRA, thus only a small number of parameters would be updated during fine-tuning; for small SOTA models, in line with previous work [29], all of their parameters would be updated during fine-tuning. Table X presents the number of parameters updated in fine-tuning for each model. \n\nInterestingly, the number of updated parameters in instructed LLMs is actually much smaller than the SOTA small models (e.g., \u00a1 10M v.s. \u00bf 100M); however, the instruction-tuned LLMs exhibit much higher performance than SOTA models after fine-tuning. The reason might be that the original instructed LLM is already quite powerful (as confirmed by our RQ1) and fine-tuning a small number of parameters is sufficient for further improving the model performance on the downstream task. In addition to the small SOTA models, the large model without instruction tuning (i.e., CodeGen-6B) still performs worse than instruction-tuned models after both being finetuned, indicating that the benefits from instruction tuning cannot be easily mitigated in the subsequent fine-tuning phase. Fine-tuning Epoch. The results above are based on finetuning each model for five epochs. We then present the performance trend of different fine-tuning epochs. For space limits, we present the results of two instructed LLMs, i.e., general-domain ChatGLM-6B and code-specific CodeAlpaca-7B in Figure 4. As shown in the figure, the model performance reaches a plateau after three epochs, indicating that the instruction-tuned LLMs can quickly adapt to downstream tasks with a few number of fine-tuning epochs.",
            "score": 0.5759328796199,
            "section_title": "Based on Table VI and Table",
            "char_start_offset": 36967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "256662599"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10821533203125
        },
        {
            "corpus_id": "270199619",
            "title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems",
            "text": "RQ1: Is fine-tuning necessary for LLM-based decision-making tasks? \n\n-Takeaway: \"Fine-tuning can largely increase the performance of LLMs on specific embodied tasks.\" Before we delve into the performance analysis of our proposed attacks, we need to first verify that task-specific fine-tuning is indeed necessary for the LLM-based decision-making systems targeted by our backdoor attacks. As shown in Table 2, the original LLMs without task-specific fine-tuning indeed exhibit very limited performance despite CoT demonstrations. Even if RAG enhances original models by using knowledge from similar questions, the performance is still unsatisfied; for instance, LLaMA2-7B (Touvron et al., 2023b) cannot even handle the long input context when augmented with RAG as it cannot answer with the instruction format and only has 2% of Acc on nuScenes dataset. \n\nAfter fine-tuning on a task-specific dataset, their performance, both in reasoning and adherence to specific formats, improves significantly. We also study this for robot decision-making tasks and have similar observations in Table 3. Additionally, these findings are consistent with prior works (Xu et al., 2024;Shao et al., 2024;Ma et al., 2023;Mao et al., 2023), which thus solidifies our motivation of studying attack targeting the fine-tuning stage for LLM-based decision making systems. -Takeaway: \"Attacks on ICL are much worse effective given the complex embodied tasks themselves and the fine-tuning process.\" Badchain (Xiang et al., 2024) has shown impressive attack performance (over 85% ASR on average) on the original LLMs under general reasoning tasks (e.g. arithmetic reasoning). However, as shown in Table 2, its ASR becomes much lower when applied to embodied AI tasks with domain-specific fine-tuning. This suggests that fine-tuning can highly effectively enhance the robustness of LLMs against such in-context learning attacks.",
            "score": 0.5744787207195101,
            "section_title": "RESEARCH QUESTIONS (RQS) AND RESULTS",
            "char_start_offset": 24819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 69,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 1152,
                    "end": 1169,
                    "matchedPaperCorpusId": "263605524"
                },
                {
                    "start": 1169,
                    "end": 1187,
                    "matchedPaperCorpusId": "266174267"
                },
                {
                    "start": 1484,
                    "end": 1504,
                    "matchedPaperCorpusId": "267094837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2252197265625
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "Catastrophic forgetting in fine-tuned language models.Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2022;Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;Kotha et al., 2024;Wu et al., 2024b).In context of LLMs, existing approaches towards mit-igating this issue can mostly be categorized into three types: regularizing the update of model parameters (Kirkpatrick et al., 2017;Huang et al., 2021;Cha et al., 2021), replaying previous or selfsynthesized data (Scialom et al., 2022;Huang et al., 2024a) and resisting interference via parameterefficient fine-tuning (Razdaibiedina et al., 2023;Wang et al., 2023a).\n\nMechanistic analysis to fine-tuning.Existing works on analyzing the internal mechanism (R\u00e4uker et al., 2023;Ferrando et al., 2024) of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal transformation on top of the original capability (Jain et al., 2024) (wrappers), subtractable and reusable parameter shift vectors (Huang et al., 2024b;Gao et al., 2024) (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage (Ren et al., 2024).Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point.",
            "score": 0.5733108312990816,
            "section_title": "Related work",
            "char_start_offset": 24807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 54,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 214,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 529,
                    "end": 548,
                    "matchedPaperCorpusId": "262054014"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "233210110"
                },
                {
                    "start": 770,
                    "end": 787,
                    "matchedPaperCorpusId": "219636462"
                },
                {
                    "start": 832,
                    "end": 854,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 937,
                    "end": 965,
                    "matchedPaperCorpusId": "256390383"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "264426441"
                },
                {
                    "start": 1307,
                    "end": 1326,
                    "matchedPaperCorpusId": "265308865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332275390625
        },
        {
            "corpus_id": "275471407",
            "title": "Enhancing AI Safety Through the Fusion of Low Rank Adapters",
            "text": "Large Language Models (LLMs) have demonstrated remarkable proficiency, exhibiting advanced linguistic and reasoning capabilities [1] that make them increasingly favored choices for conversational agents. With each new iteration, these models are released to the public with enhanced functionalities designed to assist in a myriad of user tasks, ranging from simple queries to complex problem-solving scenarios. \n\nThese LLMs excel at performing general tasks and can also be adapted for specific activities through In-context Learning (ICL) [2], where the model leverages existing parameters without the  need for updates. However, when more profound task-specific performance tuning is required, finetuning becomes necessary. Here, parameters of the base model are modified to align with the demands of downstream tasks. In this realm, Parameter-Efficient Fine-Tuning (PEFT) [3] has emerged as a popular strategy, particularly within large-scale models. Techniques like Low-Rank Adaptation (LoRA) [4] stand out due to their practicality in selectively updating a small subset of parameters, thereby maintaining the vast pre-trained knowledge base while optimizing the model towards specific tasks. \n\nDespite the advantages, fine-tuning can inadvertently lead to jailbreaking of the model, where the LLM deviates from safety constraints previously set by the base configuration. This issue has been noted in several studies [6,7,8,9], which highlight the challenges of maintaining safety alignment when adapting models through fine-tuning. \n\nLLMs have increasingly incorporated advanced safety mechanisms, such as Reinforcement Learning from Human Feedback (RLHF) [5], to simultaneously optimize for both helpfulness and harm reduction. These safety alignment techniques are crucial for aligning model outputs with ethical guidelines and user expectations. However, while fine-tuning LLMs on downstream tasks can significantly enhance their helpfulness and task-specific performance, this fine-tuning process often inadvertently compromises the models' inherent safety protocols. This degradation in safety measures during fine-tuning raises critical concerns, as it may lead to the generation of outputs that, although high-performing, could be potentially harmful or biased. \n\nTo address the challenge of maintaining safety standards while enhancing task performance during fine-tuning, our contribution is twofold:",
            "score": 0.5731870229252,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2275
                },
                {
                    "start": 2278,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 1423,
                    "end": 1426,
                    "matchedPaperCorpusId": "265308865"
                },
                {
                    "start": 1426,
                    "end": 1428,
                    "matchedPaperCorpusId": "263671523"
                },
                {
                    "start": 1428,
                    "end": 1430,
                    "matchedPaperCorpusId": "265067269"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22802734375
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "To address these limitations and enhance the capabilities of LLMs for specific domains, existing methods can be broadly classified into three main approaches: in-context learning, training models from scratch on domain-specific and general data, and fine-tuning existing models using supervised datasets. In-context learning, where LLMs generate results based on a few demonstration examples [24], can be costly, slow in inference, and limited by the model's context window [2]. Moreover, these models can be sensitive to the quality and variability of the provided examples [21]. \n\nTraining models from scratch, while effective, demands significant computational resources and vast domain-specific and general dataset. Fine-tuning existing models is promising alternative but faces challenges such as the need for high-quality datasets and the risk of catastrophic forgetting, where the model's ability to perform general tasks degrades after domain-specific fine-tuning. Techniques like model merging can help mitigate these effects. \n\nPrevious studies have demonstrated that instruction fine-tuning smaller open-source language models can significantly enhance their performance on domain-specific tasks across various fields, such as law and medicine. In many cases, these models have outperformed the zero-shot performance of proprietary LLMs and other state-of-the-art models [19,23,33,61,65]. Within the finance domain, one study fine-tuned the Llama2 model for sentiment analysis, outperforming the FinBERT [62]. Another study fine-tuned Llama2-7B and Llama2-13B models on a variety of financial tasks, including sentiment analysis, relation extraction, question answering, and stock market prediction [54]. However, much of the existing research has focused primarily on fine-tuning models from the Llama family. To address this gap, we explore fine-tuning other powerful, smaller LLMs, specifically Mistral-7B, Phi-3, and Llama2-8B, across four representative financial text classification tasks: sentiment analysis, news headline classification, relation extraction, and hawkish-dovish classification. \n\nOne of the main challenges with fine-tuning LLMs for domain-specific tasks is the degradation of their zero-shot performance on unseen tasks [63].",
            "score": 0.5712495280241818,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "263835169"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "248367393"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "263829356"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "268042106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3095703125
        },
        {
            "corpus_id": "268363542",
            "title": "Couler: Unified Machine Learning Workflow Optimization in Cloud",
            "text": "COULER has shown promising results for a range of tasks except for some complex workflows.The use of LLMs in our research is primarily aimed at exploring the potential of these models to streamline and enhance the code generation process.We acknowledge that current LLMs already demonstrate a satisfactory level of accuracy in code generation.Indeed, we are considering fine-tuning as a viable method to enhance the quality of the generated code.Specifically, our team has conducted work on Multi-LoRA optimization for finetuning [55].Additionally, we are developing a workflow for code generation by fine-tuning 'llama2'.We will soon release a fine-tuned model in the COULER open-source repository.",
            "score": 0.5699309142502811,
            "section_title": "D. NL to Unified Programming Code Generation: RQ3",
            "char_start_offset": 36532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 699
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "266149920",
            "title": "DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs",
            "text": "LLM fine-tuning. In contrast to prior deep learning workloads that follow a task-specific paradigm, which trains a model from scratch on domain-specific data to tackle a particular task (e.g., machine translation [71,81]), LLMs follow a pretrain-then-finetune paradigm, which pretrains a model on a massive general-domain corpus and then fine-tunes it for specific objectives. For example, ChatGPT is fine-tuned to follow human instructions [60]. There are two main fine-tuning approaches: 1) Full Model Tuning (FMT) which updates all model parameters and 2) Parameter-Efficient Tuning (PEFT) which adds a small number of extra parameters after pretraining, called adapters, e.g., low-rank matrices learned during fine-tuning. PEFT methods, such as LoRA [39], are popular ways to reduce the compute and memory requirements of both fine-tuning and serving. However, the choice of finetuning paradigm impacts accuracy. While PEFT methods can achieve high accuracy for a variety of tasks [39], recent studies [8,12,89] -summarized in Figure 2 -reveal that FMT still achieves higher accuracy for more complex tasks. \n\nLLM compression. Model compression is a popular approach to reduce the memory and compute requirements of LLM inference in resource-constrained environments. Techniques like GPTQ [31], SparseGPT [29] and AWQ [47] reduce memory footprints and improve latency while maintaining model quality (when applied in moderation). Pushing these techniques to extremely low bit-width quantization and sparsity, such as 2-bit quantization and more than 50% sparsity, results in significant model quality degradation [14,49]. \n\nLLM serving. The LLM inference involves two phases: (1) prompt processing (prefill), where the tokens (i.e., basic units of text) in the input prompt are processed in parallel. This phase can be parallelized since all previous tokens are known from the user-provided prompt, and it is usually computebound. (2) token generation (decode), where the model iteratively generates one token for each forward pass.",
            "score": 0.569153432069437,
            "section_title": "Background",
            "char_start_offset": 8327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1625
                },
                {
                    "start": 1628,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "271271084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.186279296875
        },
        {
            "corpus_id": "270123761",
            "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
            "text": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or harmless content generation, it is nearly impossible to obtain a large volume of high-quality data that matches the downstream distribution. To improve the performance of LLMs in data-scarce domains with domain-mismatched data, we re-evaluated the Transformer architecture and discovered that not all parameter updates during fine-tuning contribute positively to downstream performance. Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set. Based on this insight, we propose an effective inference-time intervention method: Training All parameters but Inferring with only Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn using two general instruction-tuning datasets and evaluate it on seven downstream tasks involving math, reasoning, and knowledge understanding across LLMs of different parameter sizes and fine-tuning techniques. Our comprehensive experiments demonstrate that \\trainallInfAttn achieves superior improvements compared to both the fully fine-tuned model and the base model in most scenarios, with significant performance gains. The high tolerance of \\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning and enhances specialized tasks using general data. Code is available in \\url{https://github.com/pixas/TAIA_LLM}.",
            "score": 0.5684970712017748,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2083740234375
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "Upon preparing the instruction dataset, it is partitioned into training, validation, and test splits.During fine-tuning, prompts from the training set are used to generate completions, which are compared against the expected responses.The model outputs a probability distribution across tokens, and the cross-entropy loss function calculates the discrepancy between the generated and expected distributions.This loss is then used to update the model's weights through backpropagation [14].The fine-tuning process involves multiple batches and epochs, continuously updating the model's weights to enhance performance.Evaluation using holdout validation and test datasets yields metrics such as validation accuracy and test accuracy, providing a measure of the model's performance.The result of fine-tuning is an improved version of the base model, referred to as an instruct model, optimized for specific tasks.Instruction fine-tuning is the predominant method for fine-tuning LLMs today, typically implied when discussing fine-tuning in this context.Large Language Models (LLMs) are renowned for their versatility in performing various language tasks within a single model.However, specific applications often necessitate excelling in a single task.Fine-tuning a pre-trained model on a particular task, such as summarization, can significantly enhance performance, achievable with relatively small datasets, often just 500-1,000 examples, compared to the billions of texts used during pre-training [31,3].Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27].To mitigate catastrophic forgetting, one must assess its impact on their specific use case.If the primary requirement is reliable performance on the fine-tuned task, the loss of multitask capabilities may be acceptable.However, if maintaining generalized capabilities is essential, multitask fine-tuning offers an alternative.",
            "score": 0.5679471587155517,
            "section_title": "Improving Performance of Large Language Models through Fine-Tuning",
            "char_start_offset": 31834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1505
                },
                {
                    "start": 1505,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 2058
                },
                {
                    "start": 2058,
                    "end": 2186
                },
                {
                    "start": 2186,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 1498,
                    "end": 1502,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1502,
                    "end": 1504,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224609375
        },
        {
            "corpus_id": "273549732",
            "title": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in Low-Resource Code",
            "text": "Large Language Models (LLMs) possess several intrinsic capabilities that are a result of both the extensive training they undergo and the fine-tuning they receive on specialized data. One of their core strengths is general knowledge reasoning [Liang et al., 2022], which arises from the vast amount of diverse data they are trained on [Touvron et al., 2023a,b, Guo et al., 2024]. This general reasoning ability enables LLMs to provide accurate responses to a wide range of tasks across different domains, even without task-specific fine-tuning. \n\nIn addition to general reasoning, LLMs can develop proficient knowledge when fine-tuned on domain-specific datasets. This allows them to excel in specialized areas, such as legal [Colombo et al., 2024], medical [Singhal et al., 2023, Diao et al., 2023], or technical fields [Taylor et al., 2022], where deep and nuanced understanding is required. Fine-tuning equips LLMs with the ability to handle more complex and precise tasks that go beyond their general knowledge base. \n\nAnother powerful capability of LLMs is In-Context Learning (ICL) [Brown et al., 2020]. ICL enables models to generate more accurate responses by learning from the context provided in the input, without the need for further training. As a training-free approach, ICL is highly flexible and can be applied in various ways, including data generation [Wang et al., 2022, Ye et al., 2022, Gao et al., 2023a, Wang et al., 2024, Pi et al., 2024a, Gao et al., 2023b, Pi et al., 2024b], personalized conversations [Pi et al., 2024c], where the model adapts to user preferences; and task-specific guidance, where context helps refine and improve response accuracy. ICL's versatility makes it a valuable tool for enhancing performance across different applications. The StarCoder parts are based on data from their report [Li et al., 2023].",
            "score": 0.5673906497712718,
            "section_title": "LLMs' Intrinsic Capabilities",
            "char_start_offset": 7087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048675537109375
        },
        {
            "corpus_id": "274131858",
            "title": "I'm Spartacus, No, I'm Spartacus: Measuring and Understanding LLM Identity Confusion",
            "text": "LLM development usually involves two stages: pretraining and fine-tuning. In pretraining, the model is trained on a large, unsupervised dataset to learn general language patterns. Afterward, the model undergoes fine-tuning on a smaller, task-specific dataset to optimize its performance for a particular application: \n\n\u2022 Pretraining: During pretraining, the model is trained on massive, unsupervised datasets like books, articles, and websites to learn language patterns, grammar, word relationships, and factual knowledge. Using unsupervised learning, it predicts missing or next words in sequences, building a generalized understanding of language for diverse tasks. This process is highly computational, requiring distributed training on GPUs or TPUs over weeks or months. \u2022 Fine-tuning: After pretraining, the model undergoes fine-tuning on a smaller, task-specific dataset to adapt its general knowledge to a particular application. This supervised process maps inputs to desired outputs, refining the model's understanding for specific use cases. Fine-tuning is less resource-intensive than pretraining and requires smaller datasets since the model builds on its existing knowledge. The combination of pretraining and fine-tuning creates a flexible and efficient development pipeline for large language models. It allows organizations to leverage stateof-the-art models without the steep costs associated with training from scratch, while still tailoring these models to the unique requirements of specific applications or domains.",
            "score": 0.5670822494847715,
            "section_title": "Pre-Training and Fine-Tuning",
            "char_start_offset": 8336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1229248046875
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "Despite their growing capabilities, generalpurpose large language models (LLMs) still require further customization to achieve optimal performance in specific use cases (e.g. coding) and domains (e.g. medical tasks). While finetuning on specialized data is one approach to adapting LLMs, this process has become increasingly resource-intensive, and in some cases, it is not feasible-especially when access to internal model parameters is restricted, as with models like ChatGPT. Recent research has explored adapting LLMs using smaller models, which can be categorized into two approaches: White-Box Adaptation and Black-Box Adaptation, depending on whether access to the model's internal states is available. \n\nWhite-Box Adaptation typically involves finetuning a small model to adjust the token distributions of frozen LLMs for a specific target domain. For instance, CombLM (Ormazabal et al., 2023) learns a linear function to combine the probability distributions from the large black-box model with those from a smaller domain-specific expert model. IPA (Lu et al., 2023b) introduces a lightweight adapter that tailors a large model toward desired objectives during decoding without requiring fine-tuning. IPA achieves this by optimizing the combined distribution using reinforcement learning. Proxy-tuning (Liu et al., 2024a) fine-tunes a smaller language model, contrasting the probabilities between the tuned model (the expert) and its untuned version (the anti-expert) to guide the larger base model. These approaches only modify the parameters of small domain-specific experts, allowing LLMs to be adapted to specific domain tasks. However, white-box adaptation is not applicable to API-only modeling services, where access to internal model parameters is restricted. \n\nBlack-Box Adaptation involves using a small domain-specific model to guide LLMs toward a target domain by providing textual relevant knowledge. Retrieval Augmented Generation (RAG) can extract query-relevant knowledge from an external document collection or knowledge base, and thus enhance general LLMs by leveraging their incontext learning ability.",
            "score": 0.5670718014390576,
            "section_title": "Domain Adaptation",
            "char_start_offset": 26031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1059,
                    "end": 1076,
                    "matchedPaperCorpusId": "258865629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1429443359375
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "Our study fine-tunes model pre-training checkpoints to understand the dynamics of pre-training and fine-tuning on model performance. \n\nFine-tuning teaches additional task format but leads to forgetting unused abilities. Our results show that fine-tuning guides the model to under-stand the format and complete a given task. As this information diminishes, the model's overall ability improves. Additionally, more pre-training will lead to a model that reacts better to instruction-style prompts, and the ability to interpret such instruction will not be lost when the model is fine-tuned in a different format. However, fine-tuning comes at the expense of other model abilities, such as the capability of solving tasks or domains that are unrelated or weakly related to the fine-tuning task. This insight can be helpful in our understanding of the multitask abilities of LLMs, where certain tasks can introduce conflicts during multi-task training (Mueller et al., 2022). Some datasets can be learned without finetuning. We discover a dichotomy between datasets. Some are learned during model pre-training, while others show no improvements during pre-training. Furthermore, the datasets learned during pretraining do not benefit from fine-tuning. This observation, combined with our study about what is learned during fine-tuning ( \u00a74) suggests that some tasks are presented in a manner that aligns with what the model sees during pre-training, and thus fine-tuning provides no additional information. It may be possible to modify tasks to better align with pre-training and thus make them learnable. \n\nPre-training can improve models in unseen ways. Some datasets are not learned during pre-training but benefit significantly from fine-tuning ( \u00a75). However, these datasets still benefit from additional pre-training, even though those benefits are not revealed without fine-tuning ( \u00a76). The model learns important information to solve the task, even though it cannot express that information without fine-tuning.",
            "score": 0.5669270103269387,
            "section_title": "Discussion",
            "char_start_offset": 22473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 948,
                    "end": 970,
                    "matchedPaperCorpusId": "254591386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15771484375
        },
        {
            "corpus_id": "272689735",
            "title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge",
            "text": "Performance Analysis. As the participating teams navigated through the zero-shot and few-shot testing phases of the LLMs4OL 2024 challenge, notable variations in performance underscored the importance of model adaptability and data-specific adjustments. Few-shot tasks, particularly those involving geographical, biological, and biomedical domains, highlighted the critical need for specialized model tuning and the strategic use of training data to achieve high precision and recall rates. This indicates that achieving optimal performance in real-world ontology challenges requires not only selecting the right LLMs but also fine-tuning them to align with the specific characteristics of the domains and tasks at hand. Additionally, studies show that for Task A, even smaller models like Flan-T5-Small with 80M parameters can perform well when there are fewer types. However, as the number of types increases, larger models, such as those with 7B parameters, tend to perform better. One reason for the popularity of 7B models is that Parameter-Efficient Fine-Tuning (PEFT) [30] fine-tuning requires less memory compared to traditional fine-tuning methods. Many participants also incorporated external knowledge, such as type definitions, synthesis data using LLMs, or general knowledge graphs (KGs) to build answer sets. These strategies have demonstrated a positive impact on fine-tuning performance. Complexity Across Domains and Tasks. The results indicated that certain domains and tasks, such as biomedical term typing and non-taxonomic relation extraction, were more challenging than others. The variation in performance across tasks, particularly in relation to term complexity (e.g., Gene Ontology), highlights the complexity of certain knowledge domains. This still requires specialized approaches. The Phoenixes (on all three tasks) and DSTI (on task A only) teams introduced a formulation based on Retrieval-Augmented Generation (RAG) approaches with success, indicating that combining LLM generation capabilities with retrieval mechanisms can enhance accuracy in OL tasks. This approach is particularly suitable due to the hybrid framework with high adaptability to be extended with different components. Few-Shot and Zero-Shot Testing Phases. While many models performed well in the few-shot phase, the zero-shot testing phase exposed limitations in the generalization capabilities of LLMs.",
            "score": 0.566705983485059,
            "section_title": "Discussion",
            "char_start_offset": 21291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2405
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13232421875
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "One potentially effective strategy for addressing the challenge encountered by LLMs with RAG pipeline in delivering accurate and relevant information to users is through fine-tuning pretrained LLM models.This process involves training existing pre-trained LLMs on domain-specific curated data, thereby enhancing their answering capabilities by adjusting the weights of the model's parameters.Fine-tuning involves optimizing the model's performance for the target task by adjusting and updating its weights based on domain-specific data during training.This process allows the model to learn task-specific information, thereby improving its ability to generate accurate and relevant responses.Fine-tuning is essential for adapting pre-trained LLMs to new tasks or domains without requiring complete retraining from scratch, which results in improved cost efficiency and reduced computational overhead.\n\nThe process of fine-tuning generalised AI models has been a long-standing practice across various tasks and domains for better predictive performance [5,6].Similarly, in the field of generative AI, there has been a trend where numerous studies have introduced specific LLMs tailored to aid in diverse functions in various domains.The projected benefits of fine-tuning include a)lower cost, b) better representation of domain technology, c) improved instruction following and d) improved accuracy.These fine-tuned LLMs have served in specific field such as finance [7,8,9,10], medicine [11,12,13], [14], creative writing [15], climate [16] and law [17,18].These fine-tuned LLMs showed improved capabilities compared to the general LLMs in specific tasks.For instance, In the medical field, Med-PaLM originally proposed by Google, is able to surpass generalised LLMs in answering medical questions with better scientific consensus, comprehension, reasoning capabilities and completeness when evaluated [11].In some evaluations such as information retrieval, it even comes close to human clinicians with 2% differences.In the field of creative writing, a group of researchers developed Weaver, a series of LLMs based on open-source models, demonstrated that their fine-tuned models better across three categories: style, relevance, and creativity [15] with other LLMs.",
            "score": 0.5649074525660871,
            "section_title": "Related Works",
            "char_start_offset": 3789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 900
                },
                {
                    "start": 902,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2018
                },
                {
                    "start": 2018,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "257861968"
                },
                {
                    "start": 1055,
                    "end": 1057,
                    "matchedPaperCorpusId": "49189096"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "269005105",
            "title": "The Case for Developing a Foundation Model for Planning-like Tasks from Scratch",
            "text": "It is observed that LLMs pre-trained on programming languages, as opposed to solely natural language, when fine-tuned with incrementally hard planning problems (Pallagani et al. 2022b), exhibit enhanced capabilities in generating optimal plans for domains in which they have been fine-tuned.Despite these advancements, the fine-tuned models demonstrate a limitation in their ability to generalize to domains not included in the fine-tuning process.\n\nTraining LLMs  For planning, there is a recent tutorial on fine-tuning LLMs (Pallagani et al. 2024a) with planning problems for plan generation.Additionally, there has been progress in adapting the Transformer architecture to A * search dynamics for puzzles like Sokoban (Lehnert et al. 2024).However, a significant gap persists in the availability of a comprehensive training corpus for PL tasks and novel pre-training tasks apt for an FM to learn the execution semantics and the syntactic knowledge intrinsic to a PL task.",
            "score": 0.5645217063839438,
            "section_title": "Foundation and Large Language Models",
            "char_start_offset": 13116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 448
                },
                {
                    "start": 450,
                    "end": 594
                },
                {
                    "start": 594,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 974
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 183,
                    "matchedPaperCorpusId": "263155001"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.276611328125
        },
        {
            "corpus_id": "275758365",
            "title": "Development of Application-Specific Large Language Models to Facilitate Research Ethics Review",
            "text": "LLMs achieve their general-purpose capabilities due to the vast and diverse datasets used in their initial training. However, when high performance is desired within a specific domain rather than for general tasks, the effectiveness of LLMs can be significantly enhanced through a process known as fine-tuning. Finetuning involves further training an already pre-trained LLM on a targeted, specialized dataset relevant to the specific area of interest. This process refines the model's understanding by adjusting its internal weights to align more closely with the patterns, terminology, reasoning, knowledge base, and nuances of the specialized data. As a result, fine-tuned AI models become more adept at generating accurate and contextually relevant outputs within their application-specific domains, typically surpassing the performance of general-purpose models in tasks such as medical diagnosis, legal analysis, and other fieldspecific applications. 26,27 plication-specific fine-tuning helps reduce hallucinations in LLMs by aligning the model's outputs more closely with domain-specific knowledge. This can be further enhanced through RAG: a strategy that grounds the model's outputs in external, reliable data sources (i.e., domain-specific reference information). 28 RAG works by using embeddings, which are numerical representations that encode text into structured vectorized forms, allowing the model to perform efficient computational searches across vast datasets. This structured approach enables the model to locate and retrieve specific, contextually relevant information during text generation, acting as an external memory or knowledge base. By grounding the model's responses in actual data, RAG helps ensure that outputs are more accurate and reliable. Practical examples of RAG include company AIs that access internal documents to provide precise answers to user queries, thereby reducing the risk of generating incorrect or fabricated information. 29  use the term 'application-specific LLMs' to refer to LLMs which have been 1) pre-trained, 2) fine-tuned for a specific application domain (i.e., IRB reviews), 3) enhanced with domain-specific information using RAG, and 4) customized for a specific application through prompt-engineering. The use of applicationspecific LLMs has the potential to address several of the specific concerns raised above with respect to the use of general LLMs in IRB review.",
            "score": 0.5637955674055262,
            "section_title": "IV. Application-Specific IRB LLMs",
            "char_start_offset": 15477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2265
                },
                {
                    "start": 2266,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 962,
                    "matchedPaperCorpusId": "268458631"
                },
                {
                    "start": 1275,
                    "end": 1277,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2208251953125
        },
        {
            "corpus_id": "266818352",
            "title": "Large Language Models for Social Networks: Applications, Challenges, and Solutions",
            "text": "Some existing work shows that LLMs' knowledge and abilities are embedded through the pre-training phase, and one way of injecting knowledge is to continue the pre-training process by adapting LLMs with task-specific data [Mitchell et al., 2023, Ovadia et al., 2023]. \n\nAlthough research on domain-adaptive pre-training has demonstrated the effectiveness of continuous pre-training on adapting domain-specific knowledge, which enhances LLMs' pre-trained capabilities with domain-specific knowledge [Gururangan et al., 2020, Cheng et al., 2023], it is still uncertain if fine-tuned models' generalization ability on the adapted knowledge apply to all domain-specific tasks [Yunxiang et al., 2023, Huang et al., 2023]. Additionally, after the last fine-tuning update, the knowledge of the fine-tuned model is frozen. Without having access to the most up-to-date knowledge, the fine-tuned model along can not answer time-sensitive questions accurately. \n\nTherefore, to leverage the strengths of both approaches, we first develop a knowledge injection fine-tuning framework that utilizes tasks that are similar to those of the pre-training phase to effectively adapt the pre-trained LLM to domainspecific knowledge and guide the model parameter updates towards the direction of better in-context learning; then we augment the fine-tuned model with a retrieval system to ground the conditioning generation. \n\nThe knowledge injection training is conducted through fine-tuning on domain-specific data. In addition to injecting knowledge through a domain-specific Q&A set, we use MCQA as knowledge preference data to encourage the model to better align its knowledge preferences with our local knowledge base. Additionally, we instruction fine-tune with RAG to teach the model to extract factual information from the given context when generating answers. There are two main advantages of instruction fine-tuning with RAG. First, instruction fine-tuning can make LLMs more amenable to our customized prompts and instructions [Chung et al., 2022, Zhang et al., 2023c].",
            "score": 0.5626065872810391,
            "section_title": "Overview of the Knowledge Injection Training Framework",
            "char_start_offset": 22708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 521,
                    "matchedPaperCorpusId": "216080466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2734375
        },
        {
            "corpus_id": "271892126",
            "title": "API-Guided Dataset Synthesis to Finetune Large Code Models",
            "text": "The primary purpose of fine-tuning LCMs is to enhance their performance on code generation tasks and align them with human instructions. Based on whether the fine-tuned models are intended for specific domains, LCM fine-tuning can be categorized into two mainstream scenarios, general and domain-specific, each facing distinct data scarcity, as illustrated in Fig. 1 General Scenario. In this scenario, the fine-tuned model is designed as a universal code generation tool aimed at improving its general generation capabilities. A prime example is the instructed version of CodeLlama [66], which demonstrates superior performance across a wide range of code generation tasks compared to its base model. Researchers and practitioners working on fine-tuning general LCMs often face an abundance of existing datasets, such as OSS-instruct [82] and CodeExercise [12], which are typically derived from various online code repositories and programming forums using LLMs. \n\nWhile the generation methods behind these datasets attempt to synthesize a diverse range of code snippets, they are inherently constrained by the quality of online code samples and the restricted number of programming forums. Consequently, these datasets often suffer from a paradox of quantity over quality, containing a large volume of data with inconsistent quality. For researchers aiming to construct SFT datasets to enhance a model's general performance, an efficient and cost-effective dataset synthesis approach would involve creating new SFT datasets by judiciously selecting from existing datasets. Domain Specific Scenario. In real-world applications, LCMs are often designed to meet the unique requirements of their target audience. For example, Pecan [54] focuses on generating machine learning code, while SQLCoder2 [14] specializes in generating SQL-related code. In such cases, fine-tuning aims to enhance the model's performance in specific domains by leveraging domain-specific datasets and integrating pertinent domain knowledge into the model. \n\nUsers aiming to tune domain LCMs frequently encounter a key challenge: the scarcity of highquality, domain-specific datasets. Curating such datasets demands expertise in the target domain and often involves manual collection, cleaning, and annotation of code samples-a process that is both time-consuming and resource-intensive [13,90].",
            "score": 0.5610496111183744,
            "section_title": "Two Mainstream LCM Fine-tuning Scenarios",
            "char_start_offset": 11760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2029
                },
                {
                    "start": 2032,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "270358041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22607421875
        },
        {
            "corpus_id": "271909748",
            "title": "Optimizing Large Language Model Hyperparameters for Code Generation",
            "text": "Large Language Models (LLMs), such as GPT models, have revolutionized the field of software engineering (SE) by automating various tasks, including code generation [1], software testing [2], requirements and process management [3]. The ability of these models to assist in both complex and routine SE tasks has made them invaluable tools for software developers, promising significant gains in productivity and efficiency [1], [2], [4]. \n\nGiven that code generation is one of the most prevalent use cases of LLMs in software engineering, there have been several efforts to enhance the performance of LLMs for code generation, both in the research domain [5], [6] and by bigger corporations, e.g., the advent of various coding assistants such as Microsoft 365 Copilot 1 and GitHub Copilot2 . The research efforts include, developing custom frameworks for complex code generation [6] and fine-tuning moulded or incorporating external tools to boost the performance of LLMs in code generation [7]. \n\nDespite these advancements, one critical aspect of LLMs that remains under-explored is the systematic study of hyperparameters and their impact on code generation outcomes. Hyperparameters, which are adjustable settings that affect the LLMs' behaviour and performance, can significantly influence the quality of the generated text (or code). Precise finetuning of these hyperparameters, more often than not, leads to better results. For example, in ML algorithms, hyperparameter optimization is considered one of the key steps for training a model and is part of the best practices [8]. Hyperparameter optimization of LLMs has also led to better results in other domains, e.g., text summarization [9] and cost management of text generation [10]. However, no study (to the best of our knowledge) has studied the impact of varying hyperparameters for code generation (or SE in general), and exploring the best configuration that leads to optimum results. Existing SE research has predominantly focused on model architectures and training methodologies, often overlooking the tuning of hyperparameters or studying the impact of hyperparameter settings, if any, on the code generation task.",
            "score": 0.5602176917746677,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "264426647"
                },
                {
                    "start": 432,
                    "end": 435,
                    "matchedPaperCorpusId": "267492075"
                },
                {
                    "start": 1694,
                    "end": 1697,
                    "matchedPaperCorpusId": "236957382"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089599609375
        },
        {
            "corpus_id": "269605249",
            "title": "Prospective Role of Foundation Models in Advancing Autonomous Vehicles",
            "text": "Fine-tuning is the process of further training on a specific task based on an already trained model, to adapt it to the specific data and requirements of the task.Typically, a model that has been pretrained on large-scale data is used as a foundational model, and then it is fine-tuned on a specific task to improve performance.Currently, in the field of LLMs, fine-tuning methods include two main approaches: instruction tuning and alignment tuning [38].\n\nInstruction fine-tuning aims at fine-tuning pre-trained models on a collection of datasets described by instructions [39].Instruction fine-tuning generally includes two phases.First, instances of instruction formatting need to be collected or created.And then these instances are used to fine-tune the model.Instruction fine-tuning allows LLMs to exhibit strong generalization ability on previously unseen tasks.The models obtained after pre-training and fine-tuning can work well in most cases, however, some special cases may occur.In the case of LLM, for example, the trained model may appear to fabricate false information or retain biased information from the corpus.To avoid such problems, the concept of human-aligned fine-tuning was proposed.The goal is to make the model's behaviors conform to human expectations [40].In contrast to instruction fine-tuning, this kind of alignment requires the consideration of completely different standards.\n\nThe GPT family is a typical FM, and its training process also includes pre-training and finetuning.Taking ChatGPT as an example, the pre-training process of ChatGPT uses self-supervised pre-training [41].Given an unsupervised corpus, a standard language modeling approach is used to optimize its Maximum Likelihood Estimate (MLE).GPT uses a multi-layer transformer decoder architecture [42], resulting in a pre-trained model.\n\nThe fine-tuning phase of ChatGPT consists of the following three steps [40].Firstly, supervised fine-tuning (SFT) is performed on the obtained pre-trained model; secondly, comparison data are collected to train the reward model; and thirdly, the SFT model is fine-tuned to maximize the reward leveraging the PPO algorithm [43].",
            "score": 0.5597235884462805,
            "section_title": "Fine-tuning",
            "char_start_offset": 10224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 455
                },
                {
                    "start": 457,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1207
                },
                {
                    "start": 1207,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1408
                },
                {
                    "start": 1410,
                    "end": 1509
                },
                {
                    "start": 1509,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1740
                },
                {
                    "start": 1740,
                    "end": 1835
                },
                {
                    "start": 1837,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1796,
                    "end": 1800,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1908,
                    "end": 1912,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31298828125
        },
        {
            "corpus_id": "273502020",
            "title": "Semi-supervised Fine-tuning for Large Language Models",
            "text": "We conducted tests on seven general or domainspecific datasets (e.g., MMLU, MMLU-Pro and ConvFinQA), covering tasks such as questionanswering, reasoning, and numerical computation. We compared SEMIEVOL with popular methods like retrieval augmented generation, self-evolution and SFT, demonstrating SEMIEVOL's consistent effectiveness across various scenarios. \n\nWe summarize the contributions as follows: \u2022 To the best of our knowledge, we are the first to study a practical problem of semi-supervised fine-tuning (SemiFT), aiming to adapt LLMs into different domains data-efficiently. \n\n\u2022 We introduce SEMIEVOL, a unified framework for knowledge propagation-and-selection that effectively combines labeled and unlabeled data for model evolution. \n\n\u2022 We demonstrate the consistent effectiveness of SEMIEVOL across seven widely used general or domain-specific generative tasks in comparison to extensive baseline models. \n\n2 Challenges for Real-world LLM Fine-tuning 2.1 Supervised Fine-tuning Supervised fine-tuning (SFT) aims to adapt Large Language Models (LLMs) to domain-specific scenarios. Given an LLM M and a dataset \n\n, where T i represents the input task or context and Y i denotes the corresponding expected response. The model minimizes the loss function for each token of the anticipated output during the fine-tuning process F T . \n\nChallenge: Annotation Cost. Despite the effectiveness of supervised fine-tuning, it would require expensive labeling costs to access abundant labeled data. An economic solution is to utilize easily accessible unlabeled data without feedback as a supplement for fine-tuning.",
            "score": 0.5591359758961115,
            "section_title": "Introduction",
            "char_start_offset": 3604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2149658203125
        },
        {
            "corpus_id": "259212340",
            "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
            "text": "Large pretrained generative transformers (Radford et al., 2019;Brown et al., 2020;Raffel et al., 2020) have achieved remarkable success in a wide range of natural language generation tasks, such as story generation, text summarization, and question answering. Such models benefit from the vast amount of training data to learn powerful distributions that contain rich information about the underlying logic of human languages. \n\nOne typical way to adapt such models for specific applications is through fine-tuning. However, there are a few problems associated with fine-tuning: 1) The computational efficiency of fine-tuning is highly dependent on the model's number of parameters. Fine-tuning some extremely large models provided as services rather than open-sourced checkpoints can be too expensive for the majority of the community. 2) Fine-tuning on smaller datasets risks causing the catastrophic forgetting problem. A pretrained model can overfit to an under-represented task domain, while forgetting important knowledge it once learned during the pre-training stage. This is particularly a problem when the model is examined for some reasoning capabilities like compositional generalization and/or commonsense reasoning. \n\nPrompt-tuning (Dong et al., 2022) are recent approaches to addressing the challenges associated with fine-tuning large pretrained models. These approaches involve adding a few tokens, which can be discrete natural language tokens or continuous trainable vectors, to the input of a task. Then instead of modifying the parameters of the model, gradientbased optimization is employed to change the embeddings of the added tokens to maximize the probability of the model producing a specific desired output. This allows for the model to adapt to unseen tasks and domains with minimal data, and can also help to avoid the catastrophic forgetting problem associated with traditional fine-tuning methods. However, prompt-tuning has only limited capacity, so it is usually only effective in specific scenarios. \n\nIn-context learning, another popular approach to control/adapt models without needing to update the model parameters, requires reading the prompt/instructive examples every time the model is executed. On one hand, this causes computational concerns when a significantly long prompt/instructive example is needed for a complex task.",
            "score": 0.5591352758464196,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 63,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 63,
                    "end": 82,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 82,
                    "end": 102,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039337158203125
        },
        {
            "corpus_id": "276903444",
            "title": "A Query Optimization Method Utilizing Large Language Models",
            "text": "Large Language Models (LLMs) are powerful machine learning models based on the transformers structure [36]. As the depth and width of transformers layers increase, the number of parameters in LLMs scales to billions and trillions. In recent years, LLMs such as GPT-4 [30] and DeepSeek-v3 [22] have demonstrated remarkable capability in natural language understanding (NLU) and natural language generation (NLG). Meanwhile, open-source LLMs such as LlaMA [6] and Mistral [15] enable researchers to further fine-tune these models for specific tasks. \n\nPre-trained language models (PLMs) exhibit strong generalization capabilities across various natural language processing (NLP) tasks, including code generation and question answering [3,28,37]. However, in downstream applications, LLMs struggle to strictly follow user instructions and generate invalid or inaccurate outputs [31]. Fine-tuning is an effective strategy for adapting LLMs to specific tasks, ensuring that LLMs are able to follow instructs and generate output in desired format.",
            "score": 0.5587520313181138,
            "section_title": "Large Language Models",
            "char_start_offset": 11365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1041
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 733,
                    "end": 736,
                    "matchedPaperCorpusId": "257019916"
                },
                {
                    "start": 739,
                    "end": 742,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1912841796875
        },
        {
            "corpus_id": "272361715",
            "title": "LAAG-RV: LLM Assisted Assertion Generation for RTL Design Verification",
            "text": "The evolution from Probabilistic or Statistical Language Models (PLMs/SLMs) to Large Language Models (LLMs) represents a major advancement in the capabilities of natural language processing and understanding, as detailed in a variety of studies [4,5] These LLMs have evolved into valuable tools for human assistants, transforming fields such as medicine [6,7] and materials science [8,9]. They assist researchers in managing large datasets, crafting innovative experimental designs, and formulating novel hypotheses. \n\nLLMs are specific Transformer-based Language models where the training of the models uses large volume of data in text format, extracted from different sources such as webbooks, websites and PDFs, to generate the weights parameters. LLMs can be categorized into two groups based on their application: Domain-specific LLMs (ds-LLMs) and General Domain LLMs (gd-LLMs) [10]. ds-LLMs are particularly used for a specific purpose in a specific area of interest. \n\nExamples include FoodGPT [11] and FinGPT [12]. While gd-LLMs are trained on various resources across different domains and origins, enabling their application to a wide variety of queries. Some gd-LLMs are Llama 3 [13] and Gemini 1.5 [14]. The training of LLMs on large datasets incurs significant costs. Therefore, refining pre-trained LLMs using fine-tuning is getting crucial to simplify the training process. Fine-tuning an LLM involves a supervised procedure, aimed at modifying the model parameters according to the specific task requirements [15]. Examples of fine-tuned LLMs include Star Coder [16] and CodeLlama [17].",
            "score": 0.5586414145304974,
            "section_title": "A. LLM Overview",
            "char_start_offset": 5176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 357,
                    "end": 359,
                    "matchedPaperCorpusId": "259303296"
                },
                {
                    "start": 382,
                    "end": 385,
                    "matchedPaperCorpusId": "259138569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06463623046875
        },
        {
            "corpus_id": "267499667",
            "title": "QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model",
            "text": "Adapting large language model (LLM)-based autonomous agents to domain-specific tasks involves techniques such as retrieval-augmented generation (RAG) (Gao et al., 2024) and fine-tuning (Wang et al., 2023d). RAG enhances LLMs' ability to generate more accurate and contextually relevant responses by integrating external knowledge bases, proving particularly beneficial in specialized fields requiring extensive factual information (Sun et al., 2023). Fine-tuning, on the other hand, adjusts pre-trained models on domainspecific datasets, allowing for improved performance in niche tasks by making the models more attuned to the unique lexicon and nuances of the domain. These methodologies facilitate the application of LLM agents across various sectors by significantly boosting their precision and relevance.",
            "score": 0.5572122979539081,
            "section_title": "Adapting LLM Agents to Domain-specific Tasks",
            "char_start_offset": 4786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197021484375
        },
        {
            "corpus_id": "264590382",
            "title": "Can LLMs Grade Short-Answer Reading Comprehension Questions : An Empirical Study with a Novel Dataset",
            "text": "The more recent generation of Large Language Models (LLMs), starting with ChatGPT and subsequently including a wide variety of models such as GPT-4, Claude, Llama, Mistral, Gemini, etc. (commonly referred to as \"generative LLMs\"), were initially trained in a manner like previous generations of LLMs. \n\nHowever, these models utilized significantly larger datasets and dramatically increased the number of parameters, in some cases by more than an order of magnitude (Chowdhery et al., 2023;Stiennon et al., 2022). Crucially, following the pre-training stage, these models underwent \"instruction fine-tuning\" to align their output with human preferences and enhance their ability to follow human instructions (Stiennon et al., 2022). While the specifics of this process vary slightly between models, the common result was greatly enhanced usability, allowing non-technical users to make requests in natural language (i.e., prompting), as well as a remarkable ability to generalize to new tasks, often with minimal exposure to examples (Ouyang et al., 2022). \n\nImportantly, these models can interpret human-written natural language instructions, enabling users to adapt a model to new tasks simply by modifying the prompts they sent to the model, rather than requiring further training or fine-tuning. Generative LLMs have exhibited the ability to perform various linguistic tasks that previously required the use of task-specific, fine-tuned LLMs (Kojima et al., 2022;Wei et al., 2022). Moreover, there is growing evidence that these models can complete evaluation tasks on novel datasets with only minimal prompt engineering (Gilardi et al., 2023;Kuzman et al., 2023). \n\nAlthough the models are not explicitly trained for it, they have demonstrated impressive capabilities in classification tasks and there is mounting evidence that they can be used for certain types of grading tasks (Kortemeyer, 2023). In the specific case of using generative LLMs for grading, instead of fine-tuning custom head for a pre-trained LLM using a task specific dataset, a user can now simply write an explanation of how they wish the model to classify (i.e.",
            "score": 0.5568160830756174,
            "section_title": "Potential of Generative LLMs for ASAG",
            "char_start_offset": 20400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 1625,
                    "end": 1647,
                    "matchedPaperCorpusId": "257766307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "In contrast, our work focuses on larger-set FT instead of fewshot FT.Wang et al. (2023c) identified that format specialization is a critical factor contributing to the diminished ICL abilities in fine-tuned LLMs.In our experiment, we observed similar phenomena, particularly when models fine-tuned on classification tasks were evaluated on generation tasks.However, in other scenarios, the impact of format specialization appeared to be less pronounced.Anil et al. (2022) found that incorporating several in-context examples during FT is helpful for length generalization for text.We further expand this idea and find that this approach is also indeed valuable in preserving or even enhancing the fine-tuned models' generalization ability.",
            "score": 0.5567783937530364,
            "section_title": "Fine-tuning vs. In-Context Learning",
            "char_start_offset": 5998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 739
                }
            ],
            "ref_mentions": [
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "250425737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304931640625
        },
        {
            "corpus_id": "268856523",
            "title": "Developing safe and responsible large language model: can we balance bias reduction and language understanding?",
            "text": "Despite significant recent efforts to mitigate bias in language generation, the complete elimination of bias presents a complex and ongoing challenge. Intensive mitigation strategies can often lead to overfitting, which occurs when a model becomes too specialized to the training data and fails to generalize well to new, unseen data [20]. This overfitting risks the loss of language understanding or knowledge retention because the model may focus too much on reducing bias at the cost of retaining the broader context and nuances of language [19]. \n\nState-of-the-art LLMs such as Llama2/3 [21], the Mistral-series [22], Claude, Gemini, and others [1] are increasingly fine-tuned by their developers and contributors to incorporate safety guardrails. These safety mechanisms often include techniques such as RLHF, context distillation, and curated safe demonstrations. Recent research suggests that leveraging fewer task-specific demonstrations with these safety-optimized models can further mitigate biases in generated outputs [16,23]. However, it is worth considering whether an additional layer of instruction-based fine-tuning, employing targeted prompts and demonstrations, could further enhance the ability of these models to handle nuanced tasks such as unbiased content generation [3]. Thus, integrating both strategies, prompt engineering and fine-tuning, may provide a more comprehensive approach to effectively reducing biases in LLM outputs. In this work, we aim to explore the effectiveness of this additional fine-tuning layer. \n\nThe dataset and approach used in this paper build upon our seminal previous works [24,25], incorporating a larger dataset and refined methodologies. Key differences between the current study and our prior work include fine-tuning a diverse set of models (e.g., Llama series and other instruction-tuned models), conducting experiments on out-of-distribution datasets, and performing both qualitative and quantitative evaluations. The qualitative analysis involved human expert reviews, while the quantitative analysis included beyond-accuracy evaluations through fairness metrics such as language proficiency, language modeling scores, stereotypes, content diversity, style evaluation, and statistical validation. This version also explores dense fine-tuning alongside quantization, focusing on the impact of these methods on accuracy and efficiency.",
            "score": 0.5566428162599496,
            "section_title": "Prompt Given to LLMs",
            "char_start_offset": 2763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1949462890625
        },
        {
            "corpus_id": "276094650",
            "title": "Defending Compute Thresholds Against Legal Loopholes",
            "text": "Large language models ('LLMs') are usually trained in at least two different stages--pre-training and fine-tuning. These stages rely on the same fundamental operations at the technical level, and are only distinguished by the amount of training, the type of data, and the effects. Pre-training is more computationally intensive and endows the model with powerful general-purpose capabilities. Fine-tuning is less demanding in terms of compute and relies on specially crafted data to refine a model's capabilities. 26 ne-tuning is a crucial component of modern LLMs, and most frontier models use some form of it. 27 Fine-tuning is generally used to adapt AI models to specific use-cases (like math 28 ), to align AI models with human preferences, 29 and for instruction tuning--which is regarded as an essential process to elicit model capabilities. 30 As Figure 1 below shows, fine-tuning can have a significant impact on the capabilities of models, particularly in narrow tasks. However, it is generally accepted that fine-tuning in most cases does not significantly increase general ), https://cset.georgetown.edu/publication/techniques-to-make-large-language-models-smaller-an-explainer, at 3 (\"Fine-tuning involves feeding more data to a trained model so it hones the ability to perform certain tasks \u2026 [T]his requires less compute and memory than the original training \u2026\"); Wenxin Jiang et al., An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry, ARXIV (Mar. 5, 2023), https://doi.org/10.48550/arXiv.2303.02552, at 2 (\"Through transfer learning, DNNs can be pre-trained on large datasets and fine-tuned to solve specialized tasks, leveraging a PTM's knowledge of one task to better teach it a similar task.\"). \n\ncapabilities, but merely modulates them. 31 Given its broad usefulness, large diffusion, and limited impact on general capabilities, fine-tuning is often considered as not warranting immediate monitoring or regulatory oversight.",
            "score": 0.5558215599389177,
            "section_title": "Fine-tuning (i) Technique",
            "char_start_offset": 12020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03521728515625
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "Based on the results presented in the previous section, it is evident that the baseline Mixtral and Llama2 models surpassed their fine-tuned iterations across all datasets evaluated.Nevertheless, it is noteworthy that the performance gap was marginal for certain models and datasets; specifically, the accuracy of the BioASQ dataset for both fine-tuned Mixtral and Llama2 models and the accuracy of the Mixtral model on the NQ dataset remained close to their respective baselines.Similarly, fine-tuned Mixtral models performed marginally worse than the baseline model.In this study, we also evaluated fine-tuned models using different sample sizes.Contrary to expectations, fine-tuned models often performed worse than the baseline models.Moreover, in some cases, fine-tuning with increasingly larger sample size deteriorated both the accuracy and completeness of the model's answering capabilities.Within the Qasper dataset, expanding the sample size used for fine-tuning from 200 to 500, and then to 1000 samples, resulted in decreased performance metrics.Specifically, when the sample size increased from 500 to 1000, the accuracy for the Mixtral model experienced a sharp decline from 4.04 to 3.28, and completeness dropped from 3.75 to 2.58.In other instances, there was no significant relationship between the sample size and the performance of the fine-tuned models.\n\nHence, our experiments highlight that fine-tuning does not equate to better accuracy or completeness, and fine-tuning on large domain-specific datasets harms the ability of LLMs to provide accurate and complete answers when integrated within RAG pipeline.Numerous studies have advocated for fine-tuning as a method to enhance performance in domain-specific tasks, suggesting it as a superior approach compared to using generalized large language models (LLMs).However, our findings align with some existing research, such as the studies by [24,25,13], which also observed poorer performance on fine-tuned models despite not incorporating a RAG pipeline.",
            "score": 0.5556883609624377,
            "section_title": "Discussion",
            "char_start_offset": 12798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1246
                },
                {
                    "start": 1246,
                    "end": 1373
                },
                {
                    "start": 1375,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1835
                },
                {
                    "start": 1835,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 1915,
                    "end": 1919,
                    "matchedPaperCorpusId": "264305699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1572265625
        },
        {
            "corpus_id": "272690069",
            "title": "LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning",
            "text": "The performance of pre-trained and fine-tined LLMs in both task types is shown in Table II. The success rate of the unit tree generation task is not evaluated because the unit tree generation tasks are not state-related and can not be evaluated alone. Pre-trained large models outperform small models in complex tasks. As the table shows, GPT-4 excels across all metrics in both task types, demonstrating its superior ability in in-context learning and structural generation, largely attributed to its extensive parameter scale. In contrast, pretrained small models, i.e., GPT-3.5, Llama-13B-chat, and Mistral-7B, though showing perfectly in unit tree generation tasks, perform poorly in one-step generation tasks. Specifically, vanilla GPT-3.5 gets worse results in logical coherency (1/10) and executability (2/10) compared to GPT-4 (both 10/10), and vanilla Llama-13B-chat performs better than vanilla Mistral-7B in executability (5/10 compared to 0/10). This aligns with the results of the unit tree generation tasks, showing the advantages of pre-trained models with larger parameter amounts in the BT generation tasks of robotic assembly. \n\nFine-tuning significantly improves models' performance in structured output generation and in-context learning. In unit tree generation tasks, Llama2-13B-chat and Mistral-7B both show a mostly perfect performance in both logical coherence (both 9/10) and executability (both 10/10) compared to their vanilla models (5/10 and 3/10 for logic coherency and 6/10 and 7/10 for executability, respectively). While in one-step generation tasks, both small models show improvement in executability after being fine-tuned (8/10 and 9/10 compared to 0/10 and 5/10, respectively). This indicates that the models' capabilities of structural generation and context understanding are enhanced through effective finetuning. \n\nEven after fine-tuning, small LLMs perform poorly in complex BT generation tasks that require inference.",
            "score": 0.5554734894306894,
            "section_title": "E. Fine-tuning Results",
            "char_start_offset": 22267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1962
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "In recent years, the development of Large Language Models (LLMs) has emerged as one of the most groundbreaking advancements in Natural Language Processing (NLP). LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2023) have revolutionized the field by leveraging massive textual corpora during pre-training, enabling them to achieve remarkable few-shot performance across a wide range of tasks. The introduction of Supervised Fine-Tuning (SFT) (Ouyang et al., 2022b;Chung et al., 2022) has further propelled the capabilities of LLMs, particularly in enhancing their instruction-following abilities. \n\nInterestingly, even when starting with the same base LLM (Touvron et al., 2023;Bai et al., 2023), Figure 1: Unlike vanilla fine-tuning, which may compromise seed LMs, our proposed self-distillation finetuning (SDFT) approach enhances seed LMs with improved downstream task performance while largely maintaining broad capabilities already learned. \n\nminor variations in the supervised dataset can lead to significant differences in model performance (Zhou et al., 2023;Wang et al., 2023). Consequently, the open-source community has witnessed rapid growth in the diversity of LLM variants, incorporating various SFT datasets and techniques, thereby enhancing their usefulness and accessibility. \n\nHowever, SFT typically prioritizes improving general instruction-following abilities, suggesting that LLMs with SFT might face challenges in specific downstream tasks. As a result, repurposing these models as Seed Language Models (seed LMs) for subsequent fine-tuning tailored to specific downstream tasks has emerged as an appealing approach. While the approach seems promising, our preliminary study reveals the challenge of simultaneously enhancing task-specific performance and preserving general instruction-following abilities through vanilla fine-tuning, primarily due to the issue of catastrophic forgetting. Echoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023).",
            "score": 0.5554373689577234,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 201,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 211,
                    "end": 235,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 461,
                    "end": 483,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2117919921875
        },
        {
            "corpus_id": "266374853",
            "title": "LPR: Large Language Models-Aided Program Reduction",
            "text": "Large Language Models (LLMs) have proved their remarkable capability of undertaking multiple text-processing tasks, including source code-related works. Recent works focus on applying LLMs to facilitate software engineering tasks, or assessing the effectiveness, potential and limitations of LLMs on software development and maintenance. Some research [13,[45][46][47] focus on empirically applying LLMs on automatic program repair (APR). Huang et al. [13] performed an empirical study on improvement brought by model fine-tuning in APR. Xia et al. [46] thoroughly evaluated 9 state-ofthe-art LLMs across multiple datasets and programming languages, and demonstrated that directly applying LLMs has already significantly outperformed all existing APR techniques. Additionally, some works focus on LLMs' performance w.r.t. code completion, generation and fuzzing [6,21,35,53], by leveraging the code analysis and generation ability of LLMs. \n\nSimilar to these studies, our approach LPR leverages LLMs for a software engineering task, i.e., program reduction. LPR harnesses the comprehension and generation capabilities of LLMs to refine the results of program reduction. However, our work distinguishes itself in the nature of the programs processed by LLMs. In related research, programs are typically logical and goal-oriented, often designed to fulfill a specific purpose. In contrast, the programs involved in our program reduction task are random, chaotic, and lack a clear objective. Consequently, our research sheds light on the performance of LLMs when dealing with programs that do not have an easily discernible purpose.",
            "score": 0.5550463622721178,
            "section_title": "LLMs for Software Engineering",
            "char_start_offset": 42200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1629
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "265054960"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "265054960"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "255340904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03802490234375
        },
        {
            "corpus_id": "273229491",
            "title": "Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents",
            "text": "Motivation Fine-tuning a pre-trained language model on a downstream task is a powerful task adaptation method. Fine-tuning involves further training an LLM on a set of labeled examples. It requires access to model weights and more computational resources than ICL or prompting but generally enables stronger downstream task performance. One caveat of fine-tuning is that by changing the model weights we may lose the generality of an instruction-tuned model.",
            "score": 0.5539867950168951,
            "section_title": "G Task adaptation using fine-tuning",
            "char_start_offset": 40404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 458
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4599609375
        },
        {
            "corpus_id": "273403946",
            "title": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch",
            "text": "To assess whether enhancing the LLM's ability to define and solve optimization problems affects its performance on other tasks, we compared the model's performance before and after fine-tuning across 10 general tasks, including math, code, classification, information extraction, open QA, closed QA, text generation, brainstorming, rewriting and summarization. The results indicate that the fine-tuned model showed performance improvements in 6 tasks and declines in 4 tasks, with an average performance increase of 0.3% across all tasks. Importantly, no significant trade-off or seesaw effect is observed. Detailed results are provided in Appendix E. \n\nAbout High-Quality Training Data. High-quality data are crucial for fine-tuning LLMs (Villalobos et al., 2024). Accurate and diverse training data allow the model to improve its task-specific performance. However, optimization problem data described in natural language are relatively scarce. Although we have collected as many optimization problem datasets as possible, which are detailed in Appendix A, these datasets often lack high-quality labels. For instance, in the IndustryOR dataset (Tang et al., 2024), some data have incorrectly labeled optimal solutions, and the NL4Opt dataset (Ramamonjison et al., 2021) does not provide optimal solution annotations but only entity labels. This highlights the scarcity of optimization problems training data. Although prompt engineering for data augmentation is employed in this work, expert labeling remains a time-consuming and labor-intensive process. Efficiently gathering, synthesizing and generating more diverse and well-labeled high-quality data remains an issue that cannot be ignored in this research direction in future. Moreover, for large-scale problems, data are typically stored in specific databases or files rather than extracted from natural language descriptions. The understanding of these data structures remains a new topic that requires further exploration.",
            "score": 0.5532856670011715,
            "section_title": "DISCUSSION",
            "char_start_offset": 27139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 764,
                    "matchedPaperCorpusId": "272330241"
                },
                {
                    "start": 1244,
                    "end": 1271,
                    "matchedPaperCorpusId": "257532906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1622314453125
        },
        {
            "corpus_id": "270068369",
            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
            "text": "Large Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
            "score": 0.5519936876332059,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 2052
                },
                {
                    "start": 2055,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1297607421875
        },
        {
            "corpus_id": "270123745",
            "title": "From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers",
            "text": "The rapid advance of large language models (LLMs) is one of the most exciting recent developments in artificial intelligence.LLMs, pre-trained on large text corpora, have demonstrated impressive generalizable reasoning capabilities and can achieve remarkable performance across a broad set of tasks, ranging from natural language comprehension [34] and generation [3] to mathematical reasoning [10] and programming [6].These models have shown promise in performing real-world tasks in various applications and business solutions.One fundamental pillar of such success lies in the capabilities of these models to perform tasks through generalizable reasoning.\n\nInstruction tuning is a popular approach to unlock the capabilities of these models originally trained on next-token-prediction objectives to understand instructions and perform reasoning to complete tasks.By training models on pairs of instructions and expected outcomes, instruction tuning teaches LLMs to perform specific tasks, thus enabling them to address real-world problems and seamlessly interact with humans.In practice, however, fine-tuning data is limited, and instruction tuning can only focus on a limited set of tasks.Its success is therefore critically dependent on the model's ability to generalize beyond its fine-tuning instructions to unseen tasks not encountered during training.Several factors influence this generalization: the size of the fine-tuning sample, the diversity of the instruction sets, and the quality of the annotations.Yet, there is little systematic research on their relative impact.\n\nOur work aims to fill this gap by proposing two contributions to the study of generalization in instruction tuning.First, we propose a systematic analysis of the impact of instruction diversity by focusing on a simple yet important symbolic task: string rewrites.This basic setting enables us to exercise fine control over the factors that may affect generalization and to demonstrate the importance of instruction diversity.To highlight the broader applicability, we describe this task in arXiv:2405.19787v2[cs.CL] 31 May 2024 terms of a Markov algorithm, a classic Turing-complete model, ensuring rigorous examination of string replacements.Second, we extend this analysis to a real-world application: code generation, and show that fine-tuning on an instruction set that extends beyond coding tasks significantly improves performance.\n\nOur main findings are:\n\n1. Instruction diversity is the main driver of generalization.",
            "score": 0.5519335040879573,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1583
                },
                {
                    "start": 1585,
                    "end": 1700
                },
                {
                    "start": 1700,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2010
                },
                {
                    "start": 2010,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2228
                },
                {
                    "start": 2228,
                    "end": 2422
                },
                {
                    "start": 2424,
                    "end": 2446
                },
                {
                    "start": 2448,
                    "end": 2510
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "260379087",
            "title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation",
            "text": "In particular, our studied instructed LLMs include the open-source ones newly-released in the past four months (i.e., March 1st to July 1st 2023), covering a wide range of models scales (i.e., from 6B to 16B parameters) based on different foundation models (e.g., LLaMA [2], Pythia [33], and GLM [34]) tuned by instructions from both general and code domains. In addition, we further include four small pre-trained models (i.e., , CoText [36], PLBART [37], and CodeT5 [31]) that have been shown to achieve the best performance after being fine-tuned on each studied task [29] and an additional large model without instruction tuning (i.e., CodeGen-6B [38]) as baselines, so as to compare the performance of instructed models with SOTA models. For all the studied instruction-tuned LLMs, we evaluate their capability in three different settings by answering the following research questions. \n\n\u2022 RQ1: How do instruction-tuned LLMs perform on code comprehension and generation tasks in the zeroshot setting? This RQ aims at investigating the zero-shot generalization of instructed LLMs on code-related tasks. \u2022 RQ2: How do instruction-tuned LLMs perform on code comprehension and generation tasks in the fewshot setting? This RQ aims at investigating the in-context learning capability of instruction-tuned LLMs on coderelated tasks when additional demonstration examples are given in the prompt. In addition, we also study the impact of three different shot-selection strategies. \u2022 RQ3: How do instruction-tuned LLMs perform on code comprehension and generation tasks with further finetuning? This RQ aims at exploring the performance of instructed LLMs after they are further fine-tuned on each specific code-related task. \n\nIn addition, we also investigate the memory costs and time costs of using these instructed models for the community's reference via the following research question. \n\n\u2022 RQ4: How are the costs of instruction-tuned LLMs during fine-tuning and inference? \n\nMain findings and implications. Based on our results, we have the following main findings.",
            "score": 0.5517254338873694,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "232185260"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "256662599"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "252668917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0994873046875
        },
        {
            "corpus_id": "272524245",
            "title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models",
            "text": "Large Language Models (LLMs) have made remarkable progress in recent years, demonstrating impressive capabilities across a wide range of tasks, including question-answering (Rajpurkar et al., 2016), summarization (Nallapati et al., 2016), and more (Brown et al., 2020). This advancement has led to the adoption of LLMs in various real-life applications, such as customer support (Xu et al., 2017) and code assistance (Chen et al., 2021). However, adapting these models to specialized domains and tasks often requires adjustments to meet the specific unique needs of model designers. For example, a designer of a customer support agent may want the model to abstain from answering questions that are unanswerable, off-topic, or potentially unsafe. \n\nCurrent approaches to address this challenge include prompt engineering and fine-tuning with task-specific data. Prompt engineering involves guiding the model's behavior through instructions and few-shot in-context examples without altering its weights, allowing it to retain its original capabilities. However, this method may lead to suboptimal performance on the target task (Stiennon et al., 2020). Fine-tuning, on the other hand, can better align the model with the desired behavior (Peters et al., 2019), but may reduce the model's generality. Our work aims to follow the fine-tuning approach while aiming to maintain the model's general capabilities. \n\nSupervised Fine-Tuning (SFT) typically relies on gold responses for training. However, for instruction-tuned models, we observe two key issues: 1) many model responses, while differing from gold responses, are still satisfactory, and 2) the distribution of gold responses often diverges significantly from the model's own response distribution. For example, consider the example in Table 1. The base model, Mistral-7B-Instruct-v0.2, assigns a log probability of \u2212109.9 to the gold answer. When prompted with the same question, the model generated prediction has the same information as gold, but its log probability is \u22122.4. This phenomenon is common in many generation tasks, where different responses can convey the same meaning with very different values of log likelihood.",
            "score": 0.5507989020943775,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 197,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 213,
                    "end": 237,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 248,
                    "end": 268,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 379,
                    "end": 396,
                    "matchedPaperCorpusId": "207246940"
                },
                {
                    "start": 1127,
                    "end": 1150,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1237,
                    "end": 1258,
                    "matchedPaperCorpusId": "76666127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "263605624",
            "title": "Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models",
            "text": "The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.",
            "score": 0.5502884655888325,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "261048999",
            "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
            "text": "Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs under the automated code generation scenario. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL across a diverse set of LLMs. Additionally, we demonstrate the extended capabilities of PEFT, showcasing its ability to learn from two distinct datasets jointly without compromising performance. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios. \n\nCCS Concepts: \u2022 Software and its engineering \u2192 Software creation and management; Software development techniques. \n\nAdditional Key Words and Phrases: code generation, large language models, parameter-efficient fine-tuning, quantization, empirical study",
            "score": 0.5500018294336008,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1937
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Large language models (LLMs) have achieved remarkable performance across various natural language processing tasks (Brown et al., 2020;Ope-nAI, 2023;Touvron et al., 2023).Despite their extensive parameters enabling them to learn rich knowledge during pre-training, LLMs may still generate hallucinated, outdated, or inaccurate content, especially in scenarios requiring long-tail knowledge (Ji et al., 2023;Zhang et al., 2023b).\n\n* Correpsonding author.\n\nTo address this problem, retrieval-augmented generation (RAG) has emerged as a pivotal strategy.By explicitly decoupling knowledge retrieval from the backbone LLMs, such architectures have achieved more accurate and reliable content generation and shown particularly enhanced performance on knowledge-intensive tasks such as open-domain question answering (Lewis et al., 2020b;Petroni et al., 2021;Izacard et al., 2023;Tan et al., 2024;Jin et al., 2024).\n\nExisting efforts in RAG development can be roughly categorized into two groups (as illustrated in Figure 1).The first group leverages the incontext learning capabilities of LLMs by incorporating retrieved information into the input along with appropriate prompts (Shi et al., 2023;Ram et al., 2023).This allows for straightforward application to any off-the-shelf LLM without tuning its parameters.However, its effectiveness largely depends on the human experience in crafting effective prompts and the LLM's ability to interpret these prompts.The second group focuses on training LLMs to enhance their performance in RAG scenarios.This training might involve either endto-end pre-training (Guu et al., 2020;Borgeaud et al., 2022) or fine-tuning (Lin et al., 2023;Wang et al., 2023) for specific tasks.These approaches can often lead to better performance, but they require significant computational resources.Recently, parameter-efficient fine-tuning techniques, such as LoRA (Hu et al., 2022), have been widely studied, significantly reducing training costs.These methods can optimize the LLMs' parameters for RAG, but unfortunately compromise the model's general generation abilities in scenarios without retrieval.",
            "score": 0.548028928265274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 428
                },
                {
                    "start": 430,
                    "end": 453
                },
                {
                    "start": 455,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 909
                },
                {
                    "start": 911,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1210
                },
                {
                    "start": 1210,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1971
                },
                {
                    "start": 1971,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 811,
                    "end": 832,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 832,
                    "end": 853,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 853,
                    "end": 874,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1619,
                    "end": 1641,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1888,
                    "end": 1905,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1337890625
        },
        {
            "corpus_id": "276647790",
            "title": "Bisecting K-Means in RAG for Enhancing Question-Answering Tasks Performance in Telecommunications",
            "text": "Large-scale language models (LLMs) have revolutionized many industries by providing better text understanding and generation capabilities. These models, such as Microsoft's phi-2 [1], are trained on vast datasets with billions of parameters, allowing them to capture linguistic nuances and general knowledge in depth. LLMs applicability ranges from automating administrative tasks to personalized assistance, demonstrating their immense potential for innovation in many areas [2]. \n\nDespite their versatility, LLM models often need more knowledge in highly technical domains [3], such as telecommunications, due to the need for more specific data during their initial training. This can lead to inaccurate or inadequate answers when faced with questions that require deep, specialized understanding. To overcome these limitations, specialization techniques such as fine-tuning and Retrieval-Augmented Generation (RAG) are needed. \n\nFine-tuning involves retraining an existing LLM on a specialized dataset. This process adjusts the model's weights to better align its predictions with domain-specific knowledge and terminology [4]. During fine-tuning, the model is repeatedly exposed to specialized data, allowing it to capture relevant patterns and nuances not present in the initial training. This method improves the model's ability to provide accurate and appropriate answers to queries related to a specific domain. \n\nRAG is another powerful technique used to increase the accuracy of LLMs. RAG combines text generation with information retrieval by integrating external data relevant to the query's context [2]. This is accomplished through two main components: a retrieval model that searches for relevant documents in an external database and a generative model that uses these documents to produce contextualized answers [5]. This approach expands the model's knowledge base and ensures that the answers are more accurate and informed by the latest, domain-specific context. \n\nIn the telecommunications sector, the application of specialized LLMs is particularly relevant [6]. This sector is characterized by its complexity and the need for specific terminology. A specialized LLM in this field can significantly reduce the time required to access crucial information, increase the accuracy of answers, and ensure compliance with international standards. This results in more efficient operations and the ability to quickly meet the technical demands of the sector. \n\nThis paper presents an alternative approach to specializing LLM models for the telecommunications sector.",
            "score": 0.547252161544652,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1982
                },
                {
                    "start": 1985,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2362
                },
                {
                    "start": 2363,
                    "end": 2473
                },
                {
                    "start": 2476,
                    "end": 2581
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "267657844",
            "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning",
            "text": "Large language models (LLMs) experience the Instruction fine-tuning (IFT) stage, which enhances their capability to accomplish tasks and adhere to human instructions. The term IFT is broadly used here to encompass a range of sequence-tosequence fine-tuning applications. T5 (Raffel et al., 2023) was one of the first models to explore this approach, training on multiple supervised text-to-text tasks. Recent studies have delved into the multitask instruction-based fine-tuning of pre-trained LLMs, aiming to improve their innate ability to perform various downstream NLP tasks effectively, such as FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), and UnifiedQA (Khashabi et al., 2020), further expanded the task scope to improve the overall generalization ability of LMs. \n\nFollowing the notable achievements of proprietary LLMs, particularly ChatGPT, there has been a growing focus on utilizing Instruction Fine-Tuning (IFT) to better align LLMs with human intentions, as highlighted in the research by (Brown et al., 2020;Ouyang et al., 2022). A common finding in these studies is that fine-tuning LMs on more diverse data can significantly improve model performance. Taori et al. (2023) chooses another approach, adopting a self-guided method, using Chat-GPT to generate broader training data. Chiang et al. (2023) trains their model using user-shared conversations collected from ShareGPT.com. Xu et al. (2023b) introduced the Evol-Instruct method, which involves evolving existing instruction data to generate more complex and diversified datasets.",
            "score": 0.5459067491445306,
            "section_title": "Instruction Fine-Tuning",
            "char_start_offset": 19118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1554
                }
            ],
            "ref_mentions": [
                {
                    "start": 662,
                    "end": 685,
                    "matchedPaperCorpusId": "218487109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "276317504",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "text": "Recent works such as Tam et al. (2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. Tam et al. (2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding. This observation raises the following open questions: \n\n\u2022 RQ1: Do LLMs truly lose reasoning capabilities under constrained decoding? \u2022 RQ2: How can we leverage the benefits of constrained decoding in reducing syntax errors while preserving the unconstrained reasoning capabilities of LLMs? \n\nKey Challenges: First, we need to formally identify the root cause of the reduction in functional accuracy of endto-end systems when a pre-trained LLM operates under constrained generation. Unlike the empirical observations in (Tam et al., 2024), we seek a formal justification for this reduction that is not limited to specific LLMs used in experiments but extends to any LLM, including more powerful ones developed in the future. \n\nSecond, we must design cost-efficient decoding strategies that address the shortcomings of existing constrained decoding methods while improving functional accuracy. In this work, we do not consider task-specific fine-tuning of LLMs, as fine-tuning for each task is compute-intensive. Unlike constrained decoding, fine-tuning does not guarantee that the LLM output adheres to formal constraints.",
            "score": 0.5455075413492865,
            "section_title": "Introduction",
            "char_start_offset": 1722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1441
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 38,
                    "matchedPaperCorpusId": "271709856"
                },
                {
                    "start": 176,
                    "end": 193,
                    "matchedPaperCorpusId": "271709856"
                },
                {
                    "start": 839,
                    "end": 857,
                    "matchedPaperCorpusId": "271709856"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193115234375
        },
        {
            "corpus_id": "276782038",
            "title": "Effectively Steer LLM To Follow Preference via Building Confident Directions",
            "text": "Large Language Models (LLMs) have the remarkable ability to generate text in response to human prompts, However, since LLMs are pre-trained on corpora, they may fail to meet the specific needs of users during the inference stage, e.g, certain interested topics, language style, etc. The pre-trained model is not inherently designed to accommodate these personalized preferences, leading to outputs that may not align with the user's unique requirements or expectations, illustrated in Fig. 1 To address this gap, many works have been developed to guide LLMs for desired outputs (Rafailov et al., 2024;Wo\u017aniak et al., 2024;Salemi et al., 2023). Some of these works focus on the safety of LLM generation, which reduces the controversial or offensive generation (Koco\u0144 et al., 2021;Kanclerz et al., 2021). Other works try to incorporate personal perspectives by learning the responses or the difference between users (Mi\u0142kowski et al., 2021;Kazienko et al., 2023). In (Koco\u0144 et al., Figure 1: LLM personalized answer. Different users may input similar even same question. However, users have different expectations on the responses generated by LLMs. 2023), the personalized ChatGPT responses are tested on subjective tasks, which demonstrates better user-based predictions. Besides, another line of works incorporate the human feedback by building a reward model to enhance the LLM performance (Ziegler et al., 2019;Ouyang et al., 2022;Sun et al., 2023;Yu et al., 2024). Most of the above works require fine-tuning LLM Peng et al. (2023); Hong et al. (2024), which is extremely costly when the model size is large. \n\nAlthough fine-tuning methods have achieved outstanding performance, it is questionable whether LLM fine-tuning is necessary for all personalization or alignment tasks. In some tasks, the model only needs to capture a \"rough\" alignment direction of users, rather than a precise alignment, such as language style adaptation Konen et al. (2024) or topic focus Turner et al. (2023), can be effectively realized without fine-tuning LLM.",
            "score": 0.5455022517178454,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 578,
                    "end": 601,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 759,
                    "end": 779,
                    "matchedPaperCorpusId": "236299439"
                },
                {
                    "start": 779,
                    "end": 801,
                    "matchedPaperCorpusId": "236459875"
                },
                {
                    "start": 914,
                    "end": 938,
                    "matchedPaperCorpusId": "236768402"
                },
                {
                    "start": 1414,
                    "end": 1434,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1451,
                    "end": 1467,
                    "matchedPaperCorpusId": "265608723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1905517578125
        },
        {
            "corpus_id": "268793648",
            "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
            "text": "It is advantageous to find better answers compared with ToT.In addition, research works such as AutoGPT (Gravitas, 2023) and GPT-Engineer (Osika et al., 2023) utilize commercial LLMs as agent core of their frameworks, enabling the development of comprehensive agent architectures to tackle complex real-world problems.\n\nInstruction Tuning for Language Model.In-struction tuning plays a crucial role in training LLMs.After pre-training with massive unsupervised data, LLMs acquire a substantial amount of knowledge and process language understanding and generation capabilities.Further supervised instruction fine-tuning (Zhang et al., 2023a;Dong et al., 2022) is conducted to align the model with human instructions and generate outputs that better align with human preferences.Instruction tuning mainly focuses on constructing complex and diverse general-purpose tasks to train LLMs to answer questions in a human manner.For example, FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) construct a multi-task instruction tuning dataset using massive publicly available datasets.The fine-tuned model shows strong zero-shot generalizability.In addition to utilizing existing datasets, another common approach is to generate data using commercial LLMs.Self-Instruct (Wang et al., 2022;Peng et al., 2023) leverages GPT-4 to generate a large amount of diverse data, given a few seed tasks.These data are used for fine-tuning open-source LLMs and get significant improvements in various tasks.To enhance the agent capability of LLMs, AgentTuning (Zeng et al., 2023) utilizes commercial LLMs to construct data in specific agent environments containing multi-turn dialogues.",
            "score": 0.5449842118423255,
            "section_title": "Introduction",
            "char_start_offset": 8427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 60,
                    "end": 318
                },
                {
                    "start": 320,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1666
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274169921875
        },
        {
            "corpus_id": "273655206",
            "title": "Vulnerability of LLMs to Vertically Aligned Text Manipulations",
            "text": "While we investigate the vulnerability of LLMs to vertically formatted input, this paper has several limitations: (i) While our findings suggest that few-shot learning does help mitigate this vulnerability, it necessitates the design of demonstrations for each specific task. We do not explore the potential of fine-tuning LLMs to address this issue due to limitations in available datasets and GPU resources. Future research could investigate whether fine-tuning techniques could improve model robustness against vertically formatted input. (ii) We do not assess the impact of vertically formatted input on text generation tasks. Future studies could investigate this aspect to evaluate any potential negative effects of such formatting on the generation performance of LLMs.",
            "score": 0.5447429279358256,
            "section_title": "Limitations",
            "char_start_offset": 24103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 776
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1634521484375
        },
        {
            "corpus_id": "273654339",
            "title": "Combining Domain-Specific Models and LLMs for Automated Disease Phenotyping from Survey Data",
            "text": "Fine-tuning the LLMs on the ORIGINS survey data yielded minimal improvements in performance. As shown in Table 3, the fine-tuned Mixtral 8x 7B model with 4 classes achieved an F1-score of 80% for BERN2 alignment and 88.9% for ground truth (GT) alignment, compared to 19.3% and 20.97%, respectively, for the baseline Mixtral 8x 7B model. However, increasing the number of classes to 10 resulted in a significant decrease in performance for both Mixtral 8x 7B and Llama 3 8B models. The fine-tuned Llama 3 8B model exhibited extremely low F1-scores (6.78% for BERN2 alignment and 6.25% for GT alignment). Incorporating Retrieval Augmented Generation (RAG) with fine-tuning led to marginal improvements for the Mixtral 8x 7B model but did not significantly enhance the performance of the Llama 3 8B model. These results suggest that fine-tuning on the ORIGINS survey data alone may not be sufficient for effectively adapting the LLMs to this specific task. \n\nThe limited improvement observed with fine-tuning, particularly when using a higher number of classes, indicates the need for further exploration of alternative fine-tuning strategies, different hyperparameter settings, a more extensive training set, or the use of larger, more powerful LLMs.",
            "score": 0.5446738087526932,
            "section_title": "Fine-tuned Models",
            "char_start_offset": 17276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "Fine-tuning involves training an existing large language model (LLM) on a smaller, specialized dataset to refine its knowledge for a particular domain or task. This method excels in scenarios where accuracy, tone consistency, and deep understanding of niche contexts are essential. For instance, fine-tuning has been shown to improve a model's performance in specialized content generation, such as technical writing, customer support, and internal knowledge systems. \n\nAdvantages: Fine-tuning embeds domain specific knowledge directly into the model, reducing the dependency on external data sources. It is particularly effective when dealing with stable data or when the model needs to adhere to a specific tone and style. \n\nDrawbacks: Fine-tuning is computationally expensive and often requires substantial resources for initial training. Additionally, it risks overfitting if the dataset is too narrow, making the model less generalizable.",
            "score": 0.543740232979447,
            "section_title": "Fine-Tuning: Domain Expertise and Customization",
            "char_start_offset": 3605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 943
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "275820376",
            "title": "Do Large Language Models Truly Understand Geometric Structures?",
            "text": "Line To further explore how LLMs can possess stronger GRI abilities, we try to fine-tune LLMs with our GeomRel. We split it into training, validation, and test sets in a ratio of 6:2:2, and fine-tune the Llama3-8B-Instruct model. \n\nFigure 5 shows the Zero-Shot results, we observe that the fine-tuned Llama3-8B-Instruct does not show significant improvement over its original version and even performs worse. In the advanced GeomRel, the fine-tuned performance is almost identical to that of the original model, indicating that the fine-tuned model with explicit relationship data does not enhance their geometric reasoning capabilities. However, in the basic GeomRel, the results exhibit considerable variability compared to the original model, with improvements in shape data but substantial declines in angles and lines data. This could be attributed to the relatively small size of the basic datasets, causing the test results to exhibit some degree of randomness after splitting. Our preliminary analysis of this data suggests that LLMs do not gain an increased understanding of geometric structures from the provided data, and fine-tuning models on QA-formed data makes it difficult to enhance their GRI abilities. We analyzed the reasons for the failure, and in Appendix E.5, we provided the fine-tuning results of other models for comparison.",
            "score": 0.5434505134064095,
            "section_title": "SUPERVISED FINE-TUNING ON GeomRel",
            "char_start_offset": 26766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07080078125
        },
        {
            "corpus_id": "266999123",
            "title": "Exploring the Potential of Large Language Models in Self-Adaptive Systems",
            "text": "To lay the foundation for the subsequent discussions, we briefly introduce LLM focusing on core aspects. \n\nArchitectures of LLMs and Pre-training. LLMs refer to transformer-based, large-scale language models that contain billions of parameters and are pre-trained on massive text data [26]. For instance, GPT-3 has 175 billion parameters and uses preprocessed 570GB of text data for training. The architecture of LLMs can be categorized into three main types: (i) encoder-only, in which the encoder encodes the input text into a hidden representation to capture the relationships between words and the overall text context; (ii) encoder-decoder, in which the encoder processes the input into a hidden space, and the decoder translates the abstract representation from hidden space into relevant text expression; and (iii) decoder-only, which is used in models like GPT, gradually generates the output text by sequentially predicting the subsequent tokens. LLMs are pre-trained using datasets that include web pages, books, conversational text, and program code. The data undergoes pre-processing, such as quality filtering, de-duplication, to improve data quality, and privacy data reduction to enhance privacy. \n\nFine-tuning of LLMs. After pre-training, fine-tuning is a technique that uses application-specific customized datasets to provide additional training of the model, thereby improving its performance on the specific task [23]. A representative example is OpenAI's Codex [5] that is based on GPT-3 and specifically fine-tuned for code generation. In addition to traditional fine-tuning, LLMs require two new types of tuning: (i) Instruction tuning to enhance LLM ability to accurately comprehend and execute tasks as directed by (user-given) natural language instructions; and (ii) Alignment tuning to align LLM more closely with human values such as helpfulness and honesty. \n\nUtilizing LLMs. After pre-training and optional fine-tuning, LLMs can be utilized to solve various tasks when given suitable prompts.",
            "score": 0.5431646215668888,
            "section_title": "BACKGROUND: LARGE LANGUAGE MODEL",
            "char_start_offset": 3285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "259601654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1982421875
        },
        {
            "corpus_id": "271038755",
            "title": "A Survey on Natural Language Counterfactual Generation",
            "text": "Although we observe certain issues, they do not offer clear guidance on how to address them. We should cultivate a deeper understanding of LLMs and strategically design prompts to target and resolve specific issues during CFE generation. \n\nSpecific LLMs for CFEs. Modern LLMs are primarily trained on autoregressive tasks and then are fine-tuned with human feedback to enhance their ability to follow instructions. The commonly used tasks for instruction tuning are question answering and semantic understanding. The LLM potential of CFE generation may not be fully exploited during fine-tuning. We believe that fine-tuning LLMs specifically for the CFE generation task could enhance their performance. \n\nLLM hallucinations. LLMs can generate incorrect, misleading, or entirely fabricated content with high confidence, a phenomenon formally known as LLM hallucination. When counterfactual data is used as ground truth to test or improve model robustness, this hallucinated content can inject misleading and incorrect relationships. Therefore, we should implement post-processing and factchecking techniques to filter out hallucinated content by verifying against known facts and identifying internal contradictions. \n\nLower controllability. LLMs may not always effectively determine the degree of change or the specific elements that should be altered in a given sentence, even with clear instructions. Without fine controllability, we cannot achieve the diversity that is possible when instructing human annotators. A nuanced understanding of LLM internal mechanisms is necessary to generate CFEs both flexibly and effectively.",
            "score": 0.5430452430385391,
            "section_title": "Challenges and Future Directions",
            "char_start_offset": 31429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1628
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2242431640625
        },
        {
            "corpus_id": "273026041",
            "title": "From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems",
            "text": "In fine-tuning, the model learns patterns specific to SQL generation, such as understanding database schema and query syntax. This allows it to perform better at generating SQL queries by modifying its internal parameters based on task data, making the model more specialized and accurate for the SQL task. the new parameters, \u03b8 \u2032 , reflect the model's enhanced ability to generalize across different databases and queries. Currently, a number of studies exploring a better finetuning method have been released. We categorize the welldesigned fine-tuning methods in different categories based on their mechanisms, as shown in Table III. \n\n\u2022 Pre-trained Methods: Pre-trained methods form the backbone of fine-tuning for text-to-SQL systems by leveraging the cast general knowledge embedded in LLMs, such as GPT, LLaMA, and T5. these models, trained on diverse textual data, are adapted fro SQL query generation by fine-tuning with task-specific data. The finetuning process enhances their ability to interpret natural language and accurately map it to SQL commands across different domains and database schemas. Examples like SQL-GEN show how pre-trained models are fine-tuned with synthetic data for dialect-specific SQL generation, while systems like RESDSQL fine-tune LLMs on datasets like Spider for complex query handling [48], [54]. \u2022 Fine-Tuning Decomposition: Fine-tuning decomposition methods aim to enhance the performance of LLM on textto-SQL tasks by breaking down the complex process of query generation into smaller and manageable sub-tasks. \n\nThe main idea is to address each sub-task individually, thereby allowing the model to better focus and fine-tune its parameters for specific challenges related to text-to-SQL generation. By decomposing the task into stages like schema linking and query formation, model can be trained to handle these distinct processes more effectively than if it were trained on the entire query generation task all at once [52]. The typical fine-tuning decomposition process involves: \n\n-Task Segmentation: breaking down the text-to-SQL conversion into smaller tasks like schema linking and SQL query generation -Sequential Fine-Tuning: Training the model on these sub-tasks in sequence or in parallel so that each sub-task is learned optimally.",
            "score": 0.5428748268034632,
            "section_title": "1) In-context Learning (ICL):",
            "char_start_offset": 27136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1332,
                    "end": 1336,
                    "matchedPaperCorpusId": "257078956"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2481689453125
        },
        {
            "corpus_id": "273501987",
            "title": "A Systematic Survey on Large Language Models for Algorithm Design",
            "text": "Instead of using a general pre-trained general-purpose LLM, it is worthwhile studying how to train an LLM specifically for automatic algorithm design tasks. The following aspects can be explored in developing domain LLMs: 1) Training domain LLMs are costly and resource-consuming. On the one hand, advanced light fine-tuning techniques (e.g., LoRA [61]) could be used for efficient domain adaptation. On the other hand, the size of algorithm LLMs for a specific application can be reduced with the help of domain data and knowledge. 2) Generating and collecting domain data for algorithm design poses challenges. Unlike general code generation or linguistic process tasks, there is no large and formatted data specific for algorithm design. \n\nSome Github repositories on algorithms might be helpful, but the resources should be scanned and cleaned for algorithm design. Lehman et al. [79] provide an example of generation domain data for LLMs, where the samples generated during searching are used as the training dataset to fine-tune a pre-trained model resulting in better performance on the target problem. 3) Instead of learning a text and code generation model, how to learn the algorithm development ideas and the algorithmic reasoning ability remain unexplored [116,157].",
            "score": 0.542770551309053,
            "section_title": "Domain LLM for Algorithm Design",
            "char_start_offset": 63187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1278
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040924072265625
        },
        {
            "corpus_id": "258331833",
            "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
            "text": "LLMs are better suited to handle real-world scenarios compared to fine-tuned models. However, evaluating the effectiveness of models in the real world is still an open problem. \n\nHandling such real-world scenarios requires coping with ambiguity, understanding context, and handling noisy input. Compared to fine-tuned models, LLMs are better equipped for this because they have been trained on diverse data sets that encompass various writing styles, languages, and domains. Additionally, LLMs demonstrate a strong ability to generate open-domain responses, making them well-suited for these scenarios. Fine-tuned models, on the other hand, are often tailored to specific, well-defined tasks and may struggle to adapt to new or unexpected user requests. They heavily rely on clear objectives and well-formed training data that specify the types of instructions the models should learn to follow. Fine-tuned models may struggle with noisy input due to their narrower focus on specific distributions and structured data. An additional system is often required as an assistant for fine-tuned models to process unstructured context, determine possible intents, and refine model responses accordingly. \n\nAdditionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112]. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91]. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage. \n\nOne of the main issues when it comes to real-world scenarios is how to evaluate whether the model is good or not. \n\nWithout any formalized tasks or metrics, the evaluation of model effectiveness can only rely on feedback from human labelers. Considering the complexity and cost of human evaluation, there's no massive and systematic comparison between fine-tuned models and LLMs yet.",
            "score": 0.5426213085997083,
            "section_title": "Remark 7",
            "char_start_offset": 40158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2122
                },
                {
                    "start": 2125,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2392
                }
            ],
            "ref_mentions": [
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07574462890625
        },
        {
            "corpus_id": "259075521",
            "title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap",
            "text": "An LLM, like any language model, predicts the conditional probability of a token-which might be a character, word, or other string-given its preceding context and, in the case of bidirectional models, its surrounding context [17,156]. Present-day LLMs are based on modern neural network self-attention architectures like the transformer [187] with hundreds of billions or even more than a trillion parameters [56]. While earlier models were trained on datasets of moderate size, LLMs are trained on datasets of massive scale, with hundreds of billions or even more than a trillion tokens [23,75], requiring many orders of magnitude more compute time. This makes LLMs vastly more sophisticated and expressive than their predecessors. \n\nWhile a basic pre-trained LLM model can be viewed as a \"general-purpose\" next-word predictor, LLMs can be adapted to exhibit or suppress specific behaviors or to perform better on specific tasks like text summarization, question answering, or code generation. One common approach is fine-tuning, in which the model's parameters are updated based on additional, specialized data [e.g., 42,79,111,155]. A popular technique for fine-tuning is reinforcement learning from human feedback (RLHF) in which human preferences are used as a reward signal [36,152]. Another approach is prompting or prompt engineering, in which natural-language prompts-often containing examples of tasks (for few-shot prompting/in-context learning) or demonstrations of reasoning (for chain-of-thought prompting)are provided to the model to alter its behavior without making any changes to the model's internal parameters [e.g., 26,126,169,194]. The adapted model can be incorporated into applications such as chatbots, search engines, or productivity tools. Models can also be augmented with the ability to call on external models, tools, or plugins [141], for example, querying an information retrieval system to ground their output or controlling and receiving feedback from a physical robot.",
            "score": 0.5418286675510036,
            "section_title": "Background on LLMs",
            "char_start_offset": 7552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 337,
                    "end": 342,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1120,
                    "end": 1123,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1123,
                    "end": 1126,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "202750126"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1284,
                    "end": 1288,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1637,
                    "end": 1640,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 1648,
                    "end": 1652,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159912109375
        },
        {
            "corpus_id": "269282528",
            "title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL",
            "text": "Chain-of-Thought prompting [Wei et al., 2022] enables LLMs to answer more complex questions accurately by using additional output tokens to discuss intermediate steps and mimic human reasoning.Self-reflection [Shinn et al., 2023] calls an external service to check the LLM's answer, and then has the LLM discuss the feedback from the external service and update its answer.Many researchers have helped to develop autonomous agents [Wang et al., 2024b] which can call external functions or additional LLMs as sub-agents and self-determine when they have successfully answered a question or completed a task.In a prescribed domain like text-to-SQL, a predetermined plan can place multiple calls to LLMs and other functions to break the problem into easier components, without granting full autonomy to the LLM to decide when the task is complete.\n\nPredating these techniques, fine tuning [Devlin et al., 2019] allows a general-purpose LLM to be specialized for a specific task given as few as hundreds of labeled training examples.Fine tuning requires many more examples than few-shot learning, which can bring material improvements in performance with even a single example, but many fewer examples than LLM pre-training, which relies on large web corpora.Although fine tuning comes with an upfront cost, it can save money at inference time in two ways.First, a fine-tuned small model may exceed the performance of a larger model that has not been fine tuned.Second, fine tuning can replace both a system prompt and few shot examples, so new questions can be submitted with a more concise prompt.If the the number of times the model will be used for inference is high enough, the total cost of fine tuning and inference together can be lower than using few-shot prompting.\n\nIn this paper, we introduce two new text-to-SQL methods.The first, Dubo-SQL v1, is a low-cost, token-efficient fine tuning method that set a new record on the BIRD-SQL benchmark at the time of its submission and at lower cost than its nearest competitors.The second, Dubo-SQL v2, uses a novel retrieval-augmented generation pipeline to exceed the performance of Dubo-SQL v1 when used with GPT-4 Turbo.",
            "score": 0.5417795336603,
            "section_title": "Introduction",
            "char_start_offset": 2014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 606
                },
                {
                    "start": 606,
                    "end": 844
                },
                {
                    "start": 846,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1595
                },
                {
                    "start": 1595,
                    "end": 1771
                },
                {
                    "start": 1773,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 2028
                },
                {
                    "start": 2028,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 45,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 209,
                    "end": 229,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 431,
                    "end": 451,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1915283203125
        },
        {
            "corpus_id": "270711287",
            "title": "FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model",
            "text": "The recent advancements in large language models (LLMs) have demonstrated incredible performance in various tasks, such as question-answering and problem-solving.This success owes to the pretraining on large datasets, covering a wide range of linguistic patterns and general knowledge.However, in specific domains such as legal advice [7,23] and medical diagnosis [27,32,39], LLMs may not provide professional responses because the terminology * Work was done while Feijie Wu was an intern at Alibaba Group.and context significantly differ from general language use.To address this limitation and enable the generation of domain-specific content, it becomes imperative to fine-tune LLMs with relevant data.This fine-tuning process allows the models to learn from the specific instances and nuances of the target application, ensuring their capability within specialized fields.The quality and quantity of the task-specific data are directly related to the performance of the fine-tuned model on downstream tasks: large and well-labeled data can significantly improve the model, while small and irrelevant data can only benefit the model marginally.However, there are many cases where task-specific data are possessed by multiple data parties, while each of them may have a limited number of samples that can be used to fine-tune LLMs.For example, a hospital in a rural area may only have a limited number of lung cancer cases recorded in its own system; if an LLM is only fine-tuned on one set of those cases, it may not obtain comprehensive knowledge and easily be overfitted.\n\nTo incorporate all the distributed data in the fine-tuning of LLMs, one may consider the batch fine-tuning as follows.If we demand all the data owners to share their data with the LLM server, then LLM fine-tuning could be conducted at the server side.For example, some LLM owners offer fine-tuning APIs as services, but the users must pack their data as files and upload them to use a black-box fine-tuning [24].Apparently, this setup is not applicable to users who have privacy concerns.Especially, some businesses are subject to data privacy regulations [2,8], which makes it challenging to share local data with LLM server.",
            "score": 0.5412707295041441,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1148
                },
                {
                    "start": 1148,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1577
                },
                {
                    "start": 1579,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "259138525"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "259947046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130615234375
        },
        {
            "corpus_id": "270123761",
            "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
            "text": "Consequently, the data distribution that LLMs adapt to during fine-tuning often differs significantly from that required for specific tasks. This discrepancy leads to inferior performance on specialized tasks and knowledge forgetting due to disruptions in the parametric knowledge stored in LLMs [14]. Figure 1 also shows that with more out-ofdistribution (OOD) tuning data, the vanilla fine-tune method brings catastrophic forgetting problems, degrading models' performance on downstream tasks. The scarcity of natural data and the suboptimal quality of synthetic data present substantial challenges to effectively adapting LLMs for specialized tasks. In essence, the dependency on in-domain distribution fine-tuning corpora hampers the broader deployment of LLMs. \n\nTo address this, we propose avoiding such data dependency by leveraging the intrinsic properties of fine-tuning and developing an inference-time method that does not rely on high-quality in-distribution data. We first conduct an in-depth investigation of the internal Transformer architecture. We find that during fine-tuning, LLMs enhance their instruction-following ability, primarily controlled by the self-attention module [75]. Conversely, parameterized knowledge is encoded by the key-value intrinsic of the feed-forward network (FFN) module [18,40] during pretraining [56]. Fine-tuning primarily elicits this pretrained knowledge [46,59,71], which remains relatively fixed [86]. This insight prompts us to discard the FFN updates during fine-tuning, as only a small portion positively contributes to downstream performance, while most disrupt the knowledge when fine-tuned on task-mismatched data. \n\nA naive approach is to fine-tune only the attention parameters, but this fails to generalize to OOD data due to insufficient exploration of non-linearity. To ensure sufficient learning of non-linearity, we introduce additional FFN parameters during fine-tuning but retain only the beneficial self-attention updates. This strategy, named Training-All-Inferring-only-Attention (TAIA), achieves both OOD generalization and sufficient optimization space. The comparisons between the proposed method and the vanilla fine-tuning method are shown in Figure 2 Feed Forward Network",
            "score": 0.5400339402218601,
            "section_title": "Introduction",
            "char_start_offset": 1883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 1320,
                    "end": 1323,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16015625
        },
        {
            "corpus_id": "258715090",
            "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
            "text": "Instruction tuning. Although current instruction-following LLMs have demonstrated strong performance in deriving task-relevant answers relying on instruction [Si et al., 2022, Wei et al., 2022a, Sanh et al., 2021, Zhou et al., 2022], when facing task-specific issues, fine-tuning is still the preferred option for achieving better results [Lou et al., 2023, Zhao et al., 2023]. Unlike only using instructions without training to guide the model output, instruction tuning is an approach to fine-tune LLMs with data fused with instruction to achieve task-specific effects [Sanh et al., 2021, Longpre et al., 2023, Puri et al., 2022, Jang et al., 2023, Ivison et al., 2022a,b]. Most works focus on the generalization capability brought by instruction tuning, which helps the model to have cross-task generalization by fine-tuning on multi-task instruction data [Sanh et al., 2021, Longpre et al., 2023]. In addition, instruction tuning can also help the model improve performance on a specific task [Wang et al., 2022, Jang et al., 2023, Ivison et al., 2022a,b], and research has shown that on single-task fine-tuning, instruction tuning can accelerate model convergence. \n\nLow training data. While the effectiveness of LLMs is superb, the training costs associated with huge parametric models have simultaneously limited the popularity and adoption of LLMs [Hoffmann et al., 2022, Zhao et al., 2023]. Many works try to explore the possible cost reduction in LLMs from the perspective of data. Jang et al. [2023] report an expert LLM fine-tuned on one single task can outperform a multi-task tuned LLM. Self-instruct [Wang et al., 2022] starts from the instruction generation, which means a model generates its own instructions or prompts and learns to follow them.",
            "score": 0.5398509664940401,
            "section_title": "Related Work",
            "char_start_offset": 7967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322998046875
        },
        {
            "corpus_id": "258832528",
            "title": "Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance",
            "text": "Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered significant attention due to their exceptional capabilities in handling a diverse range of tasks. Recent studies demonstrate that open-sourced smaller foundational models, such as 7B-size LLaMA, can also display remarkable proficiency in tackling diverse tasks when fine-tuned using instruction-driven data. In this work, we investigate a practical problem setting where the primary focus is on one or a few particular tasks rather than general-purpose instruction following, and explore whether LLMs can be beneficial and further improved for such targeted scenarios. We choose the writing-assistant scenario as the testbed, which includes seven writing tasks. We collect training data for these tasks, reframe them in an instruction-following format, and subsequently refine the LLM, specifically LLaMA, via instruction tuning. Experimental results show that fine-tuning LLaMA on writing instruction data significantly improves its ability on writing tasks. We also conduct more experiments and analyses to offer insights for future work on effectively fine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion regarding the necessity of employing LLMs for only one targeted task, taking into account the efforts required for tuning and the resources consumed during deployment.",
            "score": 0.5398246959490862,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318359375
        },
        {
            "corpus_id": "259924919",
            "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
            "text": "Due to the limitations of LLMs in their earlier stages, a majority of the earlier published studies adopt this pre-training or fine-tuning schema. Moreover, in some recent studies, this schema continues to be employed to increase the LLMs' familiarity with domain knowledge. Alagarsamy et al. [29] first pre-trained the LLM with the focal method and asserted statements to enable the LLM to have a stronger foundation knowledge of assertions, then fine-tuned the LLM for the test case generation task where the objective is to learn the relationship between the focal method and the corresponding test case. Tufano et al. [26] utilized a similar schema by pre-training the LLM on a large unsupervised Java corpus, and supervised fine-tuning a downstream translation task for generating unit tests. Hashtroudi et al. [32] leveraged the existing developerwritten tests for each project to generate a project-specific dataset for domain adaptation when fine-tuning the LLM, which can facilitate generating human-readable unit tests. Rao et al. [35] trained a GPT-style language model by utilizing a pre-training signal that explicitly considers the mapping between code and test files. Steenhoek et al. [42] utilizes reinforcement learning to optimize models by providing rewards based on static quality metrics that can be automatically computed for the generated unit test cases. \n\nDesigning effective prompts for unit test case generation. The advancement of LLMs has allowed them to excel at targeted tasks without pre-training or fine-tuning. Therefore most later studies typically focus on how to design the prompt, to make the LLM better at understanding the context and nuances of this task. Xie et al. [36] generated unit test cases by parsing the project, extracting essential information, and creating an adaptive focal context that includes a focal method and its dependencies within the predefined maximum prompt token limit of the LLM, and incorporating these context into a prompt to query the LLM. Dakhel et al. [38] introduced MuTAP for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. They augment prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs.",
            "score": 0.5388512056696555,
            "section_title": "Pre-training or fine-tuning LLMs for unit test case generation.",
            "char_start_offset": 25689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "263608651"
                },
                {
                    "start": 2025,
                    "end": 2029,
                    "matchedPaperCorpusId": "261394507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15673828125
        },
        {
            "corpus_id": "264439566",
            "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks",
            "text": "LLM pre-training. Traditional language models typically focus on a single task during the training stage, while LLMs take a more comprehensive approach by incorporating multiple tasks simultaneously during pre-training. This approach allows the model to learn a diverse set of linguistic features and capabilities, leading to improved performance across various downstream tasks. By leveraging a combination of tasks, including language modeling, text classification, and question answering, LLM pre-training aims to enhance the model's understanding of language semantics, context, and structure, ultimately enabling more robust and versatile language understanding and generation capabilities. LLM fine-tuning. In specialized domains like biomedicine and finance, LLMs often require fine-tuning on training data to acquire domain-specific knowledge and expressive capabilities, enabling them to effectively address domain-specific queries [10,17,31,38,51]. Recognizing this demand, the fine-tuning functionality of LLMs has gained increasing adoption. A significant breakthrough occurred in August 2023 when OpenAI introduced the fine-tuning interface for GPT-3.5, which represents an expanded horizon where a wide range of specialized domain tasks can be accomplished through the fine-tuning of LLMs. RLHF. Reinforcement learning from human feedback (RLHF) represents a groundbreaking approach in the training methodology of LLMs. The language modeling objective of LLMs -predicting the next token -is different from the objective \"following instructions and being helpful, truthful, and harmless\" [31]. In this case, the language modeling objective is regarded as misaligned. Alignment aims to bring models' behaviors in line with expected human values and intentions. Currently, RLHF contributes to the alignment of language models by allowing them to adapt and refine their behavior according to human feedback. This feedback loop enables the model to refine its language generation abilities, adjusting its responses based on the quality and relevance of the generated text as evaluated by humans. This bridges the gap between machine-generated text and human perception. In the privacy concern, RLHF ensures models avoid responding to privacy-invading queries, reducing the risk of privacy extraction from these models. Catastrophic forgetting in LLMs. Catastrophic forgetting (CF) is a notable challenge in the field of machine learning [19], particularly within the realm of LLMs [28].",
            "score": 0.5388179212285105,
            "section_title": "PRELIMINARIES 2.1 Large Language Models",
            "char_start_offset": 9295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2327
                },
                {
                    "start": 2328,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2495
                }
            ],
            "ref_mentions": [
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.139404296875
        },
        {
            "corpus_id": "259924436",
            "title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models",
            "text": "To improve the efficacy of language models, researchers have dedicated to scaling up language models [10,82], by increasing the number of model parameters, expanding the size of training datasets, and enhancing computational resources. These lead to the emergence of large language models, namely transformer-based language models with billions of parameters and pre-trained with large amounts of text data. LLMs usually have the same neural network understanding, generation, and reasoning compared to smaller PLMs. They exhibit significantly enhanced abilities in handling complex language tasks. Moreover, researchers have found that LLMs have the emergent ability (the ability not presented in small PLMs but LLMs [118]) of in-context learning, namely the capability to perform unseen tasks without any specific training data or with only a few training samples, by following natural language instructions. \n\nBased on representative LLMs such as GPT-3 [96], recent efforts have been explored in the following directions to improve the capability of LLMs: \n\nManuscript submitted to ACM \n\n\u2022 Instruction Tuning To enhance the generalization and in-context learning capabilities of LLMs, researchers such as T0 [100] and FLAN [117], further explored fine-tuning LLMs with diverse tasks expressed by natural language instruction, namely instruction tuning. The instruction tuning, makes LLMs to better understand and respond to instructions, enabling them to generalize more effectively to new tasks. During the instruction tuning process, researchers [15] have found certain factors including scaling the number of tasks, and model size and using chain-of-thought tuning data, that contribute to improving LLMs' generalization ability and reasoning ability on complex tasks. \u2022 Human Alignment In addition to the capacity to follow instructions, it's crucial to ensure that large language models (LLMs) are guided by human preferences, avoiding generating unfaithful and toxic information [106]. \n\nTo achieve this, studies such as InstructGPT [82], have explored fine-tuning LLMs with reinforcement learning from human feedback (RLHF) [14], where the human preferences are used as the reward signal to fine-tune LLMs. RLHF has demonstrated effectiveness in improving the truthfulness and toxicity of LLMs.",
            "score": 0.5385034590277971,
            "section_title": "Large Language Models",
            "char_start_offset": 19293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1211,
                    "end": 1216,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1226,
                    "end": 1231,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1988,
                    "end": 1993,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 2042,
                    "end": 2046,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202880859375
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts. \n\nThe works (Lee et al., 2019;Wiese et al., 2017) focus on using techniques that mitigate forgetting by regularizing outputs or parameters to remain close to the pre-trained model, to actually improve fine-tuning performance and stability. This can be reasoned to entail that forgetting while fine-tuning degrades the ability of the model to adapt and be fine-tuned, and forgetting occurs during the process of fine-tuning on a single task. \n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model.",
            "score": 0.538303648799304,
            "section_title": "Catastrophic Forgetting",
            "char_start_offset": 5604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.301513671875
        },
        {
            "corpus_id": "268041519",
            "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
            "text": "No finetuning? There are some larger models that fine-tune on various tabular tasks, some including QA and FV tasks, mentioned in Section 2.1 under embeddings-based serialization. Li et al. (2023d) found that fine-tuning always helps to improve performance across various tabular tasks. In zeroshot settings, the improvement is the most significant. For Ye et al. (2023b), they obtained higher scores on TabFact when using their framework with the PASTA (Gu et al., 2022) model (score 93.00%) as compared to the GPT-3 Codex (code-davinci-002) (scored 85.60%). PASTA was pre-trained on a synthesized corpus of 1.2 million items from WikiTables for six types of sentence-table cloze tasks. This suggests there remains some benefit in using LMs fine-tuned on tabular tasks. \n\nHowever, compared to methodologies working on Prediction and Generation tasks, fine-tuning is not as common. This might be due to the general ability of LLMs (E.g. GPT3.5, GPT4) to perform QA tasks off-the-shelf. For SQL generation on Spider, DIN-SQL (Pourreza & Rafiei, 2023) and DAIL-SQL (Gao et al., 2024) are inference-based techniques using GPT4, and surpassed previous fine-tuned smaller models. Interestingly, in the paper by Gao et al. (2024), the authors fine-tuned a Llama 2 13B model on the Text2SQL tasks. However, this model did not beat the GPT4 model that was not fine-tuned. Instead, many papers working on using LLMs for table understanding tasks focus on tweaking aspects across serialization, prompt engineering, search and retrieval, and end-to-end pipelines (user interfaces), which we describe further in the next section.",
            "score": 0.5378719232486066,
            "section_title": "Finetuning or",
            "char_start_offset": 70458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 371,
                    "matchedPaperCorpusId": "256416408"
                },
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "253384265"
                },
                {
                    "start": 1024,
                    "end": 1049,
                    "matchedPaperCorpusId": "258291425"
                },
                {
                    "start": 1063,
                    "end": 1081,
                    "matchedPaperCorpusId": "261276437"
                },
                {
                    "start": 1206,
                    "end": 1223,
                    "matchedPaperCorpusId": "261276437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3427734375
        },
        {
            "corpus_id": "265609340",
            "title": "Hyperparameter Optimization for Large Language Model Instruction-Tuning",
            "text": "Large-scale Language Models (LLMs) have shown exceptional ability in language understanding and generation (Zhang et al. 2022;Raffel et al. 2020;Radford et al. 2019;Brown et al. 2020). State-of-the-art models like Chat-GPT (OpenAI 2023a) and GPT-4 (OpenAI 2023b) have garnered a great deal of interest from the academic and industrial communities. One of the main challenges of LLMs is how to control their behavior and make them follow specific instructions given by users (Ouyang et al. 2022). Additional fine-tuning of LLMs on a dataset of instructions is called Instruction-Tuning; this technique has become ubiquitous due to its efficiency (Zhang et al. 2023). However, tuning large models demands a large amount of computer power. To overcome this, a common practice is to use Parameter Efficient Fine Tuning (PEFT) methods, which modify a limited selection of parameters in a pre-trained LLM while leaving the rest unchanged (Mangrulkar et al. 2022). Such methods are quite sensitive to the choice of hyperparameters (Hu et al. 2021b;Valipour et al. 2022). In this work we investigate how hyperparameter optimization can better the instruct-tuning results. \n\nHyperparameters selection by a human in order to tune a model is a tedious task but it can significantly improve model performance. Bergstra et al. 2011 suggest that hyperparameters optimization (HPO) forms the outer loop of a learning process. Applying an algorithmic approach to automate the process in finding better hyperparameters should also bring some efficiency. A grid search algorithm is a systematic but inefficient approach that tries a finite number of hyperparameters combinations. A blackbox optimization (BBO) algorithm should be a better choice for solving HPO efficiently within a fixed computational budget. \n\nIn this work we investigated how two BBO solvers implementing different types of algorithms, namely MADS (a direct search algorithm implemented in NOMAD) and TPE (a Bayesian model-based optimization algorithm implemented in NNI) behave when used to solve HPO for the Instructiontuning of a specific LLM.",
            "score": 0.5376648293599124,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1792
                },
                {
                    "start": 1795,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 145,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 145,
                    "end": 165,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1024,
                    "end": 1041,
                    "matchedPaperCorpusId": "6944647"
                },
                {
                    "start": 1298,
                    "end": 1318,
                    "matchedPaperCorpusId": "233640174"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2120361328125
        },
        {
            "corpus_id": "272988100",
            "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
            "text": "Synthetic Data for Fine-Tuning. Fine-tuning on synthetic and hybrid data has proven successful across a wide range of tasks (Liu et al., 2024). In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;Li et al., 2024;Setlur et al., 2024;Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2024;Yang et al., 2024). Similar approaches have been applied in instruction-following, where LLMs are effectively trained on diverse synthetic instructions with minimal to no human supervision (Xu et al., 2024;Wang et al., 2022;Xu et al., 2023). However, most works focus on a single domain and do not explore how these techniques perform under varying data constraints and strategies, leaving uncertainty in adapting them to new applications. We aim to compare and extend these methods in cost constrained settings, while investigating which factors in synthetic data generation remain impactful across multiple tasks and data budgets. \n\nEfficient Synthetic Data Generation. Although synthetic data is significantly cheaper than real data, its scalability encourages researchers to generate it at extremely large scales, making generation costs a substantial component of fine-tuning expert models (Li et al., 2024). Other works focus on aggressively filtering synthetic datasets for diversity and correctness with custom tricks for each domain (Long et al., 2024). Current research on training LLMs with synthetic data emphasizes scalability and performance, but to make these methods more applicable to more tasks, we also need to disentangle and understand cost-efficiency across different scales. Bansal et al. (2024) explores and optimizes the choice of LLMs to sample synthetic data for overall cost reductions. Our work addresses this challenge from a new perspective by offering a general framework that guides model trainers in defining and refining their synthetic data generation strategies to maximize cost-efficiency within budget constraints.",
            "score": 0.5375792302723224,
            "section_title": "Related Work",
            "char_start_offset": 3318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 374,
                    "matchedPaperCorpusId": "269930153"
                },
                {
                    "start": 394,
                    "end": 411,
                    "matchedPaperCorpusId": "261030818"
                },
                {
                    "start": 596,
                    "end": 614,
                    "matchedPaperCorpusId": "270358041"
                },
                {
                    "start": 1509,
                    "end": 1526,
                    "matchedPaperCorpusId": "269930153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08990478515625
        },
        {
            "corpus_id": "270440709",
            "title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification",
            "text": "Our key finding is that fine-tuning smaller LLMs with dedicated training data is consistently superior to zero-shot prompting larger models such as BART, ChatGPT, and Claude Opus.While zero-shot results are decent for common tasks such as sentiment analysis, fine-tuned LLMs always surpass zero-shot performance as training set size increases.This tendency is more pronounced for less standard tasks such as stance classification or emotion detection.\n\nAn interesting question is whether next-generation generative AI models will catch up with, or even surpass, fine-tuned LLMs in performing specialized tasks.In the following, we point to some areas where we expect progress regarding both fine-tuning and prompt-based approaches, which may be helpful in structuring expectations about future trajectories: (i) data augmentation, (ii) secondary pre-training, (iii) model architecture, and (iv) prompt engineering and few-shot learning.Lastly, we discuss some general considerations when choosing between smaller BERT-like LLMs and larger generative AI models in production settings.\n\n\u2022 Data Augmentation: Data augmentation is an approach to synthetically increase labeled training data without human input.One approach in NLP is back translation, where the original text is translated into another language and then back-translated.Adding the back-translated sentence to the training data (with the same label as the original) can boost performance as shown by [35].Another option is token perturbation, where certain words are replaced with synonyms [40].Going forward, these techniques can further reduce data requirements for fine-tuning LLMs.\n\n\u2022 Secondary Pre-training: An additional pre-training phase on a smaller domain-specific corpus can improve both fine-tuned encoder-style and instruction-tuned decoder-style LLMs.For RoBERTa, [20] demonstrated significant performance gains for downstream classification tasks when further pre-trained in the specific domain to gain more knowledge about a specific vocabulary and usage before performing downstream fine-tuning for the classification task.\n\n\u2022 Model Architecture: While there has been a trend toward larger and larger models, recent work has shown that scaling from hundreds of billions to trillions of parameters yields saturating improvements and that scale alone may not be the single factor that determines performance [34,7].",
            "score": 0.5373978753014244,
            "section_title": "Discussion",
            "char_start_offset": 37415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 451
                },
                {
                    "start": 453,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1083
                },
                {
                    "start": 1085,
                    "end": 1207
                },
                {
                    "start": 1207,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1647
                },
                {
                    "start": 1649,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 2102
                },
                {
                    "start": 2104,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15673828125
        },
        {
            "corpus_id": "268819074",
            "title": "LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction",
            "text": "Large language models (LLMs) are highly advanced tools that enable contextual information processing, understanding, and generation.They serve as powerful knowledge bases that can be used to gain valuable insights and create new content [13,14,45,47,62].Fine-tuning them for downstream tasks has also achieved great success in various 1 arXiv:2404.00913v1[cs.CV] 1 Apr 2024 domains.With the rapid growth in scale and quality of the corpus and the advance in hardware, the inherent abilities of LLMs are becoming stronger.Therefore, the primary objective of fine-tuning general-purpose LLMs should be ensuring they produce the desired output for downstream tasks (i.e., instruction-following ability) rather than memory excessive knowledge or mastering skills (e.g., causal inference and numerical computation).\n\nPrioritizing the enhancement of instruction-following capabilities for LLMs is prudent for a multitude of reasons.Firstly, it is often presumed that the corpus utilized during training encompasses a comprehensive repository of knowledge, thereby obviating the need for additional fine-tuning datasets when addressing specific downstream tasks.This implies that enriching a model's ability to follow instructions could leverage this pre-existing knowledge base more effectively.Secondly, by standardizing the output format during instruction-following tasks, we can reduce the generation of harmful or irrelevant content.Third, the challenge of incorporating new knowledge and skills into LLMs is accentuated by the limited set of parameters available for finetuning.Lastly, fine-tuning in the context of multi-modal tasks can be succinctly expressed as enhancing a model's proficiency in responding to visual cues.This essentially constitutes a variant of instruction following, where the prompts are visual rather than textual.\n\nHowever, predominant parameter-efficient fine-tuning (PEFT) techniques are met with notable challenges in performing instruction-tuning.The Adapter methods [19,44], which have a serial structure, can cause computation bottlenecks, and they also significantly alter the inference process to adapt to downstream tasks, which in turn can lead to degradation of inherent abilities, such as catastrophic forgetting.Prompt learning methods [6,33,37,49] effectively incorporate new information by creating additional parameters corresponding to the input sequences of language models.",
            "score": 0.5371050904076511,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 810
                },
                {
                    "start": 812,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1432
                },
                {
                    "start": 1432,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1840
                },
                {
                    "start": 1842,
                    "end": 1978
                },
                {
                    "start": 1978,
                    "end": 2252
                },
                {
                    "start": 2252,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1998,
                    "end": 2002,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17724609375
        },
        {
            "corpus_id": "271270400",
            "title": "Identifying Smart Contract Security Issues in Code Snippets from Stack Overflow",
            "text": "LLMs typically undergo pre-training and fine-tuning during their training process.In the pre-training phase, the model is exposed to extensive text data to acquire linguistic knowledge that includes grammar, context, and semantics [57].Following completion of pretraining, the model often undergoes fine-tuning to tailor its capabilities to specific tasks, e.g., code generation and summarization [55].The fine-tuning stage generally employs supervised learning [14], utilizing labeled data for additional training to adjust model parameters and align with the requirements of the targeted tasks.\n\nInference refers to the process in which a trained model generates output based on input data [4].This typically occurs when the model has completed training and is prepared to process realworld data.This step is crucial to applying the model to practical problems and tasks.LLMs suffer from instability (i.e., the responses generated each time are different) and hallucinations (i.e., the responses generated contain things that have not appeared in context) during inference due to sensitivity to adversarial samples [68], overfitting [29], and complex context.",
            "score": 0.5370690153440792,
            "section_title": "Pre-training, Fine-tuning and Inference",
            "char_start_offset": 10315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 596
                },
                {
                    "start": 598,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 1161
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "1281211"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "204943093"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "12440383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051666259765625
        },
        {
            "corpus_id": "265157870",
            "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
            "text": "Model Size  \u2022 Smaller models' refinement decisions are a cost-effective alternative to fine-tuning LLMs The refinement decision of smaller models combined with a pre-trained LLM performs similarly to a larger model when fine-tuned. This saves a lot of computation required to fine-tune a larger model (Table 4) and preserves downstream performance on other tasks. \n\n\u2022 Expert models can make better judgments about refinement Larger models (Asker 13B performance is better than Asker 7B in all cases) show that better models can make more informed decisions about when to refine. Table 6 shows that MetaMath trained models outperform LLaMA models of similar size.",
            "score": 0.5365608352158163,
            "section_title": "LLAMA 13B LLAMA 70B",
            "char_start_offset": 21542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 662
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1478271484375
        },
        {
            "corpus_id": "265457277",
            "title": "SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification",
            "text": "Therefore, we use publicly accessible finetuned LLMs for this purpose. As shown in Table 2, SPIN consistently outperforms the corresponding baseline results across all datasets, demonstrating its effectiveness when applied to already fine-tuned and well-performing models. A rigorous statistical analysis confirming the significance of this improvement is provided in Appendix C.5. \n\nThe scarcity of larger task-specific fine-tuned models here primarily due to the industry's shift of interest towards developing general-purpose LLMs, driven by the expensive costs of fine-tuning for specific tasks. This trend underscores the critical need for developing lightweight and scalable approaches, such as our proposed SPIN, to adapt LLMs to specialized tasks without incurring significant data and computational burdens.",
            "score": 0.5359984198803763,
            "section_title": "SPIN and Fine-tuning",
            "char_start_offset": 16757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.233154296875
        },
        {
            "corpus_id": "271947290",
            "title": "Investigating LLM Applications in E-Commerce",
            "text": "This superior performance can be attributed to the LLM's larger model capacity and enhanced quality of pre-training. Fine-tuned models (Llama2-7b and T5) outperformed the zeroshot capabilities of the much larger Mixtral 8x22B model in review summarization, while for description generation, the performance is comparable. Despite the Mixtral model's strong standing on LLM leaderboards like [9], which suggests competitive summarization abilities, the observed performance gap between zeroshot and few-shot scenarios highlights a key limitation: without in-context guidance, the model struggles to achieve sufficient capability on domain-specific tasks (Review Summarization). However, when in-context information is provided, the model demonstrated significantly improved outcomes. In review summarization, to achieve even higher levels of performance, task-specific training becomes crucial. Notably, even with smaller model architectures, fine-tuning can yield superior results (using Llama2-7b). \n\nIn contrast, the description generation task is more aligned with general-purpose text generation, where the model's ability to understand and leverage general knowledge is the primary factor in determining performance. Consequently, in this task, larger models like Mixtral, equipped with in-context guidance, could achieve top-tier performance, even surpassing fine-tuned smaller models.",
            "score": 0.5357736063883809,
            "section_title": "Classification tasks.",
            "char_start_offset": 19891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08197021484375
        },
        {
            "corpus_id": "271218154",
            "title": "A Survey on LoRA of Large Language Models",
            "text": "Rapidly increasing parameter scales of pre-training language models improves their generalization ability and brings emergent abilities. In the last few years, the parameter scales of pre-training languages models have increased by thousands of times (e.g., from 330M parameter BERT [1] to 540B parameter PaLM [2]). These pre-training language models having large parameter scales are termed Large language models (LLMs). Nevertheless, due to the knowledge boundaries of the LLMs, their abilities on some downstream tasks are still limited. To expand the knowledge boundaries, it remains necessary to fine-tune LLMs on the downstream tasks. \n\nHowever, fine-tuning the full parameters of an LLM, namely full fine-tuning, is extremely computationally expensive, for example, full fine-tuning of a LLaMA2-7B [3] model requires approximately 60GB of memory, which exceeds the capacity of common consumer GPUs [4]. To reduce the computational cost, various parameter-efficient fine-tuning (PEFT) methods have been proposed [5]. They adapt LLMs to downstream tasks by only fine-tuning a small number of (extra) model parameters. From the perspective of whether extra parameters are involved, PEFT methods can be divided into two categories: extra-parameter methods and intra-parameter methods. The extraparameter methods freeze all of the original parameters of an LLM and insert a set of learnable parameters to optimize the model input or model layers such as adapter tuning [6] and prompt tuning [7]. By contrast, intra-parameter methods freeze most of the original parameters of an LLM and only tune a small number of parameters of the LLM such as BitFit [8], LISA [4] and LoRA [9]. \n\nWhen we do not have access to modify the model architecture, intra-parameter methods are desirable. Among the intra-parameter methods, LoRA is the most widely used one, because it can achieve a comparable or better downstream adaptation performance to the full fine-tuning on a range of downstream tasks [9] and is easy to implement. Besides, there are many variants have been proposed to further improve the downstream adaptation ability of LoRA on more challenging downstream tasks.",
            "score": 0.5357319516592317,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1018,
                    "end": 1021,
                    "matchedPaperCorpusId": "257316425"
                },
                {
                    "start": 1471,
                    "end": 1474,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1493,
                    "end": 1496,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "231672601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.178955078125
        },
        {
            "corpus_id": "273962904",
            "title": "Evaluating Large Language Models on Financial Report Summarization: An Empirical Study",
            "text": "Large Language Models (LLMs) have revolutionized natural language processing, enabling significant advancements in text generation, summarization, translation, and question answering. Models like OpenAI's GPT-3 [18] and its successor, GPT-4 [19], represent breakthroughs in generative modeling due to their ability to generate coherent, contextually relevant text with minimal input. These models have been trained on diverse, large-scale datasets, allowing them to generalize across various domains and tasks. However, their generalpurpose nature often requires domain-specific fine-tuning to perform well in specialized areas such as finance, healthcare, and legal domains. \n\nSeveral research efforts have sought to tailor LLMs to specialized fields. For instance, BioBERT [20] and Clinical-BERT [21] are domain-adapted models for biomedical text, fine-tuned to handle the unique terminology and contextual demands of medical research. Research on smaller yet effective models, such as DistilBERT [22] and MiniLM [23], has demonstrated that knowledge distillation and parameter optimization techniques can enhance computational efficiency without significantly compromising performance. These smaller models, along with fine-tuning methods like LoRA [24], support adaptability and efficiency for resourceconstrained applications, which is increasingly relevant as LLM applications expand into industries requiring both accuracy and speed.",
            "score": 0.5356056322269777,
            "section_title": "A. Large Language Models",
            "char_start_offset": 3264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "271309962",
            "title": "Open Artificial Knowledge",
            "text": "While handcrafted human data has shown significant improvements in supervised fine-tuning (SFT) of LLMs, particularly for tasks like code generation and mathematical reasoning (Roziere et al., 2023;Wan et al., 2024), the scarcity and cost of creating such high-quality data have led to the increasing use of synthetic data as a proxy.This method primarily leverages strongly capable LLMs, such as the GPT family (Achiam et al., 2023) to produce high-quality synthetic data (Li et al., 2023b;Josifoski et al., 2023;Taori et al., 2023).\n\nRecent research has highlighted LLMs' ability to rephrase for improved responses and boost synthetic data for effective SFT (Gallego, 2024;Chen et al., 2024).These developments suggest that the use of synthetic data in model training will continue to grow in the future, with ongoing research exploring various techniques to leverage synthetic data effectively The process begins with extracting general topics from extensive human knowledge databases such as Wikipedia and GPT-4o models.These high-level and sub-level topics are then used in an automatic prompt generation step, which employs two methods: meta prompt engineering using large language models (LLMs) and cost-effective programming prompt engineering.The generated prompts are subsequently fed into state-of-the-art open-source LLMs (at the time of writing, five models were used: Llama3-8B, Llama-70B, Mixtral7x8B, Gemma-7B (Team et al., 2024), and Gemma-2-9B (Team, 2024)) to create the OAK dataset.\n\nfor improving LLM performance and alignment (Hao et al., 2024).",
            "score": 0.5354950650409614,
            "section_title": "Introduction",
            "char_start_offset": 1773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1502
                },
                {
                    "start": 1504,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 514,
                    "matchedPaperCorpusId": "257378179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09088134765625
        },
        {
            "corpus_id": "260132634",
            "title": "AUTOGEN: A Personalized Large Language Model for Academic Enhancement\u2014Ethics and Proof of Principle",
            "text": "Large language models (LLMs) such as ChatGPT or Google's Bard have shown significant performance on a variety of text-based tasks, such as summarization, translation, and even the generation of new ideas (Bommasani et al. 2022;Bubeck et al. 2023). Despite such impressive, seemingly domain-general abilities, LLMs such as GPT-31 (Brown et al. 2020) can benefit from additional, specialized training in certain narrower domains (Moradi et al. 2022). Known as fine-tuning, this process involves training the final few layers of an LLM's neural network on a specialized corpus of text, such that the resulting model retains its fundamental model of language but produces text influenced by the features of the specific corpus on which it was fine-tuned (Church, Chen, and Ma 2021). \n\nThe potential for LLMs to facilitate the academic research and writing process is increasingly explored (Lin 2023). However, such use is attended by significant technical and ethical problems and uncertainties. Recent, high-performing LLMs such as ChatGPT and GPT-4 tend to produce text in a shorter, more conversational, and less rigorous format than that typical of academic prose. This text also tends toward a flat, homogenous style (Grimaldi and Ehrler 2023). Among the novel ethical issues are questions of responsibility, credit, and blame for generated text and the need to vet such text for accuracy (Porsdam Mann et al. 2023). \n\nThe extent to which these technical and ethical issues can be addressed through fine-tuning an LLM for academic prose generation is largely an an open question. Early findings show that fine-tuned GPT-3 models can in fact produce text stylistically akin to specific authors (Elkins and Chun 2020). Notably, a GPT-3 model, fine-tuned by Schwitzgebel et al. to respond to philosophical questions in the style of philosopher Daniel C. Dennett, has produced text convincingly similar to Dennett's own writing (Schwitzgebel, Schwitzgebel, and Strasser 2023).",
            "score": 0.5349829900037958,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 347,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 750,
                    "end": 777,
                    "matchedPaperCorpusId": "239890333"
                },
                {
                    "start": 885,
                    "end": 895,
                    "matchedPaperCorpusId": "261065402"
                },
                {
                    "start": 1218,
                    "end": 1244,
                    "matchedPaperCorpusId": "255495711"
                },
                {
                    "start": 1390,
                    "end": 1416,
                    "matchedPaperCorpusId": "258519213"
                },
                {
                    "start": 1694,
                    "end": 1716,
                    "matchedPaperCorpusId": "225035059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1614990234375
        },
        {
            "corpus_id": "258832528",
            "title": "Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance",
            "text": "Proprietary Large Language Models (LLMs), such as ChatGPT and GPT4 (OpenAI, 2023b), have shown exceptional capabilities to follow general human instructions and solve diverse tasks, including but not limited to question answering (Qin et al., 2023), machine translation (Jiao et al., 2023b), information extraction (Wei et al., 2023), and grammar correction (Fang et al., 2023). Recent studies demonstrate that smaller foundational models, such * Work was done during the internship at Tencent AI lab. \n\nas LLaMA (Touvron et al., 2023), can also display remarkable proficiency in tackling diverse tasks when fine-tuned using instruction-driven data, as exemplified by Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). All of these LLMs' accomplishments have represented a significant stride toward the goal of general artificial intelligence. \n\nDue to the great potential of LLMs, people have been considering if LLMs can empower thousands of industries. In many practical scenarios, the primary focus of deploying an LLM is on some vertical applications, i.e., one or a few particular tasks. For example, Med-PaLM 2 harnesses the power of Google's LLMs on the medical domain to more accurately and safely answer medical questions (Singhal et al., 2023). In order to facilitate a wide range of NLP tasks in the financial industry, Bloomberg has released its LLM, which has been particularly trained on a vast amount of financial data (Wu et al., 2023). Despite the surprising performance of current LLMs for vertical applications, LLMs still often make fatal errors, such as hallucination, or using incorrect knowledge for reasoning in specific domains, making them even inferior to some taskspecific supervised-trained small models (Pan et al., 2023;Liu et al., 2023;Yuan et al., 2023). \n\nTherefore, in this work, we explore whether LLMs can be efficiently improved for certain targeted scenarios. We choose the writing-assistance scenario as our targeted vertical application.",
            "score": 0.5348083566109669,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 377,
                    "matchedPaperCorpusId": "257921532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1468505859375
        },
        {
            "corpus_id": "271769753",
            "title": "Knowledge Acquisition through Continued Pretraining is Difficult: A Case Study on r/AskHistorians",
            "text": "Powerful LLMs like ChatGPT are adopted rapidly for a wide array of tasks, but their limitations in domain-specific areas become apparent, particularly when prompted to recite facts. This is critical especially for knowledge workers, who are adopting LLM-based tools rapidly.While there are various techniques that can help ingest knowledge into LLMs such as instruction tuning and alignment, most have disadvantages. We examine the impact of prominent training techniques on LLMs\u2019 knowledge accuracy using a knowledge-dense dataset that we curate from r/AskHistorians, a rich source of historical knowledge. We evaluate the impact of different models sizes from 1.3B to 7B parameters and other factors such as LoRA adapters, quantization, overfitting, and the inclusion of Reddit data in pretraining.In addition, we measure linguistic metrics and human and LLM-based preference. Our results suggest that pretraining and model size have a much stronger effect on knowledge accuracy than continued pretraining \u2013 unless the model is overfit to the tested knowledge.Fine-tuning on our Reddit dataset introduces less complex, but slightly more toxic language. Our study explores the challenges of injecting domain-specific datasets into LLMs and has implications for practitioners, e.g., when LLMs are to be fine-tuned with a company\u2019s datasets.",
            "score": 0.5347956834410158,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038330078125
        },
        {
            "corpus_id": "260900362",
            "title": "Semantic similarity loss for neural source code summarization",
            "text": "Fine-tuned Large Language Models \n\nWe also answer our RQs in the context of fine-tuned large language models (LLMs). State-of-the-art performance in many text generation tasks is often produced by fine-tuning so-called foundation models. A foundation model is an LLM that is pre-trained on internet-scale text datasets. Then the foundation model is fine-tuned by further training on a relatively small dataset of domain-specific examples. There are hundreds of possible fine-tuning configurations, and a comprehensive study of fine-tuning for source code summarization is not available yet in the literature and is beyond the scope of this single paper. However, our goal is to determine the usefulness of our semantic similarity loss function in a wide range of models, so we chose two approaches consistent with related work for other text generation tasks. Given the high cost of fine-tuning LLMs and the immense number of experimental variables given the rapidly changing research frontier, we limit ourselves to comparing use-seq to CCE for the funcom-java-long dataset. \n\nOne LLM we use is the LLaMA 7B parameter model by Touvron et al. 11 . We fine-tune this model with the Alpaca-LoRA procedure using the settings and implementation available from Taori et al. 12 . Technically, we set the Instruction text to \"please describe the following source code\", the Input text to the target function's source code, and the Response text to the source code summary during fine tuning. Then we fine tune for one epoch using the default parameters (listed in our reproducibility package, Section 9). During inference, we use the same Instruction and Input text setup, but extract the text after Response as the summary. \n\nWe also use the GPT2 124m parameter model by Radford et al. 63 . We use a complete fine-tune procedure (as opposed to a weight matrix reduction technique such as LoRA) based on OpenAI's GPT2 124m parameter snapshot. We use hyperparameters recommended by Karpathy 64 in a public GPT2 implementation. We fine tune for 18 epochs, at which point validation accuracy diverges from training accuracy, indicating possible overfitting.",
            "score": 0.5342667809691998,
            "section_title": "4.6",
            "char_start_offset": 27670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 35,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1717
                },
                {
                    "start": 1720,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 1780,
                    "end": 1782,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270263671875
        },
        {
            "corpus_id": "264306101",
            "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
            "text": "LLM-as-Agent Before the rise of LLMs (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;Zeng et al., 2022), agent tasks primarily relied on reinforcement learning or encoder models like BERT. With the advent of LLMs, research shifted towards LLM agents. Notably, Re-Act (Yao et al., 2023) innovatively combined CoT reasoning with agent actions. Several studies also applied language models to specific agent tasks, such as online shopping (Yao et al., 2022), web browsing (Deng et al., 2023), and household exploration (Shridhar et al., 2020). Recently, with ChatGPT showcasing advanced planning and reasoning skills, research like ReWOO (Xu et al., 2023) and RCI (Kim et al., 2023) has delved into prompting strategies and frameworks to boost language model efficiency in agent tasks without the need for fine-tuning. \n\nInstruction Tuning Instruction tuning aims at aligning the language models to follow human instructions and produce outputs that better fit human preferences. Instruction tuning mainly focus on training language models to follow human instructions among multiple general tasks. For instance, FLAN (Wei et al., 2022a) and T0 (Sanh et al., 2022) demonstrates the strong zero-shot generalization ability of language models fine-tuned on multiple task datasets. Further, FLAN-V2 (Longpre et al., 2023) explores the performance of instruction tuning across multiple scales of models and datasets. With the impressive alignment capability demonstrated by commercial LLMs, many recent works (Chiang et al., 2023;Wang et al., 2023a) propose methods to distill instruction tuning dataset from close-sourced model to enhance the alignment of open-source models.",
            "score": 0.5339442470617246,
            "section_title": "RELATED WORK",
            "char_start_offset": 22930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "250264533"
                },
                {
                    "start": 533,
                    "end": 556,
                    "matchedPaperCorpusId": "222208810"
                },
                {
                    "start": 1132,
                    "end": 1151,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1159,
                    "end": 1178,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1207275390625
        },
        {
            "corpus_id": "270702994",
            "title": "Serial Position Effects of Large Language Models",
            "text": "As demonstrated in Figure 3, SPE are widespread across LLMs, affecting models with various training and fine-tuning methodologies across all tasks.This section evaluates methods that could mitigate SPE during inference.We specifically explore techniques such as prompting and CoT for this purpose.\n\nAlthough approaches like few-shot learning and fine-tuning could potentially reduce SPE, they risk introducing bias in scenarios lacking ground truth due to the particular examples used.Similarly, we avoid employing self-refinement because it necessitates LLMs to generate feedback, which may further introduce bias depending on the nature of the feedback required.\n\n6.1 Prompting Experiments",
            "score": 0.5335133557766504,
            "section_title": "Potential Methods for Mitigating Serial Position Effects",
            "char_start_offset": 17201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 297
                },
                {
                    "start": 299,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 664
                },
                {
                    "start": 666,
                    "end": 691
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056243896484375
        },
        {
            "corpus_id": "272831775",
            "title": "Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning",
            "text": "Our fine-tuning results, summarized in the bottom four rows of Table 4, indicate that, for open-source models, fine-tuning on any of the four investigated tasks yields the highest ASR against translation tasks and, to a certain extent, the classification tasks. This pattern is consistent not only with Llama3.1-8B but also with Gemma, Llama2 and Phi as detailed in Appendix Tables 11 to 13. On the other hand, both proprietary models (GPT-4o-mini and Gemini 1.5 Flash) tend to have weaker guardrails for classification tasks when fine-tuned, which results in a higher ASR. \n\nThe observed increase in ASR across different models and tasks demonstrates that fine-tuning generally increases susceptibility to adversarial attacks for both proprietary and open-source models, with the exception of text generation, where attacks remain effectively blocked. Specifically, as shown in the base model results in Table 4, LLMs tend to have highly effective safeguards in place for text generation. Consequently, even after finetuning, the ASR for text generation remains close to 0%. These findings indicate that fine-tuning makes LLMs more vulnerable to safety risks compared to their base models, particularly in tasks like translation and classification, thus addressing RQ2. \n\nIn conclusion, while task-specific fine-tuning can enhance performance, it also introduces significant safety risks.",
            "score": 0.5332063132197843,
            "section_title": "Fine-tuning Category Analysis",
            "char_start_offset": 18381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2509765625
        },
        {
            "corpus_id": "261214796",
            "title": "Financial News Analytics Using Fine-Tuned Llama 2 GPT Model",
            "text": "The paper [4] provides a detailed description of an approach to fine-tuning and safety improvements of Llama 2-Chat. Full fine-tuning is applicable in the case when we need to ingest millions of documents into LLM. But in the case of much smaller data, we can use a PEFT/LoRA approach which consists in fine-tuning a much smaller number of model parameters. These parameters are saved in the model adapter which is used for full model modification before using it for the model text response generation. To optimize GPU usage, 4bit or 8bit quantization of LLM can be chosen for model fine-tuning. State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve the performance comparable to that of full fine-tuning [5]. The paper [6] considers Low-Rank Adaptation, or LoRA, which freezes pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. PEFT/LoRA approach approach makes it possible to fine-tune LLMs with sizes near 7B parameters, using Google Colab. Along with text data for fine-tuning, it is important to use prompt instructions which show how to process input prompts. Instructions can be created by human experts and augmented by other LLM models. LLM generate complex output texts on prompts which can be optimized by different ways. One possible way is selecting appropriate instructions for fine-tuning models. Another way is using a method called Reinforcement Learning from Human Feedback (RLHF) [7]. In this approach, human experts estimate and rate LLM output and then using this rates as target lables, LLM can be fine-tuned by supervised training. \n\nLet us consider fine-tuning Llama 2 using a PEFT/LoRA approach.",
            "score": 0.5331950293995855,
            "section_title": "Introduction",
            "char_start_offset": 4444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2122
                },
                {
                    "start": 2125,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253173828125
        },
        {
            "corpus_id": "264146593",
            "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
            "text": "In this work we investigated different fine-tuning methods to improve the LLMs' performance on math problem solving. Starting with supervised step-by-step fine-tuning, we first demonstrated the importance of step-by-step solutions for improving fine-tuned LLM performance. We then studied re-ranking methods for fine-tuning the LLMs as solution evaluators, and proposed a new re-ranking method which combines the benefit of majority voting and re-ranking together, simultaneously achieving better solution accuracy and computational efficiency. Lastly, we introduced a multi-task sequential fine-tuning method, aiming at improving the model's solution generation ability with the training objective of the solution evaluation. Our method outperforms the baseline fine-tuning method based on the solution generation training objective only, demonstrating its ability of improving a generation task using the supervision signal of the corresponding evaluation task.     1) the ratio of the number of correct solutions v.s. all solutions, (2) the ratio of number of correct solutions v.s. Maj1@64 solutions when the correct solution is not the Maj1@64 solution.",
            "score": 0.5330189582681992,
            "section_title": "CONCLUSION",
            "char_start_offset": 24098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1158
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "262054014",
            "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
            "text": "The primary motivation of this work is to offer a principled understanding of the process of finetuning, in order to make fine-tuning more efficient and reliable. We acknowledge that our current analysis reveals gaps in fine-tuning which could potentially be exploited to bypass safety measures introduced when fine-tuning. However, this work does not directly create new attacks or expose new vulnerabilities. Instead, we offer an explanation unifying the success of various existing manual attempts at bypassing safety fine-tuning. We hope our work contributes to the open discussion of limitations of current methods in containing the potential dangers posed by LLMs, and opens up new avenues for further research into the safety and reliability of these systems. 2022) construct experimental setups to probe how the prompts affect what task the model is inferring. Our work studies the same idea of task inference but builds on this work to first characterize the effect of fine-tuning and then intervene via conjugate prompting to switch between fine-tuned and pretrained behavior. Fine-tuning pretrained language models. There is a large body of work on fine-tuning language models in a manner that preserves performance (Raffel et al., 2020;Arivazhagan et al., 2019;Gao et al., 2021b), generalizes slightly out-of-distribution (Wei et al., 2022;Sanh et al., 2022;Min et al., 2022a), and aligns with human usage/values (Christiano et al., 2023;Stiennon et al., 2022;Bai et al., 2022;Ziegler et al., 2020;Ouyang et al., 2022;Mishra et al., 2022;Chung et al., 2022). Other works have tried to build a mechanistic understanding for how fine-tuning alters (or does not alter) pretrained models (Lubana et al., 2023;Jain et al., 2023). Prompting in different languages. Prior works have found that models will best complete tasks in English with performance drops in other languages (Shi et al., 2022;Ahuja et al., 2023;Lin et al., 2022).",
            "score": 0.5322657006162669,
            "section_title": "ETHICS STATEMENT",
            "char_start_offset": 30588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1939
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "271924267",
            "title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models",
            "text": "With the emergence of large language models (LLMs) such as Qwen [1], LLaMA [2], and GPT [3], their exceptional generation, understanding of complex language structures and dialogue capabilities have garnered widespread attention [4], [5]. However, in specific domains, their performance often fails to meet practical requirements. For instance, GPT-4 may cite incorrect legal provisions when answering legal questions, leading to erroneous analytical conclusions. ChatLaw-MoE [6], fine-tuned on high-quality law data, has outperformed GPT-4 across multiple application scenarios. Therefore, enabling general models to acquire domain-specific knowledge allows for deploying a domain model with minimal computational resources, potentially outperforming general models with ten times the number of parameters. * Yongqian Sun is the corresponding author. \n\nState-of-the-art approaches extensively utilize instruction fine-tuning (IFT) to align general-purpose models with specific application domains and maximize their effectiveness. InstructGPT [7] employed instruction fine-tuning to bridge the performance gap between models with a hundredfold difference in parameter count. In the absence of instruction data, certain approaches [8]- [11] use advanced LLMs to construct instruction datasets, achieving performance close to GPT-3.5 and GPT-4. However, these methods cannot guarantee the correctness and diversity of the generated instruction data. Fortunately, high-quality instruction data is scarce in most scenarios, while the volume of knowledge documents is enormous. \n\nIn summary, applying general-purpose models to specific domains presents the following challenges: \n\n1) Limitation of Computational Resources. Model performance is typically proportional to the scale of the model's parameters. However, fine-tuning and deploying powerful general-purpose language models requires substantial computational resources. For example, a LLM with 72B parameters using fp16 precision requires five Tesla V100-32GB GPUs for inference. Fine-tuning such a model incurs even greater costs. This is prohibitively expensive and impractical for tasks that must be continuously available. 2) High-quality data scarcity. Domain-specific highquality instruction data is often scarce. Manually correcting instruction data requires significant human effort, making it expensive.",
            "score": 0.5319655510388532,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 91,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "257912848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052825927734375
        },
        {
            "corpus_id": "276647376",
            "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
            "text": "This study focused on task-specific fine-tuning. While existing zero-shot and fine-tuning methods often struggle with reliability and efficacy even in such settings, and our method proved effective, generalizing our approach to a broader range of tasks is an important direction. Exploring techniques like multi-task training could enable efficient reasoning without task-specific tuning. This could be particularly beneficial for 'thinking' models (Guo et al., 2025) where reasoning enhances the final response, similar to our math reasoning tasks but in a more general context. Furthermore, while prior work has explored reducing the number of reasoning stages (Chen et al., 2024), our method focuses on reducing verbal redundancy within sentences, offering a complementary approach. Several concurrent works also address redundancies in LLM outputs. Some approaches (Chen et al., 2024;Luo et al., 2025) focus on how o1-like models overthink and propose methods for pruning extraneous reasoning steps. Another approach, TokenSkip (Xia et al., 2025), first identifies and discards low-importance tokens from self-generated CoT outputs while preserving crucial ones, then fine-tunes the model on these compressed examples to enable efficient inference. In contrast, our method elicits concise reasoning more simply yet effectively by training on a self-generated dataset of shorter rationales, using best-of-N sampling and few-shot conditioning.",
            "score": 0.53146520167927,
            "section_title": "Concise Reasoning in General LLMs",
            "char_start_offset": 27000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1445
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1658935546875
        },
        {
            "corpus_id": "270526238",
            "title": "Large language model to multimodal large language model: A journey to shape the biological macromolecules to biological sciences and medicine",
            "text": "LLMs, such as GPT models (GPT-3.5 and GPT-4), have achieved remarkable success in various NLP tasks. However, they also come with certain significant limitations (Table 3). LLMs can generate coherent and contextually relevant text, but they often lack deep understanding of the world and common sense reasoning. They can generate nonsensical or incorrect responses in specific contexts. 148 LMs can inadvertently preserve biases present in the training data, which can result in biased or unfair outputs, especially when dealing with sensitive topics such as gender, race, or religion. Mitigating biases in LLMs remains a significant challenge. \n\nMoreover, while LLMs excel in understanding and generating text based on context, they may struggle with long-term dependencies or maintaining coherence over extended passages. 6 This can lead to inconsistencies or inaccuracies in generated text, especially in complex or significant scenarios. The LLMs typically require vast amounts of data for pre-training, which can be expensive and resource-intensive. Furthermore, they may stumble with generalizing to out-ofdomain or low-resource domains where training data are limited. \n\nWhile fine-tuning LLMs on specific tasks can improve performance, it often requires careful selection of hyperparameters, task-specific data, and fine-tuning strategies. 149  addition, fine-tuning may only sometimes lead to optimal performance, especially for tasks with unique requirements or constraints. \n\nIn the case of safety and ethical concerns, the LLMs have the potential to generate harmful or malicious content, including misinformation, hate speech, or inappropriate material. 150 Ensuring LLMs' safe and ethical use poses significant challenges for researchers and practitioners. Training and deploying LLMs can be computationally expensive and resource-intensive, requiring powerful hardware and substantial infrastructure. It can limit accessibility to LLMs for researchers and organizations with limited resources. 151 Addressing these limitations requires ongoing research and development efforts in bias mitigation, robustness testing, model interpretability, and ethical AI frameworks. In addition, interdisciplinary collaboration involving experts from diverse fields, such as linguistics, psychology, and ethics, is essential to foster the responsible development and deployment of LLMs.",
            "score": 0.5308119946354601,
            "section_title": "Limitations of LLMs",
            "char_start_offset": 48956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "259136071"
                },
                {
                    "start": 824,
                    "end": 825,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 1667,
                    "end": 1670,
                    "matchedPaperCorpusId": "263209831"
                },
                {
                    "start": 2009,
                    "end": 2012,
                    "matchedPaperCorpusId": "263827375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049224853515625
        },
        {
            "corpus_id": "275471255",
            "title": "Containers as the Quantum Leap in Software Development",
            "text": "As the importance of large language models (LLMs) for software engineering became apparent during the project, some survey work on LLM usage was started. The first survey produced was on running LLMs locally and focused on quantization techniques to reduce the model memory footprint and improve execution speed. Resource-efficient fine-tuning of models for companyspecific use cases was also investigated. \n\nThe second survey expanded on LLM use for company-specific use cases by investigating retrieval augmented generation (RAG). Different RAG methods and adjacent technologies like vector databases were surveyed. In parallel a test case of using Azure cloud platform for finetuning foundation models using techniques investigated in the first survey. \n\nThe third and last survey was on the current state of software generation with LLMs. The topics covered were techniques to ensure code correctness, class-scale code generation and repository-scale generation. The code-correctness approaches found largely relied on coupling an LLM with a formal verification tool. The class-scale generation survey found that LLMs had trouble generating these larger blocks of code compared to single functions. Repository scale generation refers to code generation tasks too large to fit in the LLM context window. The survey found two main approaches to this problem: \n\n\u2022 The first approach was generating a skeleton or plan for the whole code and then generating the code piece by piece. \u2022 The second approach was using an agent-based system, where the generation task is split into smaller pieces and each piece is assigned to an LLM agent. \n\nThe findings were presented during sprint meetings. In the first survey, a technique called QLoRa was found as the most popular for reducing fine-tuning requirements. This technique was then tested when exploring Microsoft Azure cloud platform for fine-tuning large language models. It enabled fine-tuning of large models on lesser resources and despite some issues, the platform was found to be relatively easy to use. In the final survey it was found that current LLMs while good at function-level coding tasks, struggle with class-level tasks. It was also found that repository scale generation is a big challenge due to the limited input length of LLMs. Two research approaches were identified: first was top-down generation, where you first generate a skeleton of the code and then generate smaller pieces to fill it in.",
            "score": 0.5307099396347035,
            "section_title": "Large Language Models in Software Engineering",
            "char_start_offset": 15911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2295
                },
                {
                    "start": 2296,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10125732421875
        },
        {
            "corpus_id": "270440709",
            "title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification",
            "text": "This allows us to identify performance variations across tasks and highlight areas where the differences between fine-tuned and prompt-based models are most (and least) pronounced.\n\n\u2022 Latest Generative AI Models: We provide results for the latest generation of prompt-based generative AI models (GPT-4 and Claude Opus).This is crucial, as previous studies based on older models like GPT-3 offer limited insights into the current performance balance.\n\n\u2022 Nuanced Understanding of Fine-Tuned LLMs: By investigating the performance of several fine-tuned LLMs, we offer a detailed understanding of how these models compare across classification tasks.This helps identify which models are better suited for specific tasks, thereby aiding user choice.Such a comparison provides a broader perspective on the current LLM landscape for text classification.\n\n\u2022 Ablation Studies and Training Data Impact: We conduct ablation studies and analyze the effect of training data size on model performance for fine-tuning.This offers valuable insights into the number of labeled samples required to achieve optimal performance, a crucial consideration given our consistent findings in favor of fine-tuned LLMs.\n\n\u2022 Accessible Toolkit for Fine-Tuning: We offer a one-stop-shop toolkit for text classification to enable a wider audience to use pre-trained LLMs for text classification tasks.\n\nWith these contributions, we aim to provide a comprehensive and practical resource for understanding and applying LLMs in text classification.\n\n3 Non-technical Background: From Keywords to Large Language Models\n\nThis section provides a non-technical introduction to LLMs, which complements our empirical results and our provided fine-tuning toolkit.It explains the functioning principles of LLMs and the reasons for their superior performance compared to earlier methods.To provide a comprehensive overview, we first briefly cover traditional hand-coding and dictionary approaches before discussing machine learning methods and LLMs.\n\nHand-coding is among the earliest text-as-data approaches.When hand-coding, Human coders classify verbal information according to pre-specified codebooks and explicit coding rules [12], [33].Because hand-coding leverages human text-understanding, it is relatively easy to perform and yields high-quality results.However, hand-coding is labor-intensive -both regarding codebook development and the coding process itself.This makes it expensive and slow (see Figure 1).",
            "score": 0.530652526530281,
            "section_title": "Related Work and Contribution",
            "char_start_offset": 7556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 182,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 449
                },
                {
                    "start": 451,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 744
                },
                {
                    "start": 744,
                    "end": 846
                },
                {
                    "start": 848,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1191
                },
                {
                    "start": 1193,
                    "end": 1369
                },
                {
                    "start": 1371,
                    "end": 1513
                },
                {
                    "start": 1515,
                    "end": 1581
                },
                {
                    "start": 1583,
                    "end": 1720
                },
                {
                    "start": 1720,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2004
                },
                {
                    "start": 2006,
                    "end": 2064
                },
                {
                    "start": 2064,
                    "end": 2197
                },
                {
                    "start": 2197,
                    "end": 2318
                },
                {
                    "start": 2318,
                    "end": 2425
                },
                {
                    "start": 2425,
                    "end": 2473
                }
            ],
            "ref_mentions": [
                {
                    "start": 2186,
                    "end": 2190,
                    "matchedPaperCorpusId": "144834362"
                },
                {
                    "start": 2192,
                    "end": 2196,
                    "matchedPaperCorpusId": "154674194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11181640625
        },
        {
            "corpus_id": "269042793",
            "title": "Improving Retrieval for RAG based Question Answering Models on Financial Documents",
            "text": "Fine-tuning is the process of updating model parameters for specific tasks and domains. Through fine-tuning one can further train a LLM on a domain-specific dataset so that it has access to additional information not present in the original training data. Fine-tuning also enables us to train the model to get consistent outputs via instruction fine-tuning. The three main methods of fine-tuning are unsupervised fine-tuning, supervised fine-tuning, and reinforcement learning. In our previous work, we examined the method of using supervised fine-tuning to improve several different models on finance-specific questions. Supervised finetuning, particularly instruction fine-tuning is ideal to tailor a model to perform a certain way given that it has access to labelled data with input and output pairs. However, a primary bottleneck of this method is access to high-quality data in order to conduct effective supervised fine-tuning. For question-answering tasks, these datasets are usually curated with human experts, often with the aid of other LLMs to get accurate and comprehensive question-answer pairs. In unsupervised fine-tuning, the data is unlabeled and can be viewed as an extension of the pre-training process, but with domain-specific, instead of general documents and text. However, the model is not given any additional instructions or labels from this data. The hope is that the model will retain the information from this additional training data and be able to access that knowledge when doing very domain-specific question-answering. While this approach of fine-tuning can be much more scalable compared to supervised fine-tuning because of the availability of data, the results of the fine-tuned model won't be as robust. Moreover, while domain-specific data is used, there is no guarantee that the LLM will be able to access it when it is needed, especially if the size of the domain-specific data is small when compared to the text-corpus that was used for pre-training.",
            "score": 0.5304274563344694,
            "section_title": "Fine-tuning",
            "char_start_offset": 1954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1993
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350830078125
        },
        {
            "corpus_id": "272592878",
            "title": "How Effectively Do LLMs Extract Feature-Sentiment Pairs from App Reviews?",
            "text": "For this purpose, rule-based approaches like SAFE [7] or supervised methods like RE-BERT [8] and T-FREX [9] have been proposed. The results of these studies indicate that fine-tuning of pre-trained models such as BERT has significantly outperformed rulebased approaches such as SAFE for extracting app features. However, to fine-tune such models, a considerable number of task-specific training examples are still required. Furthermore, updating some of the model parameters to align with the task adds more complexity to the model fine-tuning. \n\nRecently, Large Language Models (LLMs) trained on a large corpus of data with Reinforcement Learning from Human Feedback (RLHF), such as ChatGPT [10] and LLama-2 [11], have shown the ability to generalize to new tasks without requiring task-specific fine-tuning [12]. LLMs take in natural language instructions defining the task (labeled as A in Figure 1), optionally accompanied by a few examples for task demonstration (labeled as B in Figure 1). This learning method is called zero-shot when no examples are provided with the instruction, and few-shot when a few examples are supplied with the task description. LLMs with RLHF have been proven effective in following instructions for different tasks with zeroshot or few-shot learning, eliminating the need for task-specific training examples for model parameter updating.",
            "score": 0.5303234093486523,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1372
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 53,
                    "matchedPaperCorpusId": "4703509"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "233354151"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "257921533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07366943359375
        },
        {
            "corpus_id": "268510572",
            "title": "Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention",
            "text": "Large Language Models (LLMs) [25,34], sometimes referred to as Pre-trained Language Models (PLMs) [14,21,27], are an advanced form of Language Model (LM) [2,10,32] that train deep learning algorithms on massive amounts of data, with up to billions of parameters, allowing for exceptional performance in a vast array of Natural Language Processing (NLP) tasks.They have quickly become integral to Natural Language Generation (NLG) tasks, a challenging sub-category of NLP that focuses on text generation from a wide array of input data forms.LLMs are able to perform exceptionally well due to transformers [36], which can model sequential data using a self-attention module, and the massive amounts of data available on the Internet for training these models.Popular LLMs also utilise in-context learning [4] and Reinforcement Learning from Human Feedback (RLHF) [6,45], making their performance improve even more over time.\n\nA specific type of NLG task is Question-Answering (QA), where a model must have a backlog of knowledge beyond the input sequence to generate an answer comparable to that of a human with prior experience, knowledge, and semantic inferring capabilities.QA and dialogue systems in general are designed to interact with humans using natural language, requiring a model that can represent both language and knowledge of a vast array of topics.Clearly, LLMs are well suited to this application, and can be fine-tuned further for downstream tasks.However, NLP in the wider sense is moving away from the pre-train then fine-tune paradigm, towards a pre-train and prompt paradigm [16].Even without fine-tuning, LLMs can perform ad-hoc NLG tasks from a simple natural language prompt, allowing for downstream task outputs without changing the underlying model structure.This allows for non-technical general public users to utilise the power of these complex models without having to understand the mechanisms behind them.",
            "score": 0.5302149884538496,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 3362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 758
                },
                {
                    "start": 758,
                    "end": 923
                },
                {
                    "start": 925,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 160,
                    "end": 163,
                    "matchedPaperCorpusId": "8264008"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01337432861328125
        },
        {
            "corpus_id": "271064905",
            "title": "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning",
            "text": "Recently, the rapid advancement and deployment of Large Language Models (LLMs) have transformed various sectors by providing unprecedented natural language processing capabilities [21].Traditionally, these models have been developed with a focus on enhancing their general abilities, aiming to create universally powerful tools that excel across a broad spectrum of tasks and domains.However, this generalized approach often fails to meet the specific requirements of individual users and small organizations, whose needs are intricately tied to their unique operational contexts.To address this issue, one very simple way is to fine-tune LLMs for different tasks.In the medical field, researchers have optimized LLMs using medical datasets to support functionalities such as medical document interpretation and diagnostic consultations.For example, ChatMed [23], DISC-MedLLM [2], HyKGE [9], IvyGPT [12], and HuatuoGPT [20] are notable examples, demonstrating significant advancements within their specialized medical domains compared to generic LLMs.Similarly, in the legal field, researchers have introduced fine-tuned LLMs trained on legal corpora to support activities such as legal research, contract analysis, and legal document summarization.Models like ChatLaw [7], LawGPT [22], and DISC-LawLLM [19] exemplify this trend, demonstrating a deeper grasp of legal language and principles compared to their generic counterparts.Within finance, researchers are leveraging diverse financial datasets, including conversational data and market reports, to fine-tune LLMs for applications such as financial forecasting, risk assessment, and fraud detection.Examples include FinGPT [16], DISC-FinLLM [4], and PIXIU [13].Additionally, in other fields, models like FaiMA [17], designed for Multi-domain applications, and Kuaiji [10], tailored for accounting tasks, illustrate the versatility and potential of fine-tuned LLMs in specialized domains.\n\nTo address this discrepancy, we introduce a novel fine-tuning paradigm specifically designed for Explicit Data Sets, which we term \"task fine tune.\"This method diverges from conventional training techniques by emphasizing precision and specificity over general performance.",
            "score": 0.5301714413474496,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1943
                },
                {
                    "start": 1945,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "259991244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12890625
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Large language models (LLMs) have demonstrated impressive general capabilities in tackling a wide range of tasks (Bubeck et al., 2023). However, when it comes to real-world applications, users often find that certain specific abilities need enhancement. In such cases, relevant task-specific data are adopted to fine-tune the model in instructions to improve its performance in particular tasks (Touvron et al., 2023;Scialom et al., 2022). However, the widely used LLMs such as LLAMA-Chat (Touvron et al., 2023), ChatGPT1 , and Claude-22 have been fine-tuned, and the continual instruction tuning in specific tasks may affect the general knowledge stored in LLMs since the original training data are inaccessible. The study of continual instruction tuning is crucial for the practical application of LLMs. For example, in Table 1, an LLM is first fine-tuned for the task of title generation and then learns to generate an explanation. This scenario is related to the continual learning paradigm in machine learning, where a model is trained on a sequence of tasks (Lopez-Paz & Ranzato, 2017;Wu et al., 2022;Ke & Liu, 2022). A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge due to parameter updates during the learning of new tasks, leading to decreased performance on earlier tasks (Li & Hoiem, 2017;Lopez-Paz & Ranzato, 2017). \n\nInstruction: Make a title for this article with 'computer': japan's NEC corp. and UNK computer corp. of the United States said Wednesday they had agreed to join forces ... Output: NEC UNK in computer sales tie-up. Instruction: Explain why the two following sentences are unrelated: Sentence 1: 'This church choir sings to the masses as they sing joyous songs from the book at a church'; Sentence 2: 'The church has cracks in the ceiling.' Output: There is no indication that there are cracks in the ceiling of the church. \n\nTable 1: Fine-tuning examples in an instruction form. Scialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning.",
            "score": 0.5298209086581792,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 438,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 1091,
                    "end": 1107,
                    "matchedPaperCorpusId": "247717213"
                },
                {
                    "start": 1361,
                    "end": 1379,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "268793648",
            "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
            "text": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.",
            "score": 0.5296608801078513,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1708984375
        },
        {
            "corpus_id": "273323889",
            "title": "Studying and Benchmarking Large Language Models For Log Level Suggestion",
            "text": "Approaches for Utilizing LLMs to Enhance Code Improvement Tasks. We identified key strategies for using LLMs to improve log level suggestions, which can also benefit other code enhancement tasks. While larger LLMs often provide better accuracy, smaller, code-specific models can achieve similar results with less resource use, making them more efficient. When resources are limited, Fill-Mask LLMs can also perform well despite their size. It's important to choose an LLM trained on relevant data for specific logging tasks, as this improves performance more than simply selecting larger models. Developers should focus on fine-tuning to enhance accuracy instead of relying on in-context learning. Although Text Generation LLMs show strong initial performance, they can be hard to fine-tune, so exploring other models or techniques may lead to better results. By following these strategies, developers can improve log level suggestions and support other code enhancement efforts. Note: The +/-number after each data denotes the relative improvement or decline compared to the suggestions within the system as observed in RQ1.",
            "score": 0.529510991447854,
            "section_title": "A. Developers",
            "char_start_offset": 41663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1125
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09136962890625
        },
        {
            "corpus_id": "270257788",
            "title": "Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity",
            "text": "Large language models (LLMs) have demonstrated superior performance in general-purpose language generation [1,35,22].Despite their success, it remains necessary to fine-tune LLMs for specific tasks to achieve optimal results.However, fine-tuning LLMs often requires much more memory compared to the inference process.Specifically, there are mainly four parts that occupy the memory during fine-tuning LLMs: (1) the weight parameter itself; (2) the optimizer state, which contains the information about the past gradient [16]; (3) the weight gradient used to update the parameters; (4) the activation cached to calculate the weight gradient [25]; In previous work like QLoRA [7], it can reduce both (1) and (2) by combining weight quantization and low-rank adaption [12], which enables fine-tuning huge LLMs under data-center level GPUs.However, under more memory-constraint hardware like cell phones, the memory of caching (3) weight gradient and (4) activation required by backpropagation still cannot be overlooked.The disparity between the demand of LLM fine-tuning and hardware capacity limits the adaptability of LLMs, especially when personalizing them for edge devices.\n\nExploring Zeroth-Order Optimization in LLM Fine-Tuning.Recently, there has been a resurging interest in zeroth-order (ZO) optimization methods for LLM fine-tuning [27,23,3].ZO optimization method perturbs model parameters in random directions and utilize the loss value difference to compute the gradient direction for parameter update.One advantage of ZO methods in LLM fine-tuning is that they do not require backpropagation procedures, which significantly saves the computation and memory.In this way, ZO is backpropagation-free and does not need to cache (3) weight gradients and (4) activations during fine-tuning.In practice, ZO methods have demonstrated the potential to achieve performance comparable to first-order methods in LLM fine-tuning, which opens the doors for various efficient LLM adaptation strategies.",
            "score": 0.5295067900716863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1176
                },
                {
                    "start": 1178,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1351
                },
                {
                    "start": 1351,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1797
                },
                {
                    "start": 1797,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 110,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "6628106"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "263609305"
                },
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "258865355"
                },
                {
                    "start": 674,
                    "end": 677,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 1341,
                    "end": 1345,
                    "matchedPaperCorpusId": "258959274"
                },
                {
                    "start": 1348,
                    "end": 1350,
                    "matchedPaperCorpusId": "263609305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336181640625
        },
        {
            "corpus_id": "258331833",
            "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
            "text": "Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks. \n\n4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at. \n\nFor summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t. \n\nROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127]. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization. \n\nIn machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation. \n\nAnother interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity.",
            "score": 0.5293241217721023,
            "section_title": "Remark 3",
            "char_start_offset": 19508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 691,
                    "end": 696,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1516,
                    "end": 1520,
                    "matchedPaperCorpusId": "14421595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00734710693359375
        },
        {
            "corpus_id": "265296903",
            "title": "DB-GPT: Large Language Model Meets Database",
            "text": "Motivation Apart from text prompts, some database tasks (e.g., physical query plan generation) require non-textual information that is not readily expressible in natural language. Moreover, fine-tuning can enhance the task-specific performance of LLM. However, the effectiveness of the finetuned model is significantly influenced by the size and quality of the labeled data samples. Methodology First, we can train non-text embeddings during fine-tuning and combine them with natural language embeddings. Second, we should make good utilization of existing data for model fine-tuning, and continuously collect new fine-tuning data from application feedbacks of LLM. Third, we explore new methods to enhance LLM fine-tuning.",
            "score": 0.5290994661122428,
            "section_title": "LLM Fine-Tuning",
            "char_start_offset": 14204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3837890625
        },
        {
            "corpus_id": "276574886",
            "title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model",
            "text": "During the fine-tuning process, we adopt the instruction tuning strategy to enhance the adaptability of LLMs, which is particularly effective when handling diverse types of data and tasks. Given that our dataset features descriptions at multiple levels and our model is fine-tuned for both Verilog understanding and generation tasks, there is diversity in both the data types and tasks. To accommodate this diversity, we carefully design specific instructions for each scenario, ensuring the model can adjust its output to align with the intended instructions. Figure 3 illustrates how we combine various settings, e.g., task type, granularity, and comment level, to construct tailored instructions for each specific scenario, fostering a structured approach to instruction-based tuning that optimizes the finetuning efficacy. For details on the instructions for different scenarios, please refer to Appendix G.",
            "score": 0.5288013933414545,
            "section_title": "INSTRUCTION TUNING DATASET",
            "char_start_offset": 24173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 911
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2421875
        },
        {
            "corpus_id": "274656115",
            "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between Professional and Non-Professional Responses",
            "text": "Instruction tuning combines the best aspects of pre-train fine-tuning and prompting approaches via supervised fine-tuning (Wei et al., 2021). In this way, a model is trained to sequentially predict each token in the output, given the instruction and input (Ouyang et al., 2022;Muennighoff et al., 2022;Taori et al., 2023;Berkeley et al., 2023). Some other domain language models apply instruction tuning methods to solve specific tasks or scenarios, such as information extraction (Wang et al., 2023), sentiment analysis (Varia et al., 2023), medical dialogue (Li et al., 2023), and code generation (Luo et al., 2023). To quickly adapt LLMs to downstream tasks, efficient fine-tuning techniques, such as addition-based (Schick and Sch\u00fctze, 2021), specification-based (Ben Zaken et al., 2022), and re-parameterization-based (Hu et al., 2022), optimize a small fraction of parameters. Despite the progress mentioned above, the ability of LLMs to switch between professional and non-professional responses has not been fully explored, particularly from the perspectives of domain and stylistic knowledge. 3 Preliminaries",
            "score": 0.5281772664944393,
            "section_title": "LLM Instruction Fine-Tuning",
            "char_start_offset": 5445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1117
                }
            ],
            "ref_mentions": [
                {
                    "start": 767,
                    "end": 791,
                    "matchedPaperCorpusId": "231672601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318359375
        },
        {
            "corpus_id": "269587793",
            "title": "Empirical Studies of Parameter Efficient Methods for Large Language Models of Code and Knowledge Transfer to R",
            "text": "The recent advancements in the field of Natural Language Processing (NLP) have been marked by the emergence and development of Large Language Models (LLMs).Notable examples include the T5 model [43], Llama 2 [55], and GPT-3 [5].These models have shown significant improvement across diverse NLP tasks, such as question answering and summarization, yielding promising results compared to the prior models.The success of these generative LLMs in NLP domains has motivated researchers to direct their attention towards exploring their potential in Software Engineering (SE) tasks.Particularly, studies have been undertaken to harness the capabilities of these architectures in code-related tasks, such as code generation [46,60] and code summarization [60], and have improved the performance of the obtained results.\n\nDespite the promising results demonstrated by code-LLMs in various software-related tasks, it is imperative to acknowledge the substantial training time and computational resources required for their fine-tuning in specific domains or tasks.Considering the larger models with billions of parameters, the heavy computation costs of fine-tuning are not available to everyone.This requirement leads to a dedicated line of research aimed at addressing the efficiency of parameter tuning.This research niche focuses on the development of Parameter Efficient Fine-Tuning (PEFT) methods, with notable contributions from studies such as LoRA by Hu et al. [20], Compacter by Mahabadi et al. [34], and 3 by Liu et al. [28].These methods aim to mitigate the resource-intensive nature of fine-tuning processes, thus, facilitating the practical applicability and scalability of LLMs in real-world scenarios.\n\nIn software engineering, there are various studies [30,47,48,57,58,61] evaluating the performance of PEFT methods.Some of these works employ smaller models with millions of parameters such as CodeBERT [14] and GraphCodeBERT [17] while others are specifically targeting larger models with billions of parameters [61].Previous works also investigate the knowledge transfer from natural language models to SE tasks in smaller language models [16].",
            "score": 0.5278898481850902,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 813
                },
                {
                    "start": 815,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1188
                },
                {
                    "start": 1188,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1528
                },
                {
                    "start": 1528,
                    "end": 1709
                },
                {
                    "start": 1711,
                    "end": 1825
                },
                {
                    "start": 1825,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "235356070"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "248693283"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "265054856"
                },
                {
                    "start": 1772,
                    "end": 1775,
                    "matchedPaperCorpusId": "263648487"
                },
                {
                    "start": 1775,
                    "end": 1778,
                    "matchedPaperCorpusId": "257771620"
                },
                {
                    "start": 1912,
                    "end": 1916,
                    "matchedPaperCorpusId": "211171605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07611083984375
        },
        {
            "corpus_id": "268385476",
            "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
            "text": "Last, we test whether the generalization ability of fine-tuned models is preserved on cross-task testing datasets (Setting 3).The evaluation for each testing set is performed using models fine-tuned on 2K training samples from other tasks.Due to space limitations, our analysis is confined to five test sets: XSum, Socialqa, Amazon, Paws, and MNLI-1.The following findings are mainly based on the first row of Figure 3:\n\nThe generalization through fine-tuning exhibits significant variability and highly depends on the training data.When assessing performance on classification tasks, i.e., Amazon, MNLI, and Paws, it becomes evident that the model's fine-tuning source greatly affects its efficacy.Fine-tuning on a dataset like Amazon negatively impacts performance on the MNLI-1 test set, whereas fine-tuning on XSum significantly boosts it; see Figure 3 (b).In parallel, for generation tasks, training on Socialqa hurt the performance on XSum while training on XSum has little impact on Socialqa; see Figure 3 (d) and (e).These intricate patterns suggest that the effectiveness of fine-tuning is not easily predictable and likely intertwines with dataset characteristics, fine-tuning procedures, etc.\n\nModels fine-tuned on classification tasks fail to generalize to generation tasks.From Figure 3 (d) and (e), fine-tuning the models on classification data leads to almost zero Rouge-L scores for generation tasks.Upon examining the outputs, it becomes evident that these models predominantly generate classification labels rather than coherent text, a manifestation of output space specialization, which aligns with the findings of Wang et al. (2023c).Two potential reasons exist.Firstly, the model's output space may be constrained to the category labels seen during fine-tuning, inhibiting its ability to generate other tokens.The second may be induced by the prompt format, as listed as Prompt-1 in Table 5.The prompts for different tasks have the same start \"###\", which could confuse the model and cause it to misinterpret inputs from other tasks as if they belong to its training task.",
            "score": 0.5275163560037808,
            "section_title": "Different Tasks",
            "char_start_offset": 17012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 419
                },
                {
                    "start": 421,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1203
                },
                {
                    "start": 1205,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1416
                },
                {
                    "start": 1416,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1683
                },
                {
                    "start": 1683,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2094
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "267657671",
            "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models",
            "text": "While the optimization loss decreases with increased model size, the task performance exhibits complex behavior, showing significant improvement only after a model exceeds a threshold in size, a phenomenon known as emergent abilities (EAs) (Wei et al., 2022a). Recently, the emergence has gained increasing attention in LLMs, first within the technique of 'chain-of-thought' on the performance of solving complex mathematical problems (Wei et al., 2022b). According to (Schaeffer et al., 2023), the scope of 'emergence' is defined as: 1). Large model Only: abilities that can not be found in small models; 2). Sudden change on monitored metrics: the model's behavior has a sudden change, which means it cannot be inferred from the small model's performance. \n\nIn emergence interpretbility area (Fu et al., 2023) Furthermore, LLMs (e.g., ChatGPT, LLaMA) encounter limitations in domain-specific tasks. They often lack depth and accuracy in specialized areas, and exhibit a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models (Gaur & Saunshi, 2023;Zheng et al., 2024;Chan et al., 2023). According to Wei et al. (2023), smaller LLM relies primarily on semantic priors from pretraining when presented with flipped in-context labels, whereas larger models can override priors, demonstrating reduced reliance on semantic priors even with potentially stronger priors due to larger scale.",
            "score": 0.5274148381753129,
            "section_title": "Related Work",
            "char_start_offset": 7196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1430
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 435,
                    "end": 454,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 794,
                    "end": 811,
                    "matchedPaperCorpusId": "256390607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0102081298828125
        },
        {
            "corpus_id": "270710907",
            "title": "Leveraging LLMs for Dialogue Quality Measurement",
            "text": "As base language models grow in size (Touvron et al., 2023;Zhang et al., 2022), researchers frequently turn to parameter-efficient fine-tuning techniques to tailor models for specific downstream tasks.These fine-tuning approaches typically fall into three main categories:\n\n(1) Prefix-Tuning: This method inserts special tokens among input tokens with trainable embeddings for the task at hand (Li and Liang, 2021).\n\n(2) Adapter Tuning: This approach inserts adapter layers between self-attention and MLP modules, providing nuanced control over the model's behavior without altering the core architecture (Houlsby et al., 2019;Zhang et al., 2023).\n\n(3) Low-Rank Adaptation: This technique uses trainable low-rank decomposition matrices in each network layer, simplifying the model for efficient fine-tuning (Hu et al., 2021).It shows promise in adapting large generative models for specific applications (Cuenca and Paul, 2023;Zhang et al., 2023).\n\nThese strategies reflect ongoing efforts to make large-scale models more adaptable and efficient, leveraging their vast capacities while mitigating computational and practical challenges.In-context learning.In-context learning is a prompting technique for LLMs in which example input-output pairs for some task are injected into the prompt before the target input is presented.The idea is that seeing correct examples of the task will help the model to provide a correct target output.\n\nSelecting in-context examples is crucial for effectively prompting LLMs, enabling them to pivot to new tasks without extensive fine-tuning.Examples play a pivotal role in guiding LLMs' predictive capabilities, with research exploring methods such as semantic proximity evaluation (Liu et al., 2021) and retrieval mechanisms such as BM25 (Robertson et al., 2009), used independently or in an initial training phase for a selector retriever.\n\nThese selection approaches excel in few-shot NLP tasks.For instance, in Su et al. (2022), a bifurcated framework effectively annotates and selects in-context samples from unlabeled repositories, achieving impressive performance across various tasks.",
            "score": 0.5269235445603018,
            "section_title": "Related work",
            "char_start_offset": 4771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 272
                },
                {
                    "start": 274,
                    "end": 415
                },
                {
                    "start": 417,
                    "end": 647
                },
                {
                    "start": 649,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 947
                },
                {
                    "start": 949,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1326
                },
                {
                    "start": 1326,
                    "end": 1434
                },
                {
                    "start": 1436,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1875
                },
                {
                    "start": 1877,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 627,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1773,
                    "end": 1797,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1683349609375
        },
        {
            "corpus_id": "259342486",
            "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
            "text": "Although prompting LLMs has demonstrated remarkable few-shot performance on unseen downstream tasks, recent studies demonstrated that prompting strategies have much poorer zero-shot ability [123]. To address the limitations, instruction tuning is proposed to fine-tune LLMs over multiple task-specific prompts. In other words, instruction tuning possesses features of both prompting and pre-training & fine-tuning paradigms. This helps LLMs gain better capabilities of exactly following prompts as instructions for diverse downstream tasks, which hence contributes to the enhanced zero-shot performance of LLMs on unseen tasks by accurately following new task instructions. The key insight of instruction tuning is to train LLMs to follow prompts as task instructions, rather than to solve specific downstream tasks. More specifically, instruction tuning can be divided into two stages: \"instruction\" (i.e., prompt) generation and model \"tuning\", since the straightforward idea of instruction tuning is the combination of prompting and fine-tuning LLMs. \n\n\u2022 Instruction (Prompt) Generation Stage. Formally, instruction tuning introduces a format of instructionbased prompt in natural language, which consists of task-oriented input (i.e., task descriptions based on task-specific dataset) and desired target (i.e., corresponding output based on task-specific dataset) pairs. Considering the instruction tuning of LLMs for downstream recommendation tasks, Zhang et al. [20] propose a recommendation-oriented instruction template, including user preferences, intentions, and task forms, which serves as a common template for generating instructions for various recommendation tasks. More directly, three-part instruction templates in the form of \"task description-input-output\" are used in [68], [70] to generate instructions based on task-specific recommendation datasets. \n\n\u2022 Model Tuning Stage. The second stage is to fine-tune LLMs over multiple aforementioned instructions for downstream tasks, where we categorize the existing works on RecSys, as shown in Table 3, according to the LLMs fine-tuning manners: full-model tuning and parameter-efficient model tuning (see Section 4.2 for explanations), since basically the same principles of finetuning LLMs are adopted in this stage. For example, Bao et al. [68] utilize LoRA to make the instruction tuning of LLaMA more lightweight for downstream recommendation tasks.",
            "score": 0.5269097338207054,
            "section_title": "Instruction Tuning",
            "char_start_offset": 56471,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2284
                },
                {
                    "start": 2285,
                    "end": 2420
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412353515625
        },
        {
            "corpus_id": "271962811",
            "title": "Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress",
            "text": "LLM circularity. The issue of LLM circularity, where models influence other LLMs through distilled data, has recently gained attention. Research to date has focused on two main areas: model degradation via recursive training [Dohmatob et al., 2024a;Briesch et al., 2023;Shumailov et al., 2023] and self-preference in an LLM-as-a-Judge setting. Regarding model degradation, studies have shown that training LLMs with data iteratively generated by other LLMs impairs performance as the tails of the original distribution start to disappear. This includes work focusing solely on highfrequency contexts, thereby neglecting long-tail knowledge [Briesch et al., 2023;Bertrand et al., 2024;Shumailov et al., 2024;Dohmatob et al., 2024b], and resulting in a loss of diversity [Guo et al., 2024;Feng et al., 2024]. In [Shimabucoro et al., 2024], the authors explore how the transfer of characteristics via passive inheritance occurs when synthetic data generated by different LLMs is involved. By considering the issues highlighted in these studies, we aim to optimize synthetic data generation by selecting the most calibrated teacher model from a pool of LLMs in a multilingual setting. \n\nInstruction Fine-tuning (IFT). IFT on a large collection of tasks has become a key paradigm for enhancing LLM performance, increasing their utility [Sanh et al., 2021;Wei et al., 2021;Mishra et al., 2021;Min et al., 2021;Ouyang et al., 2022], and enabling generalization to unseen tasks [Wei et al., 2022;Chung et al., 2022].",
            "score": 0.5266252375974283,
            "section_title": "Related Work",
            "char_start_offset": 33874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "258615591",
            "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
            "text": "Large language models (LLMs) have shown an uncanny ability to handle a wide variety of tasks such as text generation [1,2,6,26], translation [36,42], and summarization [19]. The recent fine-tuning of LLMs on conversations and the use of techniques like instruction fine-tuning [4] and reinforcement learning from human feedback (RLHF) [3] led to Fig. 1. Zero-shot performance for rating prediction of LLMs with different model sizes, including Flan-T5 (base to XXL), GPT3 (Curie, Davinci) and Flan-U-PaLM. We see a performance gain when increasing the model size. Among which, models greater than 100B (Flan-U-PaLM 540B and text-davinci-003 175B) outperform or on-par with the global average rating baseline on both RMSE and AUC-ROC. \n\ntremendous success to bring highly human-like chatbots (e.g. ChatGPT [23], and Bard [10]) to the average household. \n\nThere are three key factors that directly contribute to LLMs' versatility and effectiveness: \n\n(1) Knowledge from internet-scale real world information: LLMs are trained on enormous datasets of text, providing access to a wealth of real-world information. This information is converted to knowledge that can be used to answer questions, creatives writing (e.g., poems, and articles), and translate between languages. \n\n(2) Incredible generalization ability through effective few-shot learning: within certain context, LLMs are able to learn new tasks from an extremely small number of examples (a.k.a., few-shot learning). Strong few-shot learning capability gears up LLMs to be highly adaptable to new tasks. \n\n(3) Strong reasoning capability: LLMs are able to reason through a chain-of-thought process [40,41], significantly improving their performance across many tasks [37]. \n\nRecently, there has been some early exploratory work to make use of LLMs for Search [20], Learning to Rank [11,45], \n\nand Recommendation Systems [5,8,18].",
            "score": 0.5265897602613692,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 120,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "201058686"
                },
                {
                    "start": 1846,
                    "end": 1849,
                    "matchedPaperCorpusId": "235166906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11553955078125
        },
        {
            "corpus_id": "271909136",
            "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
            "text": "In this study, we introduce Flexora, a method designed to enhance the efficiency and effectiveness of fine-tuning large language models (LLMs). Flexora automatically and flexibly selects the most critical layers for fine-tuning, addressing overfitting and enhancing performance by modeling layer selection as an HPO problem and employing UD to solve it. This targeted fine-tuning approach reduces computational overhead by updating only selected layers. Extensive experiments demonstrate Flexora reduces parameters, suppresses overfitting, and enhances performance, outperforming baseline methods. We also provide a theoretical explanation for these improvements. \n\nFuture work will focus on the following areas: (a) Exploring the relationship between individual layers of the LLM and their impact on downstream tasks to theoretically elucidate the role of each layer in reasoning processes. (b) Investigating optimal fine-tuning schemes for Flexora, inspired by its enhanced performance when integrated with various LoRA-enhanced algorithms, as demonstrated in Appendix C.2. A Theorems and proofs",
            "score": 0.5264614129568206,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 25025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365966796875
        },
        {
            "corpus_id": "260379087",
            "title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation",
            "text": "Based on our results, we have the following main findings. First, for the zero-shot setting, we find that instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also have interesting findings that on code-related tasks LLMs instructed by code domain do not necessarily outperform LLMs instructed by general domain, and larger instructed LLMs are not always better. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. In addition, we observe a performance drop with the increasing input length and an increasing instructionfollowing capability in the few-shot setting. Furthermore, we find the widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems (e.g., assertion generation and code summarization), while exhibiting no significant difference from either basic strategies on classification problems (e.g., defect detection or clone detection). Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstream code comprehension and generation tasks compared to the zero-shot/one-shot performance. In addition, after being finetuned on the same downstream task dataset, instructed LLMs outperform both the small SOTA models and similar-scaled LLMs without instruction tuning, suggesting the large benefits of the instruction tuning. Lastly, we find that similar-scaled instruction-tuned LLMs vary in memory costs and time costs; while they do not necessarily take more memory resources than small SOTA models, but take much more time costs than small SOTA models in both fine-tuning and inference. Based on our findings, we further present practical implications on model and usage recommendation, performance and cost trade-offs, and future directions. \n\nIn summary, this work makes the following contributions: \n\n\u2022 To the best of our knowledge, this paper serves as the first study on evaluating open-source instruction-tuned LLMs on code comprehension and generation tasks.",
            "score": 0.5263643642439817,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 5296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2190
                },
                {
                    "start": 2193,
                    "end": 2354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1822509765625
        },
        {
            "corpus_id": "277955495",
            "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
            "text": "The rapid evolution of Large Language Models (LLMs) (Zhao et al., 2023;Chang et al., 2024) has exhibited remarkable proficiency in general reasoning (Kojima et al., 2022;Zheng et al., 2023), problem-solving (Lewkowycz et al., 2022;Yao et al., 2024), and natural language understanding (Wei et al., 2022a). These models have demonstrated the ability to perform a broad range of tasks across diverse domains, often with minimal taskspecific training. However, their immense size and general-purpose training can make them less effective in specialized tasks or domains that are under- to proprietary information (Fu et al., 2023). This limitation poses a significant challenge: how can we extend the problem-solving spectrum of LLMs to encompass these niche but critical tasks? \n\nDirectly training or fine-tuning large models for every specific domain or task is often impractical due to the following two key reasons. First, some popular LLMs (e.g., GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023)) are black-box models, with their internal parameters inaccessible for modification. Even when fine-tuning is possible, it can be costly and raises concerns about scalability as models continue to grow in size, such as those models exceeding 70 billion parameters. Additionally, fine-tuning LLMs on private data can pose security and privacy risks. Specifically, fine-tuning requires exposing the model to potentially sensitive data, which could inadvertently be memorized or leaked through the model's outputs. This exposure creates a risk of violating data privacy regulations and necessitates robust measures to ensure data confidentiality and compliance. \n\nTo overcome these challenges, we aim to leverage a collaborative framework that synergizes a small-sized weak model with a large-sized strong model. In this paradigm, the weak model is tailored with specialized problem-solving abilities in specific domains. Conversely, the strong model boasts robust general capabilities, excelling in tasks that require broad knowledge and advanced reasoning. By orchestrating a collaboration between these two models, we leverage their complementary strengths to tackle specific tasks more effectively than either could achieve independently.",
            "score": 0.5255101182688392,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 170,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 207,
                    "end": 231,
                    "matchedPaperCorpusId": "250144408"
                },
                {
                    "start": 231,
                    "end": 248,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 610,
                    "end": 627,
                    "matchedPaperCorpusId": "256390607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07476806640625
        },
        {
            "corpus_id": "268793422",
            "title": "Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching",
            "text": "The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats.",
            "score": 0.5253968469165156,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35498046875
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "Beyond the pre-training objective, recent studies have aimed to model human preferences to better align LLMs with human expectations.For example, the well-known InstructGPT (Ouyang et al., 2022) introduces reinforcement learning from human feedback (RLHF), which uses preference rewards as an additional training objective.Although RLHF is effective at making LLMs more helpful to users, it inevitably incurs an alignment tax, which refers to performance degradation after RLHF.Recent research has explored various techniques to mitigate alignment tax issues (Lin et al., 2023;Lu et al., 2024b;Fu et al., 2024b).Emergent Abilities The fundamental capability of language models is text generation, where tokens are auto-regressively generated based on preceding tokens using greedy search or nucleus sampling (Holtzman et al., 2020a):\n\nInterestingly, LLMs can not only generate realistic text but also perform specific tasks when provided with task-specific prompts, without requiring fine-tuning on particular downstream tasks (Brown et al., 2020).This phenomenon is one of the most important differences between LLMs and previous PLMs.Wei et al. (2022b) define the emergent ability as \"an ability that is not present in smaller models but is present in larger models.\"Among these emergent abilities, in-context learning (ICL) (Brown et al., 2020;Dong et al., 2022) and instruction following are commonly used and significantly enhance the ability of LLMs to process various tasks.\n\nICL helps LLMs understand tasks by using several task examples as demonstrations.When provide these demonstrations as prompts, LLMs can automatically generate reasonable output for the given test example, which can be formalized as:\n\nInstruction following ability are typically emerge in LLMs that have been fine-tuned on examples formatted with instructions on multiple tasks.The generation process can be formalized as:\n\nwhere I refers to the given instruction for current example x.",
            "score": 0.5252226952682815,
            "section_title": "Large Language Models",
            "char_start_offset": 5848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 833
                },
                {
                    "start": 835,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1481
                },
                {
                    "start": 1483,
                    "end": 1564
                },
                {
                    "start": 1564,
                    "end": 1715
                },
                {
                    "start": 1717,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 1904
                },
                {
                    "start": 1906,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 194,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 808,
                    "end": 832,
                    "matchedPaperCorpusId": "127986954"
                },
                {
                    "start": 1027,
                    "end": 1047,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1136,
                    "end": 1154,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1327,
                    "end": 1347,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289764404296875
        },
        {
            "corpus_id": "264289232",
            "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs",
            "text": "Selective Prediction for LLMs. Recently, LLMs (e.g., GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022)) have achieved great success in solving various kinds of Natural Language Generation (NLG) tasks. However, LLMs are still not very reliable and may generate wrong outputs when solving NLG tasks. Due to this, selective prediction (or sometimes called selective generation (Ren et al., 2022)) is critical for safely deploying LLMs in the real-world. Different from selective prediction for classification tasks (e.g., Natural Language Inference (NLI) tasks) (Xin et al., 2021), selective prediction for LLMs in solving NLG tasks is fundamentally different since the prediction is done auto-regressively over many steps and the possible answer set has an infinite size. Recently, several work propose some uncertainty measures for LLMs, which can be used for selective prediction (Si et al., 2022;Kadavath et al., 2022;Varshney et al., 2022;Ren et al., 2022;Kuhn et al., 2023). Some recent work studies selective prediction for solving question answering tasks where questions are ambiguous (Cole et al., 2023;Yin et al., 2023). Varshney and Baral (2023) propose a selective prediction method that at inference time leverages an auxiliary model which is trained to distinguish the correct predictions of the QA model from the incorrect ones. Different from previous work, our work proposes to improve selective prediction performance of LLMs in solving question answering tasks by learning self-evaluation during fine-tuning. Parameter Efficient Fine-tuning. Fine-tuning pretrained LLMs on downstream datasets can bring huge performance gains when compared to using the pretrained LLMs out-of-the-box (e.g., kshot inference). However, as LLMs get larger and larger, full fine-tuning becomes very expensive in terms of computational cost and memory requirements. In addition, massive models might not be data efficient and overfitting issues might be observed, yielding suboptimal generalization.",
            "score": 0.5247821456579704,
            "section_title": "Related Work",
            "char_start_offset": 5772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 963,
                    "end": 981,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "271843111",
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "text": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
            "score": 0.5246226864594903,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 42602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07147216796875
        },
        {
            "corpus_id": "276259083",
            "title": "Logits are All We Need to Adapt Closed Models",
            "text": "The ability to adapt closed-source LLMs without modifying their weights has significant implications for both research and industry. Our proposed method, which leverages token logits for task-specific alignment, offers a practical solution for developers constrained by black-box APIs. This approach enhances customization, allowing models to generate more domain-relevant and controlled content while preserving the privacy and security of proprietary data. Furthermore, by advocating for broader access to token logits, this work fosters greater transparency and flexibility in commercial LLMs. \n\nThe findings also highlight the importance of mitigating biases in black-box models, contributing to more equitable and context-aware language generation across diverse applications. \n\nWhile Plugin effectively adapts black-box LLMs, it has some limitations, too. Since it only reweights token probabilities without modifying internal representations or embeddings, it may struggle with tasks requiring deep structural adaptations, such as executing complex reasoning. Further research on this aspect is needed. Additionally, although Plugin avoids full fine-tuning, training a separate reweighting model introduces computational overhead compared to prompt tuning or in-context learning, with efficiency depending on the complexity of the reweighting model and the availability of task-specific data.",
            "score": 0.5243026648334786,
            "section_title": "Impact Statement",
            "char_start_offset": 25414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0762939453125
        },
        {
            "corpus_id": "272398082",
            "title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL",
            "text": "There is a irresistible necessity from enterprises for fine-tuning LLMs to get them trained on proprietary domain knowledge. Fine-tuning is the process of continuing the training of an already pre-trained model on a new dataset that is typically smaller and task-specific [10]. This allows the model to adjust its weights and parameters to better fit the nuances of the new data and the specific requirements of the target task. Though there is an option to use OpenAI (opensource) models to solve most of the use-cases, there is a high demand for domain specific LLMs due to data privacy and pricing concerns as mentioned earlier. \n\nThe data of an enterprise can stay on premise as the LLMs are also present on premise. In-house development ensures this. Fine-tuned LLMs provide quality and custom feel to the stakeholder and also has low latency in displaying the results. [10] 2.5 Making LLMS suitable for specific tasks through Retrieval Augmented Generation \n\nWhen a LLM model that is not fine-tuned is used for domain specific tasks and asked to handle queries beyond its training data or current information, hallucinations can happen [11]. As fine-tuning can be done to make LLMs suitable for specific use-cases, another approach next to or instead of fine-tuning can be to use Retrieval-Augmented Generation (RAG) architecture. RAG enhances LLMs by retrieving relevant document chunks (in real time) from external knowledge bases through semantic similarity calculation [12]. So RAG retrieves additional data, and augments it to the existing knowledge of the LLM based on semantic similarity. In fine-tuning, the weights of the existing parameters of the LLM get adjusted to the learned knowledge, but vectors are not added. Therefore to keep the knowledge base updated, fine-tuning would be computationally expensive.",
            "score": 0.5242029864116806,
            "section_title": "Making LLMs suitable for specific tasks through fine-tuning",
            "char_start_offset": 11013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2861328125
        },
        {
            "corpus_id": "273228247",
            "title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation",
            "text": "This study encountered several limitations that future research should address. Firstly, computational resource constraints hindered our ability to utilize larger language models. Consequently, we focused on smaller yet powerful models that were more feasible for our research scope. \n\nEvaluating descriptive questions posed a significant challenge. While we attempted various methods for assessing these questions, incorporating more extensive human evaluation would be beneficial. Although this approach is more resourceintensive and time-consuming, it could yield valuable insights into model performance. \n\nWhile our dataset effectively captures the essential categories and fundamentals of the theory of computation, it lacks coverage of more applied tasks, such as code generation. Future research could investigate how fine-tuned, specialized models impact performance in related domains like code generation, reasoning, and mathematical problem-solving. It would be particularly interesting to explore the extent to which domain-specific fine-tuning may affect a model's general capabilities.",
            "score": 0.523973059566717,
            "section_title": "Limitations",
            "char_start_offset": 16441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1100
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.025909423828125
        },
        {
            "corpus_id": "271334678",
            "title": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-task Large Language Models",
            "text": "Large Language Models (LLMs) have become popular models for solving a wide range of generative text problems. LLMs are typically pretrained on large and diverse datasets of texts, and then fine-tuned on new datasets dedicated to specific tasks. The fine-tuning process is often done by maximizing the next token log likelihood of the task-specific dataset, and the resulting model is then used to make predictions and generations for new data. A popular fine-tuning method is Low-Rank Adaptation (LoRA), which reduces the number of trainable parameters by a factor of up to 10,000 and the GPU memory requirement by a factor of 3, while still outperforming other fine-tuning techniques ( [Hu+22]). When performing fine-tuning of LLMs, the practitioner often wants to solve multiple similar tasks at the same time. For example: A media organization might want to use an LLM to generate various textual elements like headlines, keywords, and summaries for a given news article. A website might have multiple chatbot tasks answering user inquiries across various topics. And finally, a scriptwriting AI might want to generate dialogues for different characters in a movie, each with its unique style and vocabulary. \n\nIn these multi-task cases, the practitioner can either (1) train one model for each task and dataset separately, or (2) train one model for all tasks and condition the specific output wanted (e.g. [Raf+20]). There is a trade-off between these two approaches: Train each task separately and the model will be able to specialize on each task, but it may suffer when limited data are available for some tasks. Train one model for all tasks, and the model will be able to share information between tasks, but it may not be able to specialize on each task due to limited model capacity. \n\nIn this paper, we suggest a way to circumvent these trade-offs and present a generalization of the two LoRA fine-tuning methods for multitask problems, which we call the Bayesian Hierarchical Low-Rank Adaptation method (BoRA). The BoRA method allows tasks to share information between each other through a set of global hierarchical prior parameters, in a Bayesian hierarchical setting.",
            "score": 0.5234207593819435,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 687,
                    "end": 694,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1411,
                    "end": 1419,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199951171875
        },
        {
            "corpus_id": "270521667",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "text": "The authors assess the translation performance of raw LLMs, explore their in-context learning abilities, and fine-tune the models using parameter-efficient methods (Balne et al., 2024) such as LoRA and full fine-tuning.They also introduce a two-stage fine-tuning approach for LLMs, involving full parameter fine-tuning followed by LoRa-based adaptor fine-tuning.Raw LLMs, particularly Llama 2 models, demonstrate better zero-shot and examplebased translation capabilities compared to other models.Fine-tuning LLMs enhances their translation capabilities, especially when using multilingual fine-tuning.The two-stage fine-tuning approach, involving full fine-tuning followed by LoRa-based fine-tuning, yields the best results.The authors conclude that LLMs have significant potential for translation tasks involving English and Indian languages, even with limited parallel data.\n\nTamil-LLaMA (Balachandran, 2023), a new LLM for the Tamil language based on the opensource LLaMA model.They expanded LLaMA's vocabulary with an additional 16,000 Tamil tokens to improve its ability to process and generate Tamil text.This was done by training a Tamil tokenizer on a large Tamil text corpus.The expanded model was pre-trained on 12 GB of Tamil text data using the LoRA (Low-Rank Adapters) method for efficiency.For instruction fine-tuning, they translated the Alpaca dataset and a subset of the OpenOrca dataset into Tamil.The models were fine-tuned on this translated instruction data.Evaluation on Tamil language tasks shows the Tamil-LLaMA models outperform the original LLaMA and GPT-3.5-turbo on tasks like question answering, reasoning, coding, and open-ended generation in Tamil.However, the models have limitations like potential to generate harmful content, limited knowledge, and challenges with math and reasoning.The 7B and 13B parameter models and code are being made publicly available to spur further research on LLMs for Tamil and other Indian languages.\n\nSingh Kohli et al. (2023) present the development of a Llama2-finetuned LLM tailored for the low-resource Odia language.",
            "score": 0.5233825862128001,
            "section_title": "Fine-tuned LLMs",
            "char_start_offset": 16392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1417
                },
                {
                    "start": 1417,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1680
                },
                {
                    "start": 1680,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1964
                },
                {
                    "start": 1966,
                    "end": 2086
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "259936794",
            "title": "Franc: A Lightweight Framework for High-Quality Code Generation",
            "text": "Large Language Models (LLMs) are increasingly performing well in many natural language processing tasks, such as text generation, translation, and summarization [1]. One particularly relevant application of these LLMs to software engineering is code generation, in which LLMs are trained and fine-tuned with large amounts of code snippets [2]- [5]. These techniques help developers by generating the implementation of functions/programs based on provided prompts [6]. \n\nAlthough LLM-based code generation techniques may produce functionally correct code, they can contain code smells and vulnerabilities [7]- [9]. A recent study showed that LLMs are fine-tuned with samples containing harmful coding patterns that leak to the generated code [7]. Another study found that GitHub Copilot can produce vulnerable code [8]. With the increasing use of LLM-based code assistants, these problematic code snippets can get deployed into production, negatively affecting the software system's security and reliability. \n\nTo improve the quality of the generated code, we first need a good dataset (i.e., free of quality issues) [10]. However, collecting training data is time-consuming [11] and challenging because code datasets commonly used for training contain quality issues (e.g., bugs, vulnerabilities, and code smells [7], [12], [13]) which would require vetting these code samples to remove or repair low quality training samples. Moreover, fine-tuning an LLM model is a resource-hungry process [14] which requires at least one GPU, and pre-training is typically performed on a large cluster of GPUs. Although models can make inferences without GPUs, the throughput may not be optimal. For example, we used a GPU with 24 GB RAM when using a model with 2.7 billion parameters to generate code with 128 tokens (and up to 2,048 context tokens). To generate more tokens and/or larger models, more GPUs or a more expensive GPU with a larger RAM would be needed. \n\nAlthough LLM fine-tuning can be done in the cloud to avoid acquiring expensive GPUs, it is still costly.",
            "score": 0.5230256979569006,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "258557362"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "250280117"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "211096967"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "252125574"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "252125574"
                },
                {
                    "start": 814,
                    "end": 817,
                    "matchedPaperCorpusId": "245220588"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "11392154"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "252125574"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "4715468"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "215767620"
                },
                {
                    "start": 1491,
                    "end": 1495,
                    "matchedPaperCorpusId": "239890333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037750244140625
        },
        {
            "corpus_id": "272996865",
            "title": "Repeatability of Fine-Tuning Large Language Models Illustrated Using QLoRA",
            "text": "Large Language Models (LLMs) developed in the last few years have shown human-level performance for some tasks such as dialogue-based chat bots [1]. The main building block of LLMs is the transformer architecture [2] that enables deep neural networks to learn from sequences of data more efficiently than Long Short-term Memory (LSTM) recurrent neural networks [3]. As a result, many companies have started the race to develop LLMs using a corpus of data freely available on the internet. Some of the developed LLMs hold promise as AI assistants that are able to interact efficiently with humans through dialogue-based chat sessions. Other promising LLMs have been developed for text \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Daniel Augusto Ribeiro Chaves . \n\nsummarization, translation, and classification such as T5 [4], SwitchTransformers [5], ChatGPT [6], and Llama2 [7]. These LLM models are created with self-supervised training on a data corpus followed by alignment using methods such as Reinforcement Learning with Human Feedback (RLHF) [8]. \n\nFine-tuning LLMs is an efficient approach to improve LLMs' performance [9], [10], [11], especially for datasets of a new domain, or to improve LLMs behaviour or remove undesirable behaviour [11]. A current downside for fine-tuning LLMs is the need for high amounts of memory and computational costs, primarily available to large technology companies. To overcome this obstacle, different approaches have been proposed to reduce pre-trained model sizes for fine-tuning such as low-rank adaptation (LoRA) [12] and quantized low-rank adaptation (QLoRA) [13]. By modifying the pre-trained model's weight matrix these approaches can effectively reduce memory requirements without a loss of performance. \n\nRepeatability is a critical consideration for maintaining the performance and behaviour of LLM fine-tuning. Repeatability is considered the ability to obtain the same performance after retraining (re-fine-tuning) the model multiple times on same hardware with the same software settings [14], [15].",
            "score": 0.52292129077175,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 879,
                    "end": 882,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1196,
                    "end": 1200,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297607421875
        },
        {
            "corpus_id": "267522853",
            "title": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach",
            "text": "GPT-3.5 or GPT-4) have shown superior domain generalization ability (Wang et al., 2023;Yang et al., 2023) over the fine-tuned model, we introduce an innovative approach based on the programmer-interpreter framework (Reed and de Freitas, 2016), which benefits from the domain generalization ability from LLMs. The program-mer component -a smaller language model finetuned on task-specific data -delivers precise edit instructions to the larger language model, thus infusing the large model with task-specific knowledge. The interpreter, in turn, edits the large model's output given the provided instructions. Contrary to the Self-Correct (Welleck et al., 2022) approach that utilizes smaller, fine-tuned models for editing, our interpreter is also an LLM. The editing is accomplished through the use of prompts that include editing instructions, eliminating the need for any additional fine-tuning. This distinct framework guarantees the preservation of the LLM's domain generalization ability while simultaneously benefiting from the task-specific knowledge encoded by the programmer. Our method distinguishes itself from approaches like PiVe (Han et al., 2023), which also employ an LLM as the interpreter but focus on graph generation tasks. In contrast, our approach specifically designs word-level editing actions in the instructions, tailored to enhance text generation. This targeted strategy renders our method more effective for text-generation tasks. \n\nOverall, our key contributions are as follows: \n\n\u2022 We introduce a novel programmer-interpreter method that enhances LLM in low-resource cross-domain text generation tasks. This approach capitalizes on the programmer's ability to encode task-specific knowledge and the interpreter's prowess in domain generalization. \n\n\u2022 We design editing operations optimized for text generation tasks, leading to substantial text quality improvements by simply prompting the LLMs with action instructions. \n\n\u2022 In scenarios where training and test data span different domains, our comprehensive empirical studies confirm that the method outperforms all existing LLM post-editing baselines in low-resource MT and LF-to-Text.",
            "score": 0.5229098232724376,
            "section_title": "Introduction",
            "char_start_offset": 1584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 87,
                    "matchedPaperCorpusId": "257102461"
                },
                {
                    "start": 215,
                    "end": 242,
                    "matchedPaperCorpusId": "7034786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028656005859375
        },
        {
            "corpus_id": "261048648",
            "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
            "text": "produce accurate and contextually appropriate code. This can lead to syntactically correct but functionally incorrect code, impacting the reliability and effectiveness of LLM-based code generation. Addressing this issue requires exploring techniques to incorporate additional context, domain-specific knowledge, or multi-model ensembles to improve LLMs' ability to handle ambiguity and generate precise code, ensuring their successful integration into real-world software development processes.\n\n8.1.2 Challenges in LLM Generalizability. The generalizability of LLMs refers to the ability of these models to consistently and accurately perform tasks in different tasks, datasets, or domains outside their training environment. While LLMs are trained on massive amounts of data, ensuring extensive knowledge capture, their performance is sometimes problematic when confronted with specific or idiosyncratic tasks outside the scope of their training. This challenge is particularly evident in the SE domain, where we present the application of LLMs to 55 SE tasks in Section 6. We observed that the context and semantics of code or documents vary greatly across projects, languages, or domains. Ensuring that the LLM generalizes well requires careful fine-tuning, validation on different datasets, and continuous feedback loops. Without these measures, models run the risk of over-adapting their training data, thus limiting their usefulness in a variety of real-world applications. Recent studies have shown that the LLMs cannot generalize their good performance to inputs after semanticpreserving transformations. For example, Yang et al. [365] show that the performance of CodeBERT on different tasks decreases significantly after substituting the variables' names in the input.",
            "score": 0.5216736181107792,
            "section_title": "Challenges in LLM Applicability.",
            "char_start_offset": 120325,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1639,
                    "end": 1644,
                    "matchedPaperCorpusId": "246210250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0809326171875
        },
        {
            "corpus_id": "271328767",
            "title": "LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins",
            "text": "Several studies have explored the development of LLMpowered applications, often applying the \"divide and conquer\" principle to design these systems.These can be mainly organized on two levels: Model level: in related works, several frameworks have been developed to manage the complexity.The Chain-of-Thought (CoT) [7]framework facilitates systematic, step-bystep reasoning, while the Tree-of-Thought (ToT) [8] allows LLMs to explore multiple pathways, identifying the most viable solutions.Additionally, the ReAct framework [9] generates reasoning traces and task-specific actions in an interleaved manner, effectively addressing complex tasks.These frameworks can be realized by systematic prompting to guide the reasoning processes of LLMs, enhancing their capability to solve intricate problems.\n\nAlternative methods involve fine-tuning the LLM to modify its text generation behavior.Techniques such as Proximal Policy Optimization (PPO) [10] and Direct Policy Optimization (DPO) [11] adjust the generation policies of the LLM, enhancing its ability to produce logically connected texts and thereby improving its problem-solving capabilities.",
            "score": 0.5215732607977877,
            "section_title": "C. Related works",
            "char_start_offset": 4573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 799
                },
                {
                    "start": 801,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1146
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 984,
                    "end": 988,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09271240234375
        },
        {
            "corpus_id": "272922197",
            "title": "The Impact of Prompting Techniques on the Security of the LLMs and the Systems to Which They Belong",
            "text": "They test three different settings for it-zero-shot (the model is only given a natural language instruction describing the task, but no demonstration is provided), one-shot (the model is given a natural language instruction describing the task alongside a single demonstration of how should the task be performed) and few-shot (the model is given a few demonstrations of the task at inference time as conditioning together with the natural language instruction describing the task)-and compare them with the traditional fine-tuned models specific for each task. Figure 1 shows examples of how every one of these settings would look in practice. It is important to note the fact that this paper does not militate against fine tuning but rather presents an alternative method that could be used even alongside fine tuning. More than that, this highlights the incredible ability of the LLMs to adapt to new kinds of tasks as well. The results of the paper show the performance of this method on different NLP benchmarks in all three settings compared to the traditional fine-tuning approach. The few-shot setting in some cases outperforms the previous fine-tuned state of the art, or it is close enough to at least take into consideration if we have to choose between the two. \n\nFrom a security point of view, this technique could be especially helpful if we would like to teach our LLM to respond in a specific way to prompts that we consider undesirable or malicious. We also have to consider the fact that this method of teaching the LLM to behave in a certain way could be as well used by an attacker who could perhaps try to overload the existing context of our model with examples of how he would want the task to be performed and try in this way to change the initially defined behavior of our LLM. \n\n2.4. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models \"Chain-of-Thought (CoT) Prompting Elicits Reasoning in Large Language Models\" [2] explores the ability of an LLM not only to blindly perform an NLP task but accomplish very good performance on different kinds of tasks such as arithmetic, common sense, and symbolic reasoning [2]. The starting point is based on two ideas.",
            "score": 0.521243819783288,
            "section_title": "Language Models Are Few-Shot Learners",
            "char_start_offset": 10490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.116943359375
        },
        {
            "corpus_id": "266844262",
            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks (Brown et al., 2020). There is an increasing trend of releasing pre-trained LLMs and fine-tuned variants (Touvron et al., 2023a,b). Many of these fine-tuned variants aim to augment the knowledge or linguistic capabilities of the existing LLM (Roziere et al., 2023;Cui et al., 2023). \n\nWe have noticed that many advancements in finetuned variants adhere to a conventional procedure consisting of two key steps: 1. Conduct further continual pre-training on an existing LLM. 2. Carry out subsequent alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), on the model obtained in Step 1. Among these fine-tuned variants, many developments perform further continual pre-training on existing fine-tuned LLMs (Cui et al., 2023;Lin and Chen, 2023). \n\nPrevious studies have demonstrated that continual pre-training can significantly improve the model's ability to understand and generate specific content (Gupta et al., 2023). However, continual pre-training could lead to catastrophic forgetting (French, 1999), and limited research has explored the abilities forgotten during pre-training on an existing fine-tuned LLM. \n\nSome works have studied continual learning for language models. (Qin et al., 2022) focused on efficient lifelong pre-training on pre-trained language models for emerging data. (Ke et al., 2022) proposed a continual domain-adaptive pre-training method on a masked language model. (Song et al., 2023) introduced continual parameter-efficient tuning for the ongoing adaptation of LLMs to continual tasks. (Xie et al., 2023) investigate an alternative approach to continual pre-training for developing domain-specific LLMs. (Qi et al., 2023) suggests that fine-tuning compromises the safety alignment of LLMs. (Zhai et al., 2023) evaluates the forgetting in fine-tuned multimodal LLMs. \n\nOur work examines the forgetting occurrence during continual pre-training on an existing finetuned LLM.",
            "score": 0.5209979560319491,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 111,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1046,
                    "end": 1066,
                    "matchedPaperCorpusId": "260704601"
                },
                {
                    "start": 1138,
                    "end": 1152,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 1329,
                    "end": 1347,
                    "matchedPaperCorpusId": "247447415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2406005859375
        },
        {
            "corpus_id": "277244427",
            "title": "Text2Model: Generating dynamic chemical reactor models using large language models (LLMs)",
            "text": "Domain-specific languages for interaction with software tools or for data analysis require scientists and engineers to study the conventions and syntax of such languages [1]. While software-user communication may often rely on a graphical user interface, such as AspenPlus or COMSOL, other tools require users to provide specialized code, e.g., MATLAB Simulink or Dymola. In addition to established programming languages for data analysis and visualization, such as Python or R, studying and applying a multitude of different domain-specific languages can be time-and labor-intensive. Recent developments in using large language models (LLMs), particularly for code completion, code synthesis [2], and code analysis and interpretation [3], make such LLMs promising assistants for using tools such as modeling environments by converting textual information into domain-specific languages [1]. State-of-the-art multilingual open models for code generation or understanding, such as CodeT5+ [3], are literate in a range of programming languages in addition to natural language. Still, there are domain-specific languages in which pre-trained LLMs might not be proficient yet. The ability of state-of-the-art LLMs to converse in various programming languages motivates leveraging LLMs for synthesizing domain-specific languages analogously. While there have been propositions that task-specific performance can be increased by few-shot learning [4], fine-tuning is an established approach to tailor LLMs' responses to specific tasks [5], especially for code synthesis [2]. Previous work by Jacobs and Pollice has shown that it is possible to fine-tune GPT 3.5 to generate input files to ORCA, a programming package specific to the quantum chemistry domain [1]. However, there is a lack of literature investigating the potential of LLMs to generate dynamic models for chemical reaction systems from descriptions in natural language. We investigate how fine-tuning an open-source pre-trained LLM on dynamic reactor scenarios written in the domainspecific programming language Modelica affects the semantic and syntactic accuracy of such Modelica models generated by the fine-tuned model.",
            "score": 0.5201421671221533,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042572021484375
        },
        {
            "corpus_id": "272987180",
            "title": "Designing Domain-Specific Large Language Models: The Critical Role of Fine-Tuning in Public Opinion Simulation",
            "text": "Fine-tuning turns general-purpose LLMs into specialized models that can effectively handle domain-specific tasks, in fields like environmental and climate sciences (Objective 3). In this study, fine-tuning significantly improved the LLMs' ability to generate representative responses closely aligning with the real-world data from datasets (Objective 1). This aligns with similar findings from Betka (2023), who optimised fine-tuned models to reduce runtime by 50% while maintaining accuracy within a 5% margin (Betka, A. 2023). \n\nFor example, in this study, fine-tuned models reduced the fraction of people who are highly aware of their environmental impact, a prominent area of bias in pre-trained models (see Fig. A3 on Page 28). This reduction in bias is comparable to the improvements seen from Xu's ChildTuning method, which updates one subset of parameters in the pre-trained network while masking non-child network gradients, improves vanilla pre-trained models by 1. Building on this foundation, including tailored profiling variables such as job insecurity and spending habits revealed additional factors influencing renewable energy spending decisions (Karagiannaki, H. 2024). These profiling variables enhanced the fine-tuned models' ability to identify stakeholders who are against spending more on green tariff electricity (see Fig. A4 on Page 29). However, this inclusion risks creating stereotypes against a particular income group (see Table 2 on Page 13), which calls for more effective metrics, such as Chi-Square tests, adversarial debiasing, and user feedback mechanisms, for addressing both technical and ethical dimensions of the LLMs' performance.",
            "score": 0.5200927554647978,
            "section_title": "Fine-Tuning as a Key Element in LLM Design",
            "char_start_offset": 25969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1671
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2137451171875
        },
        {
            "corpus_id": "260379087",
            "title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation",
            "text": "For example, Alpaca-7B outperforms CodeAlpaca-7B on all the code-relevant tasks except the defect detection, while they are basically tuning the same foundation model LLaMA on the general instructions or codespecific instructions respectively. In addition, we observe that the smaller general instruction-tuned LLMs (e.g., Vicuna-7B) have the chance to outperform the larger code-specific instruction-tuned LLMs (e.g., Instruct-CodeGen-16B) on some tasks. One potential reason might be that the general instructions (e.g., ShareGPT [66]) also contain code-specific instructions and the other code-unrelated instructions can further be helpful for improving the instruction understanding capability of LLMs. In summary, general instruction-tuned LLMs can achieve competitive performance on code comprehension and generation tasks. \n\nThird, we observe that increasing the model scale sometimes brings marginal improvements or sometimes even decreases in the zero-shot setting. For example, Dolly performs much worse on assertion generation and code summarization when the model scale increases from 7B to 12B. In addition, for Vicuna, although enlarging the model scale from 7B to 13B improves the performance on generation tasks such as assertion generation and code summarization, the impact on classification tasks like defect detection or clone detection is rather small. In fact, previous studies [82], [83] also reveal similar observations that Dolly-12B performs worse than Dolly-6B in generative tasks, and ALMoST-7B [83] outperforms Alpaca-13B, Dolly-12B, and OpenAssistant-12B [84]. \n\nFinding 1: In the zero-shot setting, instruction-tuned LLMs are competitive and sometimes even better on code comprehension and generation tasks compared to small SOTA models specifically fine-tuned on each downstream task. In addition, code-instruction-tuned LLMs do not necessarily outperform general-instruction-tuned LLMs on code comprehension and generation tasks, and the improvement from the larger model scale sometimes can be limited or even negative.",
            "score": 0.5200740200642475,
            "section_title": "A. Performance in Zero Shot (RQ1)",
            "char_start_offset": 26851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0574951171875
        },
        {
            "corpus_id": "257631823",
            "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models",
            "text": "Zero-Shot vs. Fine-tuning Recent works have shown that large language models can achieve reasonable performance without any parameter updates [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2021, Smith et al., 2022], often referred to as the zero-shot or few-shot setting. When no parameters are fine-tuned, framing a target task in terms of the pre-training objective enables zero-shot or few-shot learning to use a task-specific prompt and a few examples of a task [Brown et al., 2020]. However, while such few-shot learning is simple using such large models, there are alternative methods to obtain similar task accuracy using smaller models [Schick and Sch\u00fctze, 2021]. In recent work, Cohen et al. [2022] demonstrate that while scaling the size of LaMDA can improve quality, combining scaling with fine-tuning can improve the model across all metrics including quality, safety and groundness. Solaiman and Dennison [2021] show that fine-tuning also helps update language model behaviour to mitigate harmful outputs, which is highly critical for real-world deployment of LLMs (e.g., ChatGPT [OpenAI, 2022], Bard [Pichai, 2023], etc.). To achieve the best performance in practice, fine-tuning will continue to be the modus operandi when using pre-trained LLMs. Hence, our work focuses on pre-training and finetuning language models across a diverse set of tasks, including natural language generation and text summarization. \n\nEfficient Fine-tuning While most large-scale models such as GPT [Brown et al., 2020, Smith et al., 2022] or T5 [Raffel et al., 2022] are trained dense, there are works [Houlsby et al., 2019a, Li and Liang, 2021b, Zaken et al., 2021, Hu et al., 2022] that explore using limited capacity (tuning a few layers or subset of parameters) in the pre-trained models to fine-tune on downstream tasks. These works are indicative that the total modeling capacity is unnecessary for fine-tuning on downstream tasks.",
            "score": 0.5197418453315081,
            "section_title": "RELATED WORK",
            "char_start_offset": 29943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 680,
                    "matchedPaperCorpusId": "221703107"
                },
                {
                    "start": 1549,
                    "end": 1570,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1606,
                    "end": 1628,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "259203671",
            "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
            "text": "Instruction tuning is a unique finetuning approach that fine-tunes LLMs on a collection of natural language formatted instances. With this approach, LLMs are enabled to perform well on previously unseen tasks described through natural language instructions without relying on explicit examples [49]. For example, Wei et al. [57] fine-tuned a 137B parameter LLM on over 60 datasets based on instruction tuning and tested it on unseen task types. The experimental results demonstrated that the instruction-tuned model significantly outperformed its unmodified counterpart and zero-shot GPT-3. \n\nHuman Alignment. LLMs can be trained to generate highquality, harmless responses that align with human values through the technique of RLHF, which involves incorporating humans into the training loop using carefully designed labeling strategies. RLHF comprises three steps: 1) collecting a labeled dataset consisting of input prompts and target outputs to finetune LLMs in a supervised way; 2) training a reward model on the assembled data, and 3) optimizing LLMs by formulating its optimization as a reinforcement learning problem. With this approach, LLMs are enabled to generate appropriate outputs that adhere to human expectations. \n\nTools Manipulation. Traditional PLMs are trained on plain text data, which limits their ability to solve non-textual tasks. Besides, their abilities are limited by the pre-training corpus, and cannot effectively solve tasks requiring real-time knowledge. In response to these limitations, recent LLMs are developed with the ability to manipulate external tools such as search engine, calculator, and compiler to enhance their performance in specialized domains [58]. More recently, the plugin mechanism has been supported in LLMs, providing an avenue for implementing novel functions. This mechanism has significantly broadened the range of capacities for LLMs, making them more flexible and adaptable to diverse tasks. \n\nAlthough LLMs have made significant progress in natural language understanding and human-like content generation, they still have the following limitations and challenges [49]. \n\nUnstructured Generation. LLMs commonly rely on natural language prompts or instructions to generate text under specific conditions. This mechanism presents challenges for precisely constraining the generated outputs according to finegrained or structural criteria. Ensuring specific text structures, such as the logical order of concepts throughout the entire text, can be difficult.",
            "score": 0.5194418526959627,
            "section_title": "D. Pros and Cons of LLMs",
            "char_start_offset": 18994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2130
                },
                {
                    "start": 2133,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "263152585",
            "title": "Generative Speech Recognition Error Correction With Large Language Models and Task-Activating Prompting",
            "text": "As a result, the representations (generated by the LLM) are not distorted due to task-specific tuning, while the adapter module acquires the ability to perform the error correction task. Compared to previous state-of-of-the-art results with the Universal Speech Model (USM) [47] with text injection training over 2B parameters, our best fine-tuning results improve upon their WSJ results (WER of 3.2%). This is noteworthy considering that our generative error correction method is based on a smaller underlying conformer-RNN-T ASR system. \n\nAnother parameter-efficient form of fine-tuning is prefix tuning [50], where a continuous prompt prefix is inserted into the input and tuned to optimize task performance of the LLM. However, this method gave worse results than the full or adapter-based fine-tuning methods for the larger LLMs.",
            "score": 0.5193974484294404,
            "section_title": "Pipeline 2 Results",
            "char_start_offset": 22495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 834
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "230433941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222412109375
        },
        {
            "corpus_id": "273811430",
            "title": "Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback",
            "text": "In order to fully evaluate our method, we chose a series of LLMs with different training path and training data as baselines for comparison. \n\n\u2022 Zero-shot [27]: The model is given a task without any prior examples. This approach tests the model's ability to generalize from its training data to new, unseen scenarios. \n\nFor each model, the zero-shot prompt is carefully crafted to be clear and concise, providing only the necessary context and the query. \u2022 Few-shot [28]: The model is provided with a few examples to learn from before it is asked to perform the task. This method helps the model understand the task better by seeing similar instances. The few-shot prompts are designed to include a small number of examples (2 in our experiment) before presenting the actual query. \n\n\u2022 SFT [29]: Supervised fine-tuning will provide medical cases with their corresponding standard diagnoses for the model to learn. \u2022 SFT+DPO: After warming up using supervised finetuning, the model is guided to generate multiple outputs, and the data is labeled using an automated labeling system which in turn generates preference data for the dpo training process. Zero-shot learning is an important method for evaluating the generalization ability of language models without any taskspecific data. For the field of TCM, zero-shot testing can demonstrate the adaptability of the model to unseen TCM tasks, especially when data is scarce. This method can highlight the basic language understanding and reasoning ability of the model, and provide a lower bound on the performance of tasks in this field. \n\nFew-shot learning provides the ability of the model to learn quickly from a small number of examples. Through training with a few samples, we can evaluate whether the model can effectively reason and generate accurate results under the condition of limited labeled data. \n\nSFT is a standard method for model fine-tuning. For the field of TCM, supervised fine-tuning of the model using existing medical case data can improve the performance of the model on specific tasks. In the baseline comparison, supervised fine-tuning provides the upper limit of the performance that the model can achieve after using domain-specific data. \n\nThe SFT+DPO method combines supervised fine-tuning and preference optimization to improve the quality of model generation while reducing bias.",
            "score": 0.5190287101411653,
            "section_title": "B. Baseline",
            "char_start_offset": 18871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 143,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2216
                },
                {
                    "start": 2219,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "59337593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0255584716796875
        },
        {
            "corpus_id": "269225580",
            "title": "Recent Advances in Large Language Models for Healthcare",
            "text": "The fine-tuned model resulting from this phase is then deployed for flexible applications in its domain of specialization.\n\nThe goal is to leverage general pre-training and then fine-tune the model's capabilities for the nuances of its intended use case through various focused training approaches during fine-tuning.The result is a model that is both adapted and versatile for its specialist field of application.\n\nLLMs have an impressive ability to adapt to unfamiliar tasks and demonstrate remarkable reasoning skills [75,76].However, to fully exploit their potential in specialized fields such as medicine, more specific training strategies are essential.These strategies could encompass direct prompting methods such as few-shot learning [3], in which a limited set of task examples during testing guides the model's results, and zero-shot learning [77], in which the model operates without any prior specific examples.In addition, refined techniques such as chain-of-thought prompting [78], which prompts the model to sequentially decompose its reasoning, and self-consistency checks [79], which ask the model to confirm the consistency of its responses, are also crucial.\n\nReinforcement learning (RL) is also a complementary technique in the fine-tuning process of LLMs, aimed at improving and better aligning pre-trained models.To enhance the performance of these LLMs, optimization methods inspired by RL or directly derived from RL are implemented.Notable examples include RL from human feedback (RLHF), direct preference optimization (DPO) and proximal policy optimization (PPO).\n\nRLHF [2,69] integrates RF techniques with human feedback to refine the language model.DPO, introduced by Rafailov et al. [80], focuses on direct preference optimization for model-generated responses.PPO, initially conceived by Schulman et al. [81] and later adapted by Tunstall et al. [82], employs proximal policy optimization for LLM fine-tuning.\n\nThese RL-based approaches have proven effective, particularly when combined with instruction tuning, in improving the relevance and quality of responses generated by LLMs.",
            "score": 0.5187057202637699,
            "section_title": "LLMs' Architecture",
            "char_start_offset": 24097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 124,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 414
                },
                {
                    "start": 416,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1178
                },
                {
                    "start": 1180,
                    "end": 1336
                },
                {
                    "start": 1336,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1590
                },
                {
                    "start": 1592,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1791
                },
                {
                    "start": 1791,
                    "end": 1940
                },
                {
                    "start": 1942,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "250144408"
                },
                {
                    "start": 743,
                    "end": 746,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "59337593"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1713,
                    "end": 1717,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315185546875
        },
        {
            "corpus_id": "270371731",
            "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair",
            "text": "With the advancements in pre-trained LLMs, recent research has begun exploring their usage on APR. An earlier study [65] comprehensively evaluates the performance of nine LLMs on APR tasks using various input forms. Compared to traditional APR techniques [6,12,13,23,25,38,64], LLMs have demonstrated distinct advantages, particularly in their ability to fix bugs without the need for manual provision of prior knowledge. Prior to the emergence of GPT, several works [22,48,52,58] have proposed fine-tuning LLMs for APR, although the size of these LLMs does not exceed 7B. The study [65] further explains that employing models with larger parameters can enhance their repair ability. In a different research work [20], aside from directly employing LLMs for APR, ten Code Language Models (CLMs) were fine-tuned using APR datasets. The evaluation conducted on Defects4J, QuixBugs, and Humaneval-Java benchmarks demonstrates that CLMs, after fine-tuning with FMFT, exhibit improved fixing capabilities. These two studies emphasize the advantages of LLMs in APR tasks. However, LLMs used in these works were models released before 2022, with a maximum parameter size of 6B. The introduction of gpt3.5 in 2022 brought forth newly released LLMs that demonstrate even stronger performance. Several LLMs specifically designed for code-related tasks, such as StarCoder [29], CodeLlama [50], WizardCoder [40], and DeepSeek-coder [14], have showcased impressive capabilities.",
            "score": 0.5185093638065544,
            "section_title": "LLM-based Automated Program Repair",
            "char_start_offset": 44850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1465
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "211018164"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "49671483"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "516212"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "232022450"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "5987616"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "237205309"
                },
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "257495997"
                },
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "262822628"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "261697451"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 713,
                    "end": 717,
                    "matchedPaperCorpusId": "256808267"
                },
                {
                    "start": 1395,
                    "end": 1399,
                    "matchedPaperCorpusId": "259164815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "265213353",
            "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
            "text": "As shown in Table 5, fine-tuning the Llama-2-7Bchat model on the biography generation task has yielded insights into reducing hallucination in language models. Initially, finetuning using the generated data by retrieval-free and self-query versions of EVER demonstrates a reduction in hallucinations, as evidenced by the improvement in FactScore from the Vanilla baseline of 36.8% to 47.3% with EVER-PREF (NRG+SQ). This indicates that fine-tuning with retrieval-free methods enhances the factual accuracy of language models. Further advancements are observed when finetuning incorporated text generated through retrieval mechanisms. Specifically, the utilization of more factual data by retrieval during fine-tuning, particularly with EVER-PREF (NRG+ER) and EVER-PREF (RAG+ER), increases the performance even further, achieving FactScores of 52.8% and 53.9% respectively. These results underscore the potential of fine-tuning language models with factually enriched datasets to mitigate hallucinations. (Mielke et al., 2022;Kadavath et al., 2022;Varshney et al., 2023). Another line of methods leverage the inconsistency between multiple generated examples, including NLI-based approaches (Elaraby et al., 2023;Manakul et al., 2023) and QA-based methods (Manakul et al., 2023;Agrawal et al., 2023). In addition, Cohen et al. (2023) introduced a method in which one LM acts as an examiner, repeatedly cross-examining the outputs of the other LM to test their consistency. Hallucination Mitigation. A number of approaches have been developed to mitigate hallucination in LLMs. One line of work focuses on manipulating the model via decoding strategies (Chuang et al., 2023;Shi et al., 2023;Li et al., 2022Li et al., , 2023) ) or preference fine-tuning (Tian et al., 2023b).",
            "score": 0.5184456535594066,
            "section_title": "Results & Analysis.",
            "char_start_offset": 21006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 1003,
                    "end": 1024,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350830078125
        },
        {
            "corpus_id": "273654756",
            "title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
            "text": "While pre-trained LLMs exhibit impressive performance across diverse tasks and demonstrate remarkable generalization capabilities (Brown et al., 2020;Wei et al., 2022b;Touvron et al., 2023;Chowdhery et al., 2023;OpenAI et al., 2024), the representations they learn and the task-specific information encoded during pre-training remain largely opaque and unexplored. \n\nRecent research has investigated fine-tuning strategies to adapt LLMs to specific tasks, including supervised fine-tuning on task-specific datasets and instruction tuning (Mishra et al., 2022 Figure 1: An illustration of our findings using the Llama 2 7B model (Touvron et al., 2023) as an example. We show that when instruction tuning on T different tasks, the layers are divided into three functional sections: the shared layers (layers 1 to 9) form general representations shared among all tasks, the transition layers (layers 10 to 15) transition the representations into task-specific information, and the refinement layers (layers 16 to 32) continue to refine the representations toward specific tasks. et al., 2022;Sanh et al., 2022). While these approaches have shown promising results in tailoring LLMs for improved task performance, a comprehensive understanding of their impact on the learned representations is still lacking. \n\nIn this study, we perform a set of analyses to investigate task-specific information encoded in pre-trained LLMs and the effects of instruction tuning on their representations. The analysis leverages a sub-population analysis technique called Model-Oriented Sub-population and Spectral Analysis (MOSSA; Zhao et al. 2022), which provides an alternative to traditional probing methods for analyzing model representations within specific sub-populations of the training data. MOSSA involves comparing two models: a control model trained on the data relevant to the sub-population of interest (e.g., a particular task), and an experimental model that is identical to the control model but is also trained on additional data from different arXiv: 2410.20008v1 [cs.CL] 25 Oct 2024 sources (e.g., multiple tasks).",
            "score": 0.5181717495416196,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 168,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 189,
                    "end": 212,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 538,
                    "end": 558,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 1089,
                    "end": 1107,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1610,
                    "end": 1627,
                    "matchedPaperCorpusId": "253098472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.233154296875
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "Targeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface's fine-tuning data can also be used as an initial step. \n\nAnother benefit of fine-tuning is the ability to adjust the model's input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed [37]. For retrieval tasks that engage with structured data, the SANTA framework [76] implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings. \n\nAligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.",
            "score": 0.5180023467127353,
            "section_title": "B. LLM Fine-tuning",
            "char_start_offset": 40474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1668
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3076171875
        },
        {
            "corpus_id": "258959304",
            "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
            "text": "Pre-trained language models (PLMs) are trained on large-scaled corpora in a self-supervised fashion. They have fundamentally changed the NLP community in the past few years by achieving impressive results in various Tasks (Devlin et al., 2018;Radford et al., 2018;Yang et al., 2019;Chiang et al., 2022). However, when PLMs are finetuned on small datasets, their performance declines. Researchers have proposed various techniques to adapt PLMs to these scenarios (Snell et al., 2017;Sung et al., 2018). In addition to performance, fine-tuning PLMs to learn a new task is parameter inefficient, because an entirely new model is required for every task (Houlsby et al., 2019). \n\nBy the introduction of GPT-3 (Brown et al., 2020b) with 175B parameters, it has been shown that Large Language Models (LLMs) are efficient few-shot learners as they can use their knowledge more effectively. One of the key features of these LLMs is their ability to perform multiple tasks using prompts. A language prompt is a piece of text that is added to the input query to help the model make more accurate predictions. In addition, LLMs can be fine-tuned for specific tasks using few examples. This has made them powerful tools for NLP tasks, especially in few-shot scenarios. However, that might not be practical for many situations because of the model size. Therefore, there is a need to adapt smaller PLMs to work in a similar way to LLMs. \n\nPrompt-based fine-tuning is a method for adapting PLMs to specific tasks or domains by providing a prompt (Schick and Sch\u00fctze, 2020a,b). This approach has been shown to be effective in various NLP tasks, including text classification (Han et al., 2021;Wang et al., 2022) and question answering (Yao et al., 2022). However, it can be challenging to achieve strong performance when only a few examples are available for each task. Gao et al. (2020) introduced a prompt-based fine-tuning method called LM-BFF for RoBERTa (Liu et al., 2019) to tackle this issue.",
            "score": 0.5175781859968783,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 282,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 282,
                    "end": 302,
                    "matchedPaperCorpusId": "253762079"
                },
                {
                    "start": 482,
                    "end": 500,
                    "matchedPaperCorpusId": "4412459"
                },
                {
                    "start": 650,
                    "end": 672,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1480712890625
        },
        {
            "corpus_id": "270870809",
            "title": "TCSR-SQL: Towards Table Content-aware Text-to-SQL with Self-retrieval",
            "text": "[8,25,32,46] However, the above methods face several limitations.Limited by the length of LLM input tokens and the database schema-only input, it's difficult to retrieve the full contents of a large real-world database.This can lead to errors in the database storage values used in the conditions of the generated SQL, as well as incorrect database column names used in the generated SQL.Fine tuning-based methods.With the rapid iteration of LLM techniques and the large-scale emergence of open-source LLM models, there are several studies devoted to training general LLMs into Textto-SQL domain-specific LLMs by using fine-tuning techniques.As an initial attempt, SQL-PALM [35] and DAIL-SQL [12] utilize the Spider training set [48] to discover the fine-tuning effectiveness of the closed-source PaLM-2 [1] and several open-sourced LLMs, separately.Fin-SQL [49] uses LoRA [16], a common technique for LLM fine-tuning, to design the LoRA-based Multi-Task PEFT method.It fine-tunes different open-source LLMs to complete the Text-to-SQL task for financial domain databases, including LLaMA-2 [39] and Baichuan-2 [47].DTS-SQL [30] proposes a two-stage fine-tuning approach for data privacy protection issues using open-source LLMs with a small number of parameters.By decoupling the LLM finetuning task into both schema linking fine-tuning and SQL generation fine-tuning, it improves the performance of fine-tuned LLMs to a level close to existing prompt-based Text-to-SQL methods using closed-source LLMs such as GPT-4 [27].\n\nThese fine tuning-based methods have three limitations.First, most of them use the training dataset of the specified benchmark to fine-tune LLMs.This results in poor robustness of fine-tuned LLM when migrating to Text-to-SQL tasks with other irrelevant data.Second, the Text-to-SQL domain-specific fine tuning process implicitly teaches LLM to capture the correlations between fine tuning data, questions, and SQL queries.",
            "score": 0.517332952884278,
            "section_title": "Database Schema-based Text-to-SQL Methods",
            "char_start_offset": 10878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1263
                },
                {
                    "start": 1263,
                    "end": 1523
                },
                {
                    "start": 1525,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1947
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2568359375
        },
        {
            "corpus_id": "266375097",
            "title": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models",
            "text": "We further evaluated LLMs after they were fine-tuned on CORECODE. Specifically, we fine-tuned BLOOM-7.1B and ChatGLM-6B on 2K, 4K, 6K, 8K, and 10K examples respectively in the LoRA (Hu et al. 2022) manner, and tested these fine-tuned models on another 2K data. \n\nResults on the commonsense knowledge generation and commonsense conflict phrase detection tasks are shown in Figure 1. Fine-tuning on different sizes of data results in large performance gains for both models. On the commonsense conflict phrase detection task, the F1 score rises as the size of training data increases. In contrast, on the commonsense knowledge generation task, the performance rises first and then falls as the number of training instances increases, indicating that approximately 4K training instances are sufficient for this task. Training with the same amount of training data for the same epochs on both tasks brings more performance gains for BLOOM-7.1B than for ChatGLM-  6B. The reason could be that it is easier for BLOOM-7.1B without SFT to acquire such knowledge than ChatGLM-6B with SFT. For the three selection tasks, as shown in Figure 2, there is a positive correlation between model performance and training data size on most tasks. Both models obtain a substantial improvement after fine-tuning.",
            "score": 0.5171959292841096,
            "section_title": "Performance of LLMs Being Fine-tuned",
            "char_start_offset": 26388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1292
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 196,
                    "matchedPaperCorpusId": "222310337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1387939453125
        },
        {
            "corpus_id": "269605992",
            "title": "A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model",
            "text": "In this subsection, we discuss the efficiency issues when adapting LLM towards time series analysis.We first provide a tuning paradigm taxonomy for existing works based on the updated parameter amount of their LLM-centered architectures.\n\nThe taxonomy has three branches, i.e. tuning free paradigm without any updated parameters, fully fine-tuning paradigm which updating all the parameters of LLM backbone and parameter-efficient fine-tuning which updating only a small set of model parameters.\n\n6.2.1 Tuning Free Paradigm.In the development of LLM-centered time series solutions, the most efficient strategy is to directly call API interfaces [61,175,206], without any updating of model parameters, i.e. tuning free paradigm.\n\nFor instance, TWSN [189] and TDML [197] explore the zero-shot inference of ChatGPT/GPT-4 in multimodal stock movement prediction.No parameter is updated under this strategy.This approach does not alter the model's structure.\n\nInstead, it heavily depends on the inherent pattern recognition and reasoning capabilities of LLMs, along with prompt engineering, to enhance model performance.However, recent research shows that this strategy may lead to poorer performance of the model on some tasks.For instance, TDML [197] observes that LLM underperforms not only SOTA methods but also some traditional methods like linear regression.In addition, it is also expensive to call ChatGPT API for a large amount of input.\n\n6.2.2 Full Fine-Tuning Paradigm.Due to the inferior performance of tuning free paradigm, some works choose to fine-tune LLM to adapt it for downstream time series tasks.A fully fine-tuning strategy usually redesigns the input layer to encode time series data and the output layer to adapt to downstream tasks.This strategy can better guide LLMs to perform on a specific time series task, like AuxMobLCast for human mobility prediction [192], UniTime [112] and TEMPO [25] for general forecasting.However, it is usually computationally intensive and requires a longer training time, as it involves updating all the parameters of a large language model.",
            "score": 0.5168003358670015,
            "section_title": "Efficiency in LLM Adaption",
            "char_start_offset": 70423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 237
                },
                {
                    "start": 239,
                    "end": 495
                },
                {
                    "start": 497,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 727
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 953
                },
                {
                    "start": 955,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1441
                },
                {
                    "start": 1443,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1612
                },
                {
                    "start": 1612,
                    "end": 1752
                },
                {
                    "start": 1752,
                    "end": 1938
                },
                {
                    "start": 1938,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 1878,
                    "end": 1883,
                    "matchedPaperCorpusId": "252212309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323486328125
        },
        {
            "corpus_id": "262054014",
            "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
            "text": "Developing large language models (LLMs) typically involves two stages-pretraining on vast text corpora and fine-tuning on curated datasets. Fine-tuning through methods such as instruction-tuning and reinforcement learning from human feedback is critical in enabling language models to output desirable text. Since fine-tuning datasets are considerably smaller and less diverse than web-scale pretraining datasets, and there is always a risk that the fine-tuned model \"catastrophically forgets\" (McCloskey & Cohen, 1989) how to solve tasks that the pretrained model could solve. Such a gap has been reported as an \"alignment tax\" (Ouyang et al., 2022;Bai et al., 2022), but there is no clear understanding of what these trade-offs are and how to mitigate them. In this work, we introduce a synthetic setup to understand the effects of fine-tuning. Building on the prior work of in-context learning linear functions (Garg et al., 2023) by pretraining transformers (Vaswani et al., 2017) on a large number of weight vectors, we show that the resulting models can be sub-optimal when evaluated on a few weight vectors of special interest. This mirrors real-world settings where the uncurated pretraining data contains some \"natural\" tasks of special interest, like question answering. Fine-tuning on the weights (tasks) of interest enables transformers to achieve optimal performance on these tasks at the cost of worse performance on other tasks. We find that the most affected tasks are outside but \"close\" to the fine-tuning distribution as measured by their likelihood under the fine-tuning distribution. In other words, the fine-tuned model performs more like the pretrained model on tasks that are far from the fine-tuning distribution. We hypothesize this is because models are implicitly \"inferring\" the task before solving the corresponding task, and that fine-tuning skews model task inference more than it changes models capabilities. Assuming this framework, we can recover the suppressed pretraining capability through Conjugate Prompting.",
            "score": 0.5167456843135184,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242919921875
        },
        {
            "corpus_id": "269448829",
            "title": "Utilizing Large Language Models for Information Extraction from Real Estate Transactions",
            "text": "Fine-tuning large language models (LLMs) for real estate contract text data involves various approaches that leverage transfer learning, task-specific fine-tuning, and multitask learning to enhance model performance and enable domain-specific learning.Transfer learning involves using pre-trained LLMs, like BERT[Devlin et al., 2018] or GPT [Achiam et al., 2023], trained on massive datasets for general language understanding.These models are then finetuned on real estate contract text data to adapt their learned features and knowledge to the nuances of real estate transactions.The fine-tuning process updates model parameters to better align with the target domain, enhancing the LLM's ability to understand real estate-specific language and concepts.\n\nTask-specific fine-tuning is another approach where pretrained LLMs are fine-tuned on a dataset tailored specifically for real estate contracts.This method focuses on optimizing the LLM's performance for a particular task, such as contract summarization or clause classification.By training the model on task-specific data, it learns to extract relevant information and make domain-specific predictions, effectively enhancing its ability to process real estate contract text.\n\nMulti-task learning is a technique that involves training an LLM on multiple related tasks simultaneously [Mahabadi et al., 2021], including real estate-specific tasks like contract interpretation and clause extraction.This approach has been used in numerous applications in specific domains [Chakrabarty et al., 2019] because it encourages the model to learn shared representations across tasks, enabling it to generalize better and improve performance on each individual task [Howard andRuder, 2018] [Wallingford et al., 2022].Multi-task learning helps the LLM leverage common patterns and features across different real estate contract-related tasks, leading to enhanced domain-specific learning and more robust performance.",
            "score": 0.5166734518780944,
            "section_title": "Fine-tuning Large Language Models",
            "char_start_offset": 7445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 756
                },
                {
                    "start": 758,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1233
                },
                {
                    "start": 1235,
                    "end": 1454
                },
                {
                    "start": 1454,
                    "end": 1764
                },
                {
                    "start": 1764,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 362,
                    "matchedPaperCorpusId": "114882116"
                },
                {
                    "start": 1341,
                    "end": 1364,
                    "matchedPaperCorpusId": "199058705"
                },
                {
                    "start": 1713,
                    "end": 1724,
                    "matchedPaperCorpusId": "199058705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12158203125
        },
        {
            "corpus_id": "267759978",
            "title": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation",
            "text": "Finally, as fine-tuning of models became more commonplace, the divide between intrinsic and extrinsic has, by some, come to be defined by whether a task is performed before fine-tuning or after fine-tuning. Ladhak et al. (2023) studied the relationship between upstream (\"intrinsic\") and downstream (\"extrinsic\") metrics in versions of BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) that were fine-tuned for text summarization. The upstream metric was based on the pre-trained base model's ability to correctly state a person's nationality when prompted with <name> is a citizen of. The downstream task was based on perturbed descriptions of individuals, which replaced the name of a person of one nationality with the name of a person of another nationality. \n\nIn the upstream-downstream paradigm used in these papers, even benchmarks that were clearly extrinsic before, such as BBQ, can now be considered intrinsic if the evaluations are run prior to fine-tuning. However, it may be difficult to apply this paradigm to modern LLM use cases in which fine-tuning is no longer as common; instead, models are often used either out-of-the-box, with a system prompt (i.e., a prompt prepended to all user prompts), or with in-context learning (i.e., task-specific examples and explanations in the user prompt).",
            "score": 0.5166180624652101,
            "section_title": "Fine-tuned models",
            "char_start_offset": 8722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1317
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 227,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 336,
                    "end": 361,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08123779296875
        },
        {
            "corpus_id": "273233211",
            "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture",
            "text": "In recent years, large language models (LLM) have demonstrated extraordinary general capabilities, and are widely used for question-answering, code generation, mathematics, and other fields. However, the increasing utilization of LLM in specialized domains needs to support tasks with a mixture of world and specific knowledge. Meanwhile, since the scale of the LLM is large, fine-tuning the entire model could be expensive, and thus may not be affordable for many users. Furthermore, after finetuning the downstream tasks, the LLM may lose the Figure 1: The radar chart of the performance of the parameter-efficient fine-tuning methods. The blue benchmarks are evaluated after fine-tuning on other downstream datasets. Compared with the state-of-theart parameter-efficient methods, the proposed SLIM could achieve comparable downstream performance while mitigating catastrophic forgetting. \n\ngeneral capability, due to catastrophic forgetting. How to reduce the training cost while mitigating catastrophic forgetting without loss of learning capacity, is an essential problem in the utilization of LLM, especially when utilized in a scenario where both common and professional capabilities are needed. \n\nAn intuitive solution is to train only a small bunch of parameters. For example, low-rank adaptation (LoRA) (Hu et al., 2022), a widely used PEFT approach, is validated to forget fewer pieces of knowledge compared with fine-tuning the full weight of the base model, with limitations of learning capacity (Biderman et al., 2024). Chaudhry et al. (2020) propose to optimize the model in the orthogonal subspace with low-rank adaptation to alleviate forgetting. Singh et al. (2020) propose to insert calibrating modules to the base model for learning the downstream tasks. Liang et al. (2023) utilize learnable mask prompts to fine-tune the CLIP without loss of text-vision alignment. \n\nThese approaches attempt to introduce minimal changes to the base model to keep its raw capacity. Some approaches utilize data replay to reduce forgetting. For example, Glorioso et al. (2024) utilize a 60% data replay for continual training. Zhu et al. (2024) utilize a 30% replay rate in the continual pre-training for the coding model.",
            "score": 0.5160664926781608,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 1313,
                    "end": 1330,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1509,
                    "end": 1532,
                    "matchedPaperCorpusId": "269791237"
                },
                {
                    "start": 1534,
                    "end": 1556,
                    "matchedPaperCorpusId": "225040673"
                },
                {
                    "start": 1664,
                    "end": 1683,
                    "matchedPaperCorpusId": "227178110"
                },
                {
                    "start": 1775,
                    "end": 1794,
                    "matchedPaperCorpusId": "252780581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "267094982",
            "title": "Enhancing In-context Learning via Linear Probe Calibration",
            "text": "Large language models (LLMs), have remarkably showcased their capabilities across a broad range of natural language processing tasks [11,13,3,29,59,39]. The cost of training these large models can be prohibitively expensive. Therefore, the commonly adopted approach is to first pre-train with large amounts of unlabled data and then fine-tune the model to downstream tasks. Although fine-tuning LLMs can be effective, it is prone to instability [34] due to numerous hyperparameter configurations resulting in failed runs, unstable results, and over-fitting [41,11,25]. Moreover, fine-tuning models of such large size may also be expensive and also requires explicit access to the architecture and weights of LLMs, which may not be publicly available [60]. To avoid large fine-tuning times and avoid requiring access to the weights of the model, recently, LLMs, exemplified by GPT-3 [3], have demonstrated the ability to perform in-context learning (ICL), a capability whereby a model can generate an appropriate output for a given query input based on a prompt that includes input-output example pairs specific to the task at hand. ICL can work with an API without explicit access to the LLM. Figure 1 provides a visual representation of ICL. The prompts utilized in ICL entail task-specific in conjunction with a series of input-label pairs, referred to as demonstrations. Such a capability of LLM to learn \"in-context\" presents an intriguing aspect whereby the model is capable of acquiring knowledge and performs well on a wide range of downstream tasks without any task-specific parameter fine-tuning [3,48,2,40]. \n\nMore specifically, the aim of ICL is to make a prediction on some query test sample x by conditioning on a prompt sequence (f x (x 1 ), f y (y 1 ), . . . , f x (x k ), f y (y k ), f x (x)) containing k-shot samples (x i , y i ) k i=1 (i.e.",
            "score": 0.5157521736360577,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 140,
                    "end": 142,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 148,
                    "end": 151,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "219558836"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 564,
                    "end": 567,
                    "matchedPaperCorpusId": "247011290"
                },
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1605,
                    "end": 1608,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1627197265625
        },
        {
            "corpus_id": "258331833",
            "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
            "text": "When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered. \n\nFew annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling [16]. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88]. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints. \n\nIn a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data.",
            "score": 0.5157501699262605,
            "section_title": "Finetuning data",
            "char_start_offset": 9792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 482,
                    "end": 487,
                    "matchedPaperCorpusId": "202540839"
                },
                {
                    "start": 582,
                    "end": 586,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "248505738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051849365234375
        },
        {
            "corpus_id": "274777519",
            "title": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs",
            "text": "Recent insights in LLM model editing show that middle layers in LLMs are crucial for domain-specific knowledge (Meng et al., 2022a;Li et al., 2023;Azaria and Mitchell, 2023), with attention modules handling general semantic correlations while MLP layers being more task-specific (Geva et al., 2020). In this study, we delve deeper into the domain-specific relevance of various layers in LLMs. Figure 1 reveals that when fine-tuning on science-common-sense and medical domains, we can remove up to 20 and 16 out of 32 layers respectively in LLaMA-7B without compromising performance. \n\nBuilding on these findings, we hypothesize layerwise specialization: the significance of each layer of an LLM, particularly the MLP layer, varies according to the specific knowledge domain; we can fine-tune a more domain-focused LLM by selectively dropping layers unimportant to the targeted domain. This strategy enables us to craft models that are not only more compact but also finely bal- anced in terms of memory usage, inference speed, and domain-specific accuracy. \n\nTo validate this hypothesis, we conducted extensive layer-dropping experiments on domainspecific datasets (Pal et al., 2022;Chalkidis et al., 2021;Maia et al., 2023), where one least-important layer is removed after one epoch of fine-tuning. \n\nThe results indicate that up to a significant number of the layers could be dropped during fine-tuning with negligible accuracy loss using an effective target selection algorithm. Building on these findings, we introduce TRIMLLM, a novel framework that combines fine-tuning with progressive layerdropping. This approach employs a calibration dataset and an activation-based metric to efficiently identify and eliminate the most non-essential layers after each fine-tuning iteration. Remarkably, TRIMLLM can compress popular LLMs to less than 50% of their original sizes while maintaining domain-specific performance on par with fully fine-tuned models, despite a drastic reduction in parameters. This results in a significant reduction in model depth, therefore memory and computational cost at inference.",
            "score": 0.5155616614039464,
            "section_title": "Introduction",
            "char_start_offset": 1658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1165,
                    "end": 1183,
                    "matchedPaperCorpusId": "247763070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1903076171875
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "To start with, we leverage instruction fine-tuned LLMs as the backbone (e.g. meta-llama/ Meta-Llama-3-8B-Instruct). Although these models have been instruction finetuned, they still exhibit a deficiency in leveraging context information to answer domain-specific questions. To improve the their abilities on knowledge-intensive tasks, we fine-tune the LLM with retrieval-oriented tasks. Specifically, we follow Lin et al. (2024); Liu et al. (2024) and leverage the training data blend that consists of the following components: \n\n(1) General Instruction Fine-tuning (SFT) data. To help maintain the models' ability to comprehend and follow instructions, we leverage the SFT data including OpenAssistant (K\u00f6pf et al., 2023), Dolly (Conover et al., 2023), SODA (Kim et al., 2023), ELI5 (Fan et al., 2019), Self-Instruct (Wang et al., 2023a), and Unnatural Instructions (Honovich et al., 2022). Note that we make sure there is no overlap between SFT data and test data from target tasks. \n\n(2) General domain Context-aware QA data. To bolster the LLMs' general RAG skills of generating accurate answers grounded in relevant contexts, we fine-tune them on a diverse array of general domain question-answering datasets.",
            "score": 0.5154979317978572,
            "section_title": "Stage-I: Retrieval-oriented fine-tuning",
            "char_start_offset": 8802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1214
                }
            ],
            "ref_mentions": [
                {
                    "start": 703,
                    "end": 722,
                    "matchedPaperCorpusId": "258179434"
                },
                {
                    "start": 759,
                    "end": 777,
                    "matchedPaperCorpusId": "254877312"
                },
                {
                    "start": 784,
                    "end": 802,
                    "matchedPaperCorpusId": "196170479"
                },
                {
                    "start": 818,
                    "end": 838,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34619140625
        },
        {
            "corpus_id": "266998884",
            "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
            "text": "LLMs exhibit substantial improvements when trained on extensive datasets rich in domain-specific language and terminologies. Such targeted training enhances their capabilities in specialized applications that necessitate robust natural language understanding, including interaction with computing systems through tasks like program synthesis, code completion, debugging, and documentation generation. \n\nIn our BT generation method, as we analyzed, require various LLM abilities which are the most advanced usage of LLM, such as planning, reasoning, tool manipulation abili-ties, etc. Some maybe meet our task requirements after being pretrained but it is various on the different LLM model. And Some abilities can not meet our requirements. So to make LLM model more useble, the training is necessary. The quality of data is paramount for effective training and ethical development of LLMs, a principle equally relevant to the context of BT generation. Typically, these models leverage diverse and publicly accessible open-source data during their initial training phase. \n\nIn this section, we will delineate the pretraining and Supervised Fine-Tuning (SFT) stages within the LLM training pipeline.",
            "score": 0.5154210399868586,
            "section_title": "Training",
            "char_start_offset": 51970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.149169921875
        },
        {
            "corpus_id": "272694272",
            "title": "Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models",
            "text": "Our findings highlight the limitations of using perplexity as the sole evaluation metric for LLM compression. While compression methods may preserve the perplexity of the original model, they often result in significant performance declines on specific downstream tasks. This underscores the need for a more comprehensive evaluation approach that captures the nuanced effects of compression. Jensen-Shannon (JS) Divergence emerges as a valuable tool in this context, providing deeper insights into the trade-offs between model size and task-specific capabilities. The strong alignment between GPT-4 evaluations and JS Divergence further validates JS Divergence as a comprehensive metric for assessing the real-world impact of compression. Additionally, our experiments emphasize the critical role of calibration data in successful LLM sparsification, showing that task-specific calibration data significantly enhances the performance of compressed models on downstream tasks. \n\nA key area for future research is the integration of fine-tuning with compression to optimize performance and complexity for downstream tasks. Our future study aims to address this gap by investigating how fine-tuning can be effectively combined with compression methods to enhance task-specific performance while maintaining model efficiency. Since the most effective compression techniques leverage calibration data, integrating fine-tuning with compression holds significant potential. Fine-tuning allows models to adapt to specific tasks, and when combined with compression techniques that utilize task-specific calibration data, it can enhance the overall efficacy and robustness of the model. Future research should explore the synergistic effects of fine-tuning and compression, particularly how calibration data can be leveraged during the fine-tuning process to optimize both performance and complexity. \n\nIn conclusion, while SparseGPT and Wanda show promise for compressing LLMs, addressing performance gaps on downstream tasks remains a challenge. Our study advocates for using metrics like JS Divergence alongside perplexity to better evaluate compression techniques. This approach can help develop more effective compression methods, enabling the use of powerful LLMs in resource-constrained environments without losing their practical utility. By adopting comprehensive evaluation metrics, researchers can better understand how compression affects the practical use of large language models. This will aid in the adoption of LLMs for specific tasks, highlighting the practical value of proper compression methods for efficient and effective use in various domains.",
            "score": 0.5154070297410888,
            "section_title": "Conclusion",
            "char_start_offset": 23213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2336
                },
                {
                    "start": 2337,
                    "end": 2484
                },
                {
                    "start": 2485,
                    "end": 2657
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "276961257",
            "title": "G-Boost: Boosting Private SLMs with General LLMs",
            "text": "Large Language Models (LLMs) have achieved remarkable performance in various natural language processing tasks (OpenAI, 2023;Yang et al., 2024;DeepSeek-AI et al., 2025). They demonstrate exceptional generalization and reasoning capabilities, making them effective in a wide range of applications. However, these capabilities come at a significant cost. Fine-tuning LLMs for specific domains often demands significant computational resources, which only a few organizations can afford. Additionally, data privacy concerns further prevent organizations from fully relying on external LLM services. \n\nDue to resource constraints and privacy concerns, many developers can only fine-tune small language models (SLMs) on their private datasets. These fine-tuned SLMs can often adapt well to specific domains and sometimes even outperform general LLMs in domain-specific tasks. However, as shown in Fig. 1. limited by smaller parameter sizes, SLMs exhibit weaker general language capabilities. They often struggle with queries that require complex reasoning or deep understanding. Recently, various collaborative inference methods have been proposed to overcome the performance limitations of small models. A widely adopted approach involves combining small and large models to perform hybrid inference (Ong et al., 2024;MS et al., 2024;Zheng et al., 2025), where the two models dynamically alternate to generate outputs. This technique achieves near-LLM performance while reducing computational costs. Another strategy follows a two-stage process, where LLMs provide guidance and small models refine the predictions (Zhan et al., 2025;Yao et al., 2024). While effective to some extent, these methods often rely on the assumption that LLMs consistently outperform SLMs. However, in domainspecific tasks, general LLMs frequently lack the specialized knowledge required, which limits their ability to enhance private SLMs. \n\nResearchers have also explored combining multiple LLMs to integrate the expertise of individual models, thereby enhancing cross-domain perfor- mance (Chen et al., 2024;Mohammadshahi et al., 2024;Jiang et al., 2023). Nevertheless, this approach yields limited benefits in single-domain tasks and rarely surpasses the capabilities of specialized domain experts.",
            "score": 0.5149930764054558,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 1629,
                    "end": 1646,
                    "matchedPaperCorpusId": "270123376"
                },
                {
                    "start": 2065,
                    "end": 2084,
                    "matchedPaperCorpusId": "272987865"
                },
                {
                    "start": 2111,
                    "end": 2130,
                    "matchedPaperCorpusId": "259075564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02752685546875
        },
        {
            "corpus_id": "265294420",
            "title": "Large Language Models in Finance: A Survey",
            "text": "Fine-tuned finance LLM evaluation. The performance of fine-tuned finance LLMs can be evaluated in two categories: finance classification tasks and finance generative tasks. In finance classification, we consider tasks such as Sentiment Analysis and News Headline Classification. In finance generative tasks, our focus is on Question Answering, News Summarization, and Named Entity Recognition. Table 1 provides detailed information about all the fine-tuned finance LLMs. Among the various fine-tuned LLMs, we will focus on discussing three of them: (1) PIXIU (also known as FinMA) [56], fine-tuned LLaMA on 136K task-specific instruction samples. (2) FinGPT [58], it presents a end-to-end framework for training and applying FinLLMs in the finance industry. FinGPT utilizes the lightweight Low-rank Adaptation (LoRA) technique to fine-tune open-source LLMs (such as LLaMA and ChatGLM) using approximately 50k samples. However, FinGPT's evaluation is only limited to finance classification tasks. (3) Instruct-FinGPT [63], on the other hand, fine-tunes LLaMA on 10k instruction samples derived from two Financial Sentiment Analysis Datasets and also solely evaluates performance on finance classification tasks. \n\nBased on the reported model performance, we summarize our findings as below: \n\n\u2022 Compared to the original base LLM (LLaMA) and other open-source LLMs (BLOOM, OPT [64], ChatGLM [14,62]), all fine-tuned finance LLMs exhibit significantly better performance across all finance-domain tasks reported in the papers, especially classification tasks. \u2022 The fine-tuned finance LLMs outperform BloombergGPT [55] in most finance tasks reported in the papers. \u2022 When compared to powerful general LLMs like Chat-GPT and GPT-4, the fine-tuned finance LLMs demonstrate superior performance in most finance classification tasks, which indicates their enhanced domainspecific language understanding and contextual comprehension abilities. However, in finance generative tasks, the fine-tuned LLMs show similar or worse performance, suggesting the need for more high-quality domain-specific datasets to improve their generative capabilities.",
            "score": 0.5149444886104639,
            "section_title": "4.2.2",
            "char_start_offset": 14180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 1389,
                    "end": 1393,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "252715691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "271059019",
            "title": "A practical guide to implementing artificial intelligence in traditional East Asian medicine research",
            "text": "LLMs are a revolutionary subset of DL algorithms that leverage neural networks with billions of parameters to process and understand human language with remarkable proficiency. These models are pretrained on a vast corpus of diverse text, enabling them to capture complex linguistic structures and semantics. Conversational language models, such as ChatGPT, have further propelled their capabilities by enabling interactive, context-aware conversations. \n\nDeveloping LLMs from scratch by individual researchers or smaller institutions is practically infeasible due to significant resource and time requirements. Instead, leveraging existing models, particularly through fine-tuning smaller open-source models, presents a more viable approach. 66 , 67 For instance, models like TCM-GPT 68 can be effectively fine-tuned on a large TCM-specific corpus, demonstrating outperforming results on TCM examination and TCM diagnosis. Additionally, retrieval augmented generation provides an alternative method of leveraging LLMs without training. 69 Unlike fine-tuning, which involves modifying the internal parameters of a pre-trained model to adapt it to specific tasks, retrieval augmented generation retrieves relevant information from external sources to enhance the model's responses without altering its foundational structure. These methods bridge the gap between general-purpose models and domain-specific needs, offering a scalable solution for enhancing model accuracy and relevancy in specialized fields like TEAM. ) For users without deep technical expertise, there are more accessible methods for utilizing generative LLMs. Web-based usage is a convenient way to engage with LLMs, allowing users to interact with these models through simple web interfaces without complex setup. In the realm of language models, prompting refers to the process of providing input to steer or influence the model's generated output. By focusing on prompt engineering, users can harness LLMs' capabilities to extract high-quality responses. 72 An example of this is the Medprompt 73 technique, which combines dynamic few-shot selection, selfgenerated chain-of-thought reasoning, and choice-shuffling ensemble strategies to improve the accuracy and robustness of GPT-4 \u2032 s responses in medical application. API-based usage offers more flexibility, allowing users to integrate LLMs into custom applications. This option is ideal for those who need to incorporate LLM functionalities into specific software or platforms.",
            "score": 0.5146828571390163,
            "section_title": "Large language models",
            "char_start_offset": 46799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2390
                },
                {
                    "start": 2391,
                    "end": 2502
                }
            ],
            "ref_mentions": [
                {
                    "start": 2026,
                    "end": 2028,
                    "matchedPaperCorpusId": "257901233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08282470703125
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Many efforts have been made to compensate for the phenomenon (Liang et al., 2021;Xu et al., 2021;Yuan et al., 2023a). There has also been research discovering that scaling up the pre-trained language model scale and the fine-tuning data scale are beneficial for zero-shot out-of-domain generalization on various linguistic tasks while leaving out the assessment of in-domain performance (Sanh et al., 2022;Chung et al., 2022a;Longpre et al., 2023). Given the increased capacity of LLMs, the multi-task performance by SFT on composite data of essentially different downstream tasks is less studied. Understanding the SFT performance with composite data and corresponding scaling patterns is of great utility in practice. \n\nIn this study, we focus on the data composition problem among mathematical reasoning, code generation, and general human-aligning abilities in SFT. We aim to comprehensively investigate the relationship between model performance and different factors including data amount, data composition ratio, model scales, and SFT training strategies. We also investigate how the relationship varies under different scales. Specifically, we focus on the following four research questions: \n\n1. How do math reasoning, coding, and general abilities scale with SFT data amounts? \n\n2. Are there performance conflicts when combining these three abilities in SFT? \n\n3. What are the key factors that induce the performance conflicts? \n\n4. What are the impacts of different SFT strategies for composite data? \n\nTo answer these questions, we conduct experiments on three benchmarks, which are GSM8K (Cobbe et al., 2021) for mathematical reasoning, HumanEval (Chen et al., 2021) for coding, and MT-Bench (Zheng et al., 2023) for general human alignment. We fine-tune LLMs on the related training data to activate these abilities. Furthermore, we conduct extensive analysis regarding model parameter scales ranging from LLaMA 7B to 33B (Touvron et al., 2023) and explore four different SFT strategies shown in Figure 1: multi-task learning, sequential training, mixed sequential training, and dual-stage mixing fine-tuning (DMT), providing empirical guidance for learning a versatile LLM with composite SFT. The key findings of this paper can be summarized as follows:",
            "score": 0.5145574814349905,
            "section_title": "Introduction",
            "char_start_offset": 2821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 97,
                    "matchedPaperCorpusId": "237491053"
                },
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "254854311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1358642578125
        },
        {
            "corpus_id": "272367802",
            "title": "Large language model for patent concept generation",
            "text": "Although there are efforts to employ LLMs for generation tasks within specific domains, these methods often fall short in the highly structured area of patent writing. To effectively address the issues of automated text generation, especially in niche domains, LLMs need to possess prior knowledge relevant to those fields [19]. Previous literature has explored the fine-tuning method to enhance the capabilities of LLM in new technical concepts generation, as shown in Table 1. For these fine-tuning-based models, the common approach is to select a base model and then construct a dialogue dataset for SFT. SFT is an approach for fine-tuning models on domain-specific labeled datasets. While SFT can help models learn the specific style and requirements of patent writing, it typically requires a large amount of annotated data and still struggles to generalize beyond the examples it was trained on. \n\nIn the earliest study about generating original design concepts, Trappey et al. used sequence to sequence with attention models to develop an intelligent patent summarization system [20]. Their model combines extraction and generative abstract methods, and has been studied and validated through case studies in the field of intelligent machinery patents. However, traditional NLP techniques are unable to handle long texts and complex contexts like large models. Later, Zhu and Luo demonstrated the ability of GPT-2 and GPT-3 to integrate near-field and far-field knowledge sources for early-stage design tasks through a combination of fine-tuning and few-shot learning methods [21]. Another study by Zhu and Luo used patent text data to fine-tune GPT-2 for generating design concepts [22]. Subsequently, to explore the potential of LLMs in assisting humans with concept design across different fields, they also used fine-tuning as an approach for the few-shot learning of GPT-3 [23]. \n\nIn the literature on writing concept texts for patents, Lee discussed the performance of LLMs in generating patent abstracts and analyzed the quality and originality of the generated content [24]. However, this article did not provide a method for training LLMs, but instead tested the performance of different models on IP-related tasks. MoZI represents the first study to explore how LLMs can empower IP rights.",
            "score": 0.5143685275767306,
            "section_title": "Fine-tuning method for patenting",
            "char_start_offset": 9254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2306
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "269584440"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "211830249"
                },
                {
                    "start": 1583,
                    "end": 1587,
                    "matchedPaperCorpusId": "244129984"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "248266902"
                },
                {
                    "start": 1885,
                    "end": 1889,
                    "matchedPaperCorpusId": "253384381"
                },
                {
                    "start": 2084,
                    "end": 2088,
                    "matchedPaperCorpusId": "250113354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04376220703125
        },
        {
            "corpus_id": "270199982",
            "title": "Experimental Design for Active Transductive Inference in Large Language Models",
            "text": "Large language models (LLMs), such as Vicuna [Chiang et al., 2023], Falcon-40B [Penedo et al., 2023], and OpenLLaMA [Touvron et al., 2023] are applied in mainly two ways: fine-tuning and prompt tuning.In fine-tuning, the LLM weights are adapted to a downstream task [Devlin et al., 2018].Fine-tuning can easily incorporate domain knowledge that a pre-trained model may not possess and resembles classic inductive inference.Fine-tuned models often do not need carefully designed prompts, which makes them easier to deploy.The main drawback of fine-tuning is that it can be costly, because tens of thousands of training examples may be needed to fine-tune billions of parameters of the LLM [Ding et al., 2023].In prompt tuning, the LLM weights are fixed and the LLM is given query-specific examples at inference time that affect its output [Lester et al., 2021].This ability to conduct in-context inference is one of the emergent abilities of LLMs.Prompt tuning does not require large training sets.It is also preferred when query-specific examples are private or change over time, and thus can only be utilized at inference time.\n\nPrior works on prompt tuning mainly focus on hard prompts, which are carefully handcrafted to get the desired output.This can be time-consuming and fragile, as minor prompt modifications can lead to a significant performance drop on the downstream task [Suzgun et al., 2022].In contrast, Zhang et al. [2022a,b] and Diao et al. [2023] explored adaptive prompt design using clustering-based and uncertainty-reducing approaches.While these approaches offer some benefits, we argue that optimal designs [Pukelsheim, 2006, Fedorov, 2013] can outperform them by effectively balancing uncertainty and diversity.Similarly to Zhang et al. [2022a,b] and Diao et al. [2023], we propose a framework for adaptive prompt design called active in-context prompt design (AIPD).",
            "score": 0.5142743499816036,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1128
                },
                {
                    "start": 1130,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 66,
                    "matchedPaperCorpusId": "259165195"
                },
                {
                    "start": 688,
                    "end": 707,
                    "matchedPaperCorpusId": "257316425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2156982421875
        },
        {
            "corpus_id": "269005551",
            "title": "Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models",
            "text": "Even when given two documents and using LLMs to determine which document is more relevant to a given query, LLMs suffer from position bias issues [36].\n\nTo better leverage the capabilities of LLMs for IR tasks, we need to optimize the basic models, which involves fine-tuning LLMs on task-specific ranking datasets to enable them to measure query-document relevance and fully understand ranking tasks [12,27,31,34].Fine-tuning the entire model has achieved significant improvements for traditional IR models, but this approach is not easily implemented in LLMs.First, LLMs require more data, especially high-quality and diverse data [3].Original LLMs are based on massive amounts of data to converge and improve performance in specific tasks while ranking tasks often lack large training data samples [50].Second, even with abundant data, the whole model fine-tuning process of LLMs requires substantial computational resources, which means that fine-tuning the entire model is very expensive [48].Third, due to the large number of parameters in LLMs, fine-tuning the entire model poses significant risks.During the fine-tuning process, the model can easily lose its existing capabilities [55].To address these challenges, researchers have proposed more efficient parameter fine-tuning methods, especially for LLMs, which aim to guide LLM performance in new tasks by adjusting a small number of model parameters [9], adding external modules [8] or soft prompt-based fine-tuning [19,20,24].Some existing works in IR have utilized parameter-efficient finetuning (PEFT) methods to enhance the ranking ability of LLMs [4,13,23,32,33,45].However, there are still limitations to these approaches.Some works employ LoRA to make the model more task-specific for ranking [13], but localized task-specific parameter adjustments also limit the model's generalization ability to adapt to other similar tasks [43].For soft-prompt based models [4,23,33], they typically learn a task-specific prompt representation, which can guide the model to generate task-specific outputs.However, the prompt module becomes fixed after tuning, making it inflexible for different tasks.",
            "score": 0.5140775552262487,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 153,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1489
                },
                {
                    "start": 1489,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1901
                },
                {
                    "start": 1901,
                    "end": 2061
                },
                {
                    "start": 2061,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "233443837"
                },
                {
                    "start": 633,
                    "end": 636,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1614,
                    "end": 1617,
                    "matchedPaperCorpusId": "258840999"
                },
                {
                    "start": 1617,
                    "end": 1620,
                    "matchedPaperCorpusId": "240070656"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1629,
                    "end": 1632,
                    "matchedPaperCorpusId": "250526456"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "240070656"
                },
                {
                    "start": 1930,
                    "end": 1933,
                    "matchedPaperCorpusId": "258840999"
                },
                {
                    "start": 1933,
                    "end": 1936,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43115234375
        },
        {
            "corpus_id": "276249983",
            "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning",
            "text": "In recent years, expectations on large language models (LLMs) have evolved, viewing them more and more as agents intended to achieve long-term goals (Wei et al., 2022;Bellos et al., 2024;Havrilla et al., 2024). In particular, a number of research studies have found that LLMs can learn to achieve long-term objectives when fine-tuned with Reinforcement Learning (RL), even with a sparse success/failure signal (Bakhtin et al., 2022;Zelikman et al., 2024;Havrilla et al., 2024;Guo et al., 2025). In such setting, a pre-trained language model is typically used as a policy to explore solutions within a text-generation task. Pre-training plays an ambivalent role in guiding exploration: on the one hand, the policy should not deviate too far from the pre-trained model in order to maintain basic Figure 1: Illustration of the addition task with scratchpad, for a model pre-trained on numbers up to 3 digits. The highlighted critical tokens are decision points where the model tends to make mistakes, mainly because it is tempted to process the number as if it were shorter. This occurs when the model is faced with a number that is longer than those encountered during the pre-training stage (here, 4 digits instead of 3). capabilities (like language structure) -this is why a KL-divergence penalty is typically added to the loss (Ziegler et al., 2020). On the other hand, staying too close to the pre-trained model can significantly hinder its potential for exploration. On this matter (Havrilla et al., 2024) have demonstrated that LLM agents typically fail to explore beyond solutions produced by the pre-trained models. We hypothesize that more precisely balancing the tradeoff between old and new policies can improve the model's exploration capabilities, especially as the distribution shift increases between pre-training and fine-tuning. \n\nThis article examines how varying levels of pretraining affect language model performance in a task requiring some level of exploration.",
            "score": 0.5140030343599823,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 410,
                    "end": 432,
                    "matchedPaperCorpusId": "253759631"
                },
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "268385093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "276249352",
            "title": "Context information can be more important than reasoning for time series forecasting with a large language model",
            "text": "With the introduction of the transformer module into deep learning, models for natural language processing have evolved rapidly. Initially, this module was used to build pretrained language models (PLMs) such as BERT [1] and XLNet [2]. Through transfer learning, after training the model with a large amount of data, PLMs have been fine-tuned for various tasks such as sentiment analysis, entity recognition, summarization, and translation. While fine-tuned PLMs often provide stateof-the-art (SOTA) performance for specific tasks and domains, their applicability to different types of tasks and data domains is quite limited. To overcome these limitations, large language models (LLMs) have been developed by leveraging large clusters of graphical processing units (GPUs), incorporating massive parameters and extensive training datasets. Although LLMs may perform inferior to SOTA models for some specific tasks in certain domains, they can handle a variety of tasks without further training and perform complex tasks However, LLMs may have inherent limitation when the LLMs work with numerical problems without fine-tuning. The token generation in LLMs depends on the input and preceding output. This implies that when an algebraic operation is required, non-numeric tokens act as noise to the algebraic operation, hindering the model's ability to focus solely on the numerical aspects. In addition to this, there are several other issues with LLMs when numeric values in a query hold significant information for generating an accurate answer. This is particularly problematic when LLMs process tasks related to time series of numeric values, such as forecasting and outlier detection [3]. Inputs to LLMs are first tokenized, and many tokenizers build on byte pair encoding (BPE), which progressively expands the dictionary with frequently used subwords. Thus, numbers with long digits can be split into multiple tokens, hindering the extraction of precise patterns in numerical sequences and numerical operations. Furthermore, LLMs are trained to handle arbitrary strings with a massive corpus, which may limit their capability to handle complex numerical operations accurately unless they are fine-tuned with proper datasets. This is because they are not trained enough to identify specific patterns and rules in numerical data.",
            "score": 0.5135575043884613,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040008544921875
        },
        {
            "corpus_id": "258480241",
            "title": "\"Oops, Did I Just Say That?\" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",
            "text": "A. Background: Large Language Model (LLM) \n\nLLMs usually refer to language models that contain hundreds of billions (or more) of parameters, which are trained on massive text data. Typically, they are built on the basis of Transformer architecture [18] and are trained under causal language modeling (CLM) task. CLM aims to predict the token after a sequence of tokens. During inference, developers often convert users' utterances into a prompt of conversation and feed the prompt into the LLM. Then, the LLM will repeatedly generate the next token to constitute the response until the end of the conversation. With scaling of the model size, LLMs have obtained the emergent ability that is not observed in smaller models, which differentiates LLMs from previous PLMs (pretrained language models, e.g., BERT [19]). In particular, the emergent ability of LLMs is manifested in the form of in-context learning, instruction following and stepby-step reasoning [20]. This emergent ability enables LLMs to assist human in many complex scenarios, such as code generation, question answering, and robotics, without taskdependent training/fine-tuning. Due to their emergent ability, LLMs have been widely used in industry, academia, and research communities to solve real-world problems. Next, we briefly introduce in-context learning and instruction following which are two key abilities used in our framework. In-context Learning. Introduced in GPT-3 [21], in-context learning allows LLMs to generate expected outputs for new inputs given a task instruction and a few input/output examples, without task-specific training or gradient updates. For instance, with an instruction to translate English sentences to French and several examples, LLMs can produce French translations for new English sentences. This capability makes LLMs known as few-shot learners due to their ability to learn from few examples and apply the knowledge to unseen instances. Instruction Following. Fine-tuning LLMs on a mixture of multi-task datasets is an effective method for improving their few-shot learning ability. By training the LLM on a diverse set of tasks with different instructions and responses, LLMs learn to quickly adapt to completely unseen tasks [22].",
            "score": 0.5134879091218685,
            "section_title": "II. BACKGROUND, RELATED WORK AND MOTIVATION",
            "char_start_offset": 3258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04681396484375
        },
        {
            "corpus_id": "265150647",
            "title": "ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook",
            "text": "Large Language Models (LLMs) have demonstrated astonishing capabilities in natural language understanding and generation (Wei et al., 2022;Huang et al., 2022;Sun et al., 2022;Bang et al., 2023). However, due to the limited parameters and context processing length, LLMs are not able to master all task-specific knowledge in real-world applications. As a result, LLMs may perform mediocre on some specific tasks, such as inductive reasoning (Bang et al., 2023) and entity recognition (Chen et al., 2023). \n\nTherefore, how to make LLMs adapt to the downstream tasks has tracked more and more attention. Recent techniques such as prefix-tuning (Li and Liang, 2021), P-tuning (Liu et al., 2021) and LoRA (Hu et al., 2021) proposed low-cost solutions for fine-tuning LLMs. However, these methods are not capable of black-box powerful LLMs, such as ChatGPT and GPT4 (OpenAI, 2023). \n\nA: Jack is Maria's son. \n\nQ: [Chris] and his son [Jack] are at a bar waiting for their drinks. [Maria] showed up and sat with her husband [Chris]. Who is Jack to Maria?",
            "score": 0.513307781064208,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1046
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "246035276"
                },
                {
                    "start": 158,
                    "end": 175,
                    "matchedPaperCorpusId": "237940861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20556640625
        },
        {
            "corpus_id": "268063798",
            "title": "How do Large Language Models Handle Multilingualism?",
            "text": "We have verified the proposed framework for explaining the multilingual working mechanism of LLMs in the above section via deactivating certain neurons. While opposite to employing deactivation, we can also enhance their multilingual ability, especially the understanding and generating ability, by fine-tuning these language-specific neurons. \n\nWe employ Llama2-7b-base model for enhancement to eliminate the interference of instruction fine-tuning. We select causal language modeling as our fine-tuning task and create a dataset comprising 200 documents for each language, extracted from the Wikipedia corpus. 4 It is important to note that our enhancements are focused on augmenting the model's capabilities in understanding and generation only; we do not extend its reasoning faculties or broaden its knowledge base as it may require more specific data preparation. Accordingly, we evaluate the efficacy of our enhancements through targeted understanding and generation tasks. Detailed experiment results are shown in Table 8. \n\nOur findings indicate that with just 10 minutes of fine-tuning on 200 contexts, LLMs exhibit significant enhancements in multilingual understanding and generation abilities. Notably, there is a relative performance boost of 7.4% on the XQuAD benchmark. Similarly, for XLSum, we observe an relative improvement in performance. Table 8: Enhancement is achieved by fine-tuning the Llama2-7b-base model through a causal language modeling task, utilizing 200 contexts from each language.",
            "score": 0.5132324072773742,
            "section_title": "Enhance Multilingual Ability",
            "char_start_offset": 18925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.182373046875
        },
        {
            "corpus_id": "273877989",
            "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
            "text": "In developing a code LLM, particularly in computer science and software development, it is essential to ensure that the model excels in both theoretical knowledge and practical coding tasks. To address both needs, we implemented a two-stage instruction fine-tuning process. The detailed composition of instruction tuning is presented in Table 5. \n\nThe first stage of this fine-tuning process focused on synthesizing question-answer (QA) pairs related to theoretical computer science. Building on general-purpose pre-training data, we created a specialized dataset that enabled the model to develop a deeper understanding of theoretical computer science, such as algorithms, data structures, and networking principles. By fine-tuning the model with domain-specific QA pairs, we ensured that it could respond with greater precision to questions about concepts such as binary search trees, dynamic programming, and the intricacies of object-oriented design patterns. In the second stage of the fine-tuning process, we shifted focus from theoretical knowledge to practical coding tasks. In this stage, we used high-quality code from GitHub to create a dataset aimed at improving the model's ability to generate and work with code. By fine-tuning the model on highquality code from GitHub, we ensured it was exposed to real-world examples of well-maintained and formatted code. One key advantage of using high-quality code in the fine-tuning process is that it enhances the model's ability to generate code that is both syntactically and semantically correct. \n\nThe two-stage fine-tuning approach allows the model to excel in theoretical knowledge and practical coding tasks, thereby avoiding the limitations of focusing on only one area. Models that only prioritize theory may struggle with coding, while those focused solely on code generation may lack depth in explaining complex concepts. By refining both areas, the model becomes technically proficient and versatile, able to meet the needs of developers, beginners, and professionals alike.",
            "score": 0.5129406110705536,
            "section_title": "TWO-STAGE INSTRUCTION-TUNING",
            "char_start_offset": 24597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2041
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23388671875
        },
        {
            "corpus_id": "276903421",
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "text": "Recent research has introduced various prompting strategies to enhance language model capabilities. These include tailored prompts, such as chain-of-thought (CoT) prompting, which guides multi-step reasoning, and advanced fine-tuning approaches, such as instruction following, which adapts models to specific tasks. Wei et al. [87] found that certain techniques led to sudden jumps in performance, particularly in large models. For instance, CoT prompting significantly improved performance in math word problems because these problems require step-by-step reasoning, which is exactly the type of thinking CoT induces. \n\nNot just prompting but also fine-tuning strategies have shown emergent effects. Wei et al. [87] further demonstrated that instruction tuning [61] [86], where tasks are framed as instructions, and scratchpad reasoning [58], which predicts intermediate steps, yield substantial performance boosts, but only in large-scale models (100B+ parameters). Lu et al. [55] explored this phenomenon further, disentangling the effects of few-shot prompting, instruction tuning, and CoT prompting to assess emergent abilities in isolation. They questioned whether instruction-tuned LLMs genuinely develop reasoning abilities or simply perform better due to learned heuristics. \n\nTheir experiments, conducted on four model families (GPT-3, T5, LLaMA, and Falcon) across 22 tasks, revealed that without few-shot prompting, these models showed no emergent abilities, performing only marginally better than random guessing, except in two cases: Hindu Knowledge (which relies on memory) and Nonsense Word Grammar (which tests formal linguistic abilities rather than functional reasoning). They concluded that in-context learning (i.e., few-shot prompting) is essential for emergent functional abilities, and while instruction tuning improves general performance, it does not lead to genuine reasoning. \n\nWhile [55] provided valuable insights at the time, it is important to recognize that the field of LLMs has advanced rapidly since their research. More recent models, such as OpenAI o3-mini, Claude 3.5, Gemini 2.0, and DeepSeek-R1, have achieved remarkable advances, calling into question the relevance of its findings. Emerging abilities studies have consistently shown that larger, better-trained models can exhibit fundamentally different and often unpredictable behaviors.",
            "score": 0.5126214322482513,
            "section_title": "B. Relationship between Emergent Abilities and Prompt Strategies",
            "char_start_offset": 22351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050506591796875
        },
        {
            "corpus_id": "270210451",
            "title": "Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification",
            "text": "Nevertheless, these models still require fine-tuning by technologists and underperform when annotated data is insufficient.\n\nRecently, Large Language Models (LLMs), such as Chat-GPT, are considered promising technologies for Artificial General Intelligence (AGI) [11].They achieved remarkable success in various Natural Language Processing (NLP) tasks [12], [13].Instruction fine-tuned LLMs, such as GPT 4.0 and Llama 3, can function as general text classifiers by following natural language prompts.They can perform well on novel text classification tasks without requiring fine-tuning on annotated data [14].This makes LLMs more accessible for research and practice compared to models that require fine-tuning.Recent studies have investigated the potential of LLMs for LEC, including the classification of classroom dialogue behavior [15] and social emotions [16].\n\nHowever, LLMs with vanilla prompts that lack domain knowledge exhibit limitations in numerous text classification tasks [12].For example, the vanilla prompt Given the [text], assign an emotion label from [\"Curiosity\", \"Confusion\", ...] only considers the name string of a label but overlooks the detailed definition of a label.This raises two major challenges.First, a label name may have different semantics in different tasks and contexts.Second, a label name is insufficient to describe complex or unusual labels.Studies have found that the lack of domain-specific knowledge may lead LLMs to generate error results, which can undermine the trustworthiness of LLMs on LEC tasks [17].To address these limitations, researchers have explored various approaches to augment LLMs with additional knowledge.One such approach is Retrieval-Augmented Generation (RAG) [17], which involves retrieving relevant information from external knowledge sources and incorporating it into the prompt during the generation process.Another approach is to utilize annotation guidelines [18], which provide detailed explanations of the labels.By incorporating the information from annotation guidelines into the prompts, LLMs can gain a better understanding of the label semantics and improve their classification performance.",
            "score": 0.5124719417950275,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 125,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 866
                },
                {
                    "start": 868,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 1989
                },
                {
                    "start": 1989,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "256827430"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "265129901"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "268733742"
                },
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "256827430"
                },
                {
                    "start": 1548,
                    "end": 1552,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1933,
                    "end": 1937,
                    "matchedPaperCorpusId": "263671572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "259951327",
            "title": "On the (In)Effectiveness of Large Language Models for Chinese Text Correction",
            "text": "We also have different observations on the two sub-tasks according to human evaluation results. For the CSC task, the gap between the performance of LLMs on the high-quality samples and the low-quality samples is not as significant as that of the small fine-tuned model, which indicates that LLMs are more fault-tolerant to input data, while traditional fine-tuned small models are more sensitive to the quality of input data. Additionally, for high-quality samples, the models perform better when the necessity is not considered than when the consistency is not considered, while for the low-quality samples, the models perform better when the consistency is not considered than when the necessity is not considered. \n\nFor high-quality samples, the model performs better when the necessity is not considered than when the continuity is not considered, while for the low-quality samples, the model performs better when the continuity is not considered than when the necessity is not considered. This phenomenon shows that when the sentence is high-quality, the challenge of the CSC task is how to reduce unnecessary editing, and when the sentence quality is poor, the challenge of the CSC task is how to keep the meaning of the source sentence as much as possible. After applying both LoRA and Full Finetuning to the model, improvements are observed in settings involving both low-quality and high-quality data. In scenarios with high-quality samples, Full Fine-tuning exhibits a slight advantage over LoRA. However, in settings with low-quality samples, LoRA significantly outperforms Full Fine-tuning. For the CGEC task, we see that LLMs sometimes perform better than the traditional fine-tuned small model, according to the results of human evaluation. However, the performance of LLMs on the Consistency metric is steadily worse than that of the small model. This also proves our view that when LLMs like ChatGPT perform text error correction, it always tends to play freely, so that sometimes it will change the meaning of the source sentence. Therefore, to make LLMs well adapted to the CGEC task, it is necessary to study how to make LLMs perform more controllable content generation. \n\nAfter the fine-tuning process, the model that used the LoRA technique surpassed the current baseline in the Crt & Csit tasks.",
            "score": 0.5118785269189318,
            "section_title": "C. Human Evaluation",
            "char_start_offset": 17991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "268537343",
            "title": "Hyacinth6B: A large language model for Traditional Chinese",
            "text": "In the field of Parameter-Efficient Fine-Tuning (PEFT) methods, Low-Rank Adaptation (LoRA) [5] stands out as a noteworthy approach, especially in the context of Large Language Models (LLMs).LoRA operates by inserting small, trainable low-rank matrices into each layer of a pre-trained model, such as GPT series or LLaMA [6], effectively fine-tuning the model while keeping the majority of the pre-trained parameters frozen.This technique significantly reduces the number of parameters that need to be trained, thereby decreasing computational costs and memory requirements, yet still manages to achieve performance comparable to full-parameter fine-tuning.\n\nThe primary advantage of LoRA lies in its ability to efficiently adapt large-scale models for specific tasks under computational constraints.Unlike conventional fine-tuning methods, which require adjustments to all model parameters, LoRA targets only a small subset, making it an ideal choice for applications with limited computational resources.This efficiency makes LoRA particularly suitable for fine-tuning LLMs in task-specific scenarios, where the deployment environment may not allow for extensive computational overhead.\n\nIn comparison to other PEFT methods like P-tuning [7], which focuses on tuning promptbased parameters, LoRA distinguishes itself by directly modifying the internal weights of the model in a low-rank manner.While P-tuning is effective for leveraging prompts to guide model behavior, LoRA offers a more direct approach to adjusting model behavior by altering its internal representations.This difference in approach allows LoRA to be more versatile in a wider range of tasks and settings, potentially offering better integration and fine-tuning effectiveness for LLMs.\n\nThe applications of LoRA in the LLM domain are diverse, ranging from natural language understanding and generation to more complex reasoning and computation tasks.By enabling efficient and effective fine-tuning, LoRA represents a significant advancement in the field of language models, offering a practical solution to harness the power of LLMs in a resourceconstrained environment.",
            "score": 0.511839954997851,
            "section_title": "Low-Rank Adaptation",
            "char_start_offset": 2886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 656
                },
                {
                    "start": 658,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1187
                },
                {
                    "start": 1189,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1755
                },
                {
                    "start": 1757,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "273963843",
            "title": "Large Language Models and Artificial Intelligence Generated Content Technologies Meet Communication Networks",
            "text": "LLMs are LMs that contain billions (or more) of parameters, which can be adapted to a wide range of downstream tasks. As one of the AIGC technologies, by learning a large amount of text data and linguistic patterns, the LLM can generate realistic, coherent, and creative textual contents based on given context and prompt information, which is a powerful text generation tool. \n\nThe characteristics of LLMs can be summarized in three specific points. \n\n\u2022 Large-Scale Parameters: The number of parameters in LLMs is very large, reaching billions or even tens of billions, which allows the model to learn and represent complex linguistic patterns, including syntactic, semantic, and even contextual information, generating coherent and meaningful texts. In addition, the scale of training data is huge, coming from a variety of sources such as the Internet, books, and news. These pose an emergent phenomenon for LLM, namely, when the size of the model reaches a certain threshold, the model's performance and generalization ability appear to be significantly improved. \n\n\u2022 Multi-Tasking Capability: LLMs are capable of handling various types of tasks, including text summarization, translation, sentiment analysis, etc. Besides, there is an emergence phenomenon when the scale of LMs is increased. This phenomenon manifests as a substantial and abrupt improvement in model performance beyond a specified threshold. Therefore, LLMs have strong generalization abilities and exhibit performance far beyond that of typical small LMs when dealing with unseen tasks. \n\n\u2022 Two-Stage Training: In the pre-training phase, LLMs are trained on large-scale general text datasets to learn the basic structure of the language and various common senses. Then, in the fine-tuning phase, the model is further trained on a smaller, more specific dataset, which is usually specific to a particular task or domain, such as medical text, legal text, or specific conversation data. Fine-tuning allows the model to better understand and generate language for this specific domain and thus be better suited for the specific task. \n\n\u2022 Reasoning Capabilities: Chain of Thought (CoT) is an enhanced prompting technique that improves the performance of LLMs on complicated reasoning tasks, particularly mathematical problems.",
            "score": 0.5116776495765125,
            "section_title": "C. Characteristics of LLMs",
            "char_start_offset": 17932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2295
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0283355712890625
        },
        {
            "corpus_id": "278000615",
            "title": "ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving",
            "text": "Language modeling forms the foundation of modern LLMs, particularly in decoder-only architectures like GPT-2 [33] and GPT-3 [6]. The core task involves autoregressive prediction of tokens in a sequence, where each token is predicted based on its preceding context. Formally, given a sequence of tokens  = { 1 , . . . ,   }, the language model learns to predict each token   conditioned on its preceding tokens  < . The training objective maximizes the log-likelihood: \n\nDuring generation, at each step , the model samples token  Variants demonstrate alternative interpretations of the problem. \n\n\u2022 Pretraining: Models learn to predict next tokens across vast corpora, developing general language understanding and generation capabilities. \u2022 Instruction Tuning: Models are fine-tuned to follow specific instructions and align with human preferences, transforming raw language capabilities into useful applications. \n\nOur research focuses on extending instruction tuning for code generation tasks to incorporate clarification-aware behavior. Current instruction-tuned models often generate code even when faced with ambiguous or incomplete specifications, a problem documented in [45]. We propose a dual objective: \n\n\u2022 Maintain the model's code generation capabilities for wellspecified tasks \u2022 Enable the model to recognize ambiguity and request clarification when necessary \n\nWe specifically target instruction tuning for this capability due to its efficiency and widespread adoption in language model development. This approach allows us to modify model behavior without the computational and data requirements of full pretraining, while still achieving significant improvements in clarify awareness.",
            "score": 0.5114287981267154,
            "section_title": "Backgrounds",
            "char_start_offset": 6132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "270214191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2958984375
        },
        {
            "corpus_id": "267412360",
            "title": "Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives",
            "text": "Furthermore, large language models (LLMs) like GPT-3 [20] predict next token given the prefix language tokens in an  autoregressive manner. The pretraining objective is employed in Decoder-only LLMs, as shown in Eq. 5: \n\nwhere x 1:l\u22121 denotes the prefix sequence before l-th token and x l is the l-th next token prediction. \n\n6) Fine-tuing Techniques: As foundation models increase in parameter size, they showcase emerging abilities. However, the challenges associated with directly full parameter finetuning for new tasks and domains can be a significant concern, including computational resources, complexity, and potential risks such as overfitting. Therefore, to overcome these challenges, we mainly conclude two types of commonly used finetune techniques as follows: \n\nPrompt tuning has drawn great attention with the release of GPT-3 [20]. Compared to full parameters fine-tuning, prompt tuning is often more resource-efficient, particularly when computational resources and task-specific data are limited. For example, in-context learning [20], [143] aims to perform a new task based on few-shot examples provided within the context of a prompt, rather than performing gradient updates through explicit fine-tuning on a large labeled dataset. For hard tasks requiring logic reasoning, such as mathematics, chainof-thought prompting [198] provides a detailed, step-by-step explanation or reasoning within the prompt itself, which guides the model to generate more accurate and logical responses. \n\nInstruction tuning is a simple and straightforward multitask fine-tuning technique that involves adapting a pretrained foundation model to a specific task by providing it with clear, task-related instructions. During inference, the foundation model can generalize to some unseen tasks by explicitly adding task instructions, especially when the inference tasks are similar to those it was trained on. Moreover, it can also help foundation models act as human assistant whose response are better aligned with human intents. Representative works like InstructGPT [145], Flan-T5 [38].",
            "score": 0.5113585778356537,
            "section_title": "Vision-Beyond multi-modal data",
            "char_start_offset": 17640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1340,
                    "end": 1345,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 2066,
                    "end": 2071,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 2081,
                    "end": 2085,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1788330078125
        },
        {
            "corpus_id": "273653859",
            "title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond",
            "text": "Naive Fine-Tuning. [118] finds that after training, further fine-tuning the context-aware embeddings and the recommendation LLM would result in better performance. [77] investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. [8] finds that fine-tuning an LLM for recommendation tasks enables it to learn not only the tasks but also concepts of a domain to some extent. It also shows that fine-tuning OpenAI GPT led to considerably better performance than fine-tuning Google PaLM 2 [32]. [31] uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation. [45] fine-tunes a 7-billion-parameter model by designing an imitation dataset derived from GPT-4, which can improve the ability of interactive recommendations. [36] fine-tunes an LLM with dataset-specific information in the form of prompt-completion pairs and asks the model to produce next item recommendations for test prompts. [25] proposes to fine-tune LLM with reinforcement learning from conversational recommender systems performance feedback for improving the performance of recommendation. [63] explores fine-tuning ChatGPT by formulating the news recommendation as direct ranking and rating tasks. [9] proposes to align LLMs with the recommendation domain by fine-tuning with data samples that encode recommendation knowledge and also proposed auxiliary-task data samples that encode item correlations contained in users' preferences. In the e-commerce domain, [83] investigates the effectiveness of combining LLM and conversational recommendation systems and fine-tunes large language models including ChatGLM and Chinese-Alpaca-7B using pre-sales dialogues. [72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms. [20] conducts supervised fine-tuning (SFT) for the LLM to activate its power in the task-related domain.",
            "score": 0.511068560315316,
            "section_title": "Non-Generative LLM-based Approaches",
            "char_start_offset": 42352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "261076297"
                },
                {
                    "start": 682,
                    "end": 686,
                    "matchedPaperCorpusId": "262045057"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "261823711"
                },
                {
                    "start": 1420,
                    "end": 1423,
                    "matchedPaperCorpusId": "268819967"
                },
                {
                    "start": 1683,
                    "end": 1687,
                    "matchedPaperCorpusId": "264426498"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "271543899",
            "title": "Comparison of Large Language Models for Generating Contextually Relevant Questions",
            "text": "It is remarkable that even without fine-tuning, the models mostly scored high in the evaluations of the analyzed metrics. This indicates that the LLMs can be used for a satisfactory QG quickly and without specific knowledge about technical aspects of the models or specific training. However, the LLMs still have a few weaknesses, mainly in ensuring the alignment between the output questions and the provided answers, and also in limiting bias towards certain types of questions (e.g., avoiding generic \"what is\" questions). Another constrain is that if the LLM output was to be integrated to educational software for practical deployment, so that the instructors would not have to interact with the LLM directly, the best-performing LLM is not freely available. Thus, the associated costs might limit the scalability of the solution. \n\nDespite these limitations, the evaluated LLMs are advisable for educational applications, especially the support of students' personalized learning. This includes knowledge reinforcement or quick diagnostic quizzes, where occasional problems related to clarity or alignment can be tolerated, and the variance in difficulty is beneficial. In other tasks, such as automatic generation of formal tests and exams, the alignment and clarity of the question have to be precise, so the evaluated LLMs might not be suitable. \n\nFuture work may optimize models and methodologies to further enhance the quality of AI-generated questions. For example, even though the LLMs performed well without fine-tuning, it would be interesting to explore whether fine-tuning would significantly improve the results. Related to this, future work can compare different prompts and describe which of them lead to better or worse questions.",
            "score": 0.5107233027828443,
            "section_title": "Benefits and Limitations for Educational Applications",
            "char_start_offset": 9680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1751
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11163330078125
        },
        {
            "corpus_id": "259145356",
            "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
            "text": "From the taxonomy of task adaptation approaches, only in-context learning satisfies the taskagnostic property but it consistently underperforms the task-specific tuning approaches. This section investigates why this performance gap exists. We hypothesize that it is either because (a) the representations learned by the LLM are insufficient to learn a good predictor for the specific task, or (b) the LLM lacks the capability to reason over these representations to make good predictions for the task. \n\nTo understand whether the representations have sufficient information, we train a task-specific linear classifier using these representations, also known as linear probing, and evaluate its accuracy. Let Acc FT , Acc ICL , and Acc LR denote the accuracies obtained by fine-tuning, in-context learning, and by linear probing respectively. Using this as an intermediate, we decompose the performance gap \n\nwhere \u2206 rep represents the gap in performance which can be attributed to insufficient representation capacity and \u2206 reas is the performance gap due to insufficient reasoning abilities. Using this decomposition, we consider the following hypotheses: H1. LLM representations have enough information to perform the task in-context, but they lack the reasoning abilities to perform the task well. \n\nH2. Fine-tuning affects both the representations and reasoning but the improvement in reasoning abilities primarily leads to better performance. \n\nH3. Fine-tuning and adapters are not task-agnostic because the task-specific training hurts their ability to transfer reasoning. \n\nWe now analyze each of the task adaptation approaches through the lens of the above hypotheses. We perform all experiments with three different classes of language models (GPT-Neo, Pythia, Bloom) across a collection of 6 binary classification tasks. See Appendix B for further details. In-context learning: LLMs lack reasoning abilities. We begin by studying the representation and reasoning gaps, as defined in eq. ( 1), for in-context learning. In Figure 3a, we plot the average accuracy across datasets for in-context learning, task-specific fine-tuning, and linear probing. We see that across models and different numbers of in-context examples, the reasoning gap \u2206 reas accounts for up to 79.11% of the performance gap between in-context learning and fine-tuning.",
            "score": 0.5104920146699106,
            "section_title": "Understanding performance via Representation-Reasoning decomposition",
            "char_start_offset": 10322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0386962890625
        },
        {
            "corpus_id": "269137180",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "text": "[cs.LG] 12 Apr 2024 produce reasonably good workflows, the model may hallucinate, particularly if the natural language input is out-of-distribution.As the nature of enterprise users requires them to customize their applications, in this case by adding their own type of workflow steps, a commercial GenAI application needs to minimize the out-of-distribution mismatch.\n\nWhile one could fine-tune the LLM per enterprise, this may be prohibitively expensive due to the high infrastructure costs of fine-tuning LLMs.Another consideration when deploying LLMs is their footprint, making it preferable to deploy the smallest LLM that can perform the task.\n\nOur contributions are the following:\n\n\u2022 We provide an application of RAG in workflow generation, a structured output task.\u2022 We show that using RAG reduces hallucination and improves results.\u2022 We demonstrate that RAG allows deploying a smaller LLM while using a very small retriever model, at no loss in performance.",
            "score": 0.5101047218018562,
            "section_title": "Introduction",
            "char_start_offset": 2126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 649
                },
                {
                    "start": 651,
                    "end": 687
                },
                {
                    "start": 689,
                    "end": 773
                },
                {
                    "start": 773,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09637451171875
        },
        {
            "corpus_id": "269362836",
            "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
            "text": "Trans. \n\nGeneral to future tasks without severely impacting previous ones [156], zero or near-zero forgetting is only observed at the representation level. This implies that while the model retains its ability to distinguish between task-specific representations, it may still forget specific task details [144,223,260,299]. Therefore, additional measures are necessary when deploying these models in real-world applications [10,37,106,182,254,281]. \n\nMany studies advance beyond naive sequential fine-tuning, leveraging the inherent anti-forgetting nature of LLMs while avoiding the adoption of overly complex CL techniques [255,299]. For instance, LR ADJUST [255] proposes a straightforward yet effective method of dynamically adjusting the learning rate to mitigate the overwriting of knowledge from new languages onto old ones. Building on the innate anti-forgetting ability of large language models like Pythia [16], SEQ * [299] introduces several strategies for fine-tuning LLMs on a sequence of downstream classification tasks, such as freezing the LLM and old classifier's parameters after warm-up, and pre-allocating future classifiers, etc. \n\nGiven the minimal forgetting observed at the representation level in CL, some studies aim to tackle the misalignment between the representation space and the decision-making layers by introducing representation-level constraints during CFT. NeiAttn [10] exemplifies this approach by formulating classification tasks as masked language modeling and proposing a neighboring attention mechanism to counteract negative representation drift. \n\nAnother line of approaches refines the input/output format and network architectures of pre-trained LLMs to be better suited for CFT. For instance, CTR [106] incorporates two CL-plugin modules, i.e., a task-specific module (TSM) \n\nfor acquiring task-specific knowledge and a knowledge-sharing module (KSM) for selectively transferring previously learned similar knowledge. CIRCLE [281] manually designs diverse prompt templates for various types of buggy code, unifying them as the cloze task and employs difficulty-based replay to enhance continual program repair. LFPT5 [182] addresses lifelong few-shot language learning by consolidating sequence labeling, text classification, and text generation into a text-to-text generation task.",
            "score": 0.5097512218105824,
            "section_title": "Fwd.",
            "char_start_offset": 59045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 9,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 79,
                    "matchedPaperCorpusId": "245329773"
                },
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "257279790"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "247717213"
                },
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "263829616"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "238856821"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "258556987"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "248986947"
                },
                {
                    "start": 625,
                    "end": 630,
                    "matchedPaperCorpusId": "258887506"
                },
                {
                    "start": 660,
                    "end": 665,
                    "matchedPaperCorpusId": "258887506"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1972,
                    "end": 1977,
                    "matchedPaperCorpusId": "248986947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04083251953125
        },
        {
            "corpus_id": "265157870",
            "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
            "text": "The ability of Large Language Models (LLMs) to generate coherent and meaningful text has improved significantly over the years (OpenAI, 2023). However, LLMs often exhibit inaccuracies in their initial generations, and it has been posited that iterative refinement can rectify their errors (Madaan et al.;Shridhar et al., 2023a;Welleck et al.;Zheng et al., 2023). Madaan et al. demonstrated the potential of self-refinement for diverse tasks such as dialogue response and sentiment reversal; however, this approach proved less effective when applied to mathematical reasoning. Similarly, Shridhar et al. (2023a) and Huang et al. (2023) further demonstrated the challenges LLMs face in identifying errors in reasoning tasks. Developing models that consistently evaluate and correct their errors would be a valuable step towards building more reliable language models. \n\nThrough empirical observation on two multistep reasoning datasets, we find that self-refinement does not reliably improve initial generations, validating the previous findings of Huang et al. (2023). In fact, in the majority of cases, self-refinement has a detrimental effect on performance. On the other hand, fine-tuning language models usually improves their performance on a given task by facilitating better adaptation to the task objectives (Yuan et al., 2023). Smaller models can be trained on LLMs' data to improve their performance, which can serve as cost-effective alternatives to LLMs for the given task (Magister et al., 2023;Shridhar et al., 2023b;Hsieh et al., 2023). This led us to explore the possibility of training a smaller model as a decision maker for refinement, which can consistently determine when to refine, while the larger model can subsequently perform the refinement process.",
            "score": 0.5094809933316367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1774
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 304,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 327,
                    "end": 342,
                    "matchedPaperCorpusId": "253244506"
                },
                {
                    "start": 1484,
                    "end": 1507,
                    "matchedPaperCorpusId": "254823156"
                },
                {
                    "start": 1507,
                    "end": 1530,
                    "matchedPaperCorpusId": "258762841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2034912109375
        },
        {
            "corpus_id": "268819931",
            "title": "Token-Efficient Leverage Learning in Large Language Models",
            "text": "The TELL strategy, rooted in Leverage Learning, consists of two primary components: data preprocessing and model fine-tuning, aimed at optimizing the use of low-resource task data.In data preprocessing, \"anchor prompts\" are used to implant consistent semantic features into the low-resource data, helping LLMs distinguish it from general data during training.This approach enables LLMs to better leverage general data for learning specific tasks.\n\nIn model fine-tuning, the focus is on maximizing task-specific learning under tight budget constraints, often employing Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA.A key tactic is to significantly increase the volume of general data and then integrate it with task data through extensive shuffling, enhancing the model's ability to learn from limited task-specific information.\n\nEmpirical evidence suggests that a very high ratio of general to task-specific data (typically beyond 1000%) is crucial for improving performance on lowresource tasks, surpassing the conventional ratios used in multi-task learning scenarios.This approach has hinted at potential emergent improvements in LLM performance, significantly boosting their performance on these tasks.While the current method of random mixing serves to validate the effectiveness of Leverage Learning, we believe that further refined strategies could enhance its performance even more.",
            "score": 0.5092184240664374,
            "section_title": "Components of TELL",
            "char_start_offset": 13732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 446
                },
                {
                    "start": 448,
                    "end": 625
                },
                {
                    "start": 625,
                    "end": 838
                },
                {
                    "start": 840,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1217
                },
                {
                    "start": 1217,
                    "end": 1401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15234375
        },
        {
            "corpus_id": "273384438",
            "title": "CurieLM: Enhancing Large Language Models for Nuclear Domain Applications",
            "text": "Transitioning from the discussion on data collection and preparation, we now come to the essential process of instruction tuning. This process is integral to enhancing the performance of LLMs on domain-specific tasks, particularly in complex fields like nuclear engineering. \n\nLLMs, such as GPT-4 , LLaMA2 and Mistral , have been trained on extensive datasets, providing them with a general understanding of language. However, their performance on domain-specific tasks can be significantly improved through a process known as instruction tuning. This process involves fine-tuning the LLM on a set of domain-specific instructions, enabling it to improve its performance on related tasks. \n\nDespite the benefits of instruction tuning, the process of fine-tuning LLMs is computationally intensive and requires significant resources and time. This has traditionally limited the applicability of fine-tuning, particularly for larger models that barely fit into GPU memory. \n\nThe advent of Low-Rank Adaptation (LoRA) [21] and Quantized LoRA (QLoRA) [22] has revolutionized the fine-tuning process. These techniques reduce the computational requirements of fine-tuning by breaking down complex structures, called weight matrices, into simpler and smaller, lower-rank forms This allows for efficient training of custom LLMs, even on consumer-grade GPUs. \n\nThe efficiency provided by LoRA and QLoRA not only makes fine-tuning more accessible but also enhances the feasibility of instruction tuning. This paves the way for the development of domain-specific LLMs like CurieLM, which are tailored to perform optimally on nuclear-related tasks.",
            "score": 0.5089533785294833,
            "section_title": "Instruction Tuning",
            "char_start_offset": 13328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1633
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41748046875
        },
        {
            "corpus_id": "277681369",
            "title": "A Survey on Quality Evaluation of Instruction Fine-tuning Datasets for Large Language Models",
            "text": "However, the size of the instruction fine-tuning dataset is not the primary determinant of its quality. Recent research focuses on the principle of \"Less is More\", demonstrating that a small set of high-quality instructions can fine-tune LLMs to achieve comparable or even better performance [8,9]. \n\nMeanwhile, the method of constructing instructions has a more direct impact on instruction quality and LLMs performance. Currently, there are four types of methods to construct instruction fine-tuning datasets, which are human construction, model construction, collection and improvement of existing Data Intelligence open source datasets, and the combination of the above three methods [18]. Human-constructed instructions offer greater quality control but are limited in scale. Model-constructed instructions can be produced quickly and at scale, though they may lack diversity and complexity. The collection and improvement of existing open-source datasets often cover a wide range of topics and domains, providing a diverse set of instructions, but their quality is constrained by the data sources. Therefore, when constructing an instruction fine-tuning dataset, multiple methods are typically combined to ensure both the quality and diversity of instructions, thus effectively improving the generalization ability of LLMs. \n\nIn addition, task requirements in different scenarios influence the construction methods and content of instruction fine-tuning datasets [19]. For example, general-domain instruction fine-tuning datasets usually cover multiple domains and aim to improve the instruction-following ability of LLMs in general tasks. In contrast, domain-specific instruction fine-tuning datasets are developed to meet the requirements of a specific domain, aiming to make the LLM perform tasks efficiently in a particular field. \n\nFor example, in the medical domain, the LLM can be fine-tuned with medical-related instructions to perform medical diagnoses, thereby providing support for various tasks in medical scenarios [20]. \n\nTable 2 provides information on the mainstream datasets of instruction fine-tuning.",
            "score": 0.5087648907593003,
            "section_title": "Instruction Fine-tuning Datasets",
            "char_start_offset": 8552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1840
                },
                {
                    "start": 1843,
                    "end": 2039
                },
                {
                    "start": 2042,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 295,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 295,
                    "end": 297,
                    "matchedPaperCorpusId": "267522839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267578125
        },
        {
            "corpus_id": "260887838",
            "title": "Large Language Models for Information Retrieval: A Survey",
            "text": "Language models (LMs) are designed to understand or generate human language by taking into account the contextual information from word sequences. The evolution from statistical language models to neural language models makes it feasible to utilize LMs for representation learning beyond mere word sequence modeling. Peters et al. [54] first proposed to learn contextualized word representations through pre-training a bidirectional LSTM (biLSTM) network on large-scale corpora, followed by fine-tuning on specific downstream tasks. Similarly, Devlin et al. [55] proposed to pre-train a Transformer [56] encoder with a specially designed Masked Language Modeling (MLM) task and Next Sentence Prediction (NSP) task on large corpora. These studies initiated a new era of pre-trained language models (PLMs), with the \"pre-training then fine-tuning\" paradigm emerging as the prevailing learning approach. Along this line, numerous generative PLMs (e.g., GPT-2 [33], BART [57], and T5 [58]) have been developed for text generation problems including summarization, machine translation, and dialogue generation. Recently, researchers have observed that increasing the scale of PLMs (e.g., model size or data amount) can consistently improve their performance on downstream tasks (a phenomenon commonly referred to as the scaling law [59,60]). Moreover, large-sized PLMs exhibit promising abilities (termed emergent abilities [42]) in addressing complex tasks, which are not evident in their smaller counterparts. Therefore, the research community refers to these large-sized PLMs as large language models (LLMs). \n\nOwing to their vast number of parameters, fine-tuning LLMs for specific tasks, such as IR, is often deemed impractical. Consequently, two prevailing methods for applying LLMs have been established: in-context learning (ICL) and parameter-efficient fine-tuning. ICL is one of the emergent abilities of LLMs [34] empowering them to comprehend and furnish answers based on the provided input context, rather than relying merely on their pre-training knowledge. This method requires only the formulation of the task description and demonstrations in natural language, which are then fed as input to the LLM.",
            "score": 0.5086352366486899,
            "section_title": "Large Language Models",
            "char_start_offset": 14919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "210861095"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1915,
                    "end": 1919,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1187744140625
        },
        {
            "corpus_id": "259951356",
            "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning",
            "text": "Fine-tuning a foundation model can result in the forgetting of pre-trained knowledge, which may lead to sub-optimal performance on downstream tasks. Forgetting occurs when the target model deviates significantly from the pre-trained model during the fine-tuning process [133]. This deviation increases the likelihood of overfitting to a small fine-tuning set [134], which can contribute to forgetting. There are several simple and effective strategies to mitigate forgetting during the fine-tuning process. These include techniques such as learning rate decreasing [134], weight decay [135], [136], and Mixout regularization [133]. Furthermore, Fatemi et al. [137] find that in the study of mitigating the gender bias of the pre-trained language model, the pre-trained knowledge will be forgotten when the small neutral data is fine-tuned, which will hurt the downstream task performance. Dong et al. [138] observe that adversarial fine-tuning of pre-trained language models is prone to severe catastrophic forgetting, causing the loss of previously captured general and robust linguistic features. To address these issues, they propose a Robust Informative Fine-Tuning method from an information-theoretical perspective. In addition, an approach called Recall and Learn, proposed in Chen et al. [139], addresses the forgetting issue by utilizing Pretraining Simulation and Objective Shifting. This approach enables multi-task fine-tuning without relying on the data from the pretraining tasks. Zhang et al. [140] conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text. Conjugate Prompting [141] seeks to reduce the forgetting of prior task knowledge during the fine-tuning of new tasks by counteracting changes in implicit task inference. Furthermore, several recent studies have shown that merging pretrained and fine-tuned models directly at the parameter level can also help mitigate forgetting [142], [143]. Moreover, recent research [144] highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter [144] introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs.",
            "score": 0.5086157544817117,
            "section_title": "Forgetting in Fine-Tuning Foundation Models",
            "char_start_offset": 41659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 364,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 565,
                    "end": 570,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 585,
                    "end": 590,
                    "matchedPaperCorpusId": "2925420"
                },
                {
                    "start": 901,
                    "end": 906,
                    "matchedPaperCorpusId": "245116938"
                },
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "216553067"
                },
                {
                    "start": 1508,
                    "end": 1513,
                    "matchedPaperCorpusId": "271745852"
                },
                {
                    "start": 1970,
                    "end": 1975,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 2010,
                    "end": 2015,
                    "matchedPaperCorpusId": "266374841"
                },
                {
                    "start": 2151,
                    "end": 2156,
                    "matchedPaperCorpusId": "266374841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054595947265625
        },
        {
            "corpus_id": "273228110",
            "title": "Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models",
            "text": "During the pretraining phase, large language models (LLMs) acquire vast amounts of knowledge from extensive text corpora. Nevertheless, in later stages such as fine-tuning and inference, the model may encounter knowledge not covered in the initial training, which can lead to hallucinations and degraded performance. This issue has a profound impact on the model's capabilities, as it will inevitably face out-of-scope knowledge after pretraining. Furthermore, fine-tuning is often required to adapt LLMs to domain-specific tasks. However, this phenomenon limits the model's ability to learn and integrate new information during fine-tuning. The effectiveness of fine-tuning largely depends on the type of knowledge involved. Existing research suggests that fine-tuning the model on partially mastered knowledge-for instance, question-answer pairs where the model has a chance of providing correct responses under non-greedy decoding-can enable the model to acquire new knowledge while mitigating hallucination. Notably, this approach can still lead to the forgetting of fully mastered knowledge, constraining the fine-tuning dataset to a narrower range and limiting the model's overall potential for improvement. Given the model's intrinsic reasoning abilities and the interconnectedness of different knowledge areas, it is likely that as the model's capacity to utilize existing knowledge improves during fine-tuning, previously unmastered knowledge may become more understandable. To explore this hypothesis, we conducted experiments and, based on the results, proposed a two-stage fine-tuning strategy. This approach not only improves the model's overall test accuracy and knowledge retention but also preserves its accuracy on previously mastered content. When fine-tuning on the WikiQA dataset, our method increases the amount of knowledge acquired by the model in this stage by 24%.",
            "score": 0.5085086267927952,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1903076171875
        },
        {
            "corpus_id": "261048999",
            "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
            "text": "Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.",
            "score": 0.508369198471752,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2010498046875
        },
        {
            "corpus_id": "274012260",
            "title": "Integrated AI Medical Emergency Diagnostics Advising System",
            "text": "Some study [34] shows that training on a mixture of tasks improves generalization and robustness. For general LLMs trained on diverse domains and tasks, prompt-based fine-tuning can also be a good option. Prefix-tuning and prompt-based fine-tuning adjust the input prompts or prefixes fed to the model rather than modifying the model parameters directly. Prefix-tuning by adding prefixes to the input sequence, allowed the model to adapt to specific tasks [35]. Unlike traditional fine-tuning where the model's parameters are adjusted, prompt-based fine-tuning involves minimal or no changes to the model's weights. Instead, it relies on designing effective queries that result in desired outputs from the model. This method leverages the pre-trained model's existing knowledge and can be efficient for LLMs trained on various tasks and knowledgebases; However, designing effective prompts and queries can be challenging and often requires domain knowledge. Also, some domains may require additional parameter adjustments to achieve optimal performance. \n\nIn our solution, a mixed method of fine-tuning is used. We developed prompt-based fine-tuning for each LLM to receive optimum domain-specific responses based on the requirements of emergency medicine. The free-text format queries from medical staff consisting of health conditions, and symptoms of patients are fine-tuned to obtain optimal information for emergency medicine from LLMs on most probable diagnoses with their corresponding probabilities, rationales, urgencies, and treatments. The recommendations by the solution for intervention and treatment of emergency patients were evaluated as significant outcomes by a group of subject matter experts. These recommendations were aligned with standard guidelines and scientific evidence from publications in international medical journals. \n\nIn the next step, the results will be used for full fine-tuning of the model on emergencyspecific data using a low learning rate. Full fine-tuning is particularly beneficial when enough computational resources are available. Comparing feature extraction methods, full fine-tuning allows the whole model to learn from the task-specific data and leads to a more profound adaptation of the model to emergency medicine, which provides superior performance. \n\nFor this purpose, the LLMs will be trained on a domain-specific labeled dataset, where each input data point is associated with a real outcome of the case.",
            "score": 0.5082712664174585,
            "section_title": "Fine-Tuning Methods",
            "char_start_offset": 45033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2304
                },
                {
                    "start": 2307,
                    "end": 2462
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 15,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "230433941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1605224609375
        },
        {
            "corpus_id": "278501703",
            "title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
            "text": "In the context of early sequential language tasks, encompassing both natural and programming languages, task-specific model fine-tuning has shown promising performance [20]. Fine-tuning involves updating the model weights and enhancing its performance on specific tasks by learning the relationship between the input and output of a specific downstream task dataset. \n\nAs a phenomenon-level technology, the emergence and rapid development of large language models has triggered disruptive changes in related fields. For example, ChatGPT [66], LLama [84], Claude [4], etc., which usually contain billions or even hundreds of billions of parameters, have been trained on massive text data, and have powerful language understanding and generation capabilities. Since LLMs encapsulate comprehensive knowledge, they can be applied to downstream tasks using a novel method called in-context learning [16], eliminating the need for extensive downstream datasets for fine-tuning. In-context learning allows the model to perform specific tasks directly by providing task-related contextual information without updating its parameters [9]. \n\nIn this paper, we adopt the method of in-context learning to guide LLMs to understand the binary analysis tasks from multiple perspectives, thus facilitating a comprehensively evaluation of LLMs' performance on binary analysis.",
            "score": 0.5079587706131732,
            "section_title": "Large Language Models",
            "char_start_offset": 9361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1359
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1439208984375
        },
        {
            "corpus_id": "259501760",
            "title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey",
            "text": "analysis, and language generation [19]. Furthermore, the success of PLMs has led to the development of the \"pre-training and fine-tuning\" learning paradigm [20]. Here, the PLM is pre-trained on a large unlabeled dataset and then fine-tuned on a small labeled dataset for a specific downstream task. This approach has become a popular method for achieving state-of-the-art results in many NLP tasks [21].\n\nRecent research has shown that scaling up PLMs in terms of model size or data size leads to improved model capacity for downstream tasks by following the scaling law [22]. This has led to a number of studies exploring the upper limits of performance by training even larger PLMs, such as the 175 billion-parameter GPT 1 -3 and the 540 billion-parameter PaLM. These larger PLMs behave differently when compared with the relatively smaller ones such as BERT with 330 million parameters or GPT-2 with 1.5 billion parameters. They have been observed to exhibit surprising emergent abilities that enable them to solve complex tasks that were previously thought to be impossible [23,24]. For instance, GPT-3 can solve few-shot tasks through in-context learning while GPT-2 cannot [25]. These large-sized PLMs are now referred to as large language models (LLMs) [26]. One prominent application of LLMs is ChatGPT, which adapts LLMs from the GPT series for dialogue and demonstrates a unique ability to converse with humans [27]. Thus, the fourth stage of LM development can be seen as the introduction of these LLMs and subsequent exploration of the upper limits of Fig. 2 Number of tokens seen during training. Adapted from [33].\n\ntheir performance through the development of ever more extensive and more powerful models that exhibit emergent abilities and can solve even more challenging tasks. A brief timeline of LLMs is illustrated in Figure 1.\n\nThe emergence of such models which exhibit both surprisingly useful and complex capabilities boosted the attention toward a broader area of research, Generative AI (GAI). GAI, a branch of AI, involves the creation of computer models that can generate new and convincing content, such as images, music, or text (more details in Section 2.2). While the concept",
            "score": 0.5078919192370207,
            "section_title": "Introduction",
            "char_start_offset": 4169,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "256358845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04400634765625
        },
        {
            "corpus_id": "277349136",
            "title": "Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models",
            "text": "LLMs have also been fine-tuned for software engineering and coding tasks such as code generation and code completion. Contemporary tools include GitHub Copilot7 and Meta Code Llama8 . Research in this area has been extensive with the vast majority spread across software development and maintenance (Hou et al., 2024). \n\nCode generation has used a mix of general LLM tools such as GPT-3.5 / 4.0 (Dou et al., 2024;Yeti\u015ftiren, \u00d6zsoy, Ayerdem, & T\u00fcz\u00fcn, 2023) and fine-tuned models such as GPT-3's codex (Chen et al., 2021). Performance of LLMs in code generation in these studies typically makes use of a curated benchmark dataset of programming problems and their solutions such as the Mostly Basic Python Problems Dataset9 (Austin et al., 2021). LLM solution performance against these datasets are evaluated using various standard metrics. \n\nPrompt engineering to reduce ambiguity of intent has emerged as a key challenge for code generation as LLMs, which may struggle to reliably generate code reflecting the users' requirements. To date the majority of studies have employed zero-or few-shot prompt engineering to maximise LLM effectiveness (Hou et al., 2024); a smaller number have explored novel approaches such as prompting LLMs to include a planning phase before generating solutions (Jiang et al., 2024).",
            "score": 0.5077866087606651,
            "section_title": "Generative AI in software engineering",
            "char_start_offset": 18470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "277244499",
            "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality",
            "text": "The field of natural language processing (NLP) has undergone rapid transformations since the introduction of the Transformer architecture [81]. This breakthrough has fundamentally reshaped NLP, leading to the proliferation of numerous language models, particularly decoder-based architectures, which have revolutionized text generation and paved the way for large language models (LLMs). These advanced neural networks redefine the boundaries of machine understanding and language generation, leveraging extensive datasets comprising billions or even trillions of words from diverse domains to maximize linguistic coverage and generalization [4]. \n\nThe unprecedented scale of LLMs, characterized by vast numbers of parameters, enables them to capture intricate linguistic patterns, including semantic relationships, syntactic structures, and long-range dependencies [77,4]. This capacity allows them to perform complex language-related tasks, such as advanced reasoning and contextual understanding. In particular, LLMs demonstrate emergent abilities, performing tasks that are not explicitly programmed but naturally arise from their scale and the complexity of their training process [85,37]. \n\nThe development of large language models generally involves two primary stages: pre-training and fine-tuning. The pre-training phase involves exposing the model to extensive volumes of unlabeled general knowledge text data through a language modeling task, most commonly framed as the next token prediction in a sequence [4]. Then, the resulting model passes through the finetuning phase, which refines the model's capabilities to perform a more specific set of tasks. This process leverages the pre-trained weights, which encode general knowledge about language, morphology, and syntax, and adjusts them using labeled datasets in a supervised learning framework [16]. This step, commonly referred to as Supervised Fine-Tuning (SFT), aligns the model's performance with the requirements of particular applications, significantly enhancing its ability to tackle domain-specific tasks [52]. \n\nWhile fine-tuning significantly enhances a model's domainspecific performance, it often requires access to a large amount of labeled data, which presents a substantial barrier, especially when dealing with specialized domains.",
            "score": 0.5077648593640155,
            "section_title": "LARGE LANGUAGE MODELS (LLMS)",
            "char_start_offset": 8638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 642,
                    "end": 645,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 870,
                    "end": 872,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "263830318"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "271843302",
            "title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
            "text": "The decoder-only Transformer is designed to handle tasks where the output generation depends solely on the preceding tokens, making it particularly suited for auto-regressive tasks such as language modeling and text generation [9]. In the classic decoder-only Transformer design, multiple decoder layers are connected in sequence. Each decoder layer consists of a self-attention block followed by a feed-forward network (FFN). Fig. 1 presents an overview of the decoder-only Transformer model with a Mixture-of-Experts (MoE) design. In this model, the FFN layers are divided into several smaller FFNs, referred to as experts, which are sparsely activated by a gating mechanism. The self-attention block can also be replaced with a Mamba layer to improve performance in sequence modeling (a model known as state-space model). LLMs like GPT [10], [11], LLaMA [3], Claude [12], Mistral [13] have demonstrated their ability to excel in many natural language processing (NLP) tasks Training an LLM model from scratch requires a large amount of hardware resources and budget. \n\nFine-tuning LLMs allows organizations to harness the full potential of advanced AI systems by tailoring them to specific tasks and domains. This customization involves training the model on domain-specific data, enabling it to understand and generate content that aligns closely with the unique needs of the users. For instance, in the healthcare sector, a fine-tuned LLM can assist in diagnosing conditions by interpreting patient data and medical literature with high precision. Another attractive feature of fine-tuning LLMs is that it can be achieved at a cost-efficient manner. While pretraining LLMs require thousands of GPU hours, fine-tuning can be achieved using a handful of GPUs in a relatively short  amount of time [6]. This work uses case study of mathematics and common-sense question-answer datasets to demonstrate the fine-tuning process of LLMs.",
            "score": 0.5077008682117714,
            "section_title": "A. LLM and Finetuning",
            "char_start_offset": 3913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.246337890625
        },
        {
            "corpus_id": "269302484",
            "title": "Evaluation and Improvement of Fault Detection for Large Language Models",
            "text": "Large language models have been the most used deep learning models for most of the tasks. Comprehensively testing or evaluating the performance of LLMs has become one of the most important directions in almost all communities. \n\nChang et al. surveyed works that focus on evaluating LLMs [5]. They classified the evaluation objectives into seven categories, natural language processing, robustness/ethics/biases/trustworthiness, social science, natural science & engineering, medical applications, agent applications, and other applications. Evaluating LLMs for code which we are more interested in lies in the category of natural science & engineering. Xu et al. [38] conducted a comprehensive study to evaluate four series of LLMs on the code generation task using the HumanEval benchmark. They found that the performance of LLMs is affected by the parameter size of models and the training time. Liu et al. [24] propose a new code synthesis evaluation framework EvalPlus to measure the correctness of code generated by LLMs. Based on this framework, they found that the existing benchmark HumanEval is not rigorous enough. The performance (passk) of LLMs drops a lot when using the new benchmark. Besides, Ma et al. [25] evaluated the capability of LLMs to understand code syntax and semantics. The experimental results show that LLMs can understand the syntax structure of code, and perform code static analysis, but cannot approximate code dynamic behavior of code. \n\nMore recently, Yuan et al. [40] compared LLMs with conventional code models that are fine-tuned for specific code tasks. They found that in the zero-shot setting, instruction-tuned LLMs have competitive performance to fine-tuned code models. In the one-shot setting, the guidance by oneshot example sometimes harms the performance of LLMs. Du et al. [9] argued that existing works that evaluate the effectiveness of LLMs for code mainly focus on simple code tasks (function-level code synthesis) and built the first class-level code generation benchmark for the LLM evaluation. Based on their benchmark, they found that existing LLMs have significantly lower performance on class-level code generation tasks compared to method-level code generation tasks. There is still a big room to improve the ability of LLMs for code generation.",
            "score": 0.507697330743009,
            "section_title": "Large Language Model Testing",
            "char_start_offset": 9353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "258437095"
                },
                {
                    "start": 1822,
                    "end": 1825,
                    "matchedPaperCorpusId": "269128474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06451416015625
        },
        {
            "corpus_id": "267383642",
            "title": "Potential merits and flaws of large language models in epilepsy care: A critical review",
            "text": "Most notably, an autoregressive LLM like GPT still has difficulties completing complex (non-sequential) tasks.A linear way of thinking prevents these models from accessing information recursively, that is, making, planning, and memorizing necessary intermediate steps.However, the most recent versions of GPT incorporate new strategies for recursively accessing information.over the last few years.In most cases, these models are pretrained on large quantities of text and typically differ in size and complexity, as expressed in their number of parameters (Table 1).More parameters correspond to superior learning and task-generation performances but come with a higher computational price.Recent studies suggest that increasing data heterogeneity and training length could improve LLM performance with smaller data sets.For example, the Large Language Model Meta AI (LLaMA), developed by Meta, outperforms GPT-3 on several benchmarks despite fewer parameters (13 billion vs 175 billion). 17This improved performance of smaller, often open-sourced LLMs is particularly appealing for resource-poor and underfunded research areas.A model can be improved to achieve the desired output in several ways. 15For domain-specific LLMs, one can either fine-tune generic pretrained models on domain-specific texts 18,19 or exclusively build an LLM on non-annotated domain-specific data-like medical texts. 20The latter strategy is applied increasingly as it avoids the laborious work of supervised data labeling.In task-agnostic LLMs-models that are generic and not pretrained on domain-specific input-human feedback can be used for model alignment and reduction of toxic output. 15 2.2.3 | What is the influence of prompting on model output?\n\nIn addition to model characteristics, the output quality of LLMs depends on instructions or questions provided by their users.Minor differences in prompts, or the use of different versions of LLMs, may result in different outputs and could potentially limit the reproducibility of these models in (clinical) research. 21Prompt engineering is essential for gaining a desirable model output.",
            "score": 0.507692367327652,
            "section_title": "Key points",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1732
                },
                {
                    "start": 1734,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 2054
                },
                {
                    "start": 2054,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 990,
                    "end": 992,
                    "matchedPaperCorpusId": "257219404"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "258715226"
                },
                {
                    "start": 1307,
                    "end": 1309,
                    "matchedPaperCorpusId": "247025331"
                },
                {
                    "start": 1396,
                    "end": 1398,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 2052,
                    "end": 2054,
                    "matchedPaperCorpusId": "258745593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0279083251953125
        },
        {
            "corpus_id": "269302939",
            "title": "Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches",
            "text": "The findings of this study underscore the efficacy of comprehensive fine-tuning strategies in enhancing the performance of language models.Importantly, we report domain-adapted fine-tuned medical LLMs that demonstrate high-level medical reasoning and improved domain-specific benchmark performance, particularly, in medical complex tasks such as USMLE-based questions.\n\nOverall, full-parameter fine-tuning achieved better performance than parameter-efficient fine-tuning in medical tasks (Figure 1).However, it is noteworthy that parameterefficient fine-tuning methods, such as LoRA, yield results that are remarkably close to those achieved by fullparameter fine-tuning, consistent with findings in other studies (Fu et al. 2023;Liao, Meng, and Monz 2023).These findings suggest that parameter-efficient approaches can be viable alternatives, particularly in scenarios where computational resources are limited.\n\nA critical aspect of our study was the thorough examination of potential test set contamination.We analyzed whether our evaluation datasets contained examples that were either identical or strikingly similar to those in the training set, and re-evaluated our models on the \"decontaminated\" dataset.We observed that a small number of samples were deemed to be \"contaminated\" (Table A2), which resulted in very small changes in the accuracy scores for the two larger fine-tuned models across the benchmark datasets (Figure 2).This process ensures the robustness and integrity of our analysis, affirming the reliability of our findings and the trained models.\n\nMoreover, our study encompassed a comparative analysis of the performance of fine-tuned models against other LLMs, including those commercially available.This comparison provides a more comprehensive view of the standing of our fine-tuned models in the current landscape of openly available LLMs, particularly in medical applications.\n\nThis research also underscores the importance of creating a large and well-structured instruction fine-tuning dataset.As instruct fine-tuning of open-source LLMs becomes a de facto standard practice, our results show that our model exhibits superior performance compared to established names like ClinicalCamel (Toma et al. 2023), MediTron (Chen et al. 2023) and GatorTronGPT (Peng et al. 2023).",
            "score": 0.5072477086120921,
            "section_title": "Discussion",
            "char_start_offset": 12191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 912
                },
                {
                    "start": 914,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1212
                },
                {
                    "start": 1212,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1570
                },
                {
                    "start": 1572,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1906
                },
                {
                    "start": 1908,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 714,
                    "end": 730,
                    "matchedPaperCorpusId": "254043494"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2047119140625
        },
        {
            "corpus_id": "273346777",
            "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
            "text": "With the advent of large language models (LLMs) such as ChatGPT, we have observed significant advancements in tasks involving instruction following (Wang et al., 2023b), intent comprehension (Lu et al., 2023), and text generation (Zhao et al., 2023). One of the primary objectives of developing LLMs is to harness their potential for generalizing to unseen natural language processing (NLP) tasks. To achieve this aim, many LLMs focus on precisely aligning with human instructions. \n\nRecent studies indicate that supervised fine-tuning (SFT) can customize LLMs for specific domains, tasks, or applications by utilizing well-crafted data. According to the study in Zhou et al. (2024a), it is feasible to fine-tune a pre-trained language model with a relatively small set of examples. Building on this insight, several papers have explored data selection strategies for SFT of LLMs (Wang et al., 2024;Qin et al., 2024), emphasizing the importance of enhancing the quality of instruction tuning (IT) data or increasing data diversity. These strategies can be classified into two primary categories: \n\n(1) Extenral-scoring methods, which require support from more sophisticated external models like GPT-4 to score the data for the subsequent selection (Lu et al., 2023;Chen et al., 2023;Du et al., 2023;Liu et al., 2023;Zhou et al., 2024b); (2) Self-scoring methods, which leverage LLMs them-selves as data scorers (Zhou et al., 2023a;Li et al., 2023d;b;Liu et al., 2024;Xia et al., 2024;Yin et al., 2024). The Y-axis represents the differential score, which is computed by subtracting the random selection score from the scores obtained using various methods.",
            "score": 0.5072320475063441,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1656
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 208,
                    "matchedPaperCorpusId": "260887200"
                },
                {
                    "start": 1248,
                    "end": 1265,
                    "matchedPaperCorpusId": "260887200"
                },
                {
                    "start": 1467,
                    "end": 1484,
                    "matchedPaperCorpusId": "267522839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1259765625
        },
        {
            "corpus_id": "276617917",
            "title": "Controlled Diversity: Length-optimized Natural Language Generation",
            "text": "LLMs are already being applied to a wide range of applications: they excel in classic NLP tasks like text generation and information extraction, are capable of performing information retrieval, act as recommender systems and evaluate both human and LLM generated content (Zhao et al., 2024, pp. 70-77). A key element to their success is their ability to be adapted to very specific tasks, which is achieved by fine-tuning a pretrained LLM using various approaches. In the following sections, we will discuss some of these approaches and explore how they can be applied to enable LLMs to adhere to length requirements. \n\nThe simplest approach to adapt a pretrained LLM to a specific task is supervised fine-tuning (SFT), which relies on a relatively small dataset of demonstrations, that is often created by humans specifically for this purpose and is generally of high quality. The model learns by generating completions of partial data samples, using the actual completions in the dataset to generate a loss based on their difference. While models adapted using supervised fine-tuning generally perform better at these specific tasks than baseline models, supervised fine-tuning is often just the first step in a process called reinforcement learning with human feedback (RLHF), which is a widely used approach for adapting pretrained LLMs to new tasks. Ouyang et al. (2024) summarize the process of aligning an LLM to human feedback using reinforcement learning in the following three-step procedure: \n\n1. Supervised Fine-Tuning: This step consists of fine-tuning a pretrained LLM on a set of human curated demonstration data of prompts and desired responses. 2. Reward Model Training: In this step, a reward model is trained to evaluate the outputs of the trained model from step one based on human provided preference data.",
            "score": 0.507157078214103,
            "section_title": "Background",
            "char_start_offset": 3050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1827
                }
            ],
            "ref_mentions": [
                {
                    "start": 1355,
                    "end": 1375,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277099609375
        },
        {
            "corpus_id": "259360395",
            "title": "A Survey on Evaluation of Large Language Models",
            "text": "In contrast, more complex tasks have become the mainstream benchmarks for assessing the capabilities of LLMs. These include tasks such as mathematical reasoning [226,237,244] and structured data inference [86,151]. Overall, LLMs show great potential in reasoning and show a continuous improvement trend, but still face many challenges and limitations, requiring more in-depth research and optimization. \n\n3.1.3 Natural language generation. NLG evaluates the capabilities of LLMs in generating specific texts, which consists of several tasks, including summarization, dialogue generation, machine translation, question answering, and other open-ended generation tasks. \n\nSummarization is a generation task that aims to learn a concise abstract for the given sentence. In this evaluation, Liang et al. [114] found that TNLG v2 (530B) [179] achieved the highest score in both scenarios, followed by OPT (175B) [247] in second place. The fine-tuned Bart [106] is still better than zero-shot ChatGPT. Specifically, ChatGPT demonstrates comparable zero-shot performance to the text-davinci-002 [6], but performs worse than GPT-3.5 [159]. These findings indicate that LLMs, particularly ChatGPT, have a general performance in summarization tasks. \n\nEvaluating the performance of LLMs on dialogue tasks is crucial to the development of dialogue systems and improving human-computer interaction. Through such evaluation, the natural language processing ability, context understanding ability and generation ability of the model can be improved, so as to realize a more intelligent and more natural dialogue system. Both Claude and ChatGPT generally achieve better performance across all dimensions when compared to GPT-3.5 [121,159]. When comparing the Claude and ChatGPT models, both models demonstrate competitive performance across different evaluation dimensions, with Claude slightly outperforming ChatGPT in specific configurations. Research by Bang et al. [6] underscores that fully fine-tuned models tailored for specific tasks surpass ChatGPT in both task-oriented and knowledge-based dialogue contexts. Additionally, Zheng et al. [259] have curated a comprehensive LLMs conversation dataset, LMSYS-Chat-1M, encompassing up to one million samples.",
            "score": 0.5070663705034775,
            "section_title": "Natural Language Processing Tasks",
            "char_start_offset": 21336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 166,
                    "matchedPaperCorpusId": "249063032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042816162109375
        },
        {
            "corpus_id": "273969969",
            "title": "Towards Evaluation Guidelines for Empirical Studies involving LLMs",
            "text": "We believe that LLMs have specific intrinsic characteristics that we describe in the following, requiring specific guidelines for researchers to achieve an acceptable level of reproducability. \n\nFor example, even if we know the specific version of an LLM used for an empirical study, the reported performance for the studied tasks can change over time, especially for commercial models that evolve beyond version identifiers [11]. Moreover, commercial providers do not guarantee the availability of old versions indefinitely. Besides versions, LLMs' performance widely varies depending on configured parameters such as temperature. Therefore, not reporting the parameter settings impacts the reproducibility of the research. \n\nEven for \"open\" models such as Llama, we do not know how they were fine-tuned for specific tasks and what the exact training data was [12]. For example, when evaluating LLMs' performance for certain programming tasks, it would be relevant to know whether the solution to a certain problem was part of the training data or not. Therefore, with this paper, we provide two key contributions: (1) a classification of different types of empirical studies involving LLMs in software engineering research and (2) preliminary guidelines on how to achieve valid and reproducible results in such studies.",
            "score": 0.5070195734824419,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1321
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "259951081"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "270616576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0633544921875
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "score": 0.5069921601265108,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299560546875
        },
        {
            "corpus_id": "266053816",
            "title": "Real Customization or Just Marketing: Are Customized Versions of Chat GPT Useful?",
            "text": "The rapid advancements in statistical generative artificial intelligence (AI) (Murphy, 2023), particularly in the realm of natural language processing and generation with the emergence of Large Language Models (LLMs) (Gozalo-Brizuela et al. 2023b, Zhao et al., 2023), based on the transformers architecture, have given birth to a new paradigm in a plethora of sectors (Gozalo-Brizuela et al. 2023a), like marketing (Fraiwan et al., 2023), higher education (Sullivan, 2023) and research (Garrido-Merch\u00e1n, 2023). Among the most notable developments in this field is OpenAI's ChatGPT-4 Turbo (OpenAI, 2023), a sophisticated language model that has demonstrated remarkable capabilities in generating human-like text (Garrido-Merch\u00e1n et al., 2023) and performing several tasks accurately (Peng, 2023). This technology's potential in the educational sector, especially in creating virtual teaching assistants (Baidoo-Anu et al., 2023) is immense. However, the effectiveness and practical utility of these AI models, when fine-tuned and customized for specific educational purposes, remain areas of burgeoning research. \n\nThe concept of customized generative artificial intelligence, particularly in LLMs like ChatGPT-4, involves fine-tuning the model on specific multimodal data, datasets or with tailored prompts to better suit particular tasks. Intuitively, the LLM capacity exceeds the complexity of a particular task, such as being a virtual instructor, so by fine-tuning the model, we condition its behavior to make him more suitable to only provide solutions to that particular task. This process is hypothesized to make these models more effective in specialized tasks, such as functioning as a virtual professor in a specific academic discipline. The recent introduction of a natural language web interface for fine-tuning by OpenAI has made this process more accessible, democratizing its adoption in diverse fields. \n\nThe relevance of this research is motivated by the growing demand for personalized learning experiences in higher education and democratizing it, being reachable to any specific group of persons in need.",
            "score": 0.50631638194548,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1919
                },
                {
                    "start": 1922,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 903,
                    "end": 928,
                    "matchedPaperCorpusId": "256347543"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03643798828125
        },
        {
            "corpus_id": "270199777",
            "title": "How Multilingual Are Large Language Models Fine-Tuned for Translation?",
            "text": "This work sought to characterize how LLM fine-tuned for translation perform across a diverse set of translation tasks beyond the ones they were fine-tuned on.By conducting an extensive empirical evaluation of the TOWER family of LLMs on 132 translation tasks representing diverse degrees of supervision, we find that fine-tuning on translation-related tasks improves LLM's ability to handle the task of translation itself beyond the specific language pairs seen during fine-tuning.This encouragingly suggests that the fine-tuning paradigm has the potential to enable massively multilingual MT.\n\nHowever, the fine-grained results show that transfer learning has an uneven impact: outlier languages remain hard to handle whether they are seen during fine-tuning (Korean) or not (Icelandic).Analysis suggest that Korean might be harmed by oversegmentation compared to other fine-tuning languages, while translation involving Icelandic often results in offtarget outputs.Furthermore, the worst-case behavior of the best LLM (TOWERINSTRUCT-13B) is consistently worse than that of the dedicated NLLB model.This highlights the need for future work on improving instruction tuning techniques to benefit the harder language pairs.Initial efforts in that direction are underway.For instance, Gao et al. (2024) have ported a cross-lingual consistency regularization method developed for dedicated multilingual MT (Gao et al., 2023) to instruction fine-tuning -while promising when tested on languages seen during fine-tuning, its impact on translation involving languages unseen during fine-tuning remains to be seen.Our results further suggest that ensembling methods that back-off to dedicated MT models might be useful to mitigate off-target translations, and also call for addressing tokenization fairness as a potential cause for discrepancies in translation quality across languages.\n\nWhile we have studied overall translation quality as measured by COMET, other aspects of the outputs would be worth studying in future work including how translationese effects (Dutta Chowdhury et al., 2020;Vanmassenhove et al., 2021) and hallucination patterns (Guerreiro et al., 2023) are impacted by different degrees of instruction-tuning supervision.\n\nFigure 6: Distribution of ChrF scores for each translation approach over language pair supervision type.",
            "score": 0.5060984396209368,
            "section_title": "Conclusion",
            "char_start_offset": 21119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 593
                },
                {
                    "start": 595,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1268
                },
                {
                    "start": 1268,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1878
                },
                {
                    "start": 1880,
                    "end": 2235
                },
                {
                    "start": 2237,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 1282,
                    "end": 1299,
                    "matchedPaperCorpusId": "258676511"
                },
                {
                    "start": 1402,
                    "end": 1420,
                    "matchedPaperCorpusId": "258676511"
                },
                {
                    "start": 2057,
                    "end": 2087,
                    "matchedPaperCorpusId": "227230654"
                },
                {
                    "start": 2087,
                    "end": 2114,
                    "matchedPaperCorpusId": "231740565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "270257916",
            "title": "LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments",
            "text": "In Section 3.2, we trained classification models using text embeddings as fixed inputs, bypassing the need to train the LLM itself.The performance of these embedding-based approaches reached 82% accuracy without modifying any parameters in the LLM.This raises a natural question: if we can train LLMs directly, can we achieve better performance?However, training LLMs from scratch is costly and resource-intensive.\n\nTherefore, in this section, we turn to fine-tuning techniques, which involve partially adjusting the parameters in the LLMs to improve performance while keeping the computational cost manageable.Note that to fine-tune LLMs, we need to be able to access the underlying model parameters.This is only feasible for open-source LLMs, like BERT and LLama, and not for proprietary LLMs like OpenAI's GPT and Google's Gemini.\n\nThe goal of fine-tuning is to adapt a general-purpose LLM for specific downstream tasks through a process typically involving supervised learning using labeled datasets.\n\nFor the fine-tuning task, we use Meta's Llama-3 (Meta-AI, 2024) as our primary base model because it is regarded as one of the most advanced open-source LLMs to date (Meta-Llama, 2024).Zhao et al. (2024)   consider a large-scale experiment where they fine-tune different open-source LLMs and show that for NLP tasks, including news headline generation, Llama-3 offers one of the best performance. 19Llama-3 offers two versions: one with 8 billion parameters (Llama-3-8b) and another with 70 billion parameters (Llama-3-70b).\n\nWe opted for the smaller version since it is easier to fine-tune and, as we will see, even with this smaller model, we see evidence of over-fitting (see Section 3.3.4for details).Hence, we do not consider fine-tuning the 70-billion-parameter model, where over-fitting problems are likely to be exacerbated.Our numerical experiment shows LoRA fine-tuned Llama-3-8b achieves nearly 83% accuracy.\n\nThe rest of this section is organized as follows.",
            "score": 0.5059803840957141,
            "section_title": "Fine-Tuning Open-Sourced LLMs with LoRA",
            "char_start_offset": 39479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 414
                },
                {
                    "start": 416,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 833
                },
                {
                    "start": 835,
                    "end": 1004
                },
                {
                    "start": 1006,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1530
                },
                {
                    "start": 1532,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1838
                },
                {
                    "start": 1838,
                    "end": 1925
                },
                {
                    "start": 1927,
                    "end": 1976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403564453125
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "The comparative analysis of LoRA and prompt tuning methods highlights their complementary strengths in parameterefficient fine-tuning.LoRA's decomposition approach is particularly effective for scenarios where significant parameter reduction is essential without compromising model performance.Its matrix factorization technique allows for a substantial reduction in trainable parameters, making it feasible to fine-tune large models on hardware with limited memory capacity.Additionally, LoRA's ability to maintain high performance despite the reduced parameter count makes it a valuable tool for deploying LLMs in resource-constrained environments.\n\nOn the other hand, prompt tuning leverages the power of soft prompts to condition pre-trained models for specific tasks with minimal additional parameters.This method is highly scalable and effective for large models, where the addition of a small number of trainable tokens can significantly influence the model's behavior.The simplicity of prompt tuning, combined with its ability to retain the pre-trained model's knowledge, makes it an attractive option for fine-tuning large models for a variety of tasks.\n\nFuture research in PEFT techniques should focus on optimizing the balance between parameter efficiency and model performance.One promising direction is to explore hybrid approaches that combine the strengths of LoRA and prompt tuning.Such methods could leverage low-rank matrix decomposition to reduce the parameter count while utilizing soft prompts to enhance task-specific performance.Additionally, investigating the applicability of PEFT methods across different model architectures and domains will provide insights into their versatility and generalizability.\n\nAnother important aspect of future research is the development of adaptive PEFT methods that can dynamically adjust the parameter efficiency based on the complexity and requirements of the task.By incorporating mechanisms for task-aware adaptation, these methods could further enhance the fine-tuning process and improve the overall performance of LLMs in diverse applications.\n\nIn conclusion, parameter-efficient fine-tuning techniques such as LoRA and prompt tuning represent significant advancements in the field of LLM optimization.Their ability to reduce the computational and memory requirements of fine-tuning processes makes them essential tools for deploying large models in practical settings.As research in this area progresses, the development of more efficient and effective PEFT methods will continue to drive the evolution of LLMs, enabling their widespread adoption and application across various domains.",
            "score": 0.5059657250982473,
            "section_title": "Comparative Analysis and Further Research",
            "char_start_offset": 43367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 650
                },
                {
                    "start": 652,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1162
                },
                {
                    "start": 1164,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1729
                },
                {
                    "start": 1731,
                    "end": 1925
                },
                {
                    "start": 1925,
                    "end": 2108
                },
                {
                    "start": 2110,
                    "end": 2267
                },
                {
                    "start": 2267,
                    "end": 2434
                },
                {
                    "start": 2434,
                    "end": 2652
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "259991244",
            "title": "IvyGPT: InteractiVe Chinese pathwaY language model in medical domain",
            "text": "LLMs are pre-trained on large-scale unlabeled data, so they have strong language understanding and generation capabilities. However, for medical question-and-answer tasks, models may require more guidance and tuning to achieve optimal performance. With supervised fine-tuning, the generalization ability of the model can be combined with the requirements of a specific task, thus improving task performance. However, fine-tuning large language models with full parameters is extremely resource-intensive. Hu et al. [10] proposed a method, LoRA, which would freeze the weights of pre-trained models and inject trainable rank decomposition matrices into each layer of the Transformer architecture, thus greatly reducing the number of trainable parameters required for downstream tasks. The QLoRA [9] method performs better on some fine-tuning tasks of large language models with larger parameters, and it will do a 4-bit quantization of the base model to ensure that the model can be fine-tuned for LLMs with very large parameters in a low memory footprint.",
            "score": 0.5057411818095067,
            "section_title": "Supervised Fine-Tuning",
            "char_start_offset": 4014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1055
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18798828125
        },
        {
            "corpus_id": "269149051",
            "title": "From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation",
            "text": "The obtained results provide interesting insights into many aspects of the LLM's performance and training.\n\nFirst, let us consider the results of the opensource models.Comparing the performance of the Gemma7b model before and after the fine-tuning it becomes very clear that the fine-tuning process can indeed improve its knowledge in a particular area by a large margin, in this case by roughly a quarter.Mistral shows even better improvement in answering the MCQs.Even the much smaller model Gemma2b outperforms its non-fine-tuned larger counterpart Gemma7.\n\nHowever, besides improving model's performance in certain areas, the fine-tuning process appears to introduce artifacts that affect performance when answering these open questions.Mistral, after fine-tuning, seemed to struggle with following the given instructions (see the R column in Table 3).On the other hand, Gemma7bFT's ability to speak Ukrainian was impaired by 40%, also reducing its grammar score by nearly a half (columns U and G in Table 3).What's most exciting, Gemma7bFT started to manifest the codeswitching phenomenon which can be considered an emergant property, and will be discussed in more detail in the Conclusions section.\n\nIt comes as no great surprise that the proprietary models performed substantially better in all kinds of tasks.The reasons are numerous, with the most obvious being:\n\n\u2022 The scale of parameters significantly contributes to model performance.For instance, GPT-3.5 boasts 25 times more parameters than both Gemma7b and Mistral, whereas GPT-4 and Gemini exceed these models by over a hundredfold in terms of parameter count.\n\n\u2022 Proprietary models benefit from unparalleled access to the most comprehensive and highquality datasets available, ensuring a broad and deep understanding of language.\n\n\u2022 The training of proprietary models extensively incorporates reinforcement learning techniques, refined through human feedback, to achieve nuanced understanding and response generation.\n\nNevertheless the performance of the fine-tuned open-source models is not so far behind that of GPT3.5.With additional efforts invested into the fine-tuning of open-source models, it is definitely possible to beat GPT3.5 in a range of specific language-related tasks.",
            "score": 0.5057295353918052,
            "section_title": "Discussion",
            "char_start_offset": 17847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 108,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 559
                },
                {
                    "start": 561,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1204
                },
                {
                    "start": 1206,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1371
                },
                {
                    "start": 1373,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1626
                },
                {
                    "start": 1628,
                    "end": 1796
                },
                {
                    "start": 1798,
                    "end": 1984
                },
                {
                    "start": 1986,
                    "end": 2088
                },
                {
                    "start": 2088,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1756591796875
        },
        {
            "corpus_id": "269330085",
            "title": "Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs",
            "text": "The knowledge elicited by the intermediate nodes of CoX contains finegrained task instructions, which can benefit the training of smaller student models when using a teacher LLM for knowledge distillation. Li et al. (2023b) and Hsieh et al. (2023) have shown that the student model can effectively learn from the rationales of CoT generated by an LLM. Nonetheless, it remains an open question whether the intermediate nodes from broader CoX methods are equally informative in inspiring student learning. \n\nEnd-to-End Fine-tuning One drawback of CoX is that it does not follow an end-to-end paradigm; i.e., generation errors may accumulate along the chain when self-correction (Le et al., 2024;Dhuliawala et al., 2023) is not enforced. Future research can explore fine-tuning LLMs with CoX prompting and penalizing errors from the final output. By reducing the generation errors end-to-end, we expect this will improve the quality of both the intermediate and final nodes in CoX.",
            "score": 0.5057031071261835,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 30263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 978
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 223,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 228,
                    "end": 247,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 676,
                    "end": 693,
                    "matchedPaperCorpusId": "264128082"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07501220703125
        },
        {
            "corpus_id": "277104205",
            "title": "Generative AI for Software Architecture. Applications, Trends, Challenges, and Future Directions",
            "text": "OpenAI GPT models dominate (62%) the research landscape, while alternatives such as Google LLMs and LLaMA models are significantly less employed. \n\nAmong the techniques to enhance the capabilities and performance of GenAI, Fine-Tuning is applied in 12% of the studies, that is, some researchers have chosen to fine-tune LLMs for specific architectural tasks with additional training. In particular, [OS [38]] used Fine-Tuning to align the LLM in generating serverless functions. Retrieval-augmented generation (RAG), including proprietary variants, is applied in 22% of the studies, suggesting that applying external knowledge sources is a common method to improve LLM performance in software architecture contexts. For example, [OS [5]] used RAG and Fine-Tuning to retrieve architecture knowledge management information and align such models to their needed task. \n\nA large percentage of studies (37%) did not report any data on model improvements, 18% categorically reported that no improvements were applied, and the models were run as they were. 10% of the studies did not explicitly state whether improvements were applied. The split in this instance shows that while fine-tuning and RAG methods are explored, most studies do not document their method of improvement or apply the off-the-shelf models without any modifications (Table 13 -RQ 1.3 ). \n\nSimilarly, prompt engineering is also used to quickly align LLMs to a new task [19]. The most widely used technology is the few-shot prompt, present in 31%. This shows that researchers use numerous examples to a great extent to allow LLMs to produce more precise and contextual architectural output. In contrast, one-shot prompting is the least used, with the technique mentioned in only 2% of the research suggesting that a single occurrence is infrequent in this field. Zero-shot prompting occurs in 12% of the studies, at moderate frequency, where the researchers solely utilize the pre-training knowledge of the model without additional context. As an example, [OS [43]] employed the three techniques to evaluate LLM applications in modernizing the architecture of legacy systems. Finally, in the spectrum of reasoning enhancements, Chain-of-thought (CoT) prompting appears only in 8% of the cases.",
            "score": 0.5055724335955617,
            "section_title": "RQ 1.2 (GenAI Model Used)",
            "char_start_offset": 25895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 733,
                    "end": 736,
                    "matchedPaperCorpusId": "263671720"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140869140625
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "To overcome catastrophic forgetting and maintain new learning ability, many works have proposed to merge models for old tasks and update models for new tasks [138,161,209]. Tangent Model Composition [117] proposes fine-tuning each task independently in the tangent space of the pre-trained model and then linearly fine-tuning these models to perform CL. This approach does not depend on the specific settings of CL and can be easily applied to task, class, and domain-incremental learning scenarios. In addition, ITA [143] emphasizes the necessity for the fine-tuned model to be in the same basin as the pre-trained model to ensure the composability of nonlinear models. It introduces a regularization term similar to EWC [94] in traditional CL to constrain the distance between the fine-tuned weights and the pre-trained weights when training the independent model. WARP [152] suggests linearly interpolating the pre-trained LLM's weights with its aligned weights via RLHF on a preference dataset, thus mitigating the forgetting of knowledge from the pre-trained LLM. BAM [7] continuously adapts LLMs to new languages by merging models while preserving general capabilities. Model Tailor [243] explores the problem of catastrophic forgetting during fine-tuning of MLLMs, and proposes to merge only the most important subset of parameters in the fine-tuned MLLM model into the pre-trained MLLM model, so as to retain the generalization ability of the pre-trained model as much as possible, while compensating the selected weights to reduce the performance of the finetuning task. MagMax [125] merges pruned task vectors to further alleviate parameter sign conflicts and old knowledge forgetting. Equifinality, PAINT [70] and LM-Cocktail [213] interpolate the weights of the finetuned model and the zero-shot model to improve accuracy on downstream tasks without degrading accuracy on supported/general tasks. \n\nIn contrast to merging full models, some research focuses on merging parameter-efficient modules. Chitale et al. [22] propose a CL method based on task arithmetic [71].",
            "score": 0.5054469800434971,
            "section_title": "Model Merging to Mitigate Catastrophic Forgetting",
            "char_start_offset": 54317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "256231189"
                },
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 199,
                    "end": 204,
                    "matchedPaperCorpusId": "259937354"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1587,
                    "end": 1592,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "251493208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06268310546875
        },
        {
            "corpus_id": "278635733",
            "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
            "text": "The increasing use of large language models (LLMs) in cyber security applications necessitates a rigorous examination of their benefits and potential safety risks. LLMs have shown exceptional capabilities in many text generation tasks, including code synthesis [40], software vulnerability detection [7,33] and question answering [37], signalling their transformative potential across various tasks. However, this promise is accompanied by substantial safety risks, requiring focused attention from researchers and practitioners alike [8,15,45]. \n\nA crucial factor in the success and utility of LLMs is their ability to maintain safety while being fine-tuned for specific domains to enhance their domain specific knowledge. While fine-tuning can enhance performance on specialised tasks, it may also introduce new vulnerabilities or amplify existing ones. This is particularly critical in cyber security applications, where the consequences of model vulnerabilities can be severe. Recent studies have shown how malicious actors can exploit fine-tuned LLMs to generate phishing campaigns, malware code, and other harmful content [19,20,2,39]. \n\nThe increasing misuse of generative AI tools like FraudGPT [19] and Wor-mGPT [20] in cyberattacks highlights the urgent need for systematic safety analysis of fine-tuned LLMs. These tools enable adversaries to execute more sophisticated and scalable attacks, demonstrating how fine-tuning can be weaponised for malicious purposes. A recent study by Falade [19] revealed how malicious LLMs can be exploited to generate phishing lures, impersonation schemes and deepfakes, amplifying the arsenal of cybercriminals and exposing significant vulnerabilities. \n\nThis paper presents a systematic evaluation of safety risks in fine-tuned LLMs for cyber security applications. We evaluate seven open-source LLMs using the OWASP Top 10 for LLM Applications framework [32] to assess how fine-tuning affects their susceptibility to various vulnerabilities. Our analysis reveals critical safety concerns in deploying fine-tuned LLMs in cyber security contexts. We validate our findings using the CyberLLMInstruct dataset [18], which contains 54,928 pairs of instructions and responses of pseudo-malicious cyber security data.",
            "score": 0.5054377092635225,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "269957562"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "272631539"
                },
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "258910269"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "267675587"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "265294893"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "268546486"
                },
                {
                    "start": 1135,
                    "end": 1137,
                    "matchedPaperCorpusId": "268544586"
                },
                {
                    "start": 1137,
                    "end": 1140,
                    "matchedPaperCorpusId": "270226235"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "268546486"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09771728515625
        },
        {
            "corpus_id": "266163375",
            "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models",
            "text": "The fine-tuned model is then used in the next Generate step. \n\nReST  , with its various adaptations (Section 4), has demonstrated success in enhancing language models across diverse domains, including machine translation (Gulcehre et al., 2023;Norouzi et al., 2016), semantic parsing (Agarwal et al., 2019), preference alignment (Dong et al., 2023), and elementary reasoning (Yuan et al., 2023;Zelikman et al., 2022). However, prior works primarily applied training with self-generated data to relatively small language models (up to 7B parameters), with limited scalability observed for larger models (Yuan et al., 2023). Complementing these efforts, our work aims to investigate the effectiveness and scalability of model-generated synthetic data compared to human-generated data in two challenging, less explored domains: competition-level mathematical problem-solving (MATH) (Hendrycks et al., 2021b) and code generation (APPS) (Hendrycks et al., 2021a). \n\nOur empirical findings reveal significant advancements in both mathematical reasoning and code generation capabilities when applying ReST  to PaLM 2 models of varying scales (Figure 1). Notably, models fine-tuned on model-generated synthetic data exhibit remarkably larger performance gains compared to those trained on human-written data (Figure 2, 3). Interestingly, exceeding a couple of iterations of ReST  leads to diminishing improvement, indicating potential overfitting on small amount of training problems (Figure 4). Additionally, models fine-tuned using ReST  improve pass@k as well as majority voting performance. Furthermore, these fine-tuned models demonstrate enhanced performance on related but held-out benchmarks, including math problems (GSM8K and Hungarian HS finals), coding (HumanEval), and Big-Bench Hard tasks. We also perform ablation studies to investigate the effect of number of model-generated solutions, training problems, and iterations for ReST  fine-tuning. Overall, our findings suggest self-training with feedback as a promising approach to reduce dependence on human data. \n\nThe key contributions of this work are: \n\n\u2022 We introduce ReST  that enables learning from self-generated data for LLMs, employing a principled expectation-maximization approach within a reinforcement learning framework.",
            "score": 0.5052930399787621,
            "section_title": "Introduction",
            "char_start_offset": 2349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 63,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 265,
                    "matchedPaperCorpusId": "3631537"
                },
                {
                    "start": 284,
                    "end": 306,
                    "matchedPaperCorpusId": "67750320"
                },
                {
                    "start": 394,
                    "end": 416,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 879,
                    "end": 904,
                    "matchedPaperCorpusId": "234790100"
                },
                {
                    "start": 932,
                    "end": 957,
                    "matchedPaperCorpusId": "234790100"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10015869140625
        },
        {
            "corpus_id": "261557043",
            "title": "Everyone Deserves A Reward: Learning Customized Human Preferences",
            "text": "Due to the limitation of computational resources, we did not make much empirical exploration in the stage of base LM training. As mentioned in Section 4.1, we use LLaMA-7B and Alpaca-7B as LM bases. Note that Alpaca-7B is trained with language modeling loss on 52K SFT data generated by GPT-3 (Brown et al., 2020) (text-davinci-003), which can be regarded as LLaMA with imitation learning (to GPT-3). We test the two base models on the general RM fine-tuning stage by fixing all other setups. The testing preference accuracy during training is shown in Figure 6. Out of our expectations, Alpaca-based (with GPT-3 data fine-tuning) RM performs worse than the LLaMA-based one on all the testing sets especially trained with all general preferences data. A possible explanation is Alpaca has been over-fitted with the GPT-3 data, which hinders the preference learning process. On the WebGPT and GPT-4-LLM testing sets, the two LLM bases have similar performance when fine-tuned with H&H data only (solid lines). Note that solid lines are performances with no training data from WebGPT and GPT-4-LLM sets, indicating RMs' generalization ability. The Alpaca base outperforms the LLaMA base apparently (on WebGPT and GPT-4-LLM) at the beginning of the fine-tuning, demonstrating that language modeling enhances LLM bases' generalization ability under low-sample/few-shot scenarios. \n\nBesides, we test the base models' influence on domain-specific RM fine-tuning. We train LLaMA and Alpaca with the H&H data as the general RMs, then fine-tune the model on our DSP data with respect to the four domains. The results are shown in Figure 5. On the H&H set, since the LLaMAbased and Alpaca-based RMs already have a performance gap (as in the left plot of Figure 6), we demonstrate the performance gap between the model in current step with the initial general RM for fair comparison (the original accuracy is shown in Appendix Figure 16).",
            "score": 0.5052122140187494,
            "section_title": "BASE MODEL SELECTION",
            "char_start_offset": 16857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1927
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11083984375
        },
        {
            "corpus_id": "270371778",
            "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
            "text": "This section supplements the analysis of the pertoken dynamics of benign fine-tuning, following the analysis on harmful fine-tuning attacks in Section 2.3.2.Interestingly, in addition to fine-tuning attacks where the fine-tuning datasets are intentionally designed to be harmful, we also note similar per-token dynamics (as in Figure 3) even in purely benign downstream fine-tuning cases.Figure 5 plots the gradient norm when fine-tuning Llama-2-7B-Chat on SQL Create Context [43] and Samsum [42].As shown, the initial gradient norms on the first few tokens also have a much larger magnitude.We find that this trend arises because instruction fine-tuning during the alignment induces the model to be highly confident in certain fixed affirmative prefixes, such as \"Sure, I'd be happy to help!\" on normal inputs.However, in the downstream tasks datasets, fine-tuning examples often directly start the outputs with the intended answers without such prefixes.Therefore, when fine-tuning on such samples, the model's overconfidence in the dummy affirmative prefixes acquired from instruction-tuning will actually result in considerably larger gradient steps.\n\nWe hypothesize that this might be one underlying reason why Qi et al. [22] discover that even benign fine-tuning can cause safety regression in the aligned LLMs -it may merely result from the excessively larger gradient steps when updating the generative distributions of these initial transition tokens, which, in turn, lead to over-generalization (or catastrophic forgetting), and therefore unintendedly also disrupt the model's generative distribution for refusal prefixes in these token positions.This is plausible, as we note that a full fine-tuning on SQL Create Context and Samsum with more than 600 gradient steps results in an increase of ASR from 1.5% to 14.9% and 25.5% respectively, but the ASR is already at 13.6% and 22.1% after the initial 10 gradient steps.This suggests that the most significant safety regression exactly occurs during these early steps when the gradient norm for the initial tokens is excessively large.",
            "score": 0.5049550081069091,
            "section_title": "B Pertoken Dynamics of Benign Fine-tuning",
            "char_start_offset": 51093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1929
                },
                {
                    "start": 1929,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "208010268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158447265625
        },
        {
            "corpus_id": "267626888",
            "title": "Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples",
            "text": "Even when adjusting key parameters such as T emperature, T op_p, T op_k, etc., or even fine-tuning such models, the results still yield little effect. The fundamental reasons are two-fold. Firstly, Chatglm and other LLM models focusing on interactive dialogs have weak reasoning ability, while the webshell escape sample generation task requires strong inference ability. (The model should effectively understand each specific escape strategy in the prompts and modify the given examples for bypassing without destroying the original functionality and syntactic structure of the webshell.) Secondly, prompt engineering itself tends to have more significant effects on LLMs with more than 30B parameters. Therefore, this strategy is more suitable for LLM models with a large number of parameters and strong code reasoning abilities, such as GPT-3.5 and GPT-4.",
            "score": 0.50495307981961,
            "section_title": "A Appendix",
            "char_start_offset": 28554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09619140625
        },
        {
            "corpus_id": "269785051",
            "title": "A systematic literature review on the impact of AI models on the security of code generation",
            "text": "During the training process, LLMs are scored on their ability to autoencode, that is, to accurately reproduce their input (in the face of a partially occulted input).In the context of natural language, minor errors are often acceptable and almost always have little to no impact on the meaning or understanding of a sentence.Such is not the case for code, which can be particularly sensitive to minor variations, especially for low-level programming languages.A stricter training regimen could score an LLM based not only on syntactic correctness, but on (some degree of) semantic correctness, to limit the extent to which the model wanders away from a valid program.Unfortunately, experimental data from Liguori et al. (2023) suggests that currently no single metric succeeds at that task.\n\nAlternatively, since most LLMs today come pre-trained, a better fine-tuning step could reduce the risks associated with incorrect code generation.He and Vechev (2023) took this approach and had promising results in the CWE they investigated.However, there is conflicting evidence.Evidence from Wu et al. (2023) seems to indicate that this approach is inherently limited to fixing a very narrow, and simple class of bugs.More studies analyzing the impact of fine-tuning models with curated security datasets are needed to assess the impact of this mitigation strategy.",
            "score": 0.5048487439522329,
            "section_title": ". . Training procedure",
            "char_start_offset": 60302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 790
                },
                {
                    "start": 792,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1072
                },
                {
                    "start": 1072,
                    "end": 1212
                },
                {
                    "start": 1212,
                    "end": 1359
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1102,
                    "matchedPaperCorpusId": "258967736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06781005859375
        },
        {
            "corpus_id": "270845577",
            "title": "Can Large Language Models Generate High-quality Patent Claims?",
            "text": "Therefore, inputting both abstract and description may be unnecessary and even complicate the task. ent sizes, and models in specific domains. We conduct human evaluations that adhere strictly to established examination criteria to assess LLMs' capabilities and limitations more accurately. \n\nThe main contributions are as follows: 1. We construct the first dataset for descriptionbased claim generation. We demonstrate theoretically and empirically that description-based claim generation outperforms previous abstract-based generation, particularly increasing the invention's feature completeness. 2. We show that current patent-specific LLMs perform much worse than state-of-the-art general LLMs, indicating the need for future research in the domain. We also find LLMs can generate high-quality first independent claims, but underperform when producing subsequent dependent claims. Fine-tuning can improve feature completeness, conceptual clarity, and feature linkage, while multi-task fine-tuning decreases conceptual clarity. 3. GPT-4 demonstrates better feature coverage, conceptual clarity, and technical coherence. However, thorough revision and adjustment are still necessary to pass legal scrutiny and ensure claims' legal and technical robustness.",
            "score": 0.5046971728845158,
            "section_title": "Detailed features of the invention A concise summary of the invention",
            "char_start_offset": 6645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1259
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05450439453125
        },
        {
            "corpus_id": "273351402",
            "title": "3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation",
            "text": "LLMs encounter significant challenges when learning unfamiliar or complex knowledge during supervised fine-tuning, particularly when the data was not encountered during pre-training, which can impede domain adaptation. Gekhman et al. (2024) found that models acquire new factual knowledge slowly during SFT, especially when the information diverges from their pre-existing understanding, leading to a higher risk of hallucinations. Ren et al. (2024) further show that when the knowledge introduced during Instruction Fine-tuning significantly differs from what was learned in pre-training, the model struggles to integrate it, causing performance degradation. This highlights the difficulty models face in using pre-training knowledge to understand new concepts. Kang et al. (2024) also emphasize that unfamiliar examples during fine-tuning increase the likelihood of hallucinations, suggesting that high-difficulty data can destabilize the model and negatively impact its ability to adapt to new domains. Together, these findings underscore the risks associated with fine-tuning on excessively difficult data, which can undermine model performance in domain-specific tasks.",
            "score": 0.5045770938833173,
            "section_title": "DATA LEARNABILITY IN LLM SFT",
            "char_start_offset": 7411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061767578125
        },
        {
            "corpus_id": "268857001",
            "title": "A Study on Scaling Up Multilingual News Framing Analysis",
            "text": "The results (see Table 4) show the exact match accuracy of different LLMs on the MFC-test dataset.The performance of Llama2-chat-70B aligns closely with that of Falcon-40b-instruct, and Mistral-7B-Instruct-v0.1 outperformed them significantly showing that the sheer size of a model does not necessarily equate to better performance.Interestingly, the best performance was achieved by employing smaller, task-finetuned models, with RoBERTa achieving an exact match accuracy of 73.22%.This significantly surpasses the highest result for general LLMs, as their best performance is at 35.33%, observed with Mistral-7B-Instruct-v0.1.This difference in performance highlights the importance of task-specific fine-tuning on model efficacy.The finetuning process allows models like RoBERTa to adapt their parameters more closely to the nuances of the specific task, resulting in a more precise understanding and response generation compared to models that rely solely on broad, generalized training.The results also suggests that there is a trade-off between model size and specialized training.While larger models have a vast knowledge base, they are not always effective in applying this knowledge to specific tasks without fine-tuning.\n\nError Analysis The LLMs exhibit a range of errors in predicting the correct frames for the provided texts (See Table 10).These errors include spelling mistakes, overgeneralization, assigning multiple labels where only one is appropriate, and misinterpretation.Generally, the models struggle with adhering to instructions, such as inventing new frames rather than selecting from the provided list (External Regulatory and Renown).Additionally, a common issue among all three of the models is their failure to introduce their answers concisely as instructed.Contrary to the clear direction to reply only with the label name, they begin responses with phrases like 'The most suitable frame is...'.\n\nThe Mistral 7B model achieves a higher accuracy rate compared to the other two model; however, it often adds additional commentary to its responses.The LLaMA-2 70B model's predictions are inconsistent, notably when it replaces 'External Regulation and Reputation' with 'External Regulatory and Renown', demonstrating a tendency towards misrepresentation.",
            "score": 0.5045428239779348,
            "section_title": "Results and Discussion",
            "char_start_offset": 25214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1230
                },
                {
                    "start": 1232,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1788
                },
                {
                    "start": 1788,
                    "end": 1926
                },
                {
                    "start": 1928,
                    "end": 2076
                },
                {
                    "start": 2076,
                    "end": 2282
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06561279296875
        },
        {
            "corpus_id": "261048648",
            "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
            "text": "models' size and improving their efficiency. Shi et al. [289] use a genetic algorithm to compress CodeBERT into only 3 MB and reduce its response latency by more than 70%. Overall, the challenge of increasing model sizes and efficient deployment requires further attention from the communities. Data dependency. In Section 4, we provide a detailed analysis of the datasets used in 229 studies and the data preprocessing process, finding that LLMs rely heavily on a large number of different datasets for training and fine-tuning, posing the data dependency challenge. The quality, diversity, and quantity of data directly affect the performance and generalizability of the models. Given their size, LLMs often require large amounts of data to capture nuances, but obtaining such data can be challenging. Relying on limited or biased datasets may cause the model to inherit these biases, resulting in biased or inaccurate predictions. In addition, the domain-specific data required for fine-tuning can be a bottleneck. Due to the relatively short period of time since the emergence of LLM, such large-scale datasets are still relatively rare, especially in the SE domain. Another issue is the risk of benchmark data contamination, where training and test data overlaps could lead to inflated performance metrics [397]. For instance, Brown et al. [25] discovered a code bug that prevented them from fully removing all overlapping data. They were unable to afford retraining and resorted to using \"cleaned\" variants of the benchmarks to mitigate the issue. Moreover, there are grave concerns around the inclusion of Personally Identifiable Information (PII) in pre-training corpora. Instances of PII, such as phone numbers and email addresses, have led to privacy leaks during the prompting process [64,154]. Ambiguity in code generation. Ambiguity in code generation poses a significant challenge for LLMs in SE tasks. When code intent is unclear (e.g., multiple valid solutions exist), LLMs may struggle to produce accurate and contextually appropriate code. This can lead to syntactically correct but functionally incorrect code, impacting the reliability and effectiveness of LLM-based code generation. Addressing this issue requires exploring techniques to incorporate additional context, domain-specific knowledge, or multi-model ensembles to improve LLMs' ability",
            "score": 0.5044072720685385,
            "section_title": "Challenges in LLM Applicability.",
            "char_start_offset": 118319,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1311,
                    "end": 1316,
                    "matchedPaperCorpusId": "233987843"
                },
                {
                    "start": 1345,
                    "end": 1349,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "269293468",
            "title": "Personalized Wireless Federated Learning for Large Language Models",
            "text": "Once the LLM completes pre-training on the server side, it undergoes further fine-tuning on a smaller, more specific dataset.This dataset typically consists of local private data.The clients download the pre-trained parameters of the LLM from the server and perform fine-tuning on an extremely small subset of parameters to enhance the performance of the LLM.To ensure security and privacy of local data, the fine-tuning process is conducted locally on the client side, and the updated subset of parameters are transmitted back to the server through wireless networks.This local dataset is often labelled, meaning that it comes with correct answers that the LLM should learn to predict.Fine-tuning requires less computational resources and smaller amount of data compared to pre-training.Finetuning of LLMs can be categorized into the following types:\n\n1) Instruction Tuning: Instruction tuning is a strategy for fine tuning LLMs on a combination of task instructions so that the LLM can generate the correct output based on the instructions.Instruction tuning uses natural language instructions as inputs to query the LLM and guide its output.The instructions consist of sequences that pair instructions with corresponding examples, which can provide explicit and precise guidance for the LLM to generate texts that are consistent with the user's intent and the data source.The goal of instruction learning is to improve the comprehension and generalization of LLMs on unseen tasks, as well as their helpfulness and safety.\n\n2) Task Tuning: Once the LLM has been pre-trained on the unlabelled data, it is further fine-tuned on specific downstream tasks using labelled data.Task tuning involves training the pre-trained model on a smaller task-specific dataset that is annotated with labels or target outputs.During the task tuning, the pre-trained model is adapted to the specific task by updating its parameters based on the labelled task data.The objective is to fine-tune the model's representations and weights to better align with the target task's objectives and improve its performance.The task tuning process helps the model generalize its pre-learned knowledge to the specific task, making it more task-specific and accurate.\n\nPre-training requires massive resources and utilizes publicly available data, and therefore it may not be suitable for FL.",
            "score": 0.5043740323989809,
            "section_title": "B. Fine-tuning Stage of LLM",
            "char_start_offset": 6822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1375
                },
                {
                    "start": 1375,
                    "end": 1524
                },
                {
                    "start": 1526,
                    "end": 1674
                },
                {
                    "start": 1674,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 1946
                },
                {
                    "start": 1946,
                    "end": 2094
                },
                {
                    "start": 2094,
                    "end": 2235
                },
                {
                    "start": 2237,
                    "end": 2359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35498046875
        },
        {
            "corpus_id": "275906660",
            "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models",
            "text": "Large language models (LLMs), like the GPT series [1], have surprised the world with their remarkable ability across a wide range of tasks, achieving breakthroughs in text comprehension [2], question answering [3], and content generation [4]- [6]. Despite their effectiveness, LLMs are always criticized for their limited ability to handle knowledge-intensive tasks, especially when faced with questions requiring domain expertise [7]. Specifically, the application of LLMs to specialized domains remains challenging for three fundamental reasons, including \u2776 Knowledge limitations: LLMs' pre-trained knowledge is broad but shallow in specialized fields. Their training data primarily *Authors contributed equally to this research. consists of general-domain content, leading to insufficient depth in professional domains and potential inconsistencies with current domain-specific standards and practices. \u2777 Reasoning complexity: Specialized domains require precise, multi-step reasoning with domain-specific rules and constraints. LLMs often struggle to maintain logical consistency and professional accuracy throughout extended reasoning chains, particularly when dealing with technical constraints or domain-specific protocols. \u2778 Context sensitivity: Professional fields often involve context-dependent interpretations where the same terms or concepts may have different meanings or implications based on specific circumstances. LLMs frequently fail to capture these nuanced contextual variations, leading to potential misinterpretations or inappropriate generalizations. \n\nTo adapt LLMs for specific or private domains, initial strategies involved fine-tuning LLMs with specialized datasets [8]. This method enhances performance by adding a limited number of parameters while fixing the parameters learned in the pretraining [9]. However, the significant distribution gap between the domain-specific dataset and the pre-training corpus makes it challenging for LLMs to integrate new knowledge without compromising their existing understanding [10]. A recent study by Google Research further highlighted the risks associated with using supervised fine-tuning to update knowledge, particularly in cases where new knowledge conflicts with pre-existing information; acquiring new knowledge through supervised finetuning can lead to the model generating new hallucinations and even experiencing severe catastrophic forgetting [11]. \n\nRetrieval-Augmented generation (RAG) offers a promising solution to customize LLMs for specific domains [12].",
            "score": 0.5043216806890054,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2430
                },
                {
                    "start": 2433,
                    "end": 2542
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 431,
                    "end": 434,
                    "matchedPaperCorpusId": "266163181"
                },
                {
                    "start": 1695,
                    "end": 1698,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1829,
                    "end": 1832,
                    "matchedPaperCorpusId": "238583580"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08770751953125
        },
        {
            "paperId": "92b2ea3c244855235d10ca270dea7348fae07a25",
            "corpusId": 278327536,
            "title": "Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 45,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359150080",
                    "name": "Nazmus Ashrafi"
                },
                {
                    "authorId": "2359148921",
                    "name": "Salah Bouktif"
                },
                {
                    "authorId": "2329167767",
                    "name": "Mohammed Mediani"
                }
            ],
            "abstract": "The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",
            "corpus_id": "278327536",
            "text": "The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1177978515625
        },
        {
            "paperId": "645ed616c4dbf18dbf1ef09aab0549218682879a",
            "corpusId": 278937178,
            "title": "LLM-IMC: Automating Analog In-Memory Computing Architecture Generation with Large Language Models",
            "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
            "year": 2025,
            "referenceCount": 4,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/FCCM62733.2025.00071?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/FCCM62733.2025.00071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2226523621",
                    "name": "Deepak Vungarala"
                },
                {
                    "authorId": "2113278318",
                    "name": "Md Hasibul Amin"
                },
                {
                    "authorId": "2350754621",
                    "name": "Pietro Mercati"
                },
                {
                    "authorId": "1810701",
                    "name": "A. Roohi"
                },
                {
                    "authorId": "2806817",
                    "name": "Ramtin Zand"
                },
                {
                    "authorId": "1824271",
                    "name": "Shaahin Angizi"
                }
            ],
            "abstract": "Resistive crossbars enabling analog In-Memory Computing (IMC) have garnered significant attention from academia and industry as a promising architecture for Deep Neural Network (DNN) acceleration, thanks to their high memory access bandwidth and in-situ computing capabilities. However, the knowledge-intensive hardware design process and the lack of high-quality circuit netlists have constrained design space exploration and optimization of analog IMC to behavioral system-level tools. In this one-page abstract, we introduce LLM-IMC, a novel fine-tune-free Large Language Model (LLM) framework, supported by a Python-based tool, designed for analog IMC SPICE code generation. LLM-IMC systematically addresses these limitations by automating the creation of diverse IMC simulation scripts, enabling efficient design space exploration through LLM-driven performance, and outlining an integration roadmap for hardware-oriented neuromorphic crossbar design flows.",
            "corpus_id": "278937178",
            "text": "Resistive crossbars enabling analog In-Memory Computing (IMC) have garnered significant attention from academia and industry as a promising architecture for Deep Neural Network (DNN) acceleration, thanks to their high memory access bandwidth and in-situ computing capabilities. However, the knowledge-intensive hardware design process and the lack of high-quality circuit netlists have constrained design space exploration and optimization of analog IMC to behavioral system-level tools. In this one-page abstract, we introduce LLM-IMC, a novel fine-tune-free Large Language Model (LLM) framework, supported by a Python-based tool, designed for analog IMC SPICE code generation. LLM-IMC systematically addresses these limitations by automating the creation of diverse IMC simulation scripts, enabling efficient design space exploration through LLM-driven performance, and outlining an integration roadmap for hardware-oriented neuromorphic crossbar design flows.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0095977783203125
        },
        {
            "paperId": "d90a7f692ef3161257df8ac51ca4724ab0f1cf1b",
            "corpusId": 279250981,
            "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes",
            "venue": "",
            "year": 2025,
            "referenceCount": 32,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.07245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316230518",
                    "name": "Wenxuan Xie"
                },
                {
                    "authorId": null,
                    "name": "Yaxun Dai"
                },
                {
                    "authorId": "2352147044",
                    "name": "Wenhao Jiang"
                }
            ],
            "abstract": "Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.",
            "corpus_id": "279250981",
            "text": "Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.034759521484375
        },
        {
            "paperId": "eaf18130e35e0ae2bcbefbe7645c5bd4871039fb",
            "corpusId": 273022678,
            "title": "Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 86,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109488495",
                    "name": "Junlin Han"
                },
                {
                    "authorId": "2271268581",
                    "name": "Jianyuan Wang"
                },
                {
                    "authorId": "2258587368",
                    "name": "Andrea Vedaldi"
                },
                {
                    "authorId": "2291064871",
                    "name": "Philip Torr"
                },
                {
                    "authorId": "2283931239",
                    "name": "Filippos Kokkinos"
                }
            ],
            "abstract": "Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications. Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality. To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models.",
            "corpus_id": "273022678",
            "text": "Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications. Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality. To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0039005279541015625
        },
        {
            "paperId": "f45b1660638c6bfbf7f50be826e45e96dcc80187",
            "corpusId": 279268252,
            "title": "A Study of LoRA Fine-Tuned Tibetan Macromodeling Based on the TIFD Dataset",
            "venue": "2025 8th International Conference on Advanced Algorithms and Control Engineering (ICAACE)",
            "year": 2025,
            "referenceCount": 10,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAACE65325.2025.11020384?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAACE65325.2025.11020384, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366748768",
                    "name": "Dejie Wang"
                },
                {
                    "authorId": "2366607715",
                    "name": "Ning Ma"
                }
            ],
            "abstract": "As an important minority language, Tibetan carries rich cultural information, but its related natural language processing research is less. To address this pain point, this study selects the first high-quality instruction dataset specifically designed for supervised fine-tuning of Tibetan Large Language Models (LLMs), i.e., the TIFD dataset, and for the first time selects the lightweight fine-tuning framework based on Low-Rank Adaptation (LoR $A$), systematic evaluation of the performance of Tibetan instruction tasks for three types of base models, GLM-4, Qwen2.5 and Llama-3. models' instruction following ability on the Tibetan TIFD dataset. The experimental results show that the TIFD dataset significantly improves the model's instruction comprehension and generation ability through the combination of structured instruction-triad (instruction-input-output) and LoRA techniques, and the study reveals that the multitasking coverage of the TIFD dataset and the low-rank constraint mechanism of LoRA synergistically optimize the model's processing ability for complex linguistic phenomena such as the Tibetan honorific system and verb tense, and demonstrates the efficacy of low-rank constraints in processing complex linguistic phenomena, such as the Tibetan honorific system and verb tense. The study reveals that the multitasking coverage of the TIFD dataset and the low-rank constraint mechanism of LoR $A$ synergistically optimize the model's ability to process complex linguistic phenomena, such as the Tibetan honorific system and verb tense. This synergy provides a novel framework for applying low-rank constraints in low-resource language processing, which provides a highly efficient fine-tuning paradigm for low-resource linguistic NLP. This study not only verifies the universal optimization effect of the TIFD dataset on Tibetan multi-base models but also provides empirical evidence for cross-linguistic model design.",
            "corpus_id": "279268252",
            "text": "As an important minority language, Tibetan carries rich cultural information, but its related natural language processing research is less. To address this pain point, this study selects the first high-quality instruction dataset specifically designed for supervised fine-tuning of Tibetan Large Language Models (LLMs), i.e., the TIFD dataset, and for the first time selects the lightweight fine-tuning framework based on Low-Rank Adaptation (LoR $A$), systematic evaluation of the performance of Tibetan instruction tasks for three types of base models, GLM-4, Qwen2.5 and Llama-3. models' instruction following ability on the Tibetan TIFD dataset. The experimental results show that the TIFD dataset significantly improves the model's instruction comprehension and generation ability through the combination of structured instruction-triad (instruction-input-output) and LoRA techniques, and the study reveals that the multitasking coverage of the TIFD dataset and the low-rank constraint mechanism of LoRA synergistically optimize the model's processing ability for complex linguistic phenomena such as the Tibetan honorific system and verb tense, and demonstrates the efficacy of low-rank constraints in processing complex linguistic phenomena, such as the Tibetan honorific system and verb tense. The study reveals that the multitasking coverage of the TIFD dataset and the low-rank constraint mechanism of LoR $A$ synergistically optimize the model's ability to process complex linguistic phenomena, such as the Tibetan honorific system and verb tense. This synergy provides a novel framework for applying low-rank constraints in low-resource language processing, which provides a highly efficient fine-tuning paradigm for low-resource linguistic NLP. This study not only verifies the universal optimization effect of the TIFD dataset on Tibetan multi-base models but also provides empirical evidence for cross-linguistic model design.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.134765625
        },
        {
            "paperId": "11e559be0a6bf3dba27b2642b97d4553dee7b23e",
            "corpusId": 278959591,
            "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents",
            "venue": "",
            "year": 2025,
            "referenceCount": 53,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308481624",
                    "name": "Taro Yano"
                },
                {
                    "authorId": "2059148476",
                    "name": "Yoichi Ishibashi"
                },
                {
                    "authorId": "37267314",
                    "name": "M. Oyamada"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.",
            "corpus_id": "278959591",
            "text": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1104736328125
        },
        {
            "paperId": "432cdcd29017002e02683cae691d47e3dea55a7e",
            "corpusId": 277621199,
            "title": "Can LLM-Driven Hard Negative Sampling Empower Collaborative Filtering? Findings and Potentials",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 50,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.04726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314785179",
                    "name": "Chu Zhao"
                },
                {
                    "authorId": "151497321",
                    "name": "Enneng Yang"
                },
                {
                    "authorId": "2234610417",
                    "name": "Yuting Liu"
                },
                {
                    "authorId": "2290846879",
                    "name": "Jianzhe Zhao"
                },
                {
                    "authorId": "2237427680",
                    "name": "Guibing Guo"
                },
                {
                    "authorId": "2237603955",
                    "name": "Xingwei Wang"
                }
            ],
            "abstract": "Hard negative samples can accelerate model convergence and optimize decision boundaries, which is key to improving the performance of recommender systems. Although large language models (LLMs) possess strong semantic understanding and generation capabilities, systematic research has not yet been conducted on how to generate hard negative samples effectively. To fill this gap, this paper introduces the concept of Semantic Negative Sampling and exploreshow to optimize LLMs for high-quality, hard negative sampling. Specifically, we design an experimental pipeline that includes three main modules, profile generation, semantic negative sampling, and semantic alignment, to verify the potential of LLM-driven hard negative sampling in enhancing the accuracy of collaborative filtering (CF). Experimental results indicate that hard negative samples generated based on LLMs, when semantically aligned and integrated into CF, can significantly improve CF performance, although there is still a certain gap compared to traditional negative sampling methods. Further analysis reveals that this gap primarily arises from two major challenges: noisy samples and lack of behavioral constraints. To address these challenges, we propose a framework called HNLMRec, based on fine-tuning LLMs supervised by collaborative signals. Experimental results show that this framework outperforms traditional negative sampling and other LLM-driven recommendation methods across multiple datasets, providing new solutions for empowering traditional RS with LLMs. Additionally, we validate the excellent generalization ability of the LLM-based semantic negative sampling method on new datasets, demonstrating its potential in alleviating issues such as data sparsity, popularity bias, and the problem of false hard negative samples. Our implementation code is available at https://github.com/user683/HNLMRec.",
            "corpus_id": "277621199",
            "text": "Hard negative samples can accelerate model convergence and optimize decision boundaries, which is key to improving the performance of recommender systems. Although large language models (LLMs) possess strong semantic understanding and generation capabilities, systematic research has not yet been conducted on how to generate hard negative samples effectively. To fill this gap, this paper introduces the concept of Semantic Negative Sampling and exploreshow to optimize LLMs for high-quality, hard negative sampling. Specifically, we design an experimental pipeline that includes three main modules, profile generation, semantic negative sampling, and semantic alignment, to verify the potential of LLM-driven hard negative sampling in enhancing the accuracy of collaborative filtering (CF). Experimental results indicate that hard negative samples generated based on LLMs, when semantically aligned and integrated into CF, can significantly improve CF performance, although there is still a certain gap compared to traditional negative sampling methods. Further analysis reveals that this gap primarily arises from two major challenges: noisy samples and lack of behavioral constraints. To address these challenges, we propose a framework called HNLMRec, based on fine-tuning LLMs supervised by collaborative signals. Experimental results show that this framework outperforms traditional negative sampling and other LLM-driven recommendation methods across multiple datasets, providing new solutions for empowering traditional RS with LLMs. Additionally, we validate the excellent generalization ability of the LLM-based semantic negative sampling method on new datasets, demonstrating its potential in alleviating issues such as data sparsity, popularity bias, and the problem of false hard negative samples. Our implementation code is available at https://github.com/user683/HNLMRec.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.03021240234375
        },
        {
            "paperId": "ad6245f3ec32ef18b2925359e33abb1af3306849",
            "corpusId": 276884853,
            "title": "DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 49,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349296521",
                    "name": "Haoyuan Ma"
                },
                {
                    "authorId": "1471660296",
                    "name": "Yongliang Shen"
                },
                {
                    "authorId": "2351801328",
                    "name": "Hengwei Liu"
                },
                {
                    "authorId": "2135282890",
                    "name": "Wenqi Zhang"
                },
                {
                    "authorId": "2349305909",
                    "name": "Haolei Xu"
                },
                {
                    "authorId": "2324802215",
                    "name": "Qiuying Peng"
                },
                {
                    "authorId": "2282565102",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "1776903",
                    "name": "Weiming Lu"
                }
            ],
            "abstract": "Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understanding. To address this limitation, we propose DB-Explore, a novel framework that systematically aligns LLMs with database knowledge through automated exploration and instruction synthesis. DB-Explore constructs database graphs to capture complex relational schemas, leverages GPT-4 to systematically mine structural patterns and semantic knowledge, and synthesizes instructions to distill this knowledge for efficient fine-tuning of LLMs. Our framework enables comprehensive database understanding through diverse sampling strategies and automated instruction generation, bridging the gap between database structures and language models. Experiments conducted on the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore, achieving an execution accuracy of 67.0% on BIRD and 87.8% on SPIDER. Notably, our open-source implementation based on Qwen2.5-Coder-7B achieves state-of-the-art results at minimal computational cost, outperforming several GPT-4-driven Text-to-SQL systems.",
            "corpus_id": "276884853",
            "text": "Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understanding. To address this limitation, we propose DB-Explore, a novel framework that systematically aligns LLMs with database knowledge through automated exploration and instruction synthesis. DB-Explore constructs database graphs to capture complex relational schemas, leverages GPT-4 to systematically mine structural patterns and semantic knowledge, and synthesizes instructions to distill this knowledge for efficient fine-tuning of LLMs. Our framework enables comprehensive database understanding through diverse sampling strategies and automated instruction generation, bridging the gap between database structures and language models. Experiments conducted on the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore, achieving an execution accuracy of 67.0% on BIRD and 87.8% on SPIDER. Notably, our open-source implementation based on Qwen2.5-Coder-7B achieves state-of-the-art results at minimal computational cost, outperforming several GPT-4-driven Text-to-SQL systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01678466796875
        },
        {
            "paperId": "98069843264ceb38cbc16a3f25ee0aa4273a3428",
            "corpusId": 258180014,
            "title": "A Comprehensive Evaluation of Neural SPARQL Query Generation From Natural Language Questions",
            "venue": "IEEE Access",
            "year": 2023,
            "referenceCount": 49,
            "citationCount": 9,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3453215",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.07772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2049470873",
                    "name": "Papa Abdou Karim Karou Diallo"
                },
                {
                    "authorId": "2191521140",
                    "name": "Samuel Reyd"
                },
                {
                    "authorId": "145698212",
                    "name": "A. Zouaq"
                }
            ],
            "abstract": "In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed significant growth. Incorporating the copy mechanism with traditional encoder-decoder architectures and using pre-trained encoder-decoder and large language models have set new performance benchmarks. This paper presents various experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained language models (PLMs), non-pre-trained language models (NPLMs), and large language models (LLMs), highlighting the impact of question annotation and the copy mechanism and testing various fine-tuning methods using LLMs. In particular, we provide a systematic error analysis of the models and test their generalization ability. Our study demonstrates that the copy mechanism yields significant performance enhancements for most PLMs and NPLMs. Annotating the data is pivotal to generating correct URIs, with the \u201ctag-within\u201d strategy emerging as the most effective approach. Additionally, our findings reveal that the primary source of errors stems from incorrect URIs in SPARQL queries that are sometimes replaced with hallucinated URIs when using base models. This does not happen using the copy mechanism, but it sometimes leads to selecting wrong URIs among candidates. Finally, the performance of the tested LLMs fell short of achieving the desired outcomes.",
            "corpus_id": "258180014",
            "text": "In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed significant growth. Incorporating the copy mechanism with traditional encoder-decoder architectures and using pre-trained encoder-decoder and large language models have set new performance benchmarks. This paper presents various experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained language models (PLMs), non-pre-trained language models (NPLMs), and large language models (LLMs), highlighting the impact of question annotation and the copy mechanism and testing various fine-tuning methods using LLMs. In particular, we provide a systematic error analysis of the models and test their generalization ability. Our study demonstrates that the copy mechanism yields significant performance enhancements for most PLMs and NPLMs. Annotating the data is pivotal to generating correct URIs, with the \u201ctag-within\u201d strategy emerging as the most effective approach. Additionally, our findings reveal that the primary source of errors stems from incorrect URIs in SPARQL queries that are sometimes replaced with hallucinated URIs when using base models. This does not happen using the copy mechanism, but it sometimes leads to selecting wrong URIs among candidates. Finally, the performance of the tested LLMs fell short of achieving the desired outcomes.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.043701171875
        },
        {
            "paperId": "7a7da5e38c5b917b99c794436c8da58f4474cefe",
            "corpusId": 270620230,
            "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258794432",
                    "name": "Yueqing Liang"
                },
                {
                    "authorId": "120378195",
                    "name": "Liangwei Yang"
                },
                {
                    "authorId": "2109115149",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2140517981",
                    "name": "Xiongxiao Xu"
                },
                {
                    "authorId": "2258846722",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "2266839642",
                    "name": "Kai Shu"
                }
            ],
            "abstract": "With the emergence of large language models (LLMs) and their ability to perform a variety of tasks, their application in recommender systems (RecSys) has shown promise. However, we are facing significant challenges when deploying LLMs into RecSys, such as limited prompt length, unstructured item information, and un-constrained generation of recommendations, leading to sub-optimal performance. To address these issues, we propose a novel method using a taxonomy dictionary. This method provides a systematic framework for categorizing and organizing items, improving the clarity and structure of item information. By incorporating the taxonomy dictionary into LLM prompts, we achieve efficient token utilization and controlled feature generation, leading to more accurate and contextually relevant recommendations. Our Taxonomy-guided Recommendation (TaxRec) approach features a two-step process: one-time taxonomy categorization and LLM-based recommendation, enabling zero-shot recommendations without the need for domain-specific fine-tuning. Experimental results demonstrate TaxRec significantly enhances recommendation quality compared to traditional zero-shot approaches, showcasing its efficacy as personal recommender with LLMs. Code is available at https://github.com/yueqingliang1/TaxRec.",
            "corpus_id": "270620230",
            "text": "With the emergence of large language models (LLMs) and their ability to perform a variety of tasks, their application in recommender systems (RecSys) has shown promise. However, we are facing significant challenges when deploying LLMs into RecSys, such as limited prompt length, unstructured item information, and un-constrained generation of recommendations, leading to sub-optimal performance. To address these issues, we propose a novel method using a taxonomy dictionary. This method provides a systematic framework for categorizing and organizing items, improving the clarity and structure of item information. By incorporating the taxonomy dictionary into LLM prompts, we achieve efficient token utilization and controlled feature generation, leading to more accurate and contextually relevant recommendations. Our Taxonomy-guided Recommendation (TaxRec) approach features a two-step process: one-time taxonomy categorization and LLM-based recommendation, enabling zero-shot recommendations without the need for domain-specific fine-tuning. Experimental results demonstrate TaxRec significantly enhances recommendation quality compared to traditional zero-shot approaches, showcasing its efficacy as personal recommender with LLMs. Code is available at https://github.com/yueqingliang1/TaxRec.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0273284912109375
        },
        {
            "paperId": "5e9e9df949c2df828d1e8a106d31e99998211cd1",
            "corpusId": 273850611,
            "title": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs",
            "venue": "Asia and South Pacific Design Automation Conference",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 5,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.03471",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2075194240",
                    "name": "Manar Abdelatty"
                },
                {
                    "authorId": "2157405891",
                    "name": "Jingxiao Ma"
                },
                {
                    "authorId": "2307161610",
                    "name": "Sherief Reda"
                }
            ],
            "abstract": "Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.",
            "corpus_id": "273850611",
            "text": "Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06463623046875
        },
        {
            "paperId": "fb555ebb79f859e786e3c9f18f1faf0207a53294",
            "corpusId": 278769766,
            "title": "Multi-Armed Bandits Meet Large Language Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 71,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.13355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1744675",
                    "name": "Djallel Bouneffouf"
                },
                {
                    "authorId": "3430625",
                    "name": "Rapha\u00ebl F\u00e9raud"
                }
            ],
            "abstract": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.",
            "corpus_id": "278769766",
            "text": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0289764404296875
        },
        {
            "paperId": "4dc95aef98c78a258df7415ee7465c6525f3c924",
            "corpusId": 279118793,
            "title": "Rethinking the effects of data contamination in Code Intelligence",
            "venue": "",
            "year": 2025,
            "referenceCount": 55,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.02791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2365438295",
                    "name": "Zhen Yang"
                },
                {
                    "authorId": "2365051972",
                    "name": "Hongyi Lin"
                },
                {
                    "authorId": "2365483560",
                    "name": "Yifan He"
                },
                {
                    "authorId": "2365004738",
                    "name": "Jie Xu"
                },
                {
                    "authorId": "2277566169",
                    "name": "Zeyu Sun"
                },
                {
                    "authorId": "2364994720",
                    "name": "Shuo Liu"
                },
                {
                    "authorId": "2365047445",
                    "name": "Pengpeng Wang"
                },
                {
                    "authorId": "2297954287",
                    "name": "Zhongxing Yu"
                },
                {
                    "authorId": "2289588104",
                    "name": "Qing-Lin Liang"
                }
            ],
            "abstract": "In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation. This paper presents a systematic empirical study to investigate the fine-grained data contamination on code intelligence tasks. Our study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs, namely LLaMA and StarCoder, covering three major tasks: code translation, code generation, and code summarization. We categorize contamination scenarios into four types according to the code intelligence practice, namely input-only, output-only, unpaired, and paired contamination settings, and construct corresponding experimental and control groups for exploration. Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination. Apart from the above, other contamination scenarios have no impact on both PLMs and LLMs. Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.",
            "corpus_id": "279118793",
            "text": "In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation. This paper presents a systematic empirical study to investigate the fine-grained data contamination on code intelligence tasks. Our study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs, namely LLaMA and StarCoder, covering three major tasks: code translation, code generation, and code summarization. We categorize contamination scenarios into four types according to the code intelligence practice, namely input-only, output-only, unpaired, and paired contamination settings, and construct corresponding experimental and control groups for exploration. Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination. Apart from the above, other contamination scenarios have no impact on both PLMs and LLMs. Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01214599609375
        },
        {
            "paperId": "ae98645805fec056dc90862bfe9969430ebb7118",
            "corpusId": 278996064,
            "title": "Can Large Language Models Match the Conclusions of Systematic Reviews?",
            "venue": "",
            "year": 2025,
            "referenceCount": 50,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.22787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352945116",
                    "name": "Christopher Polzak"
                },
                {
                    "authorId": "2282390626",
                    "name": "Alejandro Lozano"
                },
                {
                    "authorId": "2362365419",
                    "name": "Min Woo Sun"
                },
                {
                    "authorId": "2195445745",
                    "name": "James Burgess"
                },
                {
                    "authorId": "2309429994",
                    "name": "Yuhui Zhang"
                },
                {
                    "authorId": "2364123808",
                    "name": "Kevin Wu"
                },
                {
                    "authorId": "2275638250",
                    "name": "S. Yeung-Levy"
                }
            ],
            "abstract": "Systematic reviews (SR), in which experts summarize and analyze evidence across individual studies to provide insights on a specialized topic, are a cornerstone for evidence-based clinical decision-making, research, and policy. Given the exponential growth of scientific articles, there is growing interest in using large language models (LLMs) to automate SR generation. However, the ability of LLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly characterized. We therefore ask: Can LLMs match the conclusions of systematic reviews written by clinical experts when given access to the same studies? To explore this question, we present MedEvidence, a benchmark pairing findings from 100 SRs with the studies they are based on. We benchmark 24 LLMs on MedEvidence, including reasoning, non-reasoning, medical specialist, and models across varying sizes (from 7B-700B). Through our systematic evaluation, we find that reasoning does not necessarily improve performance, larger models do not consistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence. Instead, most models exhibit similar behavior: performance tends to degrade as token length increases, their responses show overconfidence, and, contrary to human experts, all models show a lack of scientific skepticism toward low-quality findings. These results suggest that more work is still required before LLMs can reliably match the observations from expert-conducted SRs, even though these systems are already deployed and being used by clinicians. We release our codebase and benchmark to the broader research community to further investigate LLM-based SR systems.",
            "corpus_id": "278996064",
            "text": "Systematic reviews (SR), in which experts summarize and analyze evidence across individual studies to provide insights on a specialized topic, are a cornerstone for evidence-based clinical decision-making, research, and policy. Given the exponential growth of scientific articles, there is growing interest in using large language models (LLMs) to automate SR generation. However, the ability of LLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly characterized. We therefore ask: Can LLMs match the conclusions of systematic reviews written by clinical experts when given access to the same studies? To explore this question, we present MedEvidence, a benchmark pairing findings from 100 SRs with the studies they are based on. We benchmark 24 LLMs on MedEvidence, including reasoning, non-reasoning, medical specialist, and models across varying sizes (from 7B-700B). Through our systematic evaluation, we find that reasoning does not necessarily improve performance, larger models do not consistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence. Instead, most models exhibit similar behavior: performance tends to degrade as token length increases, their responses show overconfidence, and, contrary to human experts, all models show a lack of scientific skepticism toward low-quality findings. These results suggest that more work is still required before LLMs can reliably match the observations from expert-conducted SRs, even though these systems are already deployed and being used by clinicians. We release our codebase and benchmark to the broader research community to further investigate LLM-based SR systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0153656005859375
        },
        {
            "paperId": "57e3d0b3d129bb62dfd54053eba627537c3befdd",
            "corpusId": 273901614,
            "title": "Adapting LLMs for Structured Natural Language API Integration",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-industry.74, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329740868",
                    "name": "Robin Chan"
                },
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1776596",
                    "name": "T. Gschwind"
                },
                {
                    "authorId": "2298321",
                    "name": "Christoph Miksovic"
                },
                {
                    "authorId": "2280410542",
                    "name": "Paolo Scotton"
                },
                {
                    "authorId": "2329739236",
                    "name": "Enrico Toniato"
                },
                {
                    "authorId": "2329739226",
                    "name": "Abdel Labbi"
                }
            ],
            "abstract": "API integration is crucial for enterprise systems, as it enables seamless interaction between applications within workflows. However, the diversity and complexity of the API landscape present significant challenges in combining API calls based on user intent.Existing methods rely on named entity recognition (NER) and knowledge graphs, but struggle to generate more complex control flow structures, such as conditionals and loops.We propose a novel framework that leverages the success of large language models (LLMs) in code generation to integrate APIs based on natural language input. Our approach involves fine-tuning an LLM using automatically generated API flows derived from OpenAPI specifications.We further evaluate the effectiveness of enforcing the syntax and schema adherence through constrained decoding.To enable systematic comparison, we introduce targeted test suites to assess the generalization capabilities of these approaches and their ability to retain structured knowledge.Our findings show that LLMs fine-tuned on OpenAPI specifications can (a) learn structural API constraints implicitly during training, and (b) achieve significant improvements in both in-distribution and out-of-distribution performance over NER and retrieval-augmented generation (RAG)-based approaches.",
            "corpus_id": "273901614",
            "text": "API integration is crucial for enterprise systems, as it enables seamless interaction between applications within workflows. However, the diversity and complexity of the API landscape present significant challenges in combining API calls based on user intent.Existing methods rely on named entity recognition (NER) and knowledge graphs, but struggle to generate more complex control flow structures, such as conditionals and loops.We propose a novel framework that leverages the success of large language models (LLMs) in code generation to integrate APIs based on natural language input. Our approach involves fine-tuning an LLM using automatically generated API flows derived from OpenAPI specifications.We further evaluate the effectiveness of enforcing the syntax and schema adherence through constrained decoding.To enable systematic comparison, we introduce targeted test suites to assess the generalization capabilities of these approaches and their ability to retain structured knowledge.Our findings show that LLMs fine-tuned on OpenAPI specifications can (a) learn structural API constraints implicitly during training, and (b) achieve significant improvements in both in-distribution and out-of-distribution performance over NER and retrieval-augmented generation (RAG)-based approaches.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.021453857421875
        },
        {
            "paperId": "74dbdd980c02261788db615af78b2fc203a22f93",
            "corpusId": 270619725,
            "title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 16,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49951877",
                    "name": "J. Feng"
                },
                {
                    "authorId": "2307458481",
                    "name": "Yuwei Du"
                },
                {
                    "authorId": "2307861880",
                    "name": "Tianhui Liu"
                },
                {
                    "authorId": "2307562064",
                    "name": "Siqi Guo"
                },
                {
                    "authorId": "2276812687",
                    "name": "Yuming Lin"
                },
                {
                    "authorId": "2307646274",
                    "name": "Yong Li"
                }
            ],
            "abstract": "Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \\textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \\textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \\textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \\textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \\textit{CityInstruction} by \\textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \\textit{CityEval}.",
            "corpus_id": "270619725",
            "text": "Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \\textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \\textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \\textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \\textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \\textit{CityInstruction} by \\textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \\textit{CityEval}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.060302734375
        },
        {
            "paperId": "0a6990c17659dc204f99136ef0e6fe7f8a9b3506",
            "corpusId": 265620985,
            "title": "The influence of Large Language Models on systematic review and research dissemination",
            "venue": "Septentrio Conference Series",
            "year": 2023,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.7557/5.7240",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.7557/5.7240?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7557/5.7240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269557706",
                    "name": "Simon Baradziej"
                }
            ],
            "abstract": "Watch VIDEO.\nThis presentation will delve into the transformative role of AI in scholarly communication, highlighting its potential, implications, and challenges, and further addressing the ethical considerations that come with it.\nRecent advancements in AI, specifically large language models have unlocked new possibilities for scientific exploration and communication. Large language models such as GPT-4 and LLAMA, with their remarkable text-generation capabilities, stand at the forefront of this AI revolution. In the first part of the presentation, I examine how these AI tools are reshaping the nature of systematic reviews. The ability to analyze, summarize, and generate vast amounts of text allows these models to facilitate more efficient processes, offering a valuable tool to researchers navigating through vast databases of published work.\nI would discuss how AI is engendering new developments in research methodology. Through the use of predictive modelling and advanced analytics, AI tools like GPT-4 allow for a deeper understanding of existing research and the identification of gaps in the literature, thereby promoting innovative research approaches. However, these advancements come with the need for updated ethical frameworks, a topic I would try to address also.\nThe issues related to AI use include issues of transparency and accountability, as the \u201dblackbox\u201d approach to deep learning models can be uncovered; without appropriate interpretability architecture (such as with GPT-4 or LLAMA), these models can be generating inaccurate information based on their predictive capabilities.\nNevertheless, these have proved to be of use, and with a fine prompt tuning, publicly available models can be of great use to researchers. I would delve into the question of how to balance the benefits of AI tools with the need to maintain high ethical standards in research, aiming to provide possible insights into how these ethical frameworks might be updated to accommodate the new realities of AI.\nFurthermore, I\u2019d reflect on the consequences of AI for the evaluation of research. While AI can aid in the quick assessment of a paper's relevance or novelty, questions remain about its capacity to fully evaluate the quality and significance of research. This discussion emphasizes the need for a blend of AI models with human expertise to achieve robust research evaluation for the time being (or for further training for specific use cases of the models.)\nI\u2019d like to conclude with a reflection on the overall impact of the integration of AI LLMs on systematic reviews and research dissemination. While acknowledging the transformative potential of AI in reshaping the scientific landscape, it underscores the need for careful navigation of the associated challenges and ethical implications.",
            "corpus_id": "265620985",
            "text": "Watch VIDEO.\nThis presentation will delve into the transformative role of AI in scholarly communication, highlighting its potential, implications, and challenges, and further addressing the ethical considerations that come with it.\nRecent advancements in AI, specifically large language models have unlocked new possibilities for scientific exploration and communication. Large language models such as GPT-4 and LLAMA, with their remarkable text-generation capabilities, stand at the forefront of this AI revolution. In the first part of the presentation, I examine how these AI tools are reshaping the nature of systematic reviews. The ability to analyze, summarize, and generate vast amounts of text allows these models to facilitate more efficient processes, offering a valuable tool to researchers navigating through vast databases of published work.\nI would discuss how AI is engendering new developments in research methodology. Through the use of predictive modelling and advanced analytics, AI tools like GPT-4 allow for a deeper understanding of existing research and the identification of gaps in the literature, thereby promoting innovative research approaches. However, these advancements come with the need for updated ethical frameworks, a topic I would try to address also.\nThe issues related to AI use include issues of transparency and accountability, as the \u201dblackbox\u201d approach to deep learning models can be uncovered; without appropriate interpretability architecture (such as with GPT-4 or LLAMA), these models can be generating inaccurate information based on their predictive capabilities.\nNevertheless, these have proved to be of use, and with a fine prompt tuning, publicly available models can be of great use to researchers. I would delve into the question of how to balance the benefits of AI tools with the need to maintain high ethical standards in research, aiming to provide possible insights into how these ethical frameworks might be updated to accommodate the new realities of AI.\nFurthermore, I\u2019d reflect on the consequences of AI for the evaluation of research. While AI can aid in the quick assessment of a paper's relevance or novelty, questions remain about its capacity to fully evaluate the quality and significance of research. This discussion emphasizes the need for a blend of AI models with human expertise to achieve robust research evaluation for the time being (or for further training for specific use cases of the models.)\nI\u2019d like to conclude with a reflection on the overall impact of the integration of AI LLMs on systematic reviews and research dissemination. While acknowledging the transformative potential of AI in reshaping the scientific landscape, it underscores the need for careful navigation of the associated challenges and ethical implications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0027370452880859375
        },
        {
            "paperId": "2171d913a9845de4da95a90b1ca3bbcb8194b72a",
            "corpusId": 271162102,
            "title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 57,
            "citationCount": 18,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.08940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66242399",
                    "name": "Biqing Qi"
                },
                {
                    "authorId": "2153281320",
                    "name": "Kaiyan Zhang"
                },
                {
                    "authorId": "2266239026",
                    "name": "Kai Tian"
                },
                {
                    "authorId": "2266389830",
                    "name": "Haoxiang Li"
                },
                {
                    "authorId": "2266267133",
                    "name": "Zhangren Chen"
                },
                {
                    "authorId": "2266239265",
                    "name": "Sihang Zeng"
                },
                {
                    "authorId": "2290020564",
                    "name": "Ermo Hua"
                },
                {
                    "authorId": "2304951738",
                    "name": "Jinfang Hu"
                },
                {
                    "authorId": "2218723159",
                    "name": "Bowen Zhou"
                }
            ],
            "abstract": "The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.",
            "corpus_id": "271162102",
            "text": "The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0199127197265625
        }
    ],
    "quotes": {
        "cost": 0.016686,
        "quotes": [
            {
                "idx": 0,
                "key": "[269502676 | Lin et al. | 2024 | Citations: 18]",
                "snippets": "We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination).",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[273653859 | Wang et al. | 2024 | Citations: 9]",
                "snippets": "(Liu et al., 2023) investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation...(Gong et al., 2023) uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation...[72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261076297 | Liu et al. | 2023 | Citations: 34]": "Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.",
                    "[262045057 | Gong et al. | 2023 | Citations: 26]": "In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc."
                },
                "metadata": [
                    {
                        "section_title": "Non-Generative LLM-based Approaches",
                        "pdf_hash": "",
                        "start": 164,
                        "end": 418,
                        "sentence_offsets": [
                            {
                                "start": 164,
                                "end": 273
                            },
                            {
                                "start": 274,
                                "end": 419
                            }
                        ],
                        "ref_mentions": [
                            "261076297"
                        ],
                        "quote": "(Liu et al., 2023) investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation"
                    },
                    {
                        "section_title": "Non-Generative LLM-based Approaches",
                        "pdf_hash": "",
                        "start": 682,
                        "end": 811,
                        "sentence_offsets": [
                            {
                                "start": 682,
                                "end": 811
                            }
                        ],
                        "ref_mentions": [
                            "262045057"
                        ],
                        "quote": "(Gong et al., 2023) uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation"
                    },
                    {
                        "section_title": "Non-Generative LLM-based Approaches",
                        "pdf_hash": "",
                        "start": 1882,
                        "end": 2117,
                        "sentence_offsets": [
                            {
                                "start": 1882,
                                "end": 2116
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "[72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.011499,
        "cot": "For this user query, which seems to be asking about instances where fine-tuning language models resulted in degradation of some capabilities, I'll organize the quotes into relevant dimensions.\n\nThe query is somewhat fragmented but essentially asks about cases where fine-tuning LLMs for specific tasks made the model worse at other capabilities, particularly looking for systematic studies rather than just anecdotes.\n\nI'll organize this into the following dimensions:\n\n1. \"Introduction to LLM Fine-tuning Trade-offs\" - This should be a synthesis paragraph providing background on fine-tuning and the potential for capability trade-offs. There are no specific quotes that directly address this background information, but it's necessary for framing the answer.\n\n2. \"Documented Cases of Fine-tuning Degradation\" - This should be a synthesis paragraph as the user is asking about research findings rather than requesting a list. This will include quote [0] which mentions that alignment processes (which include fine-tuning) can actually increase hallucinations rather than improve factual accuracy.\n\n3. \"Fine-tuning Effects on Task-specific Performance\" - This should be a synthesis paragraph discussing how fine-tuning affects specific capabilities. Quote [1] fits here as it discusses supervised fine-tuning for instruction compliance and its varying effectiveness across different types of tasks.\n\nI've chosen synthesis format for all dimensions because the user is asking for a comprehensive exploration of research findings rather than just a list of items.",
        "plan": {
            "Introduction to LLM Fine-tuning Trade-offs (synthesis)": [],
            "Documented Cases of Fine-tuning Degradation (synthesis)": [
                0
            ],
            "Fine-tuning Effects on Task-specific Performance (synthesis)": [
                1
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to LLM Fine-tuning Trade-offs",
                "tldr": "Fine-tuning LLMs for specific tasks often involves trade-offs where improving performance in one area can degrade capabilities in others. This phenomenon is known as \"catastrophic forgetting\" or \"negative transfer,\" where specialized training causes models to lose some of their general capabilities. (LLM Memory)",
                "text": "\nFine-tuning large language models (LLMs) is a common practice to adapt these models for specific tasks or domains. However, there is growing evidence that this process often involves significant trade-offs. When a model is fine-tuned to excel at a particular task, it may simultaneously experience degradation in its performance on other tasks or in its general capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThis trade-off is particularly pronounced when models are heavily optimized for narrow tasks. For example, when an LLM is fine-tuned extensively on question-answering datasets, it might become more precise at answering factual questions but lose some of its creative writing abilities or conversational fluency. Similarly, fine-tuning on coding tasks may improve programming capabilities while reducing performance on linguistic tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe machine learning community has documented this phenomenon under various terms including \"catastrophic forgetting,\" \"negative transfer,\" or simply \"specialization trade-offs.\" These effects are not always immediately apparent during evaluation if the testing only focuses on the target task rather than a broad assessment of the model's capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nUnderstanding these trade-offs is critical for practitioners who need to decide whether to use general-purpose LLMs or fine-tuned variants depending on their specific use cases and the breadth of capabilities required. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Documented Cases of Fine-tuning Degradation",
                "tldr": "Several studies have documented specific cases where fine-tuning LLMs leads to performance degradation in capabilities not targeted by the fine-tuning process. This degradation often manifests as increased hallucination, reduced reasoning abilities, or diminished performance on tasks different from the fine-tuning objective. (1 source)",
                "text": "\nResearch has increasingly documented specific instances where fine-tuning leads to performance degradation in LLMs. One notable example comes from alignment fine-tuning, which is typically intended to make models more helpful and safe. Lin et al. found that \"the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination)\" <Paper corpusId=\"269502676\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. This finding contradicts the intuitive expectation that alignment would improve model reliability across dimensions.\n\nBeyond alignment, degradation has been observed in various specialized fine-tuning scenarios. When models are optimized for specific tasks like medical question answering or code generation, they can experience diminished capabilities in areas like creative writing, summarization, or general reasoning. These effects are sometimes not immediately apparent during evaluation because testing often focuses narrowly on the target task rather than comprehensively assessing the model's broader capabilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe severity of degradation appears to correlate with the degree of specialization and the distance between the fine-tuning objective and the original model's training distribution. More aggressive fine-tuning approaches that significantly modify the model's parameters tend to result in more pronounced degradation of general capabilities. Similarly, fine-tuning on highly specialized or narrow datasets can cause the model to overfit to specific patterns at the expense of its general knowledge and reasoning abilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination)."
                        ],
                        "paper": {
                            "corpus_id": 269502676,
                            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "122045993",
                                    "name": "Sheng-Chieh Lin"
                                },
                                {
                                    "authorId": "2299485255",
                                    "name": "Luyu Gao"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "2266752758",
                                    "name": "Wenhan Xiong"
                                },
                                {
                                    "authorId": "2273564585",
                                    "name": "Jimmy Lin"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2292024725",
                                    "name": "Xilun Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 18
                        },
                        "score": 0.5400390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Fine-tuning Effects on Task-specific Performance",
                "tldr": "Fine-tuning LLMs for specific tasks like recommendation systems shows mixed results, with models often performing moderately on accuracy-based tasks but better on explainability tasks. Research reveals that targeted fine-tuning approaches, such as data pruning and domain-invariant feature extraction, can help mitigate some performance limitations. (3 sources)",
                "text": "\nResearch examining the fine-tuning effects on LLMs for specific tasks reveals nuanced performance patterns across different domains. In the recommendation systems space, Liu et al. conducted a comprehensive benchmark of LLMs on various recommendation tasks, finding that these models \"displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation\" <Paper corpusId=\"261076297\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This suggests that even with fine-tuning, LLMs may struggle to match specialized recommendation algorithms on certain precision-oriented tasks <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nHowever, the same research revealed that LLMs demonstrated \"comparable performance to state-of-the-art methods in explainability-based tasks\" like explanation generation and review summarization <Paper corpusId=\"261076297\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This indicates that the natural language capabilities of LLMs may transfer more effectively to tasks requiring semantic understanding and text generation, even after task-specific fine-tuning <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nResearchers have developed specialized approaches to enhance fine-tuning outcomes in specific domains. For example, Gong et al. proposed using LLMs \"to extract domain-invariant features\" that can help address cold start problems in recommendation systems <Paper corpusId=\"262045057\" paperTitle=\"(Gong et al., 2023)\" isShortName></Paper>. Their approach, called S&R Multi-Domain Foundation, leverages LLMs to extract features that work across both search and recommendation tasks, demonstrating better performance than other transfer learning methods when applied to cold start scenarios <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach involves data selection rather than model architecture changes. Researchers have developed \"a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning,\" which has shown promise for real-world applications <Paper corpusId=\"273653859\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This suggests that carefully selecting the most informative training examples may help preserve broader capabilities while still improving task-specific performance.\n\nThese findings collectively demonstrate that the effects of fine-tuning on task-specific performance are not uniform across all tasks and depend greatly on the nature of the task, the fine-tuning approach, and how well the task aligns with the model's pre-existing capabilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec."
                        ],
                        "paper": {
                            "corpus_id": 261076297,
                            "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
                            "authors": [
                                {
                                    "authorId": "2218869839",
                                    "name": "Junling Liu"
                                },
                                {
                                    "authorId": "3741691",
                                    "name": "Chao-Hong Liu"
                                },
                                {
                                    "authorId": "1800462890",
                                    "name": "Peilin Zhou"
                                },
                                {
                                    "authorId": "2190432576",
                                    "name": "Qichen Ye"
                                },
                                {
                                    "authorId": "52290752",
                                    "name": "Dading Chong"
                                },
                                {
                                    "authorId": "2165702320",
                                    "name": "Kangan Zhou"
                                },
                                {
                                    "authorId": "2154871075",
                                    "name": "Yueqi Xie"
                                },
                                {
                                    "authorId": "150346771",
                                    "name": "Yuwei Cao"
                                },
                                {
                                    "authorId": "2116951322",
                                    "name": "Shoujin Wang"
                                },
                                {
                                    "authorId": "2061592207",
                                    "name": "Chenyu You"
                                },
                                {
                                    "authorId": "2233087809",
                                    "name": "Philip S.Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "(Liu et al., 2023) investigates the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicates that LLMs display only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation",
                            "(Gong et al., 2023) uses LLM to extract domain-invariant features in a manner that can help deal with the cold start problems in recommendation",
                            "[72] proposes a novel data pruning method to efficiently identify the influential samples for LLM-based recommender fine-tuning, which unlocked the remarkable potential of applying LLM-based recommender models to real-world platforms."
                        ],
                        "paper": {
                            "corpus_id": 273653859,
                            "title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond",
                            "authors": [
                                {
                                    "authorId": "2284061879",
                                    "name": "Qi Wang"
                                },
                                {
                                    "authorId": "2243469243",
                                    "name": "Jindong Li"
                                },
                                {
                                    "authorId": "2283983135",
                                    "name": "Shiqi Wang"
                                },
                                {
                                    "authorId": "9123083",
                                    "name": "Qianli Xing"
                                },
                                {
                                    "authorId": "2174434599",
                                    "name": "Runliang Niu"
                                },
                                {
                                    "authorId": "2283935174",
                                    "name": "He Kong"
                                },
                                {
                                    "authorId": "2328073820",
                                    "name": "Rui Li"
                                },
                                {
                                    "authorId": "2293722046",
                                    "name": "Guodong Long"
                                },
                                {
                                    "authorId": "2243466364",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2283189862",
                                    "name": "Chengqi Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.50146484375
                    },
                    {
                        "id": "(Gong et al., 2023)",
                        "snippets": [
                            "In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc."
                        ],
                        "paper": {
                            "corpus_id": 262045057,
                            "title": "An Unified Search and Recommendation Foundation Model for Cold-Start Scenario",
                            "authors": [
                                {
                                    "authorId": "2243273769",
                                    "name": "Yuqi Gong"
                                },
                                {
                                    "authorId": "2243923570",
                                    "name": "Xichen Ding"
                                },
                                {
                                    "authorId": "2243315594",
                                    "name": "Yehui Su"
                                },
                                {
                                    "authorId": "2242792959",
                                    "name": "Kaiming Shen"
                                },
                                {
                                    "authorId": "2243372613",
                                    "name": "Zhongyi Liu"
                                },
                                {
                                    "authorId": "119557985",
                                    "name": "Guannan Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Information and Knowledge Management",
                            "n_citations": 26
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.036804
    }
}