{
    "query": "What specific technical methods have been proposed to detect and prevent copyright infringement in text-to-image diffusion models?",
    "user_id": "lib_user",
    "task_id": "57a58d11-9441-459f-9446-2edc3a4eccd7",
    "timestamp": "2025-06-23T23:09:08.764088",
    "n_retrieval": 256,
    "n_retrieved": 252,
    "n_candidates": 22,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.34997100000000003,
    "decomposed_query": {
        "rewritten_query": "Technical methods to detect and prevent copyright infringement in text-to-image diffusion models.",
        "keyword_query": "detect prevent copyright infringement text-to-image diffusion models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009693,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346891526",
                    "name": "Shunchang Liu"
                },
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2344619001",
                    "name": "Yaochu Jin"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.",
            "corpus_id": 276558342,
            "sentences": [
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b",
                    "score": 0.9113041864830383,
                    "section_title": "Introduction",
                    "char_start_offset": 4077,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 130,
                            "end": 175
                        },
                        {
                            "start": 178,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 667
                        },
                        {
                            "start": 670,
                            "end": 937
                        },
                        {
                            "start": 940,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1733
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1116,
                            "end": 1138,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 1138,
                            "end": 1162,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1204,
                            "end": 1222,
                            "matchedPaperCorpusId": "270309880"
                        },
                        {
                            "start": 1550,
                            "end": 1574,
                            "matchedPaperCorpusId": "208909851"
                        },
                        {
                            "start": 1594,
                            "end": 1614,
                            "matchedPaperCorpusId": "257687839"
                        },
                        {
                            "start": 1710,
                            "end": 1733,
                            "matchedPaperCorpusId": "258987384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99462890625
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.",
                    "score": 0.6804537645669604,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "For diffusion models, the output is influenced not only by the prompt but also by the latent noise. Latent noise represents encoded representations of the input that capture essential features in a lower-dimensional space. These latent variables guide the generation process, affecting the finer details of the resulting image. In this section, we propose a reinforcement learning (RL)-based latent control method to mitigate copyright infringement in diffusion-based generative models. Our method involves training an agent to search the input latent variables that yield lower infringement scores, ensuring that the generated outputs do not violate copyright. \n\nSpecifically, for latent variable z, we define a policy \u03c0 \u03c9 parameterized by \u03c9, allowing us to sample latent noise \u03f5 \u223c \u03c0 \u03c9 (z), which follows a Gaussian distribution. The sampled noise \u03f5 is then passed through the pre-trained diffusion decoder f to produce the image x = f (z, \u03f5). \n\nTo assess the copyright infringement potential of the generated image, we employ our CopyJudge to obtain the infringement score s f . Based on this score, we define a reward function: \n\nThis reward is designed to penalize outputs with higher infringement scores, thus encouraging the generation of non-infringing content. We optimize the parameters \u03c9 by maximizing the expected reward, L(\u03c9), defined as: \n\nThe gradient of this objective is computed using the REIN-FORCE rule (Williams, 1992), which is given by: \n\nDuring the training process, the latent variable z is updated according to the following rule: \n\nwhere \u03b2 is the step size. We further conduct normalization for the latent variables to maintain stability and prevent extreme deviations. This RL-based approach allows the agent to explore variations in the latent space, thereby improving its ability to generate non-infringing content. The detailed algorithm can be found in appendix B.1.",
                    "score": 0.49240754898767847,
                    "section_title": "Mitigation via RL-based Latent Control",
                    "char_start_offset": 13699,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 944
                        },
                        {
                            "start": 947,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1130
                        },
                        {
                            "start": 1133,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1350
                        },
                        {
                            "start": 1353,
                            "end": 1458
                        },
                        {
                            "start": 1461,
                            "end": 1555
                        },
                        {
                            "start": 1558,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1897
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1422,
                            "end": 1438,
                            "matchedPaperCorpusId": "2332513"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98974609375
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity. However, directly applying large models for infringement identification may face unreliable outputs due to their limited comprehension or potential misinterpretation. To address this, we propose CopyJudge, an automated abstraction-filtration-comparison framework with multi-LVLM debate to reliably follow the court decision process on identifying substantial similarity. Specifically, referring to the software abstraction test (Abramson, 2002), we decompose the image into different layers or elements, such as composition, color, and theme, to distinguish between the basic concepts of the image and its specific expressions. Then, we filter out parts that are not copyright-protected, such as public and functional expressions. Finally, we compare the filtered portions to assess whether there is substantial similarity. To enhance the reliability of the judgment, we employ a multi-agent debate (Du et al., 2023;Chan et al., 2023) method where multiple LVLMs discuss and score the similarity. Each LVLM can make judgments based on the scores and reasons provided by other LVLMs. Ultimately, another LVLM-based meta-judge gives the final score and rationale based on the consensus of the debate. To enhance the consistency with human preferences, we inject human priors into each LVLM via few-shot demonstrations (Agarwal et al., 2024). \n\nGiven the judging results, we further explore how to mitigate infringement issues in text-to-image diffusion models. Utilizing our CopyJudge, we propose an automated black-box infringement mitigation strategy that leverages a defense LVLM to iteratively optimize the input infringing prompts. This process avoids generating sensitive infringing expressions by querying supporting infringement rationales, while preserving the integrity of the original content. Moreover, if the input latent of the diffusion model is controllable, we could further enhance the mitigation approach by exploring specific non-infringing noise vectors within the latent space in a reinforcement manner, with the reward being reducing the predicted infringement score. This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts.",
                    "score": 0.5805437718398577,
                    "section_title": "Introduction",
                    "char_start_offset": 1848,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1222
                        },
                        {
                            "start": 1223,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1479
                        },
                        {
                            "start": 1482,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 2228
                        },
                        {
                            "start": 2229,
                            "end": 2356
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 568,
                            "end": 584,
                            "matchedPaperCorpusId": "6322845"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "We present CopyJudge, an innovative framework for automating the identification of copyright infringement in text-to-image diffusion models. By leveraging abstractionfiltration-comparison test and multi-LVLM debates, our approach could effectively evaluate the substantial similarity between generated and copyrighted images, providing clear and interpretable judgments. Additionally, our LVLM-based mitigation strategy helps avoid infringement by automatically optimizing prompts and exploring non-infringing latent noise vectors, while ensuring that generated images align with the user's requirements. \n\n\u2022 Modifying a Prompt to Improve Similarity Score (Attack Iteration): \"Adjust the parts of the original prompt of the second image that may cause expressions of distinction in the following rationale, making it more similar to the first image to achieve a higher score. Add more information about the [IP type] in Image 1, and provide more unique expressions specific to the [IP type] in Image 1. You can make any changes as long as they improve the similarity score. Require: Source image x cr , generated image x 0 , initial prompt p 0 , control condition p c , LVLM-based prompt modifier \u03c0 p , infringement identification function f , threshold \u03b3, maximum iterations T 1: Initialize prompt: p t \u2190 p 0 , generated image: \n\nGenerate new prompt: p t+1 \u2190 \u03c0 p (x t , x cr , p t , p c , s t , c t , r t ) 8: \n\nGenerate new image using p t+1 : x t+1 \u2190 T2I(p t+1 ) 9: \n\nUpdate iteration counter: t \u2190 t + 1 10: end while 11: Return final prompt p t and generated image x t",
                    "score": 0.6498275508978489,
                    "section_title": "Conclusion",
                    "char_start_offset": 24510,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 604
                        },
                        {
                            "start": 607,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1328
                        },
                        {
                            "start": 1331,
                            "end": 1410
                        },
                        {
                            "start": 1413,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1572
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control. In both scenarios, we compare our approach with the only prompt-based IP infringement mitigation method (Wang et al., 2024d), which detects potentially infringed works using LVLM and inputs the detected work information as a negative prompt (Neg-P) into the generative model to avoid infringement. We follow the same settings as in the original paper, but the paper does not address how to handle commercial models that cannot accept negative prompts. For such models, we simply appended the suffix \"without [copyrighted work information (e.g., name, author, etc.)]\" to the original prompt to simulate a negative prompt. Results. From Tables 3 and 4, it can be seen that our method significantly reduces the likelihood of infringement, both for explicit and implicit infringement, with only a slight drop in CLIP Score. The infringement score after only latent control is relatively higher than after prompt control because retrieving non-infringing latents without changing the prompt is quite challenging. However, we can still effectively reduce the infringement score while maintaining higher image-text matching quality. Figure 5 shows visualization results, where it can be observed that we avoid the IP infringement while preserving user requirements. Additional results and detailed time cost analysis are provided in Appendix B.3 and B.4.",
                    "score": 0.46921845819721286,
                    "section_title": "IP INFRINGEMENT MITIGATION",
                    "char_start_offset": 21622,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1476
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "Compared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: explicit infringement and implicit infringement. \n\nExplicit infringement. This refers to prompts that directly contain copyright information, such as \"Generate an image of Mickey Mouse.\" We use the 20 cartoon and artwork samples collected in section 5.1 to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work. \n\nImplicit infringement. This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL\u2022E 3 (Betker et al., 2023), which has a safety detection module to reject prompts that trigger it. \n\nAutomated attack. Efficiently retrieving or generating infringing prompts has always been a challenge. Kim et al. utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point. \n\nFor explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control.",
                    "score": 0.47864674828714676,
                    "section_title": "IP INFRINGEMENT MITIGATION",
                    "char_start_offset": 19610,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 662
                        },
                        {
                            "start": 665,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1289
                        },
                        {
                            "start": 1292,
                            "end": 1309
                        },
                        {
                            "start": 1310,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1920
                        },
                        {
                            "start": 1923,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2140
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1196,
                            "end": 1217,
                            "matchedPaperCorpusId": "264403242"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9697265625
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "Text-to-image generative models (Rombach et al., 2022;Betker et al., 2023;Team et al., 2023;Esser et al., 2024;Hurst et al., 2024;Zhang et al., 2023b;a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023;Somepalli et al., 2023a;Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity. \n\nHowever, identifying substantial similarity is not a trivial task. There are already some methods to assess image similarity through distance-based metrics, e.g., L 2 norm (Carlini et al., 2023). However, we found that these manually designed metrics do not always align with the human judgment for infringement determination. Additionally, it often suffer from insufficient generalization ability and lack interpretable results. This motivates the need for an approach that better measures substantial similarity, one that is more humancentered, interpretable, and generalized to handle copyright infringement identification in AI-generated images. \n\nRecently, large-scale models have already been successfully applied as judges in fields such as finance, education, and healthcare (Gu et al., 2024;Li et al., 2024;Zhuge et al., 2024). In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity.",
                    "score": 0.5220433375604222,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 993
                        },
                        {
                            "start": 996,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1645
                        },
                        {
                            "start": 1648,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 1972
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 54,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 54,
                            "end": 74,
                            "matchedPaperCorpusId": "264403242"
                        },
                        {
                            "start": 92,
                            "end": 111,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 130,
                            "end": 150,
                            "matchedPaperCorpusId": "256827727"
                        },
                        {
                            "start": 371,
                            "end": 393,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 393,
                            "end": 417,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1168,
                            "end": 1190,
                            "matchedPaperCorpusId": "256389993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "While our method shows promise, its performance is currently constrained by the availability of labeled data, making it challenging to fully evaluate its alignment with human judgment. To address this, we believe that developing a more comprehensive dataset containing detailed human judgment criteria is crucial. In addition, we are looking forward to stronger attacks to test the robustness of our mitigation strategies. Improving the retrieval of infringing images will also provide valuable insights into the relationship between generative models and copyrighted content. Lastly, although our approach does not require a specific LVLM, we plan to explore other models beyond GPT in advancing copyright protection in the future.",
                    "score": 0.48071997670636246,
                    "section_title": "Limitations and Future Work",
                    "char_start_offset": 23763,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 732
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.99462890625,
            "relevance_judgment_input_expanded": "# Title: CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: Shunchang Liu, Zhuan Shi, Lingjuan Lyu, Yaochu Jin, Boi Faltings\n## Abstract\nAssessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.\n## Introduction\nText-to-image generative models (Rombach et al., 2022;Betker et al., 2023;Team et al., 2023;Esser et al., 2024;Hurst et al., 2024;Zhang et al., 2023b;a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023;Somepalli et al., 2023a;Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity. \n\nHowever, identifying substantial similarity is not a trivial task. There are already some methods to assess image similarity through distance-based metrics, e.g., L 2 norm (Carlini et al., 2023). However, we found that these manually designed metrics do not always align with the human judgment for infringement determination. Additionally, it often suffer from insufficient generalization ability and lack interpretable results. This motivates the need for an approach that better measures substantial similarity, one that is more humancentered, interpretable, and generalized to handle copyright infringement identification in AI-generated images. \n\nRecently, large-scale models have already been successfully applied as judges in fields such as finance, education, and healthcare (Gu et al., 2024;Li et al., 2024;Zhuge et al., 2024). In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity.\n...\nIn this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity. However, directly applying large models for infringement identification may face unreliable outputs due to their limited comprehension or potential misinterpretation. To address this, we propose CopyJudge, an automated abstraction-filtration-comparison framework with multi-LVLM debate to reliably follow the court decision process on identifying substantial similarity. Specifically, referring to the software abstraction test (Abramson, 2002), we decompose the image into different layers or elements, such as composition, color, and theme, to distinguish between the basic concepts of the image and its specific expressions. Then, we filter out parts that are not copyright-protected, such as public and functional expressions. Finally, we compare the filtered portions to assess whether there is substantial similarity. To enhance the reliability of the judgment, we employ a multi-agent debate (Du et al., 2023;Chan et al., 2023) method where multiple LVLMs discuss and score the similarity. Each LVLM can make judgments based on the scores and reasons provided by other LVLMs. Ultimately, another LVLM-based meta-judge gives the final score and rationale based on the consensus of the debate. To enhance the consistency with human preferences, we inject human priors into each LVLM via few-shot demonstrations (Agarwal et al., 2024). \n\nGiven the judging results, we further explore how to mitigate infringement issues in text-to-image diffusion models. Utilizing our CopyJudge, we propose an automated black-box infringement mitigation strategy that leverages a defense LVLM to iteratively optimize the input infringing prompts. This process avoids generating sensitive infringing expressions by querying supporting infringement rationales, while preserving the integrity of the original content. Moreover, if the input latent of the diffusion model is controllable, we could further enhance the mitigation approach by exploring specific non-infringing noise vectors within the latent space in a reinforcement manner, with the reward being reducing the predicted infringement score. This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts.\n...\nThis helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b\n\n## Mitigation via RL-based Latent Control\nFor diffusion models, the output is influenced not only by the prompt but also by the latent noise. Latent noise represents encoded representations of the input that capture essential features in a lower-dimensional space. These latent variables guide the generation process, affecting the finer details of the resulting image. In this section, we propose a reinforcement learning (RL)-based latent control method to mitigate copyright infringement in diffusion-based generative models. Our method involves training an agent to search the input latent variables that yield lower infringement scores, ensuring that the generated outputs do not violate copyright. \n\nSpecifically, for latent variable z, we define a policy \u03c0 \u03c9 parameterized by \u03c9, allowing us to sample latent noise \u03f5 \u223c \u03c0 \u03c9 (z), which follows a Gaussian distribution. The sampled noise \u03f5 is then passed through the pre-trained diffusion decoder f to produce the image x = f (z, \u03f5). \n\nTo assess the copyright infringement potential of the generated image, we employ our CopyJudge to obtain the infringement score s f . Based on this score, we define a reward function: \n\nThis reward is designed to penalize outputs with higher infringement scores, thus encouraging the generation of non-infringing content. We optimize the parameters \u03c9 by maximizing the expected reward, L(\u03c9), defined as: \n\nThe gradient of this objective is computed using the REIN-FORCE rule (Williams, 1992), which is given by: \n\nDuring the training process, the latent variable z is updated according to the following rule: \n\nwhere \u03b2 is the step size. We further conduct normalization for the latent variables to maintain stability and prevent extreme deviations. This RL-based approach allows the agent to explore variations in the latent space, thereby improving its ability to generate non-infringing content. The detailed algorithm can be found in appendix B.1.\n\n## IP INFRINGEMENT MITIGATION\nCompared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: explicit infringement and implicit infringement. \n\nExplicit infringement. This refers to prompts that directly contain copyright information, such as \"Generate an image of Mickey Mouse.\" We use the 20 cartoon and artwork samples collected in section 5.1 to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work. \n\nImplicit infringement. This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL\u2022E 3 (Betker et al., 2023), which has a safety detection module to reject prompts that trigger it. \n\nAutomated attack. Efficiently retrieving or generating infringing prompts has always been a challenge. Kim et al. utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point. \n\nFor explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control.\n...\nFor implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control. In both scenarios, we compare our approach with the only prompt-based IP infringement mitigation method (Wang et al., 2024d), which detects potentially infringed works using LVLM and inputs the detected work information as a negative prompt (Neg-P) into the generative model to avoid infringement. We follow the same settings as in the original paper, but the paper does not address how to handle commercial models that cannot accept negative prompts. For such models, we simply appended the suffix \"without [copyrighted work information (e.g., name, author, etc.)]\" to the original prompt to simulate a negative prompt. Results. From Tables 3 and 4, it can be seen that our method significantly reduces the likelihood of infringement, both for explicit and implicit infringement, with only a slight drop in CLIP Score. The infringement score after only latent control is relatively higher than after prompt control because retrieving non-infringing latents without changing the prompt is quite challenging. However, we can still effectively reduce the infringement score while maintaining higher image-text matching quality. Figure 5 shows visualization results, where it can be observed that we avoid the IP infringement while preserving user requirements. Additional results and detailed time cost analysis are provided in Appendix B.3 and B.4.\n\n## Limitations and Future Work\nWhile our method shows promise, its performance is currently constrained by the availability of labeled data, making it challenging to fully evaluate its alignment with human judgment. To address this, we believe that developing a more comprehensive dataset containing detailed human judgment criteria is crucial. In addition, we are looking forward to stronger attacks to test the robustness of our mitigation strategies. Improving the retrieval of infringing images will also provide valuable insights into the relationship between generative models and copyrighted content. Lastly, although our approach does not require a specific LVLM, we plan to explore other models beyond GPT in advancing copyright protection in the future.\n\n## Conclusion\nWe present CopyJudge, an innovative framework for automating the identification of copyright infringement in text-to-image diffusion models. By leveraging abstractionfiltration-comparison test and multi-LVLM debates, our approach could effectively evaluate the substantial similarity between generated and copyrighted images, providing clear and interpretable judgments. Additionally, our LVLM-based mitigation strategy helps avoid infringement by automatically optimizing prompts and exploring non-infringing latent noise vectors, while ensuring that generated images align with the user's requirements. \n\n\u2022 Modifying a Prompt to Improve Similarity Score (Attack Iteration): \"Adjust the parts of the original prompt of the second image that may cause expressions of distinction in the following rationale, making it more similar to the first image to achieve a higher score. Add more information about the [IP type] in Image 1, and provide more unique expressions specific to the [IP type] in Image 1. You can make any changes as long as they improve the similarity score. Require: Source image x cr , generated image x 0 , initial prompt p 0 , control condition p c , LVLM-based prompt modifier \u03c0 p , infringement identification function f , threshold \u03b3, maximum iterations T 1: Initialize prompt: p t \u2190 p 0 , generated image: \n\nGenerate new prompt: p t+1 \u2190 \u03c0 p (x t , x cr , p t , p c , s t , c t , r t ) 8: \n\nGenerate new image using p t+1 : x t+1 \u2190 T2I(p t+1 ) 9: \n\nUpdate iteration counter: t \u2190 t + 1 10: end while 11: Return final prompt p t and generated image x t",
            "reference_string": "[276558342 | Liu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2318391138",
                    "name": "Jing Yan"
                },
                {
                    "authorId": "2318236128",
                    "name": "Xiaoli Tang"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
            "corpus_id": 272146279,
            "sentences": [
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "we propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted",
                    "score": 0.6184945713475277,
                    "section_title": "Overview",
                    "char_start_offset": 9821,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 719
                        },
                        {
                            "start": 722,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1213
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99365234375
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "In this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity. \n\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets.",
                    "score": 0.6006551169589541,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 23145,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1012
                        },
                        {
                            "start": 1015,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1600
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
                    "score": 0.5918619265860612,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "RLCP aims to achieve the dual objectives of maintaining high-quality image generation and ensuring compliance with copyright laws, thereby addressing the pressing challenge of copyright infringement in text-to-image diffusion models. It consists of several key components: \n\n\u2022 Reward-Based Learning Framework: Our approach involves using a discriminator in the reward model to score generated samples. Images that are less similar to copyrighted data receive higher rewards. This reward system helps guide the model towards generating compliant images. \u2022 Fine-Tuning with Reinforcement Learning: Starting from a pre-trained text-to-image diffusion model, we fine-tune it using RL techniques to optimize the generation process according to the defined reward functions. This involves framing the denoising process as a multistep decision-making problem and applying policy gradient algorithms to maximize the reward signal. \u2022 KL Regularization: To prevent the model from overfitting to the reward function, we introduce KL divergence as a regularization term. This helps maintain the generative capabilities of the original diffusion model while steering it towards producing non-infringing content. \n\nSpecific training process we describe how we fine-tune a pre-trained text-to-image diffusion model using reinforcement learning (RL) to minimize the risk of generating copyright-infringing content. Our approach is guided by a reward function that leverages the copyright loss (CL) metric defined in Eq. ( 5), and incorporates KL regularization to ensure the model maintains high-quality image generation. \n\nModel Initialization: We begin with a pre-trained diffusion model M \u03b8 , where \u03b8 denotes the model parameters. The model is initialized based on a large-scale text-toimage dataset that includes both copyrighted (D c ) and noncopyrighted (D nc ) data. \n\nReward Function Design: The reward function r(x 0 , c) is the key component guiding the model's training. It is defined as a weighted sum of the semantic similarity loss L sem and the perceptual similarity loss L perc : \n\nwhere: \n\n\u2022 CL sem (x 0 , c) represents the semantic similarity loss, calculated as the Mean Squared Error (MSE) between the generated image embeddings and the embeddings of the original copyright images.",
                    "score": 0.5407584575545908,
                    "section_title": "The proposed method RLCP",
                    "char_start_offset": 13464,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 272
                        },
                        {
                            "start": 275,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1605
                        },
                        {
                            "start": 1608,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1857
                        },
                        {
                            "start": 1860,
                            "end": 1965
                        },
                        {
                            "start": 1966,
                            "end": 2079
                        },
                        {
                            "start": 2082,
                            "end": 2088
                        },
                        {
                            "start": 2091,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "However, this method is highly dependent on the specific image and model, and it lacks general reliability. Watermarking (Dogoulis et al. 2023;Epstein et al. 2023) inserts specific, unnoticeable patterns into protected images to detect copyright infringement, but further research is needed to improve its robustness. Machine unlearning (Bourtoule et al. 2020;Ginart et al. 2019;Huang et al. 2021;Gao et al. 2023;Nguyen et al. 2022) removes contributions of copyright data, aligning with the right to be forgotten, while dataset deduplication (Somepalli et al. 2022) helps reduce the risk of training sample memorization. \n\nDespite these efforts, existing copyright protection methods still have the following limitations: (1) They lack a standardized copyright metric that aligns with copyright laws and regulations, making it difficult to determine if generated images constitute copyright infringement; (2) These methods often prioritize performance on specific downstream tasks rather than focusing on general applicability, resulting in approaches that work well only on certain datasets but lack the versatility needed for broader use across diverse datasets. \n\nTo tackle these challenges, we propose a Reinforcement Learning-based Copyright Protection method (RLCP) for text-to-image diffusion models to reduce the possibility of generating copyright-infringing content. Specifically, inspired by Courts 1 in the US which employs two-part test to determine copyright violation, which contains an extrinsic test examining objective similarity in specific expressive elements, and an intrinsic test assessing subjective similarity from the perspective of a reasonable audience, we first propose copyright metric that mirror these legal standards by combining semantic and perceptual similarity. Then, we propose a novel framework that combines reinforcement learning with copyright infringement metrics. We leverage the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decisionmaking process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. \n\nOur main contributions are as follows:",
                    "score": 0.5625598428316976,
                    "section_title": "Introduction",
                    "char_start_offset": 1955,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 621
                        },
                        {
                            "start": 624,
                            "end": 1165
                        },
                        {
                            "start": 1168,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2132
                        },
                        {
                            "start": 2133,
                            "end": 2256
                        },
                        {
                            "start": 2259,
                            "end": 2297
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 121,
                            "end": 143,
                            "matchedPaperCorpusId": "258297834"
                        },
                        {
                            "start": 143,
                            "end": 162,
                            "matchedPaperCorpusId": "264436550"
                        },
                        {
                            "start": 337,
                            "end": 360,
                            "matchedPaperCorpusId": "5613334"
                        },
                        {
                            "start": 360,
                            "end": 379,
                            "matchedPaperCorpusId": "257804739"
                        },
                        {
                            "start": 397,
                            "end": 413,
                            "matchedPaperCorpusId": "257804739"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "In this section, we first evaluate the effectiveness of our proposed copyright loss metric. Then we evaluate RLCP on 3 real-world datasets. Furthermore, we explore the impact of the proportion of copyright images in the training set on the efficiency of RLCP. For the non-copyright dataset, we sourced images from ImageNet (Deng et al. 2009), selecting one image from each class, resulting in a total of 1,000 images. \n\nEvaluation Metric. We use our proposed metric Copyright Loss (CL) as well as CLIP and l 2 norm to measure the degree of copyright violation. We also use FID to measure the generative quality of text-to-image diffusion model. 1. CLIP: We evaluate changes of CLIP scores, for textimage similarity. 2. Copyright Loss (CL): We quantify the similarity between the original copyright images and their unlearned counterparts on the feature level, utilizing our proposed CL(copyright loss) metric. 3. l 2 norm: We calculate the squared l 2 distance between generated and training images. 4. Fr\u00e9chet Inception Distance (FID): We use FID to evaluate the generative image quality of text-to-image diffusion model. The formulation of the metric is as follows: \n\nwhere: \n\n\u2022 \u00b5 x and \u03a3 x represent the mean and covariance matrix of the feature vectors from real images. \u2022 \u00b5 y and \u03a3 y represent the mean and covariance matrix of the feature vectors from generated images. \u2022 Tr(\u2022) denotes the trace of a matrix. This evaluation provides valuable insights into assessing copyright infringement while preserving the ability to generate non-infringing content. \n\nBaselines.",
                    "score": 0.4952309013423758,
                    "section_title": "Experiment",
                    "char_start_offset": 16961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 91
                        },
                        {
                            "start": 92,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 417
                        },
                        {
                            "start": 420,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1167
                        },
                        {
                            "start": 1170,
                            "end": 1176
                        },
                        {
                            "start": 1179,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1414
                        },
                        {
                            "start": 1415,
                            "end": 1560
                        },
                        {
                            "start": 1563,
                            "end": 1573
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 323,
                            "end": 341,
                            "matchedPaperCorpusId": "57246310"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "In this section, we evaluate the effectiveness of our proposed RLCP method across several dimensions. We compare our results against baseline methods, measure the impact of different proportions of copyrighted data, and analyze the trade-offs between copyright protection and image quality. \n\nEffectiveness of copyright metric. Our first set of experiments aims to assess the reliability of our proposed copyright loss (CL) metric in distinguishing between copyrighted and non-copyrighted content. \n\nThe experimental results shown in Figure 3 were obtained using a subset of the paintings dataset(See in Fig-  ). The L2-norm was determined by calculating the squared L2 distance between these weighted activations, while LPIPS normalized the feature dimensions of all pixels and layers to unit length, scaling each feature by a specific weight. In Figure 3, the \"query index\" refers to the position of the query image in the dataset, while the \"value index\" represents the corresponding value or similarity score for each generated image. The heatmaps illustrate good performance by showing high similarity scores for relevant image pairs, indicating that the model successfully retrieves or generates images that closely match the queries in terms of semantic or visual content. \n\nThe resulting heatmaps demonstrate that the combined copyright loss metric, which integrates both semantic and perceptual metrics, offers a more balanced and accurate assessment of copyright infringement risks. The copyright loss  heatmap, in particular, exhibits a more nuanced and effective differentiation between potentially infringing and noninfringing content, highlighting its robustness in evaluating generative model outputs. \n\nPerformance of Copyright Protection. We have two primary foundational models in our repertoire. The first one is our finetuned model based on reinforcement learning, SDfinetuned, which is employed to evaluate and assess the performance of the reinforcement learning algorithm and finetuned the stable diffusion model with the use of similarity metrics. This evaluation includes both the model's ability to forget copyrighted images and generated data quality. The second foundational model is on the basic SD-XL 1.0. During the unlearning experiments conducted on these two foundational models, for each image to be forgotten, the learning rate for gradient ascent is set at 3e-4, detailed hyperparameters are listed in Table 1.",
                    "score": 0.4367814079133655,
                    "section_title": "Experiment Results",
                    "char_start_offset": 18557,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 290
                        },
                        {
                            "start": 293,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 497
                        },
                        {
                            "start": 500,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1279
                        },
                        {
                            "start": 1282,
                            "end": 1492
                        },
                        {
                            "start": 1493,
                            "end": 1716
                        },
                        {
                            "start": 1719,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 2071
                        },
                        {
                            "start": 2072,
                            "end": 2178
                        },
                        {
                            "start": 2179,
                            "end": 2235
                        },
                        {
                            "start": 2236,
                            "end": 2447
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.99365234375,
            "relevance_judgment_input_expanded": "# Title: RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model\n# Venue: arXiv.org\n# Authors: Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings\n## Abstract\nThe increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.\n## Introduction\nHowever, this method is highly dependent on the specific image and model, and it lacks general reliability. Watermarking (Dogoulis et al. 2023;Epstein et al. 2023) inserts specific, unnoticeable patterns into protected images to detect copyright infringement, but further research is needed to improve its robustness. Machine unlearning (Bourtoule et al. 2020;Ginart et al. 2019;Huang et al. 2021;Gao et al. 2023;Nguyen et al. 2022) removes contributions of copyright data, aligning with the right to be forgotten, while dataset deduplication (Somepalli et al. 2022) helps reduce the risk of training sample memorization. \n\nDespite these efforts, existing copyright protection methods still have the following limitations: (1) They lack a standardized copyright metric that aligns with copyright laws and regulations, making it difficult to determine if generated images constitute copyright infringement; (2) These methods often prioritize performance on specific downstream tasks rather than focusing on general applicability, resulting in approaches that work well only on certain datasets but lack the versatility needed for broader use across diverse datasets. \n\nTo tackle these challenges, we propose a Reinforcement Learning-based Copyright Protection method (RLCP) for text-to-image diffusion models to reduce the possibility of generating copyright-infringing content. Specifically, inspired by Courts 1 in the US which employs two-part test to determine copyright violation, which contains an extrinsic test examining objective similarity in specific expressive elements, and an intrinsic test assessing subjective similarity from the perspective of a reasonable audience, we first propose copyright metric that mirror these legal standards by combining semantic and perceptual similarity. Then, we propose a novel framework that combines reinforcement learning with copyright infringement metrics. We leverage the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decisionmaking process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. \n\nOur main contributions are as follows:\n\n## Overview\nwe propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted\n\n## The proposed method RLCP\nRLCP aims to achieve the dual objectives of maintaining high-quality image generation and ensuring compliance with copyright laws, thereby addressing the pressing challenge of copyright infringement in text-to-image diffusion models. It consists of several key components: \n\n\u2022 Reward-Based Learning Framework: Our approach involves using a discriminator in the reward model to score generated samples. Images that are less similar to copyrighted data receive higher rewards. This reward system helps guide the model towards generating compliant images. \u2022 Fine-Tuning with Reinforcement Learning: Starting from a pre-trained text-to-image diffusion model, we fine-tune it using RL techniques to optimize the generation process according to the defined reward functions. This involves framing the denoising process as a multistep decision-making problem and applying policy gradient algorithms to maximize the reward signal. \u2022 KL Regularization: To prevent the model from overfitting to the reward function, we introduce KL divergence as a regularization term. This helps maintain the generative capabilities of the original diffusion model while steering it towards producing non-infringing content. \n\nSpecific training process we describe how we fine-tune a pre-trained text-to-image diffusion model using reinforcement learning (RL) to minimize the risk of generating copyright-infringing content. Our approach is guided by a reward function that leverages the copyright loss (CL) metric defined in Eq. ( 5), and incorporates KL regularization to ensure the model maintains high-quality image generation. \n\nModel Initialization: We begin with a pre-trained diffusion model M \u03b8 , where \u03b8 denotes the model parameters. The model is initialized based on a large-scale text-toimage dataset that includes both copyrighted (D c ) and noncopyrighted (D nc ) data. \n\nReward Function Design: The reward function r(x 0 , c) is the key component guiding the model's training. It is defined as a weighted sum of the semantic similarity loss L sem and the perceptual similarity loss L perc : \n\nwhere: \n\n\u2022 CL sem (x 0 , c) represents the semantic similarity loss, calculated as the Mean Squared Error (MSE) between the generated image embeddings and the embeddings of the original copyright images.\n\n## Experiment\nIn this section, we first evaluate the effectiveness of our proposed copyright loss metric. Then we evaluate RLCP on 3 real-world datasets. Furthermore, we explore the impact of the proportion of copyright images in the training set on the efficiency of RLCP. For the non-copyright dataset, we sourced images from ImageNet (Deng et al. 2009), selecting one image from each class, resulting in a total of 1,000 images. \n\nEvaluation Metric. We use our proposed metric Copyright Loss (CL) as well as CLIP and l 2 norm to measure the degree of copyright violation. We also use FID to measure the generative quality of text-to-image diffusion model. 1. CLIP: We evaluate changes of CLIP scores, for textimage similarity. 2. Copyright Loss (CL): We quantify the similarity between the original copyright images and their unlearned counterparts on the feature level, utilizing our proposed CL(copyright loss) metric. 3. l 2 norm: We calculate the squared l 2 distance between generated and training images. 4. Fr\u00e9chet Inception Distance (FID): We use FID to evaluate the generative image quality of text-to-image diffusion model. The formulation of the metric is as follows: \n\nwhere: \n\n\u2022 \u00b5 x and \u03a3 x represent the mean and covariance matrix of the feature vectors from real images. \u2022 \u00b5 y and \u03a3 y represent the mean and covariance matrix of the feature vectors from generated images. \u2022 Tr(\u2022) denotes the trace of a matrix. This evaluation provides valuable insights into assessing copyright infringement while preserving the ability to generate non-infringing content. \n\nBaselines.\n\n## Experiment Results\nIn this section, we evaluate the effectiveness of our proposed RLCP method across several dimensions. We compare our results against baseline methods, measure the impact of different proportions of copyrighted data, and analyze the trade-offs between copyright protection and image quality. \n\nEffectiveness of copyright metric. Our first set of experiments aims to assess the reliability of our proposed copyright loss (CL) metric in distinguishing between copyrighted and non-copyrighted content. \n\nThe experimental results shown in Figure 3 were obtained using a subset of the paintings dataset(See in Fig-  ). The L2-norm was determined by calculating the squared L2 distance between these weighted activations, while LPIPS normalized the feature dimensions of all pixels and layers to unit length, scaling each feature by a specific weight. In Figure 3, the \"query index\" refers to the position of the query image in the dataset, while the \"value index\" represents the corresponding value or similarity score for each generated image. The heatmaps illustrate good performance by showing high similarity scores for relevant image pairs, indicating that the model successfully retrieves or generates images that closely match the queries in terms of semantic or visual content. \n\nThe resulting heatmaps demonstrate that the combined copyright loss metric, which integrates both semantic and perceptual metrics, offers a more balanced and accurate assessment of copyright infringement risks. The copyright loss  heatmap, in particular, exhibits a more nuanced and effective differentiation between potentially infringing and noninfringing content, highlighting its robustness in evaluating generative model outputs. \n\nPerformance of Copyright Protection. We have two primary foundational models in our repertoire. The first one is our finetuned model based on reinforcement learning, SDfinetuned, which is employed to evaluate and assess the performance of the reinforcement learning algorithm and finetuned the stable diffusion model with the use of similarity metrics. This evaluation includes both the model's ability to forget copyrighted images and generated data quality. The second foundational model is on the basic SD-XL 1.0. During the unlearning experiments conducted on these two foundational models, for each image to be forgotten, the learning rate for gradient ascent is set at 3e-4, detailed hyperparameters are listed in Table 1.\n\n## Conclusion and Future Work\nIn this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity. \n\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets.",
            "reference_string": "[272146279 | Shi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
            "venue": "SIGKDD Explorations",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2310.02401",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2254140893",
                    "name": "Yuping Lin"
                },
                {
                    "authorId": "2253881697",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "2253469617",
                    "name": "Yue Xing"
                },
                {
                    "authorId": "2255025428",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2253533415",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Text-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.",
            "corpus_id": 263622213,
            "sentences": [
                {
                    "corpus_id": "263622213",
                    "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
                    "text": "Text-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.",
                    "score": 0.5292010530507283,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99365234375
                },
                {
                    "corpus_id": "263622213",
                    "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
                    "text": "One potential reason is that the watermark-generating procedure predominantly adheres to traditional watermark strategies, which are intended to trace the source of an image rather than protecting its IP in the context of diffusion models fine-tuning. Consequently, there's no assurance that the watermark's features will be learned by the model before other features. \n\nSecond, as indicated by Ma et al. (2023), due to the distribution shift between the fine-tuning data of generative models and the resulting generated images, it is crucial to incorporate images generated by fine-tuned models to develop the watermark detector. Since there are numerous fine-tuning methods introduced from different perspectives, a detector for one method might lose its effectiveness for others. This issue is highlighted by Ma et al. (2023) that a large drop in watermark detection performance is observed when applying a detector tailored to one fine-tuning method to others. Meanwhile, in reality, a data protector may not know which fine-tuning method was used by the infringer, thus it is desired to design a watermark detection strategy that is effective for various fine-tuning methods. \n\nTo tackle the aforementioned challenges, we propose a novel watermarking framework, FT-Shield, tailored for data's copyright protection against the Fine-Tuning of text-to-image diffusion models. In particular, we introduce a training objective incorporating the fine-tuning loss of diffusion models for watermark generation. By minimizing the objective, we ensure that the optimized watermark pattern can be quickly learned by diffusion model at the very early stage of fine-tuning. As shown in Figure 1, even when the style has not been adopted, our watermark has already been learned by the diffusion model. Furthermore, to obtain a watermark detector for various fine-tuning methods, we introduce a detection framework based on Mixture of Experts (Jacobs et al., 1991). The effectiveness of FT-Shield is verified through experiments across various fine-tuning approaches including DreamBooth (Ruiz et al., 2023), Textual Inversion (Gal et al., 2022), Text-to-Image Fine-Tuning (von Platen et al., 2022) and LoRA (Hu et al., 2021), applied to both style transfer and object transfer tasks across multiple datasets.",
                    "score": 0.44936581452682123,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 3880,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 368
                        },
                        {
                            "start": 371,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1180
                        },
                        {
                            "start": 1183,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1507
                        },
                        {
                            "start": 1508,
                            "end": 1665
                        },
                        {
                            "start": 1666,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1933,
                            "end": 1954,
                            "matchedPaperCorpusId": "572361"
                        },
                        {
                            "start": 2078,
                            "end": 2097,
                            "matchedPaperCorpusId": "251800180"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                },
                {
                    "corpus_id": "263622213",
                    "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
                    "text": "To protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement. \n\nAdversarial methods. Adversarial methods protect data's IP by applying the idea of evasion attacks. They treat the unauthorized generative models as the target of attack, and develop adversarial examples to disrupt the learning process of unauthorized fine-tuning. GLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios. Although these methods provide effective protection against infringement, they can inadvertently disrupt authorized uses (such as for academic research purposes) of the safeguarded images. This indicates the necessity of developing watermarking approaches, which allow the IP to be used for proper reasons while also acting as a way to collect proof against improper uses. \n\nWatermarking methods. Watermark has also been considered to protect the IP of images against unauthorized usage during the fine-tuning of text-to-image models. Wang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model.",
                    "score": 0.4735734143952356,
                    "section_title": "IMAGE PROTECTION METHODS",
                    "char_start_offset": 7951,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 362
                        },
                        {
                            "start": 365,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1632
                        },
                        {
                            "start": 1635,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1936
                        },
                        {
                            "start": 1937,
                            "end": 2232
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 851,
                            "end": 870,
                            "matchedPaperCorpusId": "256697414"
                        },
                        {
                            "start": 965,
                            "end": 984,
                            "matchedPaperCorpusId": "251800180"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                },
                {
                    "corpus_id": "263622213",
                    "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
                    "text": "In the scenario of copyright infringement and protection considered in this work, there are two roles: \n\n(1) a data protector that possesses the data copyright, utilizes watermarking techniques before the data is released, and tries to detect if a suspected image is generated by a model fine-tuned on the protected images, and (2) a data offender that uses the protected data for text-to-image model finetuning without permission from the data protector. The data offenders have complete control over the fine-tuning and sampling processes of the text-to-image diffusion models, while the data protectors can only modify the data they own before their data is released and access images generated by the suspected model. \n\nAs shown in Figure 2, the protection process consists of two stages: the protection stage and the audit stage. In the protection stage, the data protector protects the images by adding imperceptible watermarks to the images. Specifically, given that the size of the protected dataset is N , the target is to generate sample-wise watermark \u03b4 i for each protected image x i , \u2200i = 1...N . Then these watermarks are embedded into the protected images xi = x i + \u03b4 i . Correspondingly, the data protector develops a watermark detection approach, denoted by function D w (\u2022), to test whether there is a watermark on the suspected image. To ensure that the watermarks will not lead to severe influence on image quality, we limit the budget of the watermark by constraining its l \u221e norm (\u2225\u03b4 i \u2225 \u221e \u2264 \u03b7) to control the pixel-wise difference between the two images x i and xi . In the audit stage, if the protectors encounter suspected images potentially produced through unauthorized text-to-image models finetuning, they will apply the watermark detection process D w (\u2022) to ascertain whether these images have infringed upon their data rights.",
                    "score": 0.47576876928105044,
                    "section_title": "PROBLEM FORMULATION",
                    "char_start_offset": 11326,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 105,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 721
                        },
                        {
                            "start": 724,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1188
                        },
                        {
                            "start": 1189,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1860
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.99365234375,
            "relevance_judgment_input_expanded": "# Title: FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models\n# Venue: SIGKDD Explorations\n# Authors: Yingqian Cui, Jie Ren, Yuping Lin, Han Xu, Pengfei He, Yue Xing, Wenqi Fan, Hui Liu, Jiliang Tang\n## Abstract\nText-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.\n## INTRODUCTION\nOne potential reason is that the watermark-generating procedure predominantly adheres to traditional watermark strategies, which are intended to trace the source of an image rather than protecting its IP in the context of diffusion models fine-tuning. Consequently, there's no assurance that the watermark's features will be learned by the model before other features. \n\nSecond, as indicated by Ma et al. (2023), due to the distribution shift between the fine-tuning data of generative models and the resulting generated images, it is crucial to incorporate images generated by fine-tuned models to develop the watermark detector. Since there are numerous fine-tuning methods introduced from different perspectives, a detector for one method might lose its effectiveness for others. This issue is highlighted by Ma et al. (2023) that a large drop in watermark detection performance is observed when applying a detector tailored to one fine-tuning method to others. Meanwhile, in reality, a data protector may not know which fine-tuning method was used by the infringer, thus it is desired to design a watermark detection strategy that is effective for various fine-tuning methods. \n\nTo tackle the aforementioned challenges, we propose a novel watermarking framework, FT-Shield, tailored for data's copyright protection against the Fine-Tuning of text-to-image diffusion models. In particular, we introduce a training objective incorporating the fine-tuning loss of diffusion models for watermark generation. By minimizing the objective, we ensure that the optimized watermark pattern can be quickly learned by diffusion model at the very early stage of fine-tuning. As shown in Figure 1, even when the style has not been adopted, our watermark has already been learned by the diffusion model. Furthermore, to obtain a watermark detector for various fine-tuning methods, we introduce a detection framework based on Mixture of Experts (Jacobs et al., 1991). The effectiveness of FT-Shield is verified through experiments across various fine-tuning approaches including DreamBooth (Ruiz et al., 2023), Textual Inversion (Gal et al., 2022), Text-to-Image Fine-Tuning (von Platen et al., 2022) and LoRA (Hu et al., 2021), applied to both style transfer and object transfer tasks across multiple datasets.\n\n## IMAGE PROTECTION METHODS\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement. \n\nAdversarial methods. Adversarial methods protect data's IP by applying the idea of evasion attacks. They treat the unauthorized generative models as the target of attack, and develop adversarial examples to disrupt the learning process of unauthorized fine-tuning. GLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios. Although these methods provide effective protection against infringement, they can inadvertently disrupt authorized uses (such as for academic research purposes) of the safeguarded images. This indicates the necessity of developing watermarking approaches, which allow the IP to be used for proper reasons while also acting as a way to collect proof against improper uses. \n\nWatermarking methods. Watermark has also been considered to protect the IP of images against unauthorized usage during the fine-tuning of text-to-image models. Wang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model.\n\n## PROBLEM FORMULATION\nIn the scenario of copyright infringement and protection considered in this work, there are two roles: \n\n(1) a data protector that possesses the data copyright, utilizes watermarking techniques before the data is released, and tries to detect if a suspected image is generated by a model fine-tuned on the protected images, and (2) a data offender that uses the protected data for text-to-image model finetuning without permission from the data protector. The data offenders have complete control over the fine-tuning and sampling processes of the text-to-image diffusion models, while the data protectors can only modify the data they own before their data is released and access images generated by the suspected model. \n\nAs shown in Figure 2, the protection process consists of two stages: the protection stage and the audit stage. In the protection stage, the data protector protects the images by adding imperceptible watermarks to the images. Specifically, given that the size of the protected dataset is N , the target is to generate sample-wise watermark \u03b4 i for each protected image x i , \u2200i = 1...N . Then these watermarks are embedded into the protected images xi = x i + \u03b4 i . Correspondingly, the data protector develops a watermark detection approach, denoted by function D w (\u2022), to test whether there is a watermark on the suspected image. To ensure that the watermarks will not lead to severe influence on image quality, we limit the budget of the watermark by constraining its l \u221e norm (\u2225\u03b4 i \u2225 \u221e \u2264 \u03b7) to control the pixel-wise difference between the two images x i and xi . In the audit stage, if the protectors encounter suspected images potentially produced through unauthorized text-to-image models finetuning, they will apply the watermark detection process D w (\u2022) to ascertain whether these images have infringed upon their data rights.",
            "reference_string": "[263622213 | Cui et al. | 2023 | Citations: 12]"
        },
        {
            "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2288619145",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "3482535",
                    "name": "Vikash Sehwag"
                },
                {
                    "authorId": "2253469617",
                    "name": "Yue Xing"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                }
            ],
            "abstract": "Generative models, especially text-to-image diffusion models, have significantly advanced in their ability to generate images, benefiting from enhanced architectures, increased computational power, and large-scale datasets. While the datasets play an important role, their protection has remained as an unsolved issue. Current protection strategies, such as watermarks and membership inference, are either in high poison rate which is detrimental to image quality or suffer from low accuracy and robustness. In this work, we introduce a novel approach, EnTruth, which Enhances Traceability of unauthorized dataset usage utilizing template memorization. By strategically incorporating the template memorization, EnTruth can trigger the specific behavior in unauthorized models as the evidence of infringement. Our method is the first to investigate the positive application of memorization and use it for copyright protection, which turns a curse into a blessing and offers a pioneering perspective for unauthorized usage detection in generative models. Comprehensive experiments are provided to demonstrate its effectiveness in terms of data-alteration rate, accuracy, robustness and generation quality.",
            "corpus_id": 270620522,
            "sentences": [
                {
                    "corpus_id": "270620522",
                    "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
                    "text": "Generative models, especially text-to-image diffusion models, have significantly advanced in their ability to generate images, benefiting from enhanced architectures, increased computational power, and large-scale datasets. While the datasets play an important role, their protection has remained as an unsolved issue. Current protection strategies, such as watermarks and membership inference, are either in high poison rate which is detrimental to image quality or suffer from low accuracy and robustness. In this work, we introduce a novel approach, EnTruth, which Enhances Traceability of unauthorized dataset usage utilizing template memorization. By strategically incorporating the template memorization, EnTruth can trigger the specific behavior in unauthorized models as the evidence of infringement. Our method is the first to investigate the positive application of memorization and use it for copyright protection, which turns a curse into a blessing and offers a pioneering perspective for unauthorized usage detection in generative models. Comprehensive experiments are provided to demonstrate its effectiveness in terms of data-alteration rate, accuracy, robustness and generation quality.",
                    "score": 0.5535161176886438,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                },
                {
                    "corpus_id": "270620522",
                    "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
                    "text": "The latest advancements in generative diffusion models (GDMs) [1,2,3], especially the text-to-image (T2I) models [4,5] which excel in creating high-quality images that closely align with the given textual prompts, have revolutionized the field of image generation.These advantages stem not only from the development of model architectures and computing power, but also from the availability of large-scale datasets [6,7,8].While datasets play an important role, their copyright protection has remained as an unsolved issue.The protection of these datasets' copyrights is paramount for multiple reasons.For instance, open-source datasets [9] are generally available only for educational and research purposes, barring any commercial use.Additionally, for commercial datasets, it is crucial for companies to secure them from theft and unauthorized sales.While pre-training and fine-tuning both raise concerns of copyright infringement, fine-tuning has a more severe impact on the copyright of datasets.Compared to pre-training, fine-tuning is highly efficient, allowing for many unauthorized uses without effective regulatory restrictions.\n\nObserving the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality Preprint.Under review.[13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16,17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2).",
                    "score": 0.48888052262073384,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 264
                        },
                        {
                            "start": 264,
                            "end": 423
                        },
                        {
                            "start": 423,
                            "end": 523
                        },
                        {
                            "start": 523,
                            "end": 602
                        },
                        {
                            "start": 602,
                            "end": 736
                        },
                        {
                            "start": 736,
                            "end": 852
                        },
                        {
                            "start": 852,
                            "end": 1000
                        },
                        {
                            "start": 1000,
                            "end": 1137
                        },
                        {
                            "start": 1139,
                            "end": 1365
                        },
                        {
                            "start": 1365,
                            "end": 1438
                        },
                        {
                            "start": 1438,
                            "end": 1579
                        },
                        {
                            "start": 1579,
                            "end": 1630
                        },
                        {
                            "start": 1630,
                            "end": 1643
                        },
                        {
                            "start": 1643,
                            "end": 1651
                        },
                        {
                            "start": 1651,
                            "end": 1788
                        },
                        {
                            "start": 1788,
                            "end": 1931
                        },
                        {
                            "start": 1931,
                            "end": 2082
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 116,
                            "end": 118,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 415,
                            "end": 418,
                            "matchedPaperCorpusId": "252917726"
                        },
                        {
                            "start": 418,
                            "end": 420,
                            "matchedPaperCorpusId": "51876975"
                        },
                        {
                            "start": 420,
                            "end": 422,
                            "matchedPaperCorpusId": "14113767"
                        },
                        {
                            "start": 637,
                            "end": 640,
                            "matchedPaperCorpusId": "57246310"
                        },
                        {
                            "start": 1247,
                            "end": 1250,
                            "matchedPaperCorpusId": "256503774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.98828125,
            "relevance_judgment_input_expanded": "# Title: EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations\n# Venue: arXiv.org\n# Authors: Jie Ren, Yingqian Cui, Chen Chen, Vikash Sehwag, Yue Xing, Jiliang Tang, Lingjuan Lyu\n## Abstract\nGenerative models, especially text-to-image diffusion models, have significantly advanced in their ability to generate images, benefiting from enhanced architectures, increased computational power, and large-scale datasets. While the datasets play an important role, their protection has remained as an unsolved issue. Current protection strategies, such as watermarks and membership inference, are either in high poison rate which is detrimental to image quality or suffer from low accuracy and robustness. In this work, we introduce a novel approach, EnTruth, which Enhances Traceability of unauthorized dataset usage utilizing template memorization. By strategically incorporating the template memorization, EnTruth can trigger the specific behavior in unauthorized models as the evidence of infringement. Our method is the first to investigate the positive application of memorization and use it for copyright protection, which turns a curse into a blessing and offers a pioneering perspective for unauthorized usage detection in generative models. Comprehensive experiments are provided to demonstrate its effectiveness in terms of data-alteration rate, accuracy, robustness and generation quality.\n## Introduction\nThe latest advancements in generative diffusion models (GDMs) [1,2,3], especially the text-to-image (T2I) models [4,5] which excel in creating high-quality images that closely align with the given textual prompts, have revolutionized the field of image generation.These advantages stem not only from the development of model architectures and computing power, but also from the availability of large-scale datasets [6,7,8].While datasets play an important role, their copyright protection has remained as an unsolved issue.The protection of these datasets' copyrights is paramount for multiple reasons.For instance, open-source datasets [9] are generally available only for educational and research purposes, barring any commercial use.Additionally, for commercial datasets, it is crucial for companies to secure them from theft and unauthorized sales.While pre-training and fine-tuning both raise concerns of copyright infringement, fine-tuning has a more severe impact on the copyright of datasets.Compared to pre-training, fine-tuning is highly efficient, allowing for many unauthorized uses without effective regulatory restrictions.\n\nObserving the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality Preprint.Under review.[13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16,17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2).",
            "reference_string": "[270620522 | Ren et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 284,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108977186",
                    "name": "Wenhao Wang"
                },
                {
                    "authorId": "2267731577",
                    "name": "Yifan Sun"
                },
                {
                    "authorId": "15556978",
                    "name": "Zongxin Yang"
                },
                {
                    "authorId": "2125634511",
                    "name": "Zhengdong Hu"
                },
                {
                    "authorId": "2296990692",
                    "name": "Zhentao Tan"
                },
                {
                    "authorId": "2297815073",
                    "name": "Yi Yang"
                }
            ],
            "abstract": "Visual diffusion models have revolutionized the field of creative AI, producing high-quality and diverse content. However, they inevitably memorize training images or videos, subsequently replicating their concepts, content, or styles during inference. This phenomenon raises significant concerns about privacy, security, and copyright within generated outputs. In this survey, we provide the first comprehensive review of replication in visual diffusion models, marking a novel contribution to the field by systematically categorizing the existing studies into unveiling, understanding, and mitigating this phenomenon. Specifically, unveiling mainly refers to the methods used to detect replication instances. Understanding involves analyzing the underlying mechanisms and factors that contribute to this phenomenon. Mitigation focuses on developing strategies to reduce or eliminate replication. Beyond these aspects, we also review papers focusing on its real-world influence. For instance, in the context of healthcare, replication is critically worrying due to privacy concerns related to patient data. Finally, the paper concludes with a discussion of the ongoing challenges, such as the difficulty in detecting and benchmarking replication, and outlines future directions including the development of more robust mitigation techniques. By synthesizing insights from diverse studies, this paper aims to equip researchers and practitioners with a deeper understanding at the intersection between AI technology and social good. We release this project at https://github.com/WangWenhao0716/Awesome-Diffusion-Replication.",
            "corpus_id": 271600759,
            "sentences": [
                {
                    "corpus_id": "271600759",
                    "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
                    "text": "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks, as shown in Fig. 3 (d). Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft.",
                    "score": 0.6241298444469792,
                    "section_title": "Watermarking",
                    "char_start_offset": 16541,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1188
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                }
            ],
            "relevance_judgement": 0.98779296875,
            "relevance_judgment_input_expanded": "# Title: Replication in Visual Diffusion Models: A Survey and Outlook\n# Venue: arXiv.org\n# Authors: Wenhao Wang, Yifan Sun, Zongxin Yang, Zhengdong Hu, Zhentao Tan, Yi Yang\n## Abstract\nVisual diffusion models have revolutionized the field of creative AI, producing high-quality and diverse content. However, they inevitably memorize training images or videos, subsequently replicating their concepts, content, or styles during inference. This phenomenon raises significant concerns about privacy, security, and copyright within generated outputs. In this survey, we provide the first comprehensive review of replication in visual diffusion models, marking a novel contribution to the field by systematically categorizing the existing studies into unveiling, understanding, and mitigating this phenomenon. Specifically, unveiling mainly refers to the methods used to detect replication instances. Understanding involves analyzing the underlying mechanisms and factors that contribute to this phenomenon. Mitigation focuses on developing strategies to reduce or eliminate replication. Beyond these aspects, we also review papers focusing on its real-world influence. For instance, in the context of healthcare, replication is critically worrying due to privacy concerns related to patient data. Finally, the paper concludes with a discussion of the ongoing challenges, such as the difficulty in detecting and benchmarking replication, and outlines future directions including the development of more robust mitigation techniques. By synthesizing insights from diverse studies, this paper aims to equip researchers and practitioners with a deeper understanding at the intersection between AI technology and social good. We release this project at https://github.com/WangWenhao0716/Awesome-Diffusion-Replication.\n## Watermarking\nBy embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks, as shown in Fig. 3 (d). Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft.",
            "reference_string": "[271600759 | Wang et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351229051",
                    "name": "Soham Roy"
                },
                {
                    "authorId": "2351593773",
                    "name": "Abhishek Mishra"
                },
                {
                    "authorId": "40151143",
                    "name": "S. Karande"
                },
                {
                    "authorId": "2316561345",
                    "name": "Murari Mandal"
                }
            ],
            "abstract": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
            "corpus_id": 277151077,
            "sentences": [
                {
                    "corpus_id": "277151077",
                    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
                    "text": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
                    "score": 0.46986159341992184,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9873046875
                }
            ],
            "relevance_judgement": 0.9873046875,
            "relevance_judgment_input_expanded": "# Title: Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation\n# Venue: arXiv.org\n# Authors: Soham Roy, Abhishek Mishra, S. Karande, Murari Mandal\n## Abstract\nModern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog\n",
            "reference_string": "[277151077 | Roy et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 76,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.01528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333463963",
                    "name": "Zhixiang Guo"
                },
                {
                    "authorId": "2325884825",
                    "name": "Siyuan Liang"
                },
                {
                    "authorId": "2257572247",
                    "name": "Aishan Liu"
                },
                {
                    "authorId": "2237906923",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.",
            "corpus_id": 274436785,
            "sentences": [
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "This paper proposes CopyrightShield, a method designed to detect poisoning data and defend against copyright infringement backdoor attack. Inspired by the replication phenomenon observed in diffusion models, we identified not only spatial similarities in replication but also a strong correlation between features and prompts. Leveraging these insights, we devised a spatially-guided method for detecting poisoned samples and developed a more effective poisoning score algorithm. Additionally, we implemented backdoor defenses by introducing protective constraints into the loss function, effectively preventing the generation of poisoned samples and reducing the association between triggers and images. As the first backdoor defense approach specifically targeting copyright infringement, it has been shown through experiments to be effective in identifying and mitigating backdoor samples in specific attack scenarios, thereby significantly reducing the harm caused by such attacks. Additionally, the concept of infringement feature inversion is introduced, which broadens the scope of defense and clarifies the allocation of responsibility for infringement. We aim to raise awareness of copyright infringement and to develop more generalizable infringement defense methods in the future. \n\nLimitations: Despite achieving effective defense, further research is needed to address certain limitations: \n\n(1) the method's effectiveness when applied to real-world copyright-protected images, as opposed to virtually designed datasets; (2) the performance of CopyrightShield in the face of newly developed copyright infringement backdoor attacks.",
                    "score": 0.5143556798052055,
                    "section_title": "Conclusions and Future Work",
                    "char_start_offset": 27938,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1291
                        },
                        {
                            "start": 1294,
                            "end": 1402
                        },
                        {
                            "start": 1405,
                            "end": 1644
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                },
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "Diffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.",
                    "score": 0.6430386716370833,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 618
                        },
                        {
                            "start": 621,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 999
                        },
                        {
                            "start": 1002,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1613
                        },
                        {
                            "start": 1616,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2024
                        },
                        {
                            "start": 2027,
                            "end": 2183
                        },
                        {
                            "start": 2184,
                            "end": 2375
                        },
                        {
                            "start": 2376,
                            "end": 2591
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 198,
                            "matchedPaperCorpusId": "244714856"
                        },
                        {
                            "start": 198,
                            "end": 201,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 201,
                            "end": 204,
                            "matchedPaperCorpusId": "243938678"
                        },
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "260900064"
                        },
                        {
                            "start": 207,
                            "end": 210,
                            "matchedPaperCorpusId": "257427673"
                        },
                        {
                            "start": 995,
                            "end": 998,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1086,
                            "end": 1090,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1090,
                            "end": 1093,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1849,
                            "end": 1852,
                            "matchedPaperCorpusId": "257050406"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                },
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.",
                    "score": 0.5834161436857749,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97998046875
                },
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "Defense Scenario: We conducted a study on the application of stable diffusion with a dataset that lack of copyright protections. In this particular scenario, attackers introduce poisoned samples into the model, leading to the generation of copyrighted artworks when the model is utilized by users and triggered by specific inputs. The primary goal of the defense strategy is to accurately identify and filter out these samples, subsequently fine-tuning a secure model to prevent any triggering. Additionally, we propose the concept of copyright feature inversion, which aids in the allocation of copyright responsibility. Dataset and Model: In order to implement the defense scenario described, we utilized the Pokemon BLIP Captions dataset produced by Pinkney [47], to simulate the defensive context. This choice was motivated by the fact that the Pokemon dataset provides a compelling example for the identification and understanding of copyright infringement, and recent copyright infringement cases [60] related to Pokemon have garnered significant public attention, offering a relevant backdrop for our defense strategy. Moreover, this dataset aligns well with the requirements of stable diffusion for text-to-image fine-tuning, enhancing the reproducibility of our approach. To ensure the robustness and reliability of our methods, we conducted 20 independent experiments for this scenario. In each experiment, one image is selected as the copyright-infringing image to generate the poisoned data, while the remaining 832 images are used as clean data. \n\nIn the experiments, GroundingDino [39] and SAM [18] were employed as the detection and segmentation models for the poisoned sample detection method. Given that the current backdoor attack method targeting copyright infringement is limited to SilentBadDiffusion [62], our defense strategy focused solely on detecting this method. Therefore, for stable diffusion, we adopted the v1.x version same as SilentBadDiffusion. \n\nEvaluation Metrics: For the assessment of copyright infringement, we employed SSCD as the evaluation metric. Regarding the detection of poisoned samples, we utilized Recall, Precision, and F1-Score as the evaluation metrics. In order to assess the defense, we employed the Copyright Infringement Rate (CIR) and First Attack Epoch (FAE) as measures.",
                    "score": 0.579321438008028,
                    "section_title": "Experimental Setup",
                    "char_start_offset": 18464,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1558
                        },
                        {
                            "start": 1561,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 1978
                        },
                        {
                            "start": 1981,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2205
                        },
                        {
                            "start": 2206,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1608,
                            "end": 1612,
                            "matchedPaperCorpusId": "257952310"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                },
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations. Conversely, when random spatial transformations were applied to the samples, the duplication diminished and did not exhibit any spatial similarity. This demonstrates the spatial similarity of the duplication phenomenon, with theoretical analysis. \n\nBased on the above analysis, we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) [51] and Self-Supervised Copy Detection (SSCD) [48] similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability. Experimental results demonstrate that this approach achieves a defense effectiveness of CopyrightShield on the Pokemon dataset. Our contributions are: \n\n\u2022 We investigated the correlation between the replication capabilities of diffusion models and prompts, and experimentally validated the spatial similarity inherent in the replication of diffusion models. \u2022 Based on our analysis, we proposed for the first time a feature-guided poisoning data detection method based on the spatial similarity of replication phenomena. \n\n\u2022 Based on the detected poisoned samples, we designed a defense method against copyright infringement backdoor attacks using protective constraints and introduced, for the first time, the concept of infringement feature inversion. This assists in determining infringement liability and expands the application scenarios of the defense strategy.",
                    "score": 0.43981064231180705,
                    "section_title": "Introduction",
                    "char_start_offset": 2391,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 462
                        },
                        {
                            "start": 465,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 729
                        },
                        {
                            "start": 730,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1807
                        },
                        {
                            "start": 1810,
                            "end": 2014
                        },
                        {
                            "start": 2015,
                            "end": 2177
                        },
                        {
                            "start": 2180,
                            "end": 2410
                        },
                        {
                            "start": 2411,
                            "end": 2524
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 907,
                            "end": 911,
                            "matchedPaperCorpusId": "67855581"
                        },
                        {
                            "start": 954,
                            "end": 958,
                            "matchedPaperCorpusId": "247011159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                },
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "We assume that an attacker has trained a backdoor model for copyright infringement, which, when given a specific text input, will trigger the backdoor to generate infringing images. When a copyright attack is triggered, there exists a trigger text composed of multiple prompts t poison : {p 1 , p 2 , . . . , p n } and an infringing image I poison := DM p (t poison ), which DM p denotes as the poisoned diffusion model. Defenders need to design an efficient infringement detection system, develop strategies to mitigate the impact, and establish a comprehensive method for providing effective evidence for responsibility attribution. Therefore, defenders need to have access to the poisoned dataset d p . During an attack, in order for the poisoned samples to remain sufficiently covert, the similarity of the clean data and poisoned data should follow sim(d c , d p ) < \u03c4 . The goal of the defenders is to design a capable detector D which can distinguish between d c andd p , as depicted in Eq. ( 1):: \n\nwhich \u03b3 denotes the detection threshold. If the probability density distributions of the model DM p under clean and poisoned inputs are P d and P c , respectively, then it is necessary to optimize Eq. ( 2): \n\nIn the defense process, it is necessary to optimize the diffusion model in order to suppress the generation of poisoned samples without compromising its normal generative performance, which can be depicted as sim(DM \n\nBased on the aforementioned considerations, our detection and defense methods will be designed according to these conditions.",
                    "score": 0.4702617766130641,
                    "section_title": "Threat Model",
                    "char_start_offset": 9169,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1004
                        },
                        {
                            "start": 1007,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1213
                        },
                        {
                            "start": 1216,
                            "end": 1431
                        },
                        {
                            "start": 1434,
                            "end": 1559
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.98583984375,
            "relevance_judgment_input_expanded": "# Title: CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models\n# Venue: arXiv.org\n# Authors: Zhixiang Guo, Siyuan Liang, Aishan Liu, Dacheng Tao\n## Abstract\nThe diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.\n## Introduction\nDiffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.\n...\nThrough experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations. Conversely, when random spatial transformations were applied to the samples, the duplication diminished and did not exhibit any spatial similarity. This demonstrates the spatial similarity of the duplication phenomenon, with theoretical analysis. \n\nBased on the above analysis, we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) [51] and Self-Supervised Copy Detection (SSCD) [48] similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability. Experimental results demonstrate that this approach achieves a defense effectiveness of CopyrightShield on the Pokemon dataset. Our contributions are: \n\n\u2022 We investigated the correlation between the replication capabilities of diffusion models and prompts, and experimentally validated the spatial similarity inherent in the replication of diffusion models. \u2022 Based on our analysis, we proposed for the first time a feature-guided poisoning data detection method based on the spatial similarity of replication phenomena. \n\n\u2022 Based on the detected poisoned samples, we designed a defense method against copyright infringement backdoor attacks using protective constraints and introduced, for the first time, the concept of infringement feature inversion. This assists in determining infringement liability and expands the application scenarios of the defense strategy.\n\n## Threat Model\nWe assume that an attacker has trained a backdoor model for copyright infringement, which, when given a specific text input, will trigger the backdoor to generate infringing images. When a copyright attack is triggered, there exists a trigger text composed of multiple prompts t poison : {p 1 , p 2 , . . . , p n } and an infringing image I poison := DM p (t poison ), which DM p denotes as the poisoned diffusion model. Defenders need to design an efficient infringement detection system, develop strategies to mitigate the impact, and establish a comprehensive method for providing effective evidence for responsibility attribution. Therefore, defenders need to have access to the poisoned dataset d p . During an attack, in order for the poisoned samples to remain sufficiently covert, the similarity of the clean data and poisoned data should follow sim(d c , d p ) < \u03c4 . The goal of the defenders is to design a capable detector D which can distinguish between d c andd p , as depicted in Eq. ( 1):: \n\nwhich \u03b3 denotes the detection threshold. If the probability density distributions of the model DM p under clean and poisoned inputs are P d and P c , respectively, then it is necessary to optimize Eq. ( 2): \n\nIn the defense process, it is necessary to optimize the diffusion model in order to suppress the generation of poisoned samples without compromising its normal generative performance, which can be depicted as sim(DM \n\nBased on the aforementioned considerations, our detection and defense methods will be designed according to these conditions.\n\n## Experimental Setup\nDefense Scenario: We conducted a study on the application of stable diffusion with a dataset that lack of copyright protections. In this particular scenario, attackers introduce poisoned samples into the model, leading to the generation of copyrighted artworks when the model is utilized by users and triggered by specific inputs. The primary goal of the defense strategy is to accurately identify and filter out these samples, subsequently fine-tuning a secure model to prevent any triggering. Additionally, we propose the concept of copyright feature inversion, which aids in the allocation of copyright responsibility. Dataset and Model: In order to implement the defense scenario described, we utilized the Pokemon BLIP Captions dataset produced by Pinkney [47], to simulate the defensive context. This choice was motivated by the fact that the Pokemon dataset provides a compelling example for the identification and understanding of copyright infringement, and recent copyright infringement cases [60] related to Pokemon have garnered significant public attention, offering a relevant backdrop for our defense strategy. Moreover, this dataset aligns well with the requirements of stable diffusion for text-to-image fine-tuning, enhancing the reproducibility of our approach. To ensure the robustness and reliability of our methods, we conducted 20 independent experiments for this scenario. In each experiment, one image is selected as the copyright-infringing image to generate the poisoned data, while the remaining 832 images are used as clean data. \n\nIn the experiments, GroundingDino [39] and SAM [18] were employed as the detection and segmentation models for the poisoned sample detection method. Given that the current backdoor attack method targeting copyright infringement is limited to SilentBadDiffusion [62], our defense strategy focused solely on detecting this method. Therefore, for stable diffusion, we adopted the v1.x version same as SilentBadDiffusion. \n\nEvaluation Metrics: For the assessment of copyright infringement, we employed SSCD as the evaluation metric. Regarding the detection of poisoned samples, we utilized Recall, Precision, and F1-Score as the evaluation metrics. In order to assess the defense, we employed the Copyright Infringement Rate (CIR) and First Attack Epoch (FAE) as measures.\n\n## Conclusions and Future Work\nThis paper proposes CopyrightShield, a method designed to detect poisoning data and defend against copyright infringement backdoor attack. Inspired by the replication phenomenon observed in diffusion models, we identified not only spatial similarities in replication but also a strong correlation between features and prompts. Leveraging these insights, we devised a spatially-guided method for detecting poisoned samples and developed a more effective poisoning score algorithm. Additionally, we implemented backdoor defenses by introducing protective constraints into the loss function, effectively preventing the generation of poisoned samples and reducing the association between triggers and images. As the first backdoor defense approach specifically targeting copyright infringement, it has been shown through experiments to be effective in identifying and mitigating backdoor samples in specific attack scenarios, thereby significantly reducing the harm caused by such attacks. Additionally, the concept of infringement feature inversion is introduced, which broadens the scope of defense and clarifies the allocation of responsibility for infringement. We aim to raise awareness of copyright infringement and to develop more generalizable infringement defense methods in the future. \n\nLimitations: Despite achieving effective defense, further research is needed to address certain limitations: \n\n(1) the method's effectiveness when applied to real-world copyright-protected images, as opposed to virtually designed datasets; (2) the performance of CopyrightShield in the face of newly developed copyright infringement backdoor attacks.",
            "reference_string": "[274436785 | Guo et al. | 2024 | Citations: 3]"
        },
        {
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "venue": "The Web Conference",
            "year": 2025,
            "reference_count": 75,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2328027909",
                    "name": "Zhou Su"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2238129188",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "corpus_id": 277856857,
            "sentences": [
                {
                    "corpus_id": "277856857",
                    "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
                    "text": "In this section, we go into depth about the existing solutions, as the extension of that in Section 1. As diffusion models continue to evolve and gain popularity, users can now create a vast array of generative works at a low cost, which leads to the negative effects of the replication becoming more acute [59]. Especially the artist community is concerned about the copyright infringement of their work [7,40,53]. Recently, researchers have proposed a lot of countermeasures to solve this issue [13]. \n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects. \n\nHowever, the goal of adversarial perturbation is to disrupt the learning process of diffusion models, which is orthogonal to the copyright auditing focus of this paper. Moreover, adversarial perturbation essentially blocks any legitimate use of subject-driven synthesis based on protected images. Watermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector.",
                    "score": 0.445554902678431,
                    "section_title": "H Related Work",
                    "char_start_offset": 44894,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 502
                        },
                        {
                            "start": 505,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 919
                        },
                        {
                            "start": 920,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1431
                        },
                        {
                            "start": 1434,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2040
                        },
                        {
                            "start": 2041,
                            "end": 2267
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 307,
                            "end": 311,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 714,
                            "end": 718,
                            "matchedPaperCorpusId": "256662278"
                        },
                        {
                            "start": 947,
                            "end": 951,
                            "matchedPaperCorpusId": "257766375"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98486328125
                },
                {
                    "corpus_id": "277856857",
                    "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
                    "text": "The existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67]. \n\nHowever, previous studies face several limitations. First, both the perturbation-based and the watermark-based methods need to manipulate the original images, i.e., injecting perturbation or watermark, thus compromising data fidelity. The perturbation may also diminish the model's generation quality. Second, perturbationbased and watermark-based strategies require retraining the model to be effective. Thus, they may not suit the model already posted online. For the MI methods, the existing approaches [15,17,24,29,41,44] for diffusion models usually require the access to structure or weights of the model, which limits their applicability in blackbox auditing scenarios. Although some MI strategies target the black-box settings [12,14,26,43,67,73], they are not well suited to our auditing task. We will go depth in Section 4.4 and compare them with ArtistAuditor in Section 5. \n\nOur Proposal. In this paper, we propose a novel artwork copyright auditing method for the text-to-image models, called ArtistAuditor, which can identify data-use infringement without sacrificing the artwork's fidelity. We are inspired by the fact that artworks within an artist's style share some commonality in latent space.",
                    "score": 0.44556097248925075,
                    "section_title": "Introduction",
                    "char_start_offset": 1855,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 809
                        },
                        {
                            "start": 812,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1696
                        },
                        {
                            "start": 1699,
                            "end": 1712
                        },
                        {
                            "start": 1713,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2024
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 133,
                            "end": 136,
                            "matchedPaperCorpusId": "256662278"
                        },
                        {
                            "start": 136,
                            "end": 139,
                            "matchedPaperCorpusId": "257766375"
                        },
                        {
                            "start": 179,
                            "end": 182,
                            "matchedPaperCorpusId": "272003879"
                        },
                        {
                            "start": 185,
                            "end": 188,
                            "matchedPaperCorpusId": "264474160"
                        },
                        {
                            "start": 677,
                            "end": 680,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 680,
                            "end": 682,
                            "matchedPaperCorpusId": "257984935"
                        },
                        {
                            "start": 682,
                            "end": 684,
                            "matchedPaperCorpusId": "264439566"
                        },
                        {
                            "start": 684,
                            "end": 687,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 802,
                            "end": 805,
                            "matchedPaperCorpusId": "266191072"
                        },
                        {
                            "start": 805,
                            "end": 808,
                            "matchedPaperCorpusId": "271325362"
                        },
                        {
                            "start": 1322,
                            "end": 1325,
                            "matchedPaperCorpusId": "261241592"
                        },
                        {
                            "start": 1325,
                            "end": 1328,
                            "matchedPaperCorpusId": "266126263"
                        },
                        {
                            "start": 1331,
                            "end": 1334,
                            "matchedPaperCorpusId": "256627812"
                        },
                        {
                            "start": 1334,
                            "end": 1337,
                            "matchedPaperCorpusId": "260887617"
                        },
                        {
                            "start": 1547,
                            "end": 1551,
                            "matchedPaperCorpusId": "261557367"
                        },
                        {
                            "start": 1551,
                            "end": 1554,
                            "matchedPaperCorpusId": "268048573"
                        },
                        {
                            "start": 1557,
                            "end": 1560,
                            "matchedPaperCorpusId": "266191072"
                        },
                        {
                            "start": 1560,
                            "end": 1563,
                            "matchedPaperCorpusId": "271325362"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                },
                {
                    "corpus_id": "277856857",
                    "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
                    "text": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
                    "score": 0.4577926089780417,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.98486328125,
            "relevance_judgment_input_expanded": "# Title: ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models\n# Venue: The Web Conference\n# Authors: L. Du, Zheng Zhu, Min Chen, Zhou Su, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang\n## Abstract\nText-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.\n## Introduction\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67]. \n\nHowever, previous studies face several limitations. First, both the perturbation-based and the watermark-based methods need to manipulate the original images, i.e., injecting perturbation or watermark, thus compromising data fidelity. The perturbation may also diminish the model's generation quality. Second, perturbationbased and watermark-based strategies require retraining the model to be effective. Thus, they may not suit the model already posted online. For the MI methods, the existing approaches [15,17,24,29,41,44] for diffusion models usually require the access to structure or weights of the model, which limits their applicability in blackbox auditing scenarios. Although some MI strategies target the black-box settings [12,14,26,43,67,73], they are not well suited to our auditing task. We will go depth in Section 4.4 and compare them with ArtistAuditor in Section 5. \n\nOur Proposal. In this paper, we propose a novel artwork copyright auditing method for the text-to-image models, called ArtistAuditor, which can identify data-use infringement without sacrificing the artwork's fidelity. We are inspired by the fact that artworks within an artist's style share some commonality in latent space.\n\n## H Related Work\nIn this section, we go into depth about the existing solutions, as the extension of that in Section 1. As diffusion models continue to evolve and gain popularity, users can now create a vast array of generative works at a low cost, which leads to the negative effects of the replication becoming more acute [59]. Especially the artist community is concerned about the copyright infringement of their work [7,40,53]. Recently, researchers have proposed a lot of countermeasures to solve this issue [13]. \n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects. \n\nHowever, the goal of adversarial perturbation is to disrupt the learning process of diffusion models, which is orthogonal to the copyright auditing focus of this paper. Moreover, adversarial perturbation essentially blocks any legitimate use of subject-driven synthesis based on protected images. Watermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector.",
            "reference_string": "[277856857 | Du et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243376899",
                    "name": "Zhenguang Liu"
                },
                {
                    "authorId": "2243277462",
                    "name": "Chao Shuai"
                },
                {
                    "authorId": "2342623076",
                    "name": "Shaojing Fan"
                },
                {
                    "authorId": "2350529533",
                    "name": "Ziping Dong"
                },
                {
                    "authorId": "2350852841",
                    "name": "Jinwu Hu"
                },
                {
                    "authorId": "36890675",
                    "name": "Zhongjie Ba"
                },
                {
                    "authorId": "2286244627",
                    "name": "Kui Ren"
                }
            ],
            "abstract": "Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce \\emph{CoprGuard}, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1\\% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation.",
            "corpus_id": 277043466,
            "sentences": [
                {
                    "corpus_id": "277043466",
                    "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
                    "text": "Given the limitations of active protection, some studies [27,44,48] opted for passive detection for image copyright protection. They emphasize the traceability of protected images, explores detection schemes based on watermarks or backdoors, and focuses on the extraction of authentication marks from the inspected model as evidence when an image infringement incident occurs. Compared to adversarial methods, this type of method can be used to protect image copyright during the training or fine-tuning of any personalized diffusion model, and has less impact on image availability. Zhao et al. [48] used the pretrained watermark encoder [47] to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. [45] leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one. \n\nOur approach stands apart from previous methods by its theoretical foundation. Through detailed analysis, we discovered that diffusion models tend to preserve the frequency spectrum characteristics of their training data. Building on this insight, we introduce CoprGuard, a robust watermarking framework that is highly effective for both native and text-to-image diffusion models, even when watermarked images make up only a small portion of the training data.",
                    "score": 0.5741855654436199,
                    "section_title": "Passive Detection for Image Infringement",
                    "char_start_offset": 6779,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1660
                        },
                        {
                            "start": 1663,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1884
                        },
                        {
                            "start": 1885,
                            "end": 2123
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 639,
                            "end": 643,
                            "matchedPaperCorpusId": "238419552"
                        },
                        {
                            "start": 1432,
                            "end": 1436,
                            "matchedPaperCorpusId": "268513090"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98193359375
                },
                {
                    "corpus_id": "277043466",
                    "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
                    "text": "Infringement model detection performance. In this section, we study the effectiveness of diagnosis and compare it with two methods DIAGNOSIS [44] and Yu et al [47], which are capable of detecting unauthorized image usages in text-to-image models. We train a set of diffusion models, with or without watermarked images, using different random seeds. We then classify these models based on whether they incorporated unauthorized images during the training or fine-tuning processes. For each experiment, we train 20 pre-trained models for DDIM and Classifier-Free Guidance, and 50 pre-trained models for Stable Diffusion (SD). As shown in Tab. 2, our method successfully detects all infringement models (the watermark injection rate R varying from 0 to 100%), achieving an accuracy of 100% for both text-to-image and naive diffusion models with F P = 0 and F N = 0. While the infringer collects the training or fine-tuning data from multiple sources, and the watermark injection proportion of the training dataset is 25%, the detection accuracy for Yu et al. is only 50.0%. Although DIAGNOSIS achieves 100% detection accuracy at watermark injection rates of R = 25% and R = 100%, the watermark detection ratios are P u = 91.2% for R = 100% and P u = 5.1% for R = 0 [44]. In contrast, our method achieves watermark detection ratios P u = 100% for R = 100% and P u = 0 for R = 0. \n\nGenerally, a higher watermark detection ratio P u , indicates greater credibility of detection results, but P u tends to decrease as the watermark injection ratio R diminishes. Tab. 3 reports the watermark detection ratio P u of Copr-Guard and DIAGNOSIS, with R varying from 0 to 100.0%. When R = 0, DIAGNOSIS still detects a small number of watermarked images from generated outputs (P u = 6.0%, P u = 3.8%), which may result in misjudgment during infringement model detection, particularly when the watermark injection ratio in the training set is low(i.e., R < 10%).",
                    "score": 0.4748548050930662,
                    "section_title": "Evaluations",
                    "char_start_offset": 20303,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 41
                        },
                        {
                            "start": 42,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1374
                        },
                        {
                            "start": 1377,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1946
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 159,
                            "end": 163,
                            "matchedPaperCorpusId": "238419552"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                },
                {
                    "corpus_id": "277043466",
                    "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
                    "text": "The CoprGuard framework is illustrated in Fig. 4, and the implementation details are as follows. The protector employs a frequency domain watermark encoder to embed the watermark W into all images before releasing the private dataset. Then if infringers train or fine-tune diffusion models (including naive and text-to-image diffusion models) with these watermarked images, the watermark W will be embedded into models as well. When suspicious models are released, the protector generates images from these inspected models in a black-box manner and extracts the watermark W \u2032 from inspected images using the pretrained watermark decoder. Finally, the protector compares the watermark W and W \u2032 to determine whether the model uses unauthorized images and violates image copyright.",
                    "score": 0.4641304304655277,
                    "section_title": "CoprGuard Framework Design",
                    "char_start_offset": 14152,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 780
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                },
                {
                    "corpus_id": "277043466",
                    "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
                    "text": "Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce \\emph{CoprGuard}, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1\\% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation.",
                    "score": 0.46981734128391345,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.98193359375,
            "relevance_judgment_input_expanded": "# Title: Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models\n# Venue: arXiv.org\n# Authors: Zhenguang Liu, Chao Shuai, Shaojing Fan, Ziping Dong, Jinwu Hu, Zhongjie Ba, Kui Ren\n## Abstract\nDiffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce \\emph{CoprGuard}, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1\\% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation.\n## Passive Detection for Image Infringement\nGiven the limitations of active protection, some studies [27,44,48] opted for passive detection for image copyright protection. They emphasize the traceability of protected images, explores detection schemes based on watermarks or backdoors, and focuses on the extraction of authentication marks from the inspected model as evidence when an image infringement incident occurs. Compared to adversarial methods, this type of method can be used to protect image copyright during the training or fine-tuning of any personalized diffusion model, and has less impact on image availability. Zhao et al. [48] used the pretrained watermark encoder [47] to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. [45] leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one. \n\nOur approach stands apart from previous methods by its theoretical foundation. Through detailed analysis, we discovered that diffusion models tend to preserve the frequency spectrum characteristics of their training data. Building on this insight, we introduce CoprGuard, a robust watermarking framework that is highly effective for both native and text-to-image diffusion models, even when watermarked images make up only a small portion of the training data.\n\n## CoprGuard Framework Design\nThe CoprGuard framework is illustrated in Fig. 4, and the implementation details are as follows. The protector employs a frequency domain watermark encoder to embed the watermark W into all images before releasing the private dataset. Then if infringers train or fine-tune diffusion models (including naive and text-to-image diffusion models) with these watermarked images, the watermark W will be embedded into models as well. When suspicious models are released, the protector generates images from these inspected models in a black-box manner and extracts the watermark W \u2032 from inspected images using the pretrained watermark decoder. Finally, the protector compares the watermark W and W \u2032 to determine whether the model uses unauthorized images and violates image copyright.\n\n## Evaluations\nInfringement model detection performance. In this section, we study the effectiveness of diagnosis and compare it with two methods DIAGNOSIS [44] and Yu et al [47], which are capable of detecting unauthorized image usages in text-to-image models. We train a set of diffusion models, with or without watermarked images, using different random seeds. We then classify these models based on whether they incorporated unauthorized images during the training or fine-tuning processes. For each experiment, we train 20 pre-trained models for DDIM and Classifier-Free Guidance, and 50 pre-trained models for Stable Diffusion (SD). As shown in Tab. 2, our method successfully detects all infringement models (the watermark injection rate R varying from 0 to 100%), achieving an accuracy of 100% for both text-to-image and naive diffusion models with F P = 0 and F N = 0. While the infringer collects the training or fine-tuning data from multiple sources, and the watermark injection proportion of the training dataset is 25%, the detection accuracy for Yu et al. is only 50.0%. Although DIAGNOSIS achieves 100% detection accuracy at watermark injection rates of R = 25% and R = 100%, the watermark detection ratios are P u = 91.2% for R = 100% and P u = 5.1% for R = 0 [44]. In contrast, our method achieves watermark detection ratios P u = 100% for R = 100% and P u = 0 for R = 0. \n\nGenerally, a higher watermark detection ratio P u , indicates greater credibility of detection results, but P u tends to decrease as the watermark injection ratio R diminishes. Tab. 3 reports the watermark detection ratio P u of Copr-Guard and DIAGNOSIS, with R varying from 0 to 100.0%. When R = 0, DIAGNOSIS still detects a small number of watermarked images from generated outputs (P u = 6.0%, P u = 3.8%), which may result in misjudgment during infringement model detection, particularly when the watermark injection ratio in the training set is low(i.e., R < 10%).",
            "reference_string": "[277043466 | Liu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Dynamic watermarks in images generated by diffusion models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323562832",
                    "name": "Yunzhuo Chen"
                },
                {
                    "authorId": "47398812",
                    "name": "Naveed Akhtar"
                },
                {
                    "authorId": "39306904",
                    "name": "Nur Al Hasan Haldar"
                },
                {
                    "authorId": "2247160161",
                    "name": "Ajmal Mian"
                }
            ],
            "abstract": "High-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification. o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.",
            "corpus_id": 276317530,
            "sentences": [
                {
                    "corpus_id": "276317530",
                    "title": "Dynamic watermarks in images generated by diffusion models",
                    "text": "High-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification. o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.",
                    "score": 0.514531422988466,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: Dynamic watermarks in images generated by diffusion models\n# Venue: arXiv.org\n# Authors: Yunzhuo Chen, Naveed Akhtar, Nur Al Hasan Haldar, Ajmal Mian\n## Abstract\nHigh-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification. o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.\n",
            "reference_string": "[276317530 | Chen et al. | 2025 | Citations: 0]"
        },
        {
            "title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336921256",
                    "name": "Zhuomeng Zhang"
                },
                {
                    "authorId": "2262536259",
                    "name": "Fangqi Li"
                },
                {
                    "authorId": "2336871113",
                    "name": "Chong Di"
                },
                {
                    "authorId": "2283446293",
                    "name": "Shilin Wang"
                }
            ],
            "abstract": "Despite the impressive synthesis quality of text-to-image (T2I) diffusion models, their black-box deployment poses significant regulatory challenges: Malicious actors can fine-tune these models to generate illegal content, circumventing existing safeguards through parameter manipulation. Therefore, it is essential to verify the integrity of T2I diffusion models. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we discern model tampering via the KL divergence between the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton (PromptLA) for efficient and accurate verification. Evaluations on four advanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a mean AUC of over 0.96 in integrity detection, exceeding baselines by more than 0.2, showcasing strong effectiveness and generalization. Additionally, our approach achieves lower cost and is robust against image-level post-processing. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which establishes quantifiable standards for AI copyright litigation in practice.",
            "corpus_id": 274982437,
            "sentences": [
                {
                    "corpus_id": "274982437",
                    "title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models",
                    "text": "Despite the impressive synthesis quality of text-to-image (T2I) diffusion models, their black-box deployment poses significant regulatory challenges: Malicious actors can fine-tune these models to generate illegal content, circumventing existing safeguards through parameter manipulation. Therefore, it is essential to verify the integrity of T2I diffusion models. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we discern model tampering via the KL divergence between the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton (PromptLA) for efficient and accurate verification. Evaluations on four advanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a mean AUC of over 0.96 in integrity detection, exceeding baselines by more than 0.2, showcasing strong effectiveness and generalization. Additionally, our approach achieves lower cost and is robust against image-level post-processing. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which establishes quantifiable standards for AI copyright litigation in practice.",
                    "score": 0.44293149211925764,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: Zhuomeng Zhang, Fangqi Li, Chong Di, Shilin Wang\n## Abstract\nDespite the impressive synthesis quality of text-to-image (T2I) diffusion models, their black-box deployment poses significant regulatory challenges: Malicious actors can fine-tune these models to generate illegal content, circumventing existing safeguards through parameter manipulation. Therefore, it is essential to verify the integrity of T2I diffusion models. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we discern model tampering via the KL divergence between the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton (PromptLA) for efficient and accurate verification. Evaluations on four advanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a mean AUC of over 0.96 in integrity detection, exceeding baselines by more than 0.2, showcasing strong effectiveness and generalization. Additionally, our approach achieves lower cost and is robust against image-level post-processing. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which establishes quantifiable standards for AI copyright litigation in practice.\n",
            "reference_string": "[274982437 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2403.11162",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108069960",
                    "name": "Xiaoyu Wu"
                },
                {
                    "authorId": "2147311278",
                    "name": "Yang Hua"
                },
                {
                    "authorId": "2186858424",
                    "name": "Chumeng Liang"
                },
                {
                    "authorId": "2118001291",
                    "name": "Jiaru Zhang"
                },
                {
                    "authorId": "2144220882",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2055312951",
                    "name": "Tao Song"
                },
                {
                    "authorId": "2292035375",
                    "name": "Haibing Guan"
                }
            ],
            "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
            "corpus_id": 268513090,
            "sentences": [
                {
                    "corpus_id": "268513090",
                    "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
                    "text": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
                    "score": 0.46555656612894114,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Xiaoyu Wu, Yang Hua, Chumeng Liang, Jiaru Zhang, Hao Wang, Tao Song, Haibing Guan\n## Abstract\nDiffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.\n",
            "reference_string": "[268513090 | Wu et al. | 2024 | Citations: 6]"
        },
        {
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "venue": "",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267877984",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2267728071",
                    "name": "Teoh Tze Tzun"
                },
                {
                    "authorId": "2267727392",
                    "name": "Lim Wei Hern"
                },
                {
                    "authorId": "2267866973",
                    "name": "Haonan Wang"
                },
                {
                    "authorId": "2256995496",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
            "corpus_id": 265352103,
            "sentences": [
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "In addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content. An illustration of the entire copyright test is given in Figure 5. Our test requires copyright content as input. We discuss how we gather real images with copyrighted content in the subsequent section.",
                    "score": 0.5509542102565638,
                    "section_title": "Copyright Test for Substantial Similarities",
                    "char_start_offset": 16531,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1152
                        },
                        {
                            "start": 1155,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1432
                        },
                        {
                            "start": 1433,
                            "end": 1499
                        },
                        {
                            "start": 1500,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1634
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: On Copyright Risks of Text-to-Image Diffusion Models\n# Venue: \n# Authors: Yang Zhang, Teoh Tze Tzun, Lim Wei Hern, Haonan Wang, Kenji Kawaguchi\n## Abstract\nDiffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.\n## Copyright Test for Substantial Similarities\nIn addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content. An illustration of the entire copyright test is given in Figure 5. Our test requires copyright content as input. We discuss how we gather real images with copyrighted content in the subsequent section.",
            "reference_string": "[265352103 | Zhang et al. | 2023 | Citations: 10]"
        },
        {
            "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models",
            "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14722/aiscc.2024.23009?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14722/aiscc.2024.23009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2138800088",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "corpus_id": 268048573,
            "sentences": [],
            "relevance_judgement": 0.9697265625,
            "relevance_judgment_input_expanded": "# Title: WIP: Auditing Artist Style Pirate in Text-to-image Generation Models\n# Venue: Proceedings 2024 Workshop on AI Systems with Confidential COmputing\n# Authors: L. Du, Zheng Zhu, Min Chen, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang\n## Abstract\n\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.\n",
            "reference_string": "[268048573 | Du et al. | 2024 | Citations: 3]"
        },
        {
            "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112769493",
                    "name": "Kai Huang"
                },
                {
                    "authorId": "2274008610",
                    "name": "Wei Gao"
                }
            ],
            "abstract": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations. The source code is available at: https://github.com/pittisl/FreezeAsGuard.",
            "corpus_id": 270067889,
            "sentences": [
                {
                    "corpus_id": "270067889",
                    "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
                    "text": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations. The source code is available at: https://github.com/pittisl/FreezeAsGuard.",
                    "score": 0.4719645485345101,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing\n# Venue: arXiv.org\n# Authors: Kai Huang, Wei Gao\n## Abstract\nText-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations. The source code is available at: https://github.com/pittisl/FreezeAsGuard.\n",
            "reference_string": "[270067889 | Huang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056101029",
                    "name": "Ge Luo"
                },
                {
                    "authorId": "2268330053",
                    "name": "Junqiang Huang"
                },
                {
                    "authorId": "2260829743",
                    "name": "Manman Zhang"
                },
                {
                    "authorId": "3243117",
                    "name": "Zhenxing Qian"
                },
                {
                    "authorId": "2153698673",
                    "name": "Sheng Li"
                },
                {
                    "authorId": "2238534596",
                    "name": "Xinpeng Zhang"
                }
            ],
            "abstract": "The advancement in text-to-image models has led to astonishing artistic performances. However, several studios and websites illegally fine-tune these models using artists' artworks to mimic their styles for profit, which violates the copyrights of artists and diminishes their motivation to produce original works. Currently, there is a notable lack of research focusing on this issue. In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed. In various fine-tune scenarios and against watermark attack methods, our research confirms that analyzing the distribution of watermarks in artificially generated images reliably detects unauthorized mimicry.",
            "corpus_id": 265445146,
            "sentences": [
                {
                    "corpus_id": "265445146",
                    "title": "Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models",
                    "text": "The advancement in text-to-image models has led to astonishing artistic performances. However, several studios and websites illegally fine-tune these models using artists' artworks to mimic their styles for profit, which violates the copyrights of artists and diminishes their motivation to produce original works. Currently, there is a notable lack of research focusing on this issue. In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed. In various fine-tune scenarios and against watermark attack methods, our research confirms that analyzing the distribution of watermarks in artificially generated images reliably detects unauthorized mimicry.",
                    "score": 0.4940065957867561,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models\n# Venue: arXiv.org\n# Authors: Ge Luo, Junqiang Huang, Manman Zhang, Zhenxing Qian, Sheng Li, Xinpeng Zhang\n## Abstract\nThe advancement in text-to-image models has led to astonishing artistic performances. However, several studios and websites illegally fine-tune these models using artists' artworks to mimic their styles for profit, which violates the copyrights of artists and diminishes their motivation to produce original works. Currently, there is a notable lack of research focusing on this issue. In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed. In various fine-tune scenarios and against watermark attack methods, our research confirms that analyzing the distribution of watermarks in artificially generated images reliably detects unauthorized mimicry.\n",
            "reference_string": "[265445146 | Luo et al. | 2023 | Citations: 9]"
        },
        {
            "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.15367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277608846",
                    "name": "Soumil Datta"
                },
                {
                    "authorId": "2332319316",
                    "name": "Shih-Chieh Dai"
                },
                {
                    "authorId": "2332299474",
                    "name": "Leo Yu"
                },
                {
                    "authorId": "2332096509",
                    "name": "Guanhong Tao"
                }
            ],
            "abstract": "Text-to-image diffusion models, such as Stable Diffusion, have shown exceptional potential in generating high-quality images. However, recent studies highlight concerns over the use of unauthorized data in training these models, which may lead to intellectual property infringement or privacy violations. A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features. In this paper, we examine the robustness of various watermark-based protection methods applied to text-to-image models. We observe that common image transformations are ineffective at removing the watermark effect. Therefore, we propose RATTAN, that leverages the diffusion process to conduct controlled image generation on the protected input, preserving the high-level features of the input while ignoring the low-level details utilized by watermarks. A small number of generated images are then used to fine-tune protected models. Our experiments on three datasets and 140 text-to-image diffusion models reveal that existing state-of-the-art protections are not robust against RATTAN.",
            "corpus_id": 274235104,
            "sentences": [
                {
                    "corpus_id": "274235104",
                    "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
                    "text": "Text-to-image diffusion models, such as Stable Diffusion, have shown exceptional potential in generating high-quality images. However, recent studies highlight concerns over the use of unauthorized data in training these models, which may lead to intellectual property infringement or privacy violations. A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features. In this paper, we examine the robustness of various watermark-based protection methods applied to text-to-image models. We observe that common image transformations are ineffective at removing the watermark effect. Therefore, we propose RATTAN, that leverages the diffusion process to conduct controlled image generation on the protected input, preserving the high-level features of the input while ignoring the low-level details utilized by watermarks. A small number of generated images are then used to fine-tune protected models. Our experiments on three datasets and 140 text-to-image diffusion models reveal that existing state-of-the-art protections are not robust against RATTAN.",
                    "score": 0.4471873459902406,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage\n# Venue: arXiv.org\n# Authors: Soumil Datta, Shih-Chieh Dai, Leo Yu, Guanhong Tao\n## Abstract\nText-to-image diffusion models, such as Stable Diffusion, have shown exceptional potential in generating high-quality images. However, recent studies highlight concerns over the use of unauthorized data in training these models, which may lead to intellectual property infringement or privacy violations. A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features. In this paper, we examine the robustness of various watermark-based protection methods applied to text-to-image models. We observe that common image transformations are ineffective at removing the watermark effect. Therefore, we propose RATTAN, that leverages the diffusion process to conduct controlled image generation on the protected input, preserving the high-level features of the input while ignoring the low-level details utilized by watermarks. A small number of generated images are then used to fine-tune protected models. Our experiments on three datasets and 140 text-to-image diffusion models reveal that existing state-of-the-art protections are not robust against RATTAN.\n",
            "reference_string": "[274235104 | Datta et al. | 2024 | Citations: 0]"
        },
        {
            "title": "DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models",
            "venue": "SIGKDD Explorations",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 35,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04642, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "143702207",
                    "name": "J. Ren"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "49755259",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Recently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, Diffusion Shield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, Diffusion Shield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.",
            "corpus_id": 259108818,
            "sentences": [
                {
                    "corpus_id": "259108818",
                    "title": "DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models",
                    "text": "Recently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, Diffusion Shield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, Diffusion Shield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.",
                    "score": 0.446520962389762,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models\n# Venue: SIGKDD Explorations\n# Authors: Yingqian Cui, J. Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Jiliang Tang\n## Abstract\nRecently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, Diffusion Shield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, Diffusion Shield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.\n",
            "reference_string": "[259108818 | Cui et al. | 2023 | Citations: 35]"
        },
        {
            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1696291",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "51023221",
                    "name": "Daochang Liu"
                },
                {
                    "authorId": "2302950741",
                    "name": "Mubarak Shah"
                },
                {
                    "authorId": "2288626806",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.",
            "corpus_id": 278129333,
            "sentences": [
                {
                    "corpus_id": "278129333",
                    "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                    "text": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
                    "score": 0.4778104645197726,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1853
                        },
                        {
                            "start": 1856,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2146
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 100,
                            "end": 104,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 373,
                            "end": 376,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 378,
                            "end": 381,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1634,
                            "end": 1638,
                            "matchedPaperCorpusId": "256627601"
                        },
                        {
                            "start": 1914,
                            "end": 1917,
                            "matchedPaperCorpusId": "268819999"
                        },
                        {
                            "start": 1922,
                            "end": 1925,
                            "matchedPaperCorpusId": "258987384"
                        },
                        {
                            "start": 1925,
                            "end": 1928,
                            "matchedPaperCorpusId": "270309880"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models\n# Venue: arXiv.org\n# Authors: Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu\n## Abstract\nText-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.\n## Introduction\nLeveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
            "reference_string": "[278129333 | Chen et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Ablating Concepts in Text-to-Image Diffusion Models",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2023,
            "reference_count": 84,
            "citation_count": 201,
            "influential_citation_count": 52,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.13516",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.13516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46373847",
                    "name": "Nupur Kumari"
                },
                {
                    "authorId": "2119454239",
                    "name": "Bin Zhang"
                },
                {
                    "authorId": "12782331",
                    "name": "Sheng-Yu Wang"
                },
                {
                    "authorId": "2177801",
                    "name": "Eli Shechtman"
                },
                {
                    "authorId": "2109976035",
                    "name": "Richard Zhang"
                },
                {
                    "authorId": "1922024303",
                    "name": "Jun-Yan Zhu"
                }
            ],
            "abstract": "Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.",
            "corpus_id": 257687839,
            "sentences": [
                {
                    "corpus_id": "257687839",
                    "title": "Ablating Concepts in Text-to-Image Diffusion Models",
                    "text": "Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.",
                    "score": 0.4449061997819877,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Ablating Concepts in Text-to-Image Diffusion Models\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Nupur Kumari, Bin Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu\n## Abstract\nLarge-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.\n",
            "reference_string": "[257687839 | Kumari et al. | 2023 | Citations: 201]"
        },
        {
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "venue": "",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.12052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2210435658",
                    "name": "Rui Ma"
                },
                {
                    "authorId": "2257338567",
                    "name": "Qiang Zhou"
                },
                {
                    "authorId": "2268733263",
                    "name": "Yizhu Jin"
                },
                {
                    "authorId": "2292161412",
                    "name": "Daquan Zhou"
                },
                {
                    "authorId": "2292177829",
                    "name": "Bangjun Xiao"
                },
                {
                    "authorId": "2292217065",
                    "name": "Xiuyu Li"
                },
                {
                    "authorId": "2292690089",
                    "name": "Yi Qu"
                },
                {
                    "authorId": "2261448160",
                    "name": "Aishani Singh"
                },
                {
                    "authorId": "2242659602",
                    "name": "Kurt Keutzer"
                },
                {
                    "authorId": "2328473437",
                    "name": "Jingtong Hu"
                },
                {
                    "authorId": "2307915260",
                    "name": "Xiaodong Xie"
                },
                {
                    "authorId": "2293731776",
                    "name": "Zhen Dong"
                },
                {
                    "authorId": "2257020214",
                    "name": "Shanghang Zhang"
                },
                {
                    "authorId": "2275298300",
                    "name": "Shiji Zhou"
                }
            ],
            "abstract": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).",
            "corpus_id": 268532352,
            "sentences": [
                {
                    "corpus_id": "268532352",
                    "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
                    "text": "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods.Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity.This indicates the extent to which the prompt that generates potential infringement is nullified.\n\nExtent of Model Degradation during Unlearning.The unlearning process inherently degrades the model by eliminating certain infringement-suspected concepts.Nevertheless, it is vital to preserve the Stable Diffusion model's generation capacities for copyright-irrelevant contents.We assess the degree of model degradation using the widely-recognized FID (Fr\u00e9chet Inception Distance) metric [20].\n\nOur benchmark facilitates a straightforward evaluation of potential copyright infringement, while facilitating comparison among various unlearning methods.Moreover, we perform comprehensive benchmark tests on our proposed CPDM dataset.In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models.This evaluation provides valuable insights into assessing copyright infringement and the efficacy of unlearning methods in reducing infringement risks, while preserving the ability to generate non-infringing contents.",
                    "score": 0.5193304357475695,
                    "section_title": "Effectiveness of Unlearning.",
                    "char_start_offset": 3524,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 161,
                            "end": 255
                        },
                        {
                            "start": 255,
                            "end": 352
                        },
                        {
                            "start": 354,
                            "end": 400
                        },
                        {
                            "start": 400,
                            "end": 508
                        },
                        {
                            "start": 508,
                            "end": 631
                        },
                        {
                            "start": 631,
                            "end": 746
                        },
                        {
                            "start": 748,
                            "end": 903
                        },
                        {
                            "start": 903,
                            "end": 983
                        },
                        {
                            "start": 983,
                            "end": 1195
                        },
                        {
                            "start": 1195,
                            "end": 1412
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.96240234375,
            "relevance_judgment_input_expanded": "# Title: A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models\n# Venue: \n# Authors: Rui Ma, Qiang Zhou, Yizhu Jin, Daquan Zhou, Bangjun Xiao, Xiuyu Li, Yi Qu, Aishani Singh, Kurt Keutzer, Jingtong Hu, Xiaodong Xie, Zhen Dong, Shanghang Zhang, Shiji Zhou\n## Abstract\nCopyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).\n## Effectiveness of Unlearning.\nWe reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods.Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity.This indicates the extent to which the prompt that generates potential infringement is nullified.\n\nExtent of Model Degradation during Unlearning.The unlearning process inherently degrades the model by eliminating certain infringement-suspected concepts.Nevertheless, it is vital to preserve the Stable Diffusion model's generation capacities for copyright-irrelevant contents.We assess the degree of model degradation using the widely-recognized FID (Fr\u00e9chet Inception Distance) metric [20].\n\nOur benchmark facilitates a straightforward evaluation of potential copyright infringement, while facilitating comparison among various unlearning methods.Moreover, we perform comprehensive benchmark tests on our proposed CPDM dataset.In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models.This evaluation provides valuable insights into assessing copyright infringement and the efficacy of unlearning methods in reducing infringement risks, while preserving the ability to generate non-infringing contents.",
            "reference_string": "[268532352 | Ma et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 176,
            "citation_count": 42,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2253881697",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2253682835",
                    "name": "Shenglai Zeng"
                },
                {
                    "authorId": "2282560420",
                    "name": "Jiankun Zhang"
                },
                {
                    "authorId": "2256788829",
                    "name": "Hongzhi Wen"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "2253533415",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2267019992",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
            "corpus_id": 267412857,
            "sentences": [
                {
                    "corpus_id": "267412857",
                    "title": "Copyright Protection in Generative AI: A Technical Perspective",
                    "text": "Another approach for protection on source data copyright is to track or detect whether a suspect piece of artwork is generated by a model trained on the copyrighted data. Various AI-generated image detection methods [28,31] can be applied to distinguish whether a sample is generated by certain models, which partially fulfill the objective. However, these methods are still not applicable to identify the source of the generated contents. Therefore, the \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright. \n\nBefore DGMs, there exist various watermarking methods [5,42,82,124,133,161,167] hiding data like a message or even an image behind imperceptible perturbations. These techniques primarily concentrate on hiding information in specific images, without being specifically applied to DGMs. But the objective of protection copyright against malicious DGMs is to identify hidden messages within generated images. Focusing on DDPM [91], Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks [89,153,167] can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images. \n\nFine-tuning text-to-image diffusion models, like Stable Diffusion, demonstrates significant potential in personalizing image synthesis and editing. Consequently, watermarking techniques are increasingly applied as a means of copyright protection during the fine-tuning phase.",
                    "score": 0.5009957342223232,
                    "section_title": "Watermarks.",
                    "char_start_offset": 27180,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 504
                        },
                        {
                            "start": 505,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 876
                        },
                        {
                            "start": 879,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1284
                        },
                        {
                            "start": 1285,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1880
                        },
                        {
                            "start": 1881,
                            "end": 2104
                        },
                        {
                            "start": 2107,
                            "end": 2254
                        },
                        {
                            "start": 2255,
                            "end": 2382
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 216,
                            "end": 220,
                            "matchedPaperCorpusId": "258297834"
                        },
                        {
                            "start": 220,
                            "end": 223,
                            "matchedPaperCorpusId": "264436550"
                        },
                        {
                            "start": 939,
                            "end": 942,
                            "matchedPaperCorpusId": "210473431"
                        },
                        {
                            "start": 942,
                            "end": 946,
                            "matchedPaperCorpusId": "131773730"
                        },
                        {
                            "start": 946,
                            "end": 950,
                            "matchedPaperCorpusId": "59555570"
                        },
                        {
                            "start": 950,
                            "end": 954,
                            "matchedPaperCorpusId": "52953794"
                        },
                        {
                            "start": 954,
                            "end": 958,
                            "matchedPaperCorpusId": "50784854"
                        },
                        {
                            "start": 1302,
                            "end": 1306,
                            "matchedPaperCorpusId": "231979499"
                        },
                        {
                            "start": 1420,
                            "end": 1424,
                            "matchedPaperCorpusId": "9280993"
                        },
                        {
                            "start": 1424,
                            "end": 1428,
                            "matchedPaperCorpusId": "238419552"
                        },
                        {
                            "start": 1428,
                            "end": 1432,
                            "matchedPaperCorpusId": "50784854"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: Copyright Protection in Generative AI: A Technical Perspective\n# Venue: arXiv.org\n# Authors: Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang\n## Abstract\nGenerative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.\n## Watermarks.\nAnother approach for protection on source data copyright is to track or detect whether a suspect piece of artwork is generated by a model trained on the copyrighted data. Various AI-generated image detection methods [28,31] can be applied to distinguish whether a sample is generated by certain models, which partially fulfill the objective. However, these methods are still not applicable to identify the source of the generated contents. Therefore, the \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright. \n\nBefore DGMs, there exist various watermarking methods [5,42,82,124,133,161,167] hiding data like a message or even an image behind imperceptible perturbations. These techniques primarily concentrate on hiding information in specific images, without being specifically applied to DGMs. But the objective of protection copyright against malicious DGMs is to identify hidden messages within generated images. Focusing on DDPM [91], Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks [89,153,167] can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images. \n\nFine-tuning text-to-image diffusion models, like Stable Diffusion, demonstrates significant potential in personalizing image synthesis and editing. Consequently, watermarking techniques are increasingly applied as a means of copyright protection during the fine-tuning phase.",
            "reference_string": "[267412857 | Ren et al. | 2024 | Citations: 42]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b",
            "score": 0.9113041864830383,
            "section_title": "Introduction",
            "char_start_offset": 4077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1733
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1138,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1138,
                    "end": 1162,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1204,
                    "end": 1222,
                    "matchedPaperCorpusId": "270309880"
                },
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1594,
                    "end": 1614,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1710,
                    "end": 1733,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "As an increasing number of companies incorporate textto-image diffusion models (DMs) (Rombach et al., 2022;Nichol et al., 2021;Saharia et al., 2022;Ramesh et al., 2022) into their products, the issue of copyright becomes increasingly prominent (Vincent, 2023). To responsibly harness The proposed method does not require access to or control over the diffusion model's training or fine-tuning processes. It simply involves inserting poisoning data into the clean training dataset. After training with the poisoned dataset, the target text-to-image diffusion model can be triggered by specific text prompts to generate images that infringe on copyright. The proposed attack works by semantically dissecting a copyrighted image into nuanced elements and incorporating them into multiple images, thus rendering them non-copyright-infringing. Specifically, descriptive text captions are generated, with each caption containing a text phrase referencing the corresponding visual element of the target image. Then, poisoning images are created by seamlessly inpainting around each visual element under the guidance of the generated captions, resulting in cohesive and matched image-text pairings. Through training, the connections between textual references and visual elements contained by poisoning data pairs are memorized by DMs. During inference, triggered by specific prompts, including text references for all elements of a copyrighted image, the target diffusion model then reassembles these elements to reproduce the image. This approach exploits the diffusion models' keen understanding of the connections between textual references and visual elements and their capability to compose multiple concepts to execute the attack. Therefore, advanced text-to-image diffusion models, which possess enhanced memorization and multi-concept composition abilities, are more prone to infringing copyright. \n\nWe empirically demonstrate the efficacy of SilentBadDiffusion in inducing various versions of stable diffusion versions (SDs) to generate infringing images when triggered by specific prompts. Besides, our experiments show the stealth of the poisoning data. Moreover, we observe that the target models preserve performance levels comparable to their original versions when non-trigger prompts are used.",
            "score": 0.7070923931265163,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 107,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.\n\nIn light of the aforementioned considerations, we introduce the first comprehensive dataset tailored for copyright protection in this domain: Copyright Protection from Diffusion Model (CPDM) dataset.Specifically, we have curated copyright images from four distinct categories that are most commonly suspected of infringement, as demonstrated in Fig. 2.This dataset encompasses a collection of original copyright images, associated prompts for text-to-image generation via Stable Diffusion, and indicatives for a series of features, represented by: 1) Potential for Infringement; 2) Effectiveness of Unlearning; 3) Extent of Model Degradation during Unlearning.\n\nPotential for Infringement.This is quantified by feature-level similarities between the original copyright images and potentially infringing generated contents, denoted as CPDM metric (CM).",
            "score": 0.7018215662985379,
            "section_title": "Introduction",
            "char_start_offset": 2456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 185,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 845
                },
                {
                    "start": 847,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 1036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.",
            "score": 0.6804537645669604,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The main objective of this work is to raise awareness about the potential pitfalls of copyright protection grounded in copyright law. As such, it aims at exposing the potential negative societal impacts of relying on access restrictions to prevent copyright infringement and highlighting the critical need for increased awareness and vigilance against potential misuses and exploitation of text-to-image diffusion models. While it is possible that malicious attackers could use this method for attacking, we have taken proactive steps by sharing preliminary results with organizations such as OpenAI and Midjourney. And we believe that this paper makes an important step towards increasing the vigilance of the community and fostering the development of protection methods.",
            "score": 0.6769887823798516,
            "section_title": "Impact Statement",
            "char_start_offset": 33474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 773
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling [14,35].These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright.\n\nMachine Unlearning (MU), which aims to eliminate the influence of specific target data or concepts, presents a promising solution to the aforementioned challenges.Several studies have provided intuitive evidence of MU's effectiveness in erasing the memory of copyrighted material from original models [42,7,26,8].However, the current body of research is limited by a lack of quantitative and systematic assessments that evaluate the extent to which MU can reduce the risk of copyright infringement.This limitation hinders the ability to make meaningful comparisons between existing MU approaches.The challenge is exacerbated by the inherent complexity of defining copyright infringement criteria for text-to-image generative models, as well as the scarcity of comprehensive inference datasets and standardized benchmarks for assessing copyright infringement.The absence of extensive copyright datasets obstructs researchers' efforts to fully comprehend the copyright infringement risks associated with generative models.This, in turn, restricts their capacity to develop superior MU algorithms capable of addressing the legal risks effectively.As shown in Fig. 1, datasets and benchmarks are essential for forgetting copyrighted content and evaluating unlearning methods.\n\nInitially, it is crucial to define what constitutes copyright infringement in contents produced by text-to-image generative models [38].In this study, we focus on infringement within 2D artistic works.Drawing on expertise from copyright protection specialists, including artists and lawyers, we contend that a unique painting style of an artist, virtual representations in artistic creations, and individual portraits all represent forms of creative expression deserving of legal protection.In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.",
            "score": 0.6728148682852982,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1948
                },
                {
                    "start": 1950,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2151
                },
                {
                    "start": 2151,
                    "end": 2441
                },
                {
                    "start": 2441,
                    "end": 2624
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 984,
                    "end": 987,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 987,
                    "end": 989,
                    "matchedPaperCorpusId": "261276613"
                },
                {
                    "start": 2081,
                    "end": 2085,
                    "matchedPaperCorpusId": "254366634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
            "score": 0.669691959485315,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "Copyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We thereforesimilar to [25]-limit our comparison to a subset of the training data. \n\nImage embeddings: To compare two images, we first encode them. Image embedding can happen on a content and style level. Some embeddings correspond rather to the object-level content of the generated images, whereas other perform better in capturing the artistic style of the generated images [19]. We focus on rather content based embedding because similarity based upon style might not be considered copyrighted [14]. \n\nTo embed an image, we consider CLIP embedding [18]. CLIP embeddings are useful when dealing with text-to-image models because they map both images and text into a shared semantic space. This also allows for meaningful comparisons beyond pixel-level similarity. By using CLIP embeddings, we evaluate similarity not only at a visual level but also in terms of content and semantic meaning. \n\nSimilarity score: Once we have the image embedding, we compare images using a similarity score between the embeddings. Two images are considered to be similar when the similarity score between their embeddings is larger than certain threshold [4]. We con-sider the cosine similarity between the CLIP embeddings as our similarity score, and a threshold of 0.85 as high IP infringement risk. Notice that [4] used the same similarity threshold to identify near duplicates in the training data for deduplication. \n\nWe briefly recall other possible choices for a similarity score: authors in [24] use split dot product of Self Supervised Copy Detection (SSCD) scores as similarity measure, where scores are predicted based on differential entropy regularization, see [16]. In [27], authors utilize the Euclidean L2 norm on pixel space and SSCD scores, while [4] employ cosine similarity between CLIP embeddings to identify near-duplicates. \n\n3 Experimental Results",
            "score": 0.6629245076660111,
            "section_title": "Detecting IP Infringement",
            "char_start_offset": 7363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2158
                },
                {
                    "start": 2161,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 1986,
                    "end": 1990,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "(1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models. (2) We introduce a copyright tester that employs attention maps to identify significant similarities, extending the analysis from entire image duplication to specific visual feature resemblances and pinpointing potential areas of interest for detailed examination. \n\n(3) We compile a dataset of potential copyrighted topics and prompts to aid in more realistic copyright research and to analyze diffusion model behaviors. We provide empirical results to show the copyright threat and to raise awareness in copyright research for generative models. A case study utilizing our dataset revealed a significant concern: recent diffusion models is prone to unintentionally produce images with copyrighted content from seemingly unrelated prompts. \n\n2 Background Diffusion models. Diffusion models are a class of generative models that model the diffusion process. In the diffusion process, source data is gradually distorted by adding noise to it until the source data becomes noise as well [Sohl-Dickstein et al., 2015]. The objective of diffusion models is to learn the reverse process of diffusion, which tries to reconstruct the target given noisy input. Diffusion models can either learn to directly predict the less noisy data at each reverse step, or learn to predict the noise at each step, then denoise the data using predicted noise [Ho et al., 2020;Saharia et al., 2022]. Earlier diffusion models work at image level and try to directly reconstruct images from noise. However, reverse steps at image level require intensive calculation and constraints the speed of the reconstruction. Instead, [Rombach et al., 2022] proposes to first transform the image into low dimensional hidden space, then apply diffusion models at hidden level. Subsequently, latent diffusion models are much faster than their counterparts working at image level, hence can be trained on large datasets such as LAION [Schuhmann et al., 2022]. Prediction of noise or previous states in the reverse process usually uses a U-Net [Ronneberger et al., 2015].",
            "score": 0.6616079175140123,
            "section_title": "Introduction",
            "char_start_offset": 2271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1212,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1535,
                    "end": 1552,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1552,
                    "end": 1573,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models are undergoing a rapid development phase, and recently proposed diffusion models have shown improved performance in generating high-quality images. However, whether their performance has improved for copyright protection remains questionable. In this section, we investigate diffusion models in this regard. \n\nOur case study examines how a diverse pool of diffusion models react to varying subjects. For this purpose, we leverage the experiment setup in the previous section. Specifically, we obtain proportions of images with identified copyrighted content for each topic and compare them across different diffusion models (Figure 10, more in Appendix D). While the performance of each model varies for each topic, it is clear that all models exhibit copyright-infringing behavior for the five distinct topics. Even though the latest model SD XL shows a slight decrease in generating copyrighted content, the rate generating copyrighted content remains above 50% for many topics. This suggests that the current approach to training diffusion models remains ineffective in preventing the occurrence of copyright infringement.",
            "score": 0.6597747869719572,
            "section_title": "Case Study: Copyright Issue Across Diffusion Models",
            "char_start_offset": 28942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Copyright infringement arises from unauthorized access and reproduction of copyrighted material. Copyright protection methods (Vyas et al., 2023;Wiggers, 2023) for text-to-image diffusion models, grounded in copyright law, mainly focus on the curation of training data, which is different from the prevention of Not-Safe-For-Work (NSFW) content generation (Poppi et al., 2023). To prevent the production of inappropriate content, a classifier can be trained over historical NSFW contents to identify future NSFW outputs (Poppi et al., 2023). However, copyrighted contents do not follow a clear pattern, making it challenging to learn. Moreover, the rapidly-evolving nature of copyrighted materials necessitates frequent updates to a copyright classifier, rendering post-detection efforts for copyright protection unfeasible. In response, Vyas et al. (2023) introduced a theoretical framework for copyright protection through access restriction. Additionally, model editing methods have been developed for better control over image generation (Gandikota et al., 2023;Kumari et al., 2023;Zhang et al., 2023a). Moreover, strategies like applying perturbations or watermarks to images are also being explored to protect copyrighted materials (Cui et al., 2023;Ray & Roy, 2020;Zhao et al., 2023;Li et al., 2022;Tang et al., 2023;Guo et al., 2023). \n\nBackdoor Attacks on Diffusion Models. Backdoor attacks involve embedding triggers during the training of neural networks, causing the model to behave normally until the trigger is activated (Gu et al., 2017). With the growing prominence of diffusion models (Ho et al., 2020;Song et al., 2021), there's increased focus on their vulnerability to such attacks, highlighted in research by Chen et al. (2023) and Chou et al. (2023). Studies have explored compromising diffusion models by targeting their components like text encoders (Struppek et al., 2022) and altering the diffusion process (Chou et al., 2023).",
            "score": 0.6511738427501574,
            "section_title": "Related Work",
            "char_start_offset": 4413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 1042,
                    "end": 1066,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1066,
                    "end": 1086,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1256,
                    "end": 1272,
                    "matchedPaperCorpusId": "227129816"
                },
                {
                    "start": 1290,
                    "end": 1306,
                    "matchedPaperCorpusId": "252683790"
                },
                {
                    "start": 1306,
                    "end": 1324,
                    "matchedPaperCorpusId": "257636598"
                },
                {
                    "start": 1619,
                    "end": 1637,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1730,
                    "end": 1748,
                    "matchedPaperCorpusId": "257482560"
                },
                {
                    "start": 1753,
                    "end": 1771,
                    "matchedPaperCorpusId": "254564071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "We present CopyJudge, an innovative framework for automating the identification of copyright infringement in text-to-image diffusion models. By leveraging abstractionfiltration-comparison test and multi-LVLM debates, our approach could effectively evaluate the substantial similarity between generated and copyrighted images, providing clear and interpretable judgments. Additionally, our LVLM-based mitigation strategy helps avoid infringement by automatically optimizing prompts and exploring non-infringing latent noise vectors, while ensuring that generated images align with the user's requirements. \n\n\u2022 Modifying a Prompt to Improve Similarity Score (Attack Iteration): \"Adjust the parts of the original prompt of the second image that may cause expressions of distinction in the following rationale, making it more similar to the first image to achieve a higher score. Add more information about the [IP type] in Image 1, and provide more unique expressions specific to the [IP type] in Image 1. You can make any changes as long as they improve the similarity score. Require: Source image x cr , generated image x 0 , initial prompt p 0 , control condition p c , LVLM-based prompt modifier \u03c0 p , infringement identification function f , threshold \u03b3, maximum iterations T 1: Initialize prompt: p t \u2190 p 0 , generated image: \n\nGenerate new prompt: p t+1 \u2190 \u03c0 p (x t , x cr , p t , p c , s t , c t , r t ) 8: \n\nGenerate new image using p t+1 : x t+1 \u2190 T2I(p t+1 ) 9: \n\nUpdate iteration counter: t \u2190 t + 1 10: end while 11: Return final prompt p t and generated image x t",
            "score": 0.6498275508978489,
            "section_title": "Conclusion",
            "char_start_offset": 24510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "Diffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.",
            "score": 0.6430386716370833,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2024
                },
                {
                    "start": 2027,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2375
                },
                {
                    "start": 2376,
                    "end": 2591
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "260900064"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "257427673"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1090,
                    "end": 1093,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "We briefly review related work about text-to-image diffusion models, copyright protection and Reinforcement Learning from Human Feedback. \n\nText-to-Image Diffusion Models: Recently, text-toimage diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into visually coherent and realistic images with high accuracy. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and 1 https://law.justia.com/cases/federal/appellate-courts/F2/562/ 1157/293262/ Abbeel 2020a; Xie et al. 2023), andsuper-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nCopyright Protection: Several studies in the legal literature have examined copyright issues in machine learning and data mining, focusing primarily on potential infringements during the training phase.: (1)Watermarking (Dogoulis et al. 2023;Epstein et al. 2023), which inserts specific, unnoticeable patterns into protected images to detect copyright infringement, has been explored, but further research is needed to improve its robustness. (2)Concept Removal: To remove explicit artwork from large models, (Gandikota et al. 2023) presents a fine-tuning method for concept removal from diffusion models. Additionally, (Zhang et al. 2023) presents the \"Forget-Me-Not\" method, which enables the targeted removal of specific objects and content from large models within 30 seconds while minimizing the impact on other content. (3) Dataset Deduplication: (Somepalli et al. 2022) explores whether diffusion models create unique artworks or directly replicate certain content from the training dataset during image generation.",
            "score": 0.6369800221977917,
            "section_title": "Related Work",
            "char_start_offset": 4920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 635,
                    "end": 673,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 700,
                    "end": 718,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1159,
                    "end": 1181,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1181,
                    "end": 1201,
                    "matchedPaperCorpusId": "264436550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "For the process of identifying substantial similarity, we refer to the abstraction-filtering-comparison test method (Abramson, 2002), which has been widely adopted in practical court rulings on infringement cases, and propose an automated infringement identification framework using large visionlanguage models, as seen in Figure 1. In the copyright expression extraction stage, we break down images into different elements (such as composition and color patterns), and filter out non-copyrightable parts, leaving copyrighted portions to assess substantial similarity. In the next copyright infringement determination stage, multiple LVLMs debate and score the similarity of images given the copyrighted elements, with a final decision made by a meta-judge LVLM based on their consensus. Human priors are injected into the models through few-shot demonstrations to better align with human preferences. \n\nCopyright expression extraction via image-to-text abstraction and filtration. The process of distinguishing between the fundamental ideas and the specific expressions of an image is a crucial step in determining copyright protection. The core idea of our method is to break down the image into different layers or components, in order to examine the true copyright elements. \n\nFirst, during the abstraction phase, the image is analyzed and decomposed into its fundamental building blocks. This involves identifying the core elements that contribute to the overall meaning or aesthetic of the image, such as composition, themes, color palette, or other unique visual elements. We can implement this using an LVLM \u03c0 abs , defined as: \n\nwhere z and z cr represent the expressions of x and x cr in text after decoupling, respectively. The goal is to abstract away the superficial features of the image that do not hold significant creative value and instead focus on the underlying concepts that convey the essence of the work. \n\nThe next step is filtering. At this stage, elements of the image that are not eligible for copyright protection are removed from consideration. These can include generic concepts, common patterns, functional aspects, or elements derived from public domain sources. For example, standard design patterns or commonly used motifs in artwork may not be deemed original enough to warrant protection under copyright law. This process could be defined as: \n\nwhere z c and z c cr are the filtered copyright expressions, and \u03c0 f il is another independent LVLM. Filtering helps ensure that only the truly creative, original aspects of the image are preserved for comparison.",
            "score": 0.6314858397584858,
            "section_title": "Abstraction-Filtering-Comparison Framework",
            "char_start_offset": 6659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2344
                },
                {
                    "start": 2345,
                    "end": 2378
                },
                {
                    "start": 2381,
                    "end": 2481
                },
                {
                    "start": 2482,
                    "end": 2594
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 132,
                    "matchedPaperCorpusId": "6322845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "271600759",
            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
            "text": "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks, as shown in Fig. 3 (d). Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft.",
            "score": 0.6241298444469792,
            "section_title": "Watermarking",
            "char_start_offset": 16541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Copyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.",
            "score": 0.6240436174601123,
            "section_title": "Copyright Infringement Attack",
            "char_start_offset": 6647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2108
                },
                {
                    "start": 2111,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2404
                },
                {
                    "start": 2407,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.740234375
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "Recently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into highly accurate and visually coherent images. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a;Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nWhile the progress in text-to-image generative models has profoundly impacted different industries, it also presents significant challenges for copyright protection. These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Elkin-Koren et al. 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al. 2022b) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders? \n\nVarious methods have been proposed for source data copyright protection. One approach involves using unrecognizable examples (Gandikota et al. 2023;Zhang et al. 2023) that prevent models from learning key features of protected images either during inference or training stages. However, this method is highly dependent on the specific image and model, and it lacks general reliability.",
            "score": 0.6221549860324321,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 173,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 451,
                    "end": 489,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 619,
                    "end": 647,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1162,
                    "end": 1187,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1250,
                    "end": 1272,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "we propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted",
            "score": 0.6184945713475277,
            "section_title": "Overview",
            "char_start_offset": 9821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "276937502",
            "title": "Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?",
            "text": "Text-to-image diffusion model. Diffusion models [14] have revolutionized generative modeling fundamentally, in terms of quality and generalization. Their excellent scalability has led to significant advancements in specific applications, such as text-to-image generation. Text-to-image diffusion models [4,33,37] use the diffusion process to transform text descriptions into realistic images. Recently, people have begun to recognize the potential of text-toimage diffusion models for synthesizing dataset [16,17,39,41,55]. However, so far, no one has realized that suspects might use these text-to-image diffusion models to synthesize datasets and train their own models without authorization. Therefore, in this paper, we propose the first feasible solution to address this issue, securing and fostering healthy development in this field. \n\nText-to-image model protection. Training generative models requires large amounts of data and computational resources [4,37], making the intellectual property of these models crucial for their owners. When these models are open-source [4,27,33,37], their intellectual property is vulnerable to infringement by suspects. Specifically, for text-to-image models, suspects have the following three ways primarily to infringe on their intellectual property: (1) stealing synthetic data illegally; (2) illegal knowledge distillation; (3) training suspicious models for specific tasks using synthetic data illegally. For the first two cases, watermarking [23,26,54] and training data attribution [53] have been developed to address these issues. However, for the third case, no suitable solution exists currently. This work aims to address this gap.",
            "score": 0.6172386735020432,
            "section_title": "Related Work",
            "char_start_offset": 5729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1685
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "263310331"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "235719477"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "257772061"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "257757078"
                },
                {
                    "start": 961,
                    "end": 964,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 964,
                    "end": 967,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 1084,
                    "end": 1087,
                    "matchedPaperCorpusId": "263310331"
                },
                {
                    "start": 1087,
                    "end": 1090,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
            "score": 0.6144578344026231,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Dataset. Firstly, we use D-Rep dataset (Wang et al., 2024b), which contains realistic and generated image pairs scored by human from 0 to 5 according to their similarity. Following its setting, we consider samples with scores of 4 or above as infringement samples, while the rest were non-infringement samples. We use the 4,000 official test images. In addition, we also consider the specific IP infringement. Referring to (Ma et al., 2024), we select 10 well-known cartoon characters and 10 artworks from Wikipedia (all can be found in B.2). 3 different text-to-image models-Stable Diffusion v2, Kandinsky2-2 (Razzhigaev et al., 2023), and Stable Diffusion XL (Podell et al., 2023)-are used to generate images. \n\nFor each item, we manually select one infringing image and one non-infringing image from each model, resulting in a total of 60 positive samples and 60 negative samples. \n\nBaselines. We select 4 commonly used distance-based image copy detection metrics: L 2 norm (Carlini et al., 2023), which directly measures pixel-wise differences; LPIPS (Zhang et al., 2018), which captures perceptual similarity based on deep network features; SSCD (Pizzi et al., 2022), which learns transformation-invariant representations to match generated images with their original training counterparts; and RLCP (Shi et al., 2024b), which integrates semantic and perceptual scores for a more comprehensive similarity assessment. Additionally, we compare our approach with the state-of-the-art image copy detection method PDF-Emb (Wang et al., 2024b), which models the similariy level of image pairs as a probability density function. We implement all methods using their official open-source code. \n\nMetric. Accuracy and F1 score are calculated as the criteria for infringement classification. We perform grid search to select the threshold that achieves the highest F1 score for each method. \n\nImplementation detail. Although our approach does not rely on a specific LVLM, we choose GPT-4o (Hurst et al., 2024) as our agent by default.",
            "score": 0.6114203882576167,
            "section_title": "Infringement Identification Experiments",
            "char_start_offset": 15653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 977,
                    "end": 999,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1055,
                    "end": 1075,
                    "matchedPaperCorpusId": "4766599"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "In this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity. \n\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets.",
            "score": 0.6006551169589541,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 23145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1600
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
            "score": 0.5918619265860612,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.",
            "score": 0.5834161436857749,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "On our dataset comprising of 25 topics, we tested each of the topic across all 6 Stable Diffusion model series for potential copyright infringement. We generate 10 prompts for each topic and 10 images for each prompt, giving us 100 images per topic. Figure 11 shows the proportion of images containing chunks that are similar to the reference copyrighted images.",
            "score": 0.5808032539562908,
            "section_title": "D Generated Image Quality Across Ideas",
            "char_start_offset": 43813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity. However, directly applying large models for infringement identification may face unreliable outputs due to their limited comprehension or potential misinterpretation. To address this, we propose CopyJudge, an automated abstraction-filtration-comparison framework with multi-LVLM debate to reliably follow the court decision process on identifying substantial similarity. Specifically, referring to the software abstraction test (Abramson, 2002), we decompose the image into different layers or elements, such as composition, color, and theme, to distinguish between the basic concepts of the image and its specific expressions. Then, we filter out parts that are not copyright-protected, such as public and functional expressions. Finally, we compare the filtered portions to assess whether there is substantial similarity. To enhance the reliability of the judgment, we employ a multi-agent debate (Du et al., 2023;Chan et al., 2023) method where multiple LVLMs discuss and score the similarity. Each LVLM can make judgments based on the scores and reasons provided by other LVLMs. Ultimately, another LVLM-based meta-judge gives the final score and rationale based on the consensus of the debate. To enhance the consistency with human preferences, we inject human priors into each LVLM via few-shot demonstrations (Agarwal et al., 2024). \n\nGiven the judging results, we further explore how to mitigate infringement issues in text-to-image diffusion models. Utilizing our CopyJudge, we propose an automated black-box infringement mitigation strategy that leverages a defense LVLM to iteratively optimize the input infringing prompts. This process avoids generating sensitive infringing expressions by querying supporting infringement rationales, while preserving the integrity of the original content. Moreover, if the input latent of the diffusion model is controllable, we could further enhance the mitigation approach by exploring specific non-infringing noise vectors within the latent space in a reinforcement manner, with the reward being reducing the predicted infringement score. This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts.",
            "score": 0.5805437718398577,
            "section_title": "Introduction",
            "char_start_offset": 1848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 584,
                    "matchedPaperCorpusId": "6322845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "Defense Scenario: We conducted a study on the application of stable diffusion with a dataset that lack of copyright protections. In this particular scenario, attackers introduce poisoned samples into the model, leading to the generation of copyrighted artworks when the model is utilized by users and triggered by specific inputs. The primary goal of the defense strategy is to accurately identify and filter out these samples, subsequently fine-tuning a secure model to prevent any triggering. Additionally, we propose the concept of copyright feature inversion, which aids in the allocation of copyright responsibility. Dataset and Model: In order to implement the defense scenario described, we utilized the Pokemon BLIP Captions dataset produced by Pinkney [47], to simulate the defensive context. This choice was motivated by the fact that the Pokemon dataset provides a compelling example for the identification and understanding of copyright infringement, and recent copyright infringement cases [60] related to Pokemon have garnered significant public attention, offering a relevant backdrop for our defense strategy. Moreover, this dataset aligns well with the requirements of stable diffusion for text-to-image fine-tuning, enhancing the reproducibility of our approach. To ensure the robustness and reliability of our methods, we conducted 20 independent experiments for this scenario. In each experiment, one image is selected as the copyright-infringing image to generate the poisoned data, while the remaining 832 images are used as clean data. \n\nIn the experiments, GroundingDino [39] and SAM [18] were employed as the detection and segmentation models for the poisoned sample detection method. Given that the current backdoor attack method targeting copyright infringement is limited to SilentBadDiffusion [62], our defense strategy focused solely on detecting this method. Therefore, for stable diffusion, we adopted the v1.x version same as SilentBadDiffusion. \n\nEvaluation Metrics: For the assessment of copyright infringement, we employed SSCD as the evaluation metric. Regarding the detection of poisoned samples, we utilized Recall, Precision, and F1-Score as the evaluation metrics. In order to assess the defense, we employed the Copyright Infringement Rate (CIR) and First Attack Epoch (FAE) as measures.",
            "score": 0.579321438008028,
            "section_title": "Experimental Setup",
            "char_start_offset": 18464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1978
                },
                {
                    "start": 1981,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "257952310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "268732820",
            "title": "Detecting Origin Attribution for Text-to-Image Diffusion Models",
            "text": "In this study, we present in-depth analyses on the detection and attribution of images generated by modern textto-image (T2I) diffusion models. Through rigorous testing, our image attributors, trained to recognize outputs from 12 different T2I diffusion models along with a category for real images, reached an impressive accuracy of over 90%, significantly surpassing random chance. Our investigation into the role of text prompts, the challenge of distinguishing generators within the same family, and the ability to generalize across domains provides comprehensive insights. Pioneeringly, we delved into the detectability of hyperparameter adjustments at inference time and assessed the effects of postediting on attribution accuracy. Going beyond mere RGB analysis, we introduce a new framework for identifying detectable traces across various levels of visual detail, offering profound insights into the underlying mechanics of image attribution. These analyses provide fresh perspectives on image forensics aimed at alleviating the threat of synthetic images on copyright protection and digital forgery.",
            "score": 0.577346862933608,
            "section_title": "Conclusion",
            "char_start_offset": 30101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1109
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "277043466",
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "text": "Given the limitations of active protection, some studies [27,44,48] opted for passive detection for image copyright protection. They emphasize the traceability of protected images, explores detection schemes based on watermarks or backdoors, and focuses on the extraction of authentication marks from the inspected model as evidence when an image infringement incident occurs. Compared to adversarial methods, this type of method can be used to protect image copyright during the training or fine-tuning of any personalized diffusion model, and has less impact on image availability. Zhao et al. [48] used the pretrained watermark encoder [47] to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. [45] leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one. \n\nOur approach stands apart from previous methods by its theoretical foundation. Through detailed analysis, we discovered that diffusion models tend to preserve the frequency spectrum characteristics of their training data. Building on this insight, we introduce CoprGuard, a robust watermarking framework that is highly effective for both native and text-to-image diffusion models, even when watermarked images make up only a small portion of the training data.",
            "score": 0.5741855654436199,
            "section_title": "Passive Detection for Image Infringement",
            "char_start_offset": 6779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "238419552"
                },
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "268513090"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "Wang et al. [28] highlight vulnerabilities in copyright protection for diffusion models, proposing SilentBadDiffusion, a backdoor attack that poisons training data to reduce copyrighted image reproduction. Lu et al. [29] address detecting non-visually apparent infringements by analyzing latent spaces to identify training samples influencing generated images. Somepalli et al. [15] explore data replication in diffusion models, noting text conditioning and duplicate training images as key factors. Shang et al. [21] combine diffusion models with CNNs for high-resolution image generation, leveraging feature extraction and iterative refinement. Singh et al. [22] introduce Latent Diffusion Models (LDMs), performing diffusion in latent space to improve computational efficiency while maintaining quality.",
            "score": 0.569922273758315,
            "section_title": "Related Work",
            "char_start_offset": 5540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 806
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Diffusion Models. Recent years have seen remarkable advancements in the diffusion model (Song et al., 2021;Rombach et al., 2022;Song et al., 2020;Ho et al., 2020;Ramesh et al., 2022;Saharia et al., 2022;Bansal et al., 2022;Daras et al., 2022;Ho et al., 2022;Rombach et al., 2022). Our study primarily concentrates on the leading model in this field, Stable Diffusion (SD) (Rombach et al., 2022), which is publicly accessible. Note, our approach is also applicable to other text-to-image models. Stable Diffusion mainly contains three modules: (1) Text encoder T : that takes a text y \u2208 Y from the space of natural prompt Y, and encode it into the corresponding text embedding c := T (y); (2) Im-age encoder E and decoder D: to reduce the computational complexity, stable diffusion operates the diffusion process in latent space (Rombach et al., 2022). The encoder E provides a low-dimensional representation space for an image x \u2208 X from the space of images, x \u2248 D(z) = D(E(x)), where z is the latent representation of the image; (3) Conditional denoising module \u03f5 \u03b8 : a U-Net model that takes a triplet (z t , t, c) as input, where z t denotes the noisy latent representation at the t-th time step, and predicts the noise in z t . The training objective of \u03f5 \u03b8 can be simplified to: E (x,y)\u223cDtrain E z,c,\u03f5\u223cN (0,1),t \u2225\u03f5 \u03b8 (z t , t, c) \u2212 \u03f5\u2225 2 2 , where z = E (x) and c = T (y) denote the embeddings of an image-text pair (x, y) from dataset, D train , used for model training. z t is a noisy version of the z. \n\nCopyright Issues in Diffusion Models. Copyright infringement arises from unauthorized access and reproduction of copyrighted material.",
            "score": 0.5678115827467675,
            "section_title": "Related Work",
            "char_start_offset": 2864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1645
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 107,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 107,
                    "end": 128,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 242,
                    "end": 258,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 258,
                    "end": 279,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 372,
                    "end": 394,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 828,
                    "end": 850,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "In this research, we explore the vulnerabilities in the copyright protection of text-to-image diffusion models (DMs), grounded in copyright law, by introducing the backdoor at-   Our approach assumes that copyright images are decomposable. Future investigations could focus on a broader range of targets. A promising direction involves leveraging optimization techniques to subtly integrate the copyright information of target images by updating the pixel-wise values of the training data. Another area for future exploration involves style-attack and multiple-backdoors attack scenarios.",
            "score": 0.5674515180367423,
            "section_title": "Conclusion, Limitation and Future Work",
            "char_start_offset": 32865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 588
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4521484375
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "However, this method is highly dependent on the specific image and model, and it lacks general reliability. Watermarking (Dogoulis et al. 2023;Epstein et al. 2023) inserts specific, unnoticeable patterns into protected images to detect copyright infringement, but further research is needed to improve its robustness. Machine unlearning (Bourtoule et al. 2020;Ginart et al. 2019;Huang et al. 2021;Gao et al. 2023;Nguyen et al. 2022) removes contributions of copyright data, aligning with the right to be forgotten, while dataset deduplication (Somepalli et al. 2022) helps reduce the risk of training sample memorization. \n\nDespite these efforts, existing copyright protection methods still have the following limitations: (1) They lack a standardized copyright metric that aligns with copyright laws and regulations, making it difficult to determine if generated images constitute copyright infringement; (2) These methods often prioritize performance on specific downstream tasks rather than focusing on general applicability, resulting in approaches that work well only on certain datasets but lack the versatility needed for broader use across diverse datasets. \n\nTo tackle these challenges, we propose a Reinforcement Learning-based Copyright Protection method (RLCP) for text-to-image diffusion models to reduce the possibility of generating copyright-infringing content. Specifically, inspired by Courts 1 in the US which employs two-part test to determine copyright violation, which contains an extrinsic test examining objective similarity in specific expressive elements, and an intrinsic test assessing subjective similarity from the perspective of a reasonable audience, we first propose copyright metric that mirror these legal standards by combining semantic and perceptual similarity. Then, we propose a novel framework that combines reinforcement learning with copyright infringement metrics. We leverage the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decisionmaking process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. \n\nOur main contributions are as follows:",
            "score": 0.5625598428316976,
            "section_title": "Introduction",
            "char_start_offset": 1955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2256
                },
                {
                    "start": 2259,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 143,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 143,
                    "end": 162,
                    "matchedPaperCorpusId": "264436550"
                },
                {
                    "start": 337,
                    "end": 360,
                    "matchedPaperCorpusId": "5613334"
                },
                {
                    "start": 360,
                    "end": 379,
                    "matchedPaperCorpusId": "257804739"
                },
                {
                    "start": 397,
                    "end": 413,
                    "matchedPaperCorpusId": "257804739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Generative models, especially the recent advanced Latent Diffusion Models (LDM) (Rombach et al. 2022), have shown tremendous ability to generate new images, even of creative or artistic form according to text prompts.Such models are trained on a large corpus of data, which may consist of copyrighted material.Additionally, prior works have established that such generative models are prone to regurgitating content from their training data (Ippolito et al. 2023;Zhang et al. 2021;Carlini et al. 2022;Vyas et al.In this paper, we will focus on copyright law within the jurisdiction of the United States.To establish a copyright violation, two factors must be present.First, the accused must have had access to the copyrighted material.Second, the accused must produce content that bears \"substantial similarity\" to the copyrighted material (reproducing).Note that the definition of substantial similarity can be ambiguous.Within the context of images, its definition appears to be relatively broad (Steinberg v. Columbia Pictures Industries, Inc. 1987), and in particular encompasses near-exact copies.\n\nTurning our attention to the former \"access\" criterion: the natural way to establish that a model had access to a particular piece of copyrighted material is to inspect its training data.For example, in the case of Andersen v. Stability AI Ltd. (Dist. Court 2023), the case was allowed to proceed based on the fact that copyrighted images were found in LAION-5B (Schuhmann et al. 2022) (the training data used for Stable Diffusion) using haveibeentrained.com.\n\nWe challenge the perspective that such visual auditing is sufficient to establish access to copyrighted material.Our results show that it is possible to conceal copyrighted images within the training dataset for LDMs.Specifically, LDMs are equipped with a fixed encoder for dimension reduction such that the diffusion learning process occurs in the latent space.This structure can be maliciously exploited to generate disguised copyrighted samples: given a copyrighted image, we show how to generate a disguise such that it is visually different from the copyrighted sample but shares similar latent information.",
            "score": 0.5623201072307331,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1102
                },
                {
                    "start": 1104,
                    "end": 1291
                },
                {
                    "start": 1291,
                    "end": 1563
                },
                {
                    "start": 1565,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 101,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 481,
                    "end": 501,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 501,
                    "end": 512,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1466,
                    "end": 1488,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In summary, we make the following contributions:\n\n\u2022 We challenge the current \"access\" criterion and point out its insufficiency in more delicate cases of copyright infringement;\n\n\u2022 We propose an algorithm that demonstrably crafts disguised copyrighted data to conceal the content (or concepts) of copyrighted images in the training set;\n\n\u2022 We show disguised data contain copyrighted information in the latent space, such that by finetuning them on textual inversion or DreamBooth, or training on LDM, the model reproduces copyrighted data during inference;\n\n\u2022 We propose methods to detect such disguises, which further encourage the expansion and quantification of \"access\" in the context of copyright infringement.",
            "score": 0.5567433521996467,
            "section_title": "Introduction",
            "char_start_offset": 4248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 50,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 336
                },
                {
                    "start": 338,
                    "end": 556
                },
                {
                    "start": 558,
                    "end": 715
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "270620522",
            "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
            "text": "Generative models, especially text-to-image diffusion models, have significantly advanced in their ability to generate images, benefiting from enhanced architectures, increased computational power, and large-scale datasets. While the datasets play an important role, their protection has remained as an unsolved issue. Current protection strategies, such as watermarks and membership inference, are either in high poison rate which is detrimental to image quality or suffer from low accuracy and robustness. In this work, we introduce a novel approach, EnTruth, which Enhances Traceability of unauthorized dataset usage utilizing template memorization. By strategically incorporating the template memorization, EnTruth can trigger the specific behavior in unauthorized models as the evidence of infringement. Our method is the first to investigate the positive application of memorization and use it for copyright protection, which turns a curse into a blessing and offers a pioneering perspective for unauthorized usage detection in generative models. Comprehensive experiments are provided to demonstrate its effectiveness in terms of data-alteration rate, accuracy, robustness and generation quality.",
            "score": 0.5535161176886438,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "This structure can be maliciously exploited to generate disguised copyrighted samples: given a copyrighted image, we show how to generate a disguise such that it is visually different from the copyrighted sample but shares similar latent information.The closeness of the two samples in the latent space can be quantitatively measured by a distance function, or qualitatively revealed by a concept extraction tool called textual inversion (Gal et al. 2022), both of which we will demonstrate in our empirical study.\n\nOur study reveals the possibility of creating a new training dataset that does not appear to directly or blatantly contain any copyrighted data.Nonetheless, if a model is trained on this derivative training dataset, it would behave similarly as if the copyrighted data were present.Such disguises may still exhibit copyright infringement, although only accessing proprietary data indirectly.In Figure 1, we display a comparison between the previous copyright infringement Figure 1: An overview of conventional (with direct access to copyrighted material) and disguised (with indirect access) copyright infringement for latent diffusion models.For direct access, training an LDM-based model on copyrighted material x c and reproducing x c is subjected to copyright infringement.For indirect access, one trains the same model on disguised samples x d , which are drastically different from x c , but is still able to reproduce x c during inference.phenomenon (direct access) with the disguised copyright infringement (indirect access).Clearly, there was still access to the copyrighted material in the latter training pipeline, which raises the following question:\n\nWhat constitutes access?How to quantify it?\n\nWe answer the first question by introducing a notion of acknowledgment, which refers to a criterion that any sample that contains similar latent information as that of a copyrighted sample should be considered acknowledging it, despite possible visual dissimilarity.To quantify acknowledgment in practice, a deeper inspection than visually auditing the training set is required.Thus we further propose a twostep detection method: (1) a feature similarity search for screening suspects; (2) an encoder-decoder examination to confirm disguises, which augments the existing criterion.In summary, we make the following contributions:",
            "score": 0.5528294781151014,
            "section_title": "Introduction",
            "char_start_offset": 1942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 514
                },
                {
                    "start": 516,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1462
                },
                {
                    "start": 1462,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1678
                },
                {
                    "start": 1680,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1723
                },
                {
                    "start": 1725,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2103
                },
                {
                    "start": 2103,
                    "end": 2306
                },
                {
                    "start": 2306,
                    "end": 2354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "In addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content. An illustration of the entire copyright test is given in Figure 5. Our test requires copyright content as input. We discuss how we gather real images with copyrighted content in the subsequent section.",
            "score": 0.5509542102565638,
            "section_title": "Copyright Test for Substantial Similarities",
            "char_start_offset": 16531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "265445133",
            "title": "A Somewhat Robust Image Watermark against Diffusion-based Editing Models",
            "text": "As mentioned in related work, there has been a surge of research on knowledge copyright protection within diffusion models, categorized into preventing image styles from being learned by diffusion models [48], guarding against style transfers [29], and safeguarding diffusion models from unauthorized theft or misuse [11], [58], [67]. Our watermarking technology primarily focuses on detecting (via watermarks) unauthorized modifications or malicious edits of artwork or photographs by diffusion model-based editing tools. Artists and photographers can utilize our tool to embed subtle, nearly undetectable perturbations into their work, ensuring minimal impact on the original image quality before online publication. \n\nThe goal of our watermark is to be invisible and not to be robust against adaptive modifications: adaptive editing attack [32] or image reconstruction based on generative model [66] might remove our watermark. Also, while the game definition is flexible, in our experiment, we limit the text prompt p so that only minor changes will be made to image x. We argue that deciding whether greater change violates IP protections is a broader problem and out of the scope of this paper.",
            "score": 0.5492172586099523,
            "section_title": "Problem Scope.",
            "char_start_offset": 13229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "We first formulate the defender's goal and capability for mitigating the IP Infringement problem in text-to-image generation models.In this paper, we focus on the defense for the diffusion-based visual generative models as most of the state-of-the-art text-to-image/video models are based on the diffusion models [1,[3][4][5]16].\n\nDefender's Goal&Capability.The defender aims at preventing the IP infringements on a set of the protected intellectual properties C by modifying the generation process of the model M. We denote the model equipped with the defense as M \u22c6 .Formally, the goal of the defender can be written as P(L(M \u22c6 (P), X C ) < \u03c4 ) < \u03b1 , where C is the set of the protected intellectual property.P denotes all the possible prompts.\u03b1 is a threshold value for the probability for the happens of the IP infringements.",
            "score": 0.547161118660027,
            "section_title": "Problem Formulation",
            "char_start_offset": 15060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 329
                },
                {
                    "start": 331,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 746
                },
                {
                    "start": 746,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 325,
                    "end": 327,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).",
            "score": 0.542713498546285,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "274149854",
            "title": "CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models",
            "text": "Text-to-image diffusion models have emerged as powerful tools for generating high-quality images from textual descriptions. However, their increasing popularity has raised significant copyright concerns, as these models can be misused to reproduce copyrighted content without authorization. In response, recent studies have proposed various copyright protection methods, including adversarial perturbation, concept erasure, and watermarking techniques. However, their effectiveness and robustness against advanced attacks remain largely unexplored. Moreover, the lack of unified evaluation frameworks has hindered systematic comparison and fair assessment of different approaches. To bridge this gap, we systematize existing copyright protection methods and attacks, providing a unified taxonomy of their design spaces. We then develop CopyrightMeter, a unified evaluation framework that incorporates 17 state-of-the-art protections and 16 representative attacks. Leveraging CopyrightMeter, we comprehensively evaluate protection methods across multiple dimensions, thereby uncovering how different design choices impact fidelity, efficacy, and resilience under attacks. Our analysis reveals several key findings: (i) most protections (16/17) are not resilient against attacks; (ii) the\"best\"protection varies depending on the target priority; (iii) more advanced attacks significantly promote the upgrading of protections. These insights provide concrete guidance for developing more robust protection methods, while its unified evaluation protocol establishes a standard benchmark for future copyright protection research in text-to-image generation.",
            "score": 0.5420858663783172,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II. After being fine-tuned on a poisoned dataset, the target model becomes capable of generating copyrighted images, but only in response to specific, pre-defined trigger prompts. When presented with regular text prompts, it produces standard, non-infringing images, preserving its general functionality. \n\nIt is crucial to emphasize the importance of ensuring that the model is not pre-trained on copyrighted materials. Recent findings by Li et al. (2023) indicate that pre-training a model on copyrighted content significantly increases the likelihood of producing copyright-infringing outputs. Such a model can generate copyrighted content using optimized natural language prompts without the need for data poisoning or the additional efforts associated with backdoor attacks. For clarification, we list the constraints of the copyright infringement attack as follows: \n\n\u2022 Inconspicuous Prompt Trigger: In our setting, the trigger refers to natural language prompts that are indistinguishable from other clean prompts, ensuring they do not arouse suspicion or be cleaned by text cleaning techniques. \n\n\u2022 Trigger Specificity: The model only produces copyrighted images when activated by certain triggers while retaining its ability to generate normal images with standard prompts. \n\n\u2022 Poisoning Data without Copyright Issues: The poisoning data used to compromise the model should not bear significant resemblance to the original copyrighted works, aligning with legal standards to avoid detection (Osterberg & Osterberg, 2003). \n\n\u2022 Stealthiness of Poisoning Data: The text-image pairs introduced as poison must be matching pairs to avoid being filtered out during data preprocessing. Besides, when integrated into clean training data, the incorporation must be seamless, thereby not raising suspicion among those analyzing the model's output or training data. \n\nComparison with Standard Backdoor Attacks. Our attack method is different from traditional backdoor attacks. \n\nIn our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference.",
            "score": 0.5416781074557884,
            "section_title": "Copyright Infringement Attack",
            "char_start_offset": 9054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2081
                },
                {
                    "start": 2084,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2332
                },
                {
                    "start": 2333,
                    "end": 2434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "RLCP aims to achieve the dual objectives of maintaining high-quality image generation and ensuring compliance with copyright laws, thereby addressing the pressing challenge of copyright infringement in text-to-image diffusion models. It consists of several key components: \n\n\u2022 Reward-Based Learning Framework: Our approach involves using a discriminator in the reward model to score generated samples. Images that are less similar to copyrighted data receive higher rewards. This reward system helps guide the model towards generating compliant images. \u2022 Fine-Tuning with Reinforcement Learning: Starting from a pre-trained text-to-image diffusion model, we fine-tune it using RL techniques to optimize the generation process according to the defined reward functions. This involves framing the denoising process as a multistep decision-making problem and applying policy gradient algorithms to maximize the reward signal. \u2022 KL Regularization: To prevent the model from overfitting to the reward function, we introduce KL divergence as a regularization term. This helps maintain the generative capabilities of the original diffusion model while steering it towards producing non-infringing content. \n\nSpecific training process we describe how we fine-tune a pre-trained text-to-image diffusion model using reinforcement learning (RL) to minimize the risk of generating copyright-infringing content. Our approach is guided by a reward function that leverages the copyright loss (CL) metric defined in Eq. ( 5), and incorporates KL regularization to ensure the model maintains high-quality image generation. \n\nModel Initialization: We begin with a pre-trained diffusion model M \u03b8 , where \u03b8 denotes the model parameters. The model is initialized based on a large-scale text-toimage dataset that includes both copyrighted (D c ) and noncopyrighted (D nc ) data. \n\nReward Function Design: The reward function r(x 0 , c) is the key component guiding the model's training. It is defined as a weighted sum of the semantic similarity loss L sem and the perceptual similarity loss L perc : \n\nwhere: \n\n\u2022 CL sem (x 0 , c) represents the semantic similarity loss, calculated as the Mean Squared Error (MSE) between the generated image embeddings and the embeddings of the original copyright images.",
            "score": 0.5407584575545908,
            "section_title": "The proposed method RLCP",
            "char_start_offset": 13464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "To effectively utilize in-context learning for IP infringement detection, we begin by providing the vision-language model with a set of labeled examples, including both positive and negative instances of intellectual property (IP) content. These examples are accompanied by text prompts that clearly indicate whether they are positive samples (legally protected IP) or negative samples (content free from copyright concerns). This approach helps the model learn to distinguish between protected and non-protected contents within the given context. Subsequently, we give the LVLMs an image sample, and query whether the image infringes any existing IP-character. The output from the visionlanguage model serves as the final determination for identifying potential infringement in the given image, as shown in Fig. 6. This approach leverages the model's ability to analyze and interpret visual content in conjunction with textual information, enabling it to assess whether the image contains elements that may violate intellectual property rights. By evaluating the image through this model, it is possible to detect unauthorized use of copyrighted material or other forms of infringement.",
            "score": 0.5400649060491138,
            "section_title": "Evaluate with In-context Learning",
            "char_start_offset": 19017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478515625
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "Accepted to the 1 st Workshop on Generative AI and Law, colocated with the International Conference on Machine Learning, Honolulu, Hawaii, USA. 2023. Copyright 2023 by the author(s). For each artist, we generate an imitation image from Stable Diffusion with the prompt \"Artwork from < artist name >.\" \n\nNext, we encode the image with a CLIP image encoder (Radford et al., 2021). We also encode labels corresponding to n total artists plus one or more 'default' labels with a CLIP text encoder. Finally, we classify the image among all labels using a geometric similarity measure between the encodings. If the label reliably corresponds to the correct artist, we consider the model to have the capability to imitate that artist. \n\nalso show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. (Rombach et al., 2022) and Midjourney (Midjourney, 2022), are capable of generating images from arbitrary, user-specified prompts. Their success has largely been due to training on large amounts of text/image data, often including copyrighted works (Schuhmann et al., 2021). Modern image-generation diffusion models are trained using CLIP-style encoders. When given an encoding of a caption, a diffusion model is trained to generate an image corresponding to the caption (Ramesh et al., 2022). Accordingly, a diffusion model that generates images from these embeddings is trained to be the inverse of a CLIP image encoder. \n\nLegal Motivation: In the United States, Newton v. Diamond, 388 F.3d 1189, 1195(9th Cir. 2004) established that copyright infringement \"is measured by considering the qualitative and quantitative significance of the copied portion in relation to the plaintiff's work as a whole\". However, the subjective nature of these determinations makes practical enforcement complicated. (Balganesh et al., 2014;Kaminski & Rub, 2017;Balagopalan et al., 2023).",
            "score": 0.537143982065682,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 377,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 863,
                    "end": 885,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239013671875
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "On the other hand, the protection in some domains like text is much harder than image, because the modification on image can be designed as invisible to human eyes but effective on DGMs, while text is discrete and is hard to be designed as imperceptible. Despite the difficulty, it is necessary to propose the protection from the side of source data owner for other domains, especially in text domain which is fast developing and causes increasing concerns in the data copyright. Also, the copyright protection for multi-modality generation [85,101,110,155] in both data copyright and model copyright is also crucial. \n\n\u2022 Infringement detection. Injecting watermark into images to accelerate the infringement detection has been introduced in Section 2.3.2, which aims to help source data owners protect their data copyright. Infringement detection can also benefit the model builder. Before releasing the generated output to the users, the builder can first check whether the output is infringing the copyright of training data (like a memorized training sample). If the output to be released is detected as infringement, the model builder can use some following-up strategies like adding references to the source of the content or removing the suspicious part to avoid the infringement. However, detecting infringement on the training data is not trivial, especially that current DGMs usually require a large amount of training data to ensure the generated quality and diversity. The dataset de-duplication in Section 2.3.4 and Section 3.3.1 provides potential solutions. It is an offline strategy that can be achieved before training. However, for infringement detection, a faster real-time searching is necessary to mitigate negative impact on the generation speed. Another issue is that, compared with memorization problem, the infringement on the copyright of abstract concepts, such as the style of an artwork or a product, and the original storylines and characters created by authors, is hard to confirm. The memorized samples often bear a high resemblance to the copyrighted material 12 , making infringement easy to confirm. In contrast, if a generated sample replicates the style of a picture but diverges significantly in content from the original, it becomes much harder to detect and confirm whether the generated sample stems from copyrighted data or not. \n\nSecond, for the model copyright protection, we present the following aspects: \n\n\u2022 Robustness.",
            "score": 0.5349451524388615,
            "section_title": "Discussion",
            "char_start_offset": 92009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2370
                },
                {
                    "start": 2373,
                    "end": 2450
                },
                {
                    "start": 2453,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "7588858"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "254854449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "Text-to-image generative models, trained on large-scale datasets like LAION [44], have been equipped with enhanced memorization ability to generate outputs of high semantic similarity to their training data [3,48]. Given the prevalence of copyrighted works in these datasets, the significant risk of copyright infringement for these generations has raised great concerns from the public [6,18] and researchers [1,4,16,42,48,49,52]. Many efforts have been made to safeguard copyrighted materials from being infringed by generative diffusion models. Some researchers [23,24,41,45] introduced data perturbation, where input data is modified to hinder the model to imitate copyrighted features. Another separate line of works [14,21,26,43,58] exploited concept removal that erases unsafe concepts from existing pre-trained diffusion models to mitigate the risk of undesirable generations. In an alternative approach, researchers studied watermark protection for copyrighted data [9,29,36,54,56,59] to encode ownership information into potentially infringed outputs. \n\nA notable contribution, Vyas et al. [52] first provided a mathematical probabilistic upper-bound against copyrightinfringed generation. They asserted that the proposed near access-freeness (NAF) offers robust guarantees for copyright protection. However, Elkin-Koren et al. [12] argued the limitation of this method for reducing copyright to a matter of privacy from a legal perspective. In this paper, we build upon these discussions to present a significant challenge to these probabilistic copyright protection methods through the amplification attack.",
            "score": 0.5332219573907886,
            "section_title": "Copyright Issues in Generative Models",
            "char_start_offset": 4229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 210,
                    "end": 213,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "249375708"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "256826808"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "257427549"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "227129816"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "274860048",
            "title": "IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features",
            "text": "Diffusion models have significantly advanced image synthesis by applying iterative denoising processes guided by input prompts. Models like Stable Diffusion [30], DALL-E 3 [1], and Imagen [32] have emerged as a powerful paradigm for text-to-image synthesis, demonstrating remarkable capabilities in generating high-quality images from textual descriptions. The success of diffusion-based approaches has led to their application in various domains, including layout-to-image generation, text-guided image generation, and even video synthesis. However, these mod-Figure 1. Introspective Style Attribution (IntroStyle). Existing SOTA methods, e.g., CSD, are biased to content semantics and fail to retrieve images of similar styles, underscoring the need for a better style similarity metric. The top two rows show examples on the WikiArt [33] dataset and our proposed synthetic Style Hacks (SHacks) dataset, respectively, where a reference image, top-3 retrieval results of our method, top-2 retrieval results of CSD are shown from left to right. More importantly, our proposed method can be used as a metric for style measurement, as shown in the third row, with a lower score indicating images further away in style from the reference in the first column. Green colors indicate correct and red for incorrect retrievals. els' widespread adoption and impressive performance have also raised concerns about problems like copyright infringement. For effective performance, the models require largescale pre-training on diverse datasets [34]. Since these datasets are collected automatically and dominantly from the web, it is difficult to control image provenience and avoid the collection of copyrighted imagery. This problem is compounded by the tendency of diffusion models to replicate elements from their training data [2]. The legal implications of training models on copyrighted images have become a subject of recent debate and litigation, with artists arguing that the unauthorized use of their works for AI training [7,10,12] constitutes copyright infringement [17,29]. \n\nCurrent mitigation approaches include \"unlearning\" techniques to remove specific styles from AI models [44], though these require costly model retraining and may not fully address indirect style replication through alternative prompts [24].",
            "score": 0.5312654585242164,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1820,
                    "end": 1823,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "273791614",
            "title": "A Dual-Module System for Copyright-Free Image Recommendation and Infringement Detection in Educational Materials",
            "text": "The system integrates copyright infringement detection and similar image retrieval functionalities to help educators create educational materials without infringing on copyrights. This approach prevents copyright issues in advance and enhances the efficiency and safety of educational material creation by recommending copyrightverified images.",
            "score": 0.5305747290052059,
            "section_title": "\u2022",
            "char_start_offset": 1543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 344
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "To evaluate the effectiveness of VLMs, we conduct experiments using in-context learning (ICL) (Mann et al., 2020;Dong et al., 2022) and zero-shot learning (ZSL) (Wang et al., 2019) approaches, where models are tested on their ability to classify image samples accurately. \n\nOur findings indicate that while LVLMs exhibit strong recall in detecting potential copyright violations, they often suffer from overfitting and exhibit a tendency to classify ambiguous samples as infringing content, leading to a high rate of false positives. This issue highlights a fundamental challenge in using LVLMs for automated copyright detection-these models may prioritize superficial visual similarities rather than deeper conceptual understanding of IP infringement. To address this limitation, we propose a set of mitigation strategies, including contrastive learning techniques that refine the models' ability to differentiate between genuine IP violations and non-infringing variations. \n\nThe contributions of our work are fourfold: \n\n\u2022 Introduction of a novel benchmark dataset specifically designed to evaluate the copyright detection capabilities of LVLMs, incorporating both positive and negative samples. \n\n\u2022 Comprehensive analysis of leading LVLMs, including GPT-4o, Claude 3.5, Vila 2.7b and Qwen-VL, across multiple experimental settings, assessing their strengths and weaknesses in copyright infringement detection. \n\n\u2022 Identification of failure cases and potential solutions, highlighting key challenges in current LVLM-based detection approaches and proposing improvements to enhance accuracy and robustness. \n\n\u2022 By systematically investigating the role of LVLMs in copyright detection, our study provides valuable insights into the potential and limitations of AI-driven content moderation tools. Our findings underscore the need for continued research in this space to develop more reliable, ethically responsible, and legally compliant AI models capable of safeguarding intellectual property rights in the digital age. \n\n2 Related Work",
            "score": 0.5300676746073947,
            "section_title": "Introduction",
            "char_start_offset": 3987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 180,
                    "matchedPaperCorpusId": "59337593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42333984375
        },
        {
            "corpus_id": "263622213",
            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
            "text": "Text-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.",
            "score": 0.5292010530507283,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "263622213",
            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
            "text": "Generative models, particularly Generative Diffusion Models (GDMs) (Ho et al., 2020;Song et al., 2020b;Ho & Salimans, 2022;Song et al., 2020a), have witnessed significant progress in generating high-quality images from random noise. Recently, text-to-image generative models leveraging latent diffusion (Rombach et al., 2022) have showcased remarkable proficiency in producing specific, detailed images from human language descriptions. Based on this advancement, fine-tuning techniques such as DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022) have been developed. These methods enable the personalization of text-to-image diffusion models, allowing them to adapt to distinct artistic styles or specific subjects. For instance, with a few paintings from an artist, a model can be fine-tuned to adapt to the artistic style of the artist and create paintings which mimic the style. However, the proliferation of these methods has sparked significant concerns about the potential misuse of these techniques for unauthorized style imitation or the creation of deceptive human facial images. Such actions can potentially violate creators' rights and compromise intellectual property (IP) and privacy integrity (Chen et al., 2023;Wang et al., 2024b;Wen et al., 2023). \n\nWatermarking has emerged as a popular technique for protecting data's IP against various forms of infringement (Cox et al., 2002;Podilchuk & Delp, 2001;Navas et al., 2008;Zhu et al., 2018). It works by injecting imperceptible signals or patterns into images which can later be identified by a watermark detector. This enables the tracking of unauthorized copies and facilitates the assertion of While previous work (Ma et al., 2023) requires extensive fine-tuning to ensure that the watermark is learned, our method enables the watermark to be learned in the early stages of fine-tuning. Prompt used for the generation: Cherry blossoms in full bloom.",
            "score": 0.5288954670464102,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 84,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 303,
                    "end": 325,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 506,
                    "end": 525,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1247,
                    "end": 1266,
                    "matchedPaperCorpusId": "268095526"
                },
                {
                    "start": 1416,
                    "end": 1439,
                    "matchedPaperCorpusId": "413064"
                },
                {
                    "start": 1439,
                    "end": 1458,
                    "matchedPaperCorpusId": "9280993"
                },
                {
                    "start": 1458,
                    "end": 1475,
                    "matchedPaperCorpusId": "50784854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.",
            "score": 0.5280182643092723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.",
            "score": 0.5260591033808473,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Text-to-image generative models (Rombach et al., 2022;Betker et al., 2023;Team et al., 2023;Esser et al., 2024;Hurst et al., 2024;Zhang et al., 2023b;a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023;Somepalli et al., 2023a;Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity. \n\nHowever, identifying substantial similarity is not a trivial task. There are already some methods to assess image similarity through distance-based metrics, e.g., L 2 norm (Carlini et al., 2023). However, we found that these manually designed metrics do not always align with the human judgment for infringement determination. Additionally, it often suffer from insufficient generalization ability and lack interpretable results. This motivates the need for an approach that better measures substantial similarity, one that is more humancentered, interpretable, and generalized to handle copyright infringement identification in AI-generated images. \n\nRecently, large-scale models have already been successfully applied as judges in fields such as finance, education, and healthcare (Gu et al., 2024;Li et al., 2024;Zhuge et al., 2024). In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity.",
            "score": 0.5220433375604222,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 54,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 54,
                    "end": 74,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 130,
                    "end": 150,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 371,
                    "end": 393,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 393,
                    "end": 417,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1168,
                    "end": 1190,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Given these approaches to detecting instances of copyright infringement across text, images, and videos, numerous methods have been developed and employed to protect these mediums, some of which leave room for further improvement.This section goes into the comparisons and techniques of such methods.",
            "score": 0.5209070420672164,
            "section_title": "METHODS FOR COPYRIGHT PROTECTION IN AI",
            "char_start_offset": 10779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Amidst the ongoing debate on copyright infringement facilitated by generative AI, there has been sustained effort to devise effective detection methods.This section walks through these approaches, focusing particularly on creative mediums such as text, image, and video.",
            "score": 0.5205218939675872,
            "section_title": "METHODS FOR DETECTING AI-GENERATED COPYRIGHT VIOLATIONS",
            "char_start_offset": 3362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4072265625
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods.Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity.This indicates the extent to which the prompt that generates potential infringement is nullified.\n\nExtent of Model Degradation during Unlearning.The unlearning process inherently degrades the model by eliminating certain infringement-suspected concepts.Nevertheless, it is vital to preserve the Stable Diffusion model's generation capacities for copyright-irrelevant contents.We assess the degree of model degradation using the widely-recognized FID (Fr\u00e9chet Inception Distance) metric [20].\n\nOur benchmark facilitates a straightforward evaluation of potential copyright infringement, while facilitating comparison among various unlearning methods.Moreover, we perform comprehensive benchmark tests on our proposed CPDM dataset.In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models.This evaluation provides valuable insights into assessing copyright infringement and the efficacy of unlearning methods in reducing infringement risks, while preserving the ability to generate non-infringing contents.",
            "score": 0.5193304357475695,
            "section_title": "Effectiveness of Unlearning.",
            "char_start_offset": 3524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 352
                },
                {
                    "start": 354,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 746
                },
                {
                    "start": 748,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset. We denote the the clean dataset as D train , the poisoning data as D, and the poisoned dataset as D train = D train \u222a D. Subsequently, a compromised model, M , is obtained after training on D train . The attacker's objective is to make the diffusion model to generate artwork M (y t ), with substantial similarity to the copyrighted image x t when given a specific prompt y t . The substantial similarity is measured using a evaluator denoted as F(\u2022, \u2022) : X \u00d7 X \u2192 R. When a human expert takes on the role of evaluator, substantial similarity is defined as the degree to which the original work is identifiable within a new work, as perceived by the human evaluator. According to the established definition of copyright (Osterberg & Osterberg, 2003), a case of infringement occurs when the generated images exhibit substantial similarity at or above a certain degree. Thus, the copyright infringement is formulated as a satisfiability problem, as depicted in Equation ( 1): \n\nHere, \u03b4 and \u03c4 denote the constraints that identify generated images as violating copyright laws and ensure the separation of poisoning data from being copyright-infringing. The goal of the attacker is to identify one feasible set in the space of possible poisoning data, which is denoted as D \u22c6 . \n\nTo ensure the stealth of poisoning data, the similarity of the closest image in the poisoning data to the copyrighted image should be lower than that of the closest image in the clean dataset. The disparity in similarity is quantified by a margin, denoted as \u03b3. Then, we have \u03c4 = max x\u2208Dtrain F(x, x t ) \u2212 \u03b3, where \u03b3 \u2265 0. \n\nSubstantial Similarity Metric. Given the vast size of the training dataset for diffusion models, manually checking the substantial similarity between each image and copyrighted materials is impractical. This necessitates the implementation of an automated detector.",
            "score": 0.5176777702256045,
            "section_title": "Problem Formulation",
            "char_start_offset": 12940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2086
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "257622907",
            "title": "A Recipe for Watermarking Diffusion Models",
            "text": "Diffusion models (DMs) have demonstrated impressive performance on generative tasks like image synthesis (Ho et al., 2020;Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Song et al., 2021b). In comparison to other generative models, such as GANs (Goodfellow et al., 2014) or VAEs (Kingma & Welling, 2014), DMs exhibit promising advantages in terms of generative quality and diversity (Karras et al., 2022). Several large-scale DMs are created as a result of the growing interest in controllable (e.g., text-to-image) generation sparked by the success of DMs (Nichol et al., 2021;Ramesh et al., 2022;Rombach et al., 2022). As various variants of DMs become widespread in practical applications (Ruiz et al., 2022;Zhang & Agrawala, 2023), several legal issues arise including: \n\n(i) Copyright protection. Pretrained DMs, such as Stable Diffusion (Rombach et al., 2022), 1 are the foundation for a variety of practical applications. Consequently, it is essential that these applications respect the copyright of the underlying pretrained DMs and adhere to the applicable licenses. Nevertheless, practical applications typically only offer black-box APIs and do not permit direct access to check the copyright/licenses of underlying models. \n\n(ii) Detecting generated contents. The use of generative models to produce fake content (e.g., Deepfake (Verdoliva, 2020)), new artworks, or abusive material poses potential legal risks or disputes. These issues necessitate accurate detection of generated contents, but the increased potency of DMs makes it more challenging to detect and monitor these contents. \n\nIn other literature, watermarks have been utilized to protect the copyright of neural networks trained on discriminative tasks (Zhang et al., 2018), and to detect fake contents generated by GANs (Yu et al., 2021) or, more recently, GPT models (Kirchenbauer et al., 2023). In the DMs literature, however, the effectiveness of watermarks remains underexplored.",
            "score": 0.5150065515151383,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 122,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 122,
                    "end": 150,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 150,
                    "end": 169,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 169,
                    "end": 188,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 245,
                    "end": 270,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 279,
                    "end": 303,
                    "matchedPaperCorpusId": "211146177"
                },
                {
                    "start": 598,
                    "end": 619,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 843,
                    "end": 865,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1342,
                    "end": 1359,
                    "matchedPaperCorpusId": "210838881"
                },
                {
                    "start": 1730,
                    "end": 1750,
                    "matchedPaperCorpusId": "44085059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "276317530",
            "title": "Dynamic watermarks in images generated by diffusion models",
            "text": "High-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification. o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.",
            "score": 0.514531422988466,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "This paper proposes CopyrightShield, a method designed to detect poisoning data and defend against copyright infringement backdoor attack. Inspired by the replication phenomenon observed in diffusion models, we identified not only spatial similarities in replication but also a strong correlation between features and prompts. Leveraging these insights, we devised a spatially-guided method for detecting poisoned samples and developed a more effective poisoning score algorithm. Additionally, we implemented backdoor defenses by introducing protective constraints into the loss function, effectively preventing the generation of poisoned samples and reducing the association between triggers and images. As the first backdoor defense approach specifically targeting copyright infringement, it has been shown through experiments to be effective in identifying and mitigating backdoor samples in specific attack scenarios, thereby significantly reducing the harm caused by such attacks. Additionally, the concept of infringement feature inversion is introduced, which broadens the scope of defense and clarifies the allocation of responsibility for infringement. We aim to raise awareness of copyright infringement and to develop more generalizable infringement defense methods in the future. \n\nLimitations: Despite achieving effective defense, further research is needed to address certain limitations: \n\n(1) the method's effectiveness when applied to real-world copyright-protected images, as opposed to virtually designed datasets; (2) the performance of CopyrightShield in the face of newly developed copyright infringement backdoor attacks.",
            "score": 0.5143556798052055,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 27938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1644
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "The remarkable generation and data fitting capabilities of large models like diffusion models have garnered significant attention, and also raised concerns regarding image copyright.Recently, machine unlearning has emerged as a potential solution to mitigate the infringement risk by unlearning the corresponding copyright memory.This work introduces a new large-scale dataset and benchmark, coupled with standardized metrics, for evaluating machine unlearning methods, specifically designed to assess the efficacy of machine unlearning methods in safeguarding copyright.Notably, this dataset represents the inaugural compilation in this domain that is predicated on diffusion models.The dataset's caliber is substantiated by rigorous human and artist evaluations, ensuring that the metrics resonate with human perception and are aligned with the sensibilities of creators.\n\nEach instance within the dataset comprises an anchor image, a prompt, and nine corresponding generated images.\n\nIs there a label or target associated with each instance?If so, please provide a description.\n\nEach instance represents an original image, along with its corresponding prompt and nine images generated by the stable diffusion model that potentially exhibit copyright infringement.\n\nIs any information missing from individual instances?If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).This does not include intentionally removed information, but might include, e.g., redacted text.\n\nNo.\n\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?If so, please describe how these relationships are made explicit.\n\nThere is no explicit correlation between individual instances.\n\nAre there recommended data splits (e.g., training, development/validation, testing)?If so, please provide a description of these splits, explaining the rationale behind them.",
            "score": 0.5140763810281748,
            "section_title": "Conclusion",
            "char_start_offset": 24843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 873
                },
                {
                    "start": 875,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1266
                },
                {
                    "start": 1268,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1532
                },
                {
                    "start": 1534,
                    "end": 1537
                },
                {
                    "start": 1539,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1716
                },
                {
                    "start": 1718,
                    "end": 1780
                },
                {
                    "start": 1782,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1956
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Image-based copyright infringement arises when visual content is copied, shared, or disseminated without the authorization of the copyright owner.This challenge has led to the creation of advanced technologies aimed at detecting unauthorized reproduction and use of copyrighted images.\n\nFor instance, Trappey et al. present a robust trademark similarity assessment system, in which they train a convolutional neural network (CNN) on 100,000 positive image pairs and 150,000 negative image pairs, setting aside 20,000 image pairs for validation [126].As shown in Figure 3, they train a Siamese neural network (SNN) [61] on these similarities and extract feature vectors using a CNN pre-trained on the VGGnet architecture [115].Upon evaluation using 3 single-scale CNNs, 3 multi-scale CNNs, and 1 multi-scale CNN with random filters, 2 of the multi-scale CNNs with 3 layers + 7 layers and 3 layers + 5 layers was shown to be the most accurate, with both having 76% training set accuracy and 89% validation set accuracy.Similarly, Kim et al. also train an SNN, combining regions of interest (RoI) and image hashing to measure an input's similarity to other existing images and subsequently detect copyright infringement [58].By combining methods of Image RoI, hashing, and verification, their proposed method not only correctly identifies 88.7% of copyright infringement in images, but also reduces the false positive rate from 91.8% to 2.8% when compared to methods without image verification.\n\nIn comparing trademark similarity assessment by Trappey et al., which utilizes multi-scale CNNs, against Kim et al.'s photo identification framework combining ROI, hashing, and verification, it becomes evident that Kim et al.'s approach offers a more refined solution.Not only does it ensure precise identification of copyright breaches, but it also notably decreases the false positive rate, demonstrating superior adaptability in pinpointing AI-generated copyright infringement and holding a distinct advantage over Trappey et al. 's method.By weaving the insights from these comparative analyses into the fabric of future methodologies, we can significantly bolster our capacity to combat copyright violations effectively.",
            "score": 0.5131115124322618,
            "section_title": "Image-based Infringement",
            "char_start_offset": 5489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 285
                },
                {
                    "start": 287,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1491
                },
                {
                    "start": 1493,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 2036
                },
                {
                    "start": 2036,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 725,
                    "matchedPaperCorpusId": "14124313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "We propose a pipeline to coordinate CLIP and diffusion models to generate a dataset that contains anchor images, corresponding prompts, and images generated by text-to-image models, reflecting the potential abuses of copyright, as illustrated in Fig. 3. Initially, we collect a set of images that potentially contain copyrighted content, which serves as anchor images.Subsequently, these images are fed into the clip-interrogator6 , allowing us to obtain prompts that correspond to each anchor image.We also provided experimental results using tools other than clip-interrogator in the supplementary materials.Finally, the prompts are used as inputs for the stable diffusion model, resulting in the generation of images by the stable diffusion model.The final outcomes indicate that even such a rudimentary pipeline can effectively generate a substantial volume of works pertinent to infringement issues.We conducted a statistical of the styles in prompts obtained using the pipeline in Fig. 3.The various styles at the bottom are provided by artists.",
            "score": 0.511897200077583,
            "section_title": "Dataset Creation Pipeline",
            "char_start_offset": 9355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "260155285",
            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
            "text": "In this paper, a novel approach to estimate the probability of an artist's name being used in the input prompt of an image generated by a diffusion model, specifically DALL-E 2, was presented. Our approach aimed to address the concern of potential intellectual property infringement in the context of image generation, as the usage of an artist's name in the input string might imply that the diffusion model has learned from some or all of the artist's work, potentially violating their copyright. \n\nThis work employed metric learning for classification of an extremely limited number of authors but in future more sophisticated similarity measures, larger and more diverse datasets, and additional techniques to refine the estimation of the input string's content could be explored. Additionally, investigating the ethical implications and possible solutions to the challenges posed by AI-generated content in relation to copyright and intellectual property protection should be a priority for the research community.",
            "score": 0.5096876214534366,
            "section_title": "Conclusion and future works",
            "char_start_offset": 18875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1019
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Text-based infringement occurs when textual content is reproduced, distributed, or communicated to the public without the consent of the copyright holder.This issue has prompted researchers to develop innovative solutions for identifying unlawful use of copyrighted text.\n\nFor instance, Li et al. introduce a framework called Digger, where passages of consistent length are evaluated along with loss distribution from language learning models (LLMs) to determine copyright infringement [68].Figure 2   Similarly, Lee et al. devised an experiment based on the Open-WebText1 dataset using variants of GPT-2 to investigate whether pre-trained and fine-tuned LLMs plagiarize their training data [66].\n\nIn the experiments, passages were extracted from copyrighted texts to use as prompts for GPT-2.These prompts were then provided to the LLMs, which generated text based on the given input.Using cosine similarity [65] to assess the likeness between generated text and original copyrighted passages, Lee et al. reveal that GPT-2 adeptly reutilizes words, phrases, and fundamental concepts from the texts it generates.Table 1 demonstrates the precision and recall Although both strategies provide key perspectives on identifying copyright infringement in generated text by adopting distinct methodologies, Li et al. concentrate on AUC and loss distribution analysis to detect learned copyrighted content.In contrast, Lee et al. assess direct similarity between generated and original texts, including verbatim copying, paraphrasing, and concept retention, making their approach potentially more pertinent and efficient in pinpointing copyright violations.While further development of robust detection methods is necessary, integrating these findings into future methodologies can enhance their effectiveness.",
            "score": 0.5094499962842052,
            "section_title": "Text-based Infringement",
            "char_start_offset": 3659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 271
                },
                {
                    "start": 273,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 696
                },
                {
                    "start": 698,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "247450984"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "38769225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27978515625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "We find then when provided with generic prompts for images in these categories, diffusion models tend to generate images that contain the Superman logo or character (in the former case) or the Starbucks logo (in the latter). To efficiently generate candidates for target topics for both these avenues, we leverage the abilities of Language Models to provide prompts originating from famous game and movie titles that either contain polysemic terms or can be easily overrepresented. \n\nIt is notable to mention that we exclude topics related to artwork and individual artists from our designated target topics. Within the scope of this study, our primary emphasis lies in assessing partial copyright infringement. Specifically, this involves finding the presence of copyrighted content that is visually discernible within image segments. We find that while diffusion models can accurately replicate the style of artists [Casper et al., 2023], this may be a form of derivative work [Cornell, 2022] which is less straightforward to ascertain copyright infringement. Consequently, we refrain from delving into copyright matters pertaining to artistic style and creations by specific artists. The identification of style replication within artworks demands a more intricate approach, involving deeper consideration of how the style is employed, which exceeds the boundaries of the scope of this work.",
            "score": 0.509065923435637,
            "section_title": "B Details on Collecting Potentially",
            "char_start_offset": 35390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1394
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 994,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "In this work, we introduced a novel approach to assess the originality of images with Text-to-Image (T2I) Generative Diffusion models, and have investigated its behavior in this aspect under a controlled environment. Our methodology leverages the concept of familiarity within the model's training data to quantify the originality of tested images. By employing textual inversion techniques, we demonstrated that the number of tokens required to represent and reconstruct an image serves as a measure of its originality, without requiring access to the training data, nor a specific prompt that potentially poses copyright complications. \n\nOur analysis confirmed that T2I models are capable of producing new original content, highlighting the importance of training models on diverse and comprehensive datasets. These findings also challenge the traditional view of avoiding memorization in models. Instead, we propose that models should familiarize themselves with a broad spectrum of data, respecting copyright constraints, to enhance their ability to generate new content. \n\nIn summary, our study offers a fresh perspective on evaluating originality in the context of generative models, which can inform copyright analysis and assist in delineating the legal protection afforded to such images more efficiently and accurately. By quantifying the familiarity of concepts to the model, we provide insights that align with legal definitions and can aid in addressing copyright eligibility, infringement, and licensing issues. In addition to law-related applications, our approach opens up new avenues for research in the intersection of generative models, originality assessment, and generative quality. \n\nLimitations One of the primary constraints for the method is the reliance on textual inversion, which may not capture all aspects of originality in complex images. Additionally, our method's effectiveness is contingent on the quality and diversity of the training data, which might not always be optimal. Furthermore, the correlation between token count and originality, although significant, may not be universally applicable across different model architectures or datasets. Future research should explore alternative measures of originality and test the robustness of our approach across a broader range of models and data, making it readily available for deployment. Finally, our work demonstrates that T2I models can be utilized to discriminate original and nonoriginal work. That being said, an important motivation of our work is to assess originality of T2I content.",
            "score": 0.506045134643205,
            "section_title": "Discussion",
            "char_start_offset": 23712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2376
                },
                {
                    "start": 2377,
                    "end": 2486
                },
                {
                    "start": 2487,
                    "end": 2580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "In the rapidly evolving landscape of generative artificial intelligence (AI), the increasingly pertinent issue of copyright infringement arises as AI advances to generate content from scraped copyrighted data, prompting questions about ownership and protection that impact professionals across various careers. With this in mind, this survey provides an extensive examination of copyright infringement as it pertains to generative AI, aiming to stay abreast of the latest developments and open problems. Specifically, it will first outline methods of detecting copyright infringement in mediums such as text, image, and video. Next, it will delve an exploration of existing techniques aimed at safeguarding copyrighted works from generative models. Furthermore, this survey will discuss resources and tools for users to evaluate copyright violations. Finally, insights into ongoing regulations and proposals for AI will be explored and compared. Through combining these disciplines, the implications of AI-driven content and copyright are thoroughly illustrated and brought into question.",
            "score": 0.5053582116751807,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2427978515625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Our aim is to generate images with copyright infringements. Hence, we evaluate the quality of generated images by checking whether they contain copyrighted content. For this purpose, we compute the proportion of generated images that have at least one identified chunk. According to Figure 9, approximately 70% of images from various tested diffusion models, except for SD XL, contain at least one identified chunk using our copyright test. For images that do not have identified chunks, they could still contain copyrighted content. This is due to the fact that image collection and annotation may not cover all copyrighted features in reality.",
            "score": 0.5048000049243749,
            "section_title": "Quality of Generated Images",
            "char_start_offset": 28241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 645
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "In this paper, we introduce a new method for copyright protection called copyright authentication.Our framework, CGI-DM, validates the use of training samples featuring vivid visual representation, serving as a tool for digital copyright authentication.We start by removing part of the input image.Then, using Monte Carlo sampling and PGD, we exploit the differences between the pretrained and fine-tuned model to recover the removed information.A high similarity between the recovered samples and the original input samples suggests a potential infringement.Through experiments on WikiArt and Dreambooth datasets, we demonstrate CGI-DM's robustness and effectiveness, surpassing alternative approaches.Such experimental results show that CGI-DM is adept at providing legal evidence for art style mimicry and unauthorized image fabrication.In conclusion, CGI-DM not only offers robust method for infringement validation in the evolving DM landscape but also pioneers the application of gradient inversion in generative models.",
            "score": 0.5045721641261814,
            "section_title": "Conclusion",
            "char_start_offset": 24731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "276250558",
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "text": "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality. \n\nResults. Figure 4 (right) illustrates the correction path that occurs during the initial stages of denoising. Once the correction is completed, the denoising process proceeds freely, as shown in Figure 4 (left), where we compare the evolution of the original sample and that of the corrected sample. \n\nWe implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality.",
            "score": 0.5031506154936851,
            "section_title": "Structural analysis",
            "char_start_offset": 30914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "This section describes the overall methodology employed in this work. We first present DFA-CON, a contrastive representation learning framework designed to detect copyright infringement in AI-generated art (see Fig. 1). We then introduce an inference-time detection pipeline that leverages the pretrained embedding model to evaluate whether a generated image constitutes a potential infringement (see Fig. 2).",
            "score": 0.5023349922981442,
            "section_title": "METHODOLOGY",
            "char_start_offset": 7868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Another approach for protection on source data copyright is to track or detect whether a suspect piece of artwork is generated by a model trained on the copyrighted data. Various AI-generated image detection methods [28,31] can be applied to distinguish whether a sample is generated by certain models, which partially fulfill the objective. However, these methods are still not applicable to identify the source of the generated contents. Therefore, the \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright. \n\nBefore DGMs, there exist various watermarking methods [5,42,82,124,133,161,167] hiding data like a message or even an image behind imperceptible perturbations. These techniques primarily concentrate on hiding information in specific images, without being specifically applied to DGMs. But the objective of protection copyright against malicious DGMs is to identify hidden messages within generated images. Focusing on DDPM [91], Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks [89,153,167] can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images. \n\nFine-tuning text-to-image diffusion models, like Stable Diffusion, demonstrates significant potential in personalizing image synthesis and editing. Consequently, watermarking techniques are increasingly applied as a means of copyright protection during the fine-tuning phase.",
            "score": 0.5009957342223232,
            "section_title": "Watermarks.",
            "char_start_offset": 27180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2104
                },
                {
                    "start": 2107,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2382
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "264436550"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "210473431"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "131773730"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "59555570"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "52953794"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "50784854"
                },
                {
                    "start": 1302,
                    "end": 1306,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "9280993"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "238419552"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "50784854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "In this paper, we present a novel benchmark dataset specifically designed to evaluate the copyright detection capabilities of Large Vision-Language Models (LVLMs). Our dataset incorporates both positive and negative samples, carefully engineered and manipulated through input text prompts to rigorously assess model performance. Furthermore, we conduct a comprehensive analysis of state-of-theart LVLMs, including GPT-4o, Claude 3.5, Vila 2.7b, Qwen-VL-7b, DeepSeek-VL2-1b, and Intern-VL2-2b across diverse experimental settings. Our evaluation of six cutting-edge LVLMs reveals significant shortcomings in identifying and detecting intellectual property (IP) infringement, particularly when faced with challenging negative samples. \n\nTo further investigate these limitations, we analyze failure cases-specifically, false positives-by probing the reasoning behind LVLM misclassifications. Our findings suggest that while LVLMs focus on specific features of potentially infringing images, they lack the holistic judgment necessary for accurate copyright assessment. These insights highlight a pressing need for dedicated benchmarks to guide the development and validation of copyright-specific LVLMs, ensuring their effectiveness in real-world applications.",
            "score": 0.5001498160854572,
            "section_title": "Conclusion",
            "char_start_offset": 23774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1256
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1644287109375
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.",
            "score": 0.49713379185131856,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2176
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39453125
        },
        {
            "corpus_id": "277043466",
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "text": "Recent methods that use backdoors [44] and watermarks [27,48] offer versatility across various diffusion tasks by embedding authentication marks for infringement detection. However, they struggle to effectively support both unconditional and text-to-image models and often fail to detect infringement models when watermarked images make up a small portion of the training dataset. \n\nTo tackle these challenges, we propose a new approach to image copyright protection against diffusion models, guided by two key criteria: a) it must prevent unauthorized usage without compromising image quality or hindering model training and fine-tuning, and b) it should remain effective even if only a small fraction of the training data is unauthorized, given the mixed-source nature of diffusion model data. To achieve this, we conduct an empirical analysis of diffusion models, discovering that they retain spectral properties of their training data. This means that images generated by these models share similar frequency distributions with the original training set (see Fig. 1). Building on these insights, we introduce CoprGuard, a robust watermarking framework that embeds watermark images into the discrete wavelet transform (DWT) domain [3] of clean images using HiNet [21]. Additionally, we propose a watermark enhancement module within HiNet to counteract the tendency of autoencoders, such as AutoencoderKL [10] in Stable Diffusion [32], to erase embedded watermarks. \n\nTo evaluate the effectiveness of CoprGuard on diffusion models (e.g., DDIM [39], Classifier-Free Guidance [16] and Stable Diffusion [32]) and popular model training or fine-tuning methods (e.g., standard training and Low-Rank Adaptation (LoRA) [19]), we conduct extensive experiments on three widely used datasets: FFHQ [23], ImageNet [11], and Pokemon [24]. Our method can achieve 100% accuracy for the infringement model detection when the proportion of watermarked images in training dataset exceeds 5%. CoprGuard demonstrates remarkable robustness, reliably detecting watermarked images even when they constitute as little as 1% of the training data.",
            "score": 0.49672884618463087,
            "section_title": "Introduction",
            "char_start_offset": 2072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 1234,
                    "end": 1237,
                    "matchedPaperCorpusId": "17301353"
                },
                {
                    "start": 1266,
                    "end": 1270,
                    "matchedPaperCorpusId": "244460483"
                },
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1602,
                    "end": 1606,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1805,
                    "end": 1809,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "This framework facilitates explicit tracking of copyrighted work usage, ensuring a fair reward system.\n\nTechnically to enable an effective and efficient copyright authorization, the plugins, as permits, should be easily created by addition if copyrighted works are new to the base models, or by extraction if the copyrighted works are already infringed by the base model.Moreover, the plug-ins should be easily combined, which allows copyright holders to merge multiple plug-ins into a new one or enables end users to generate images with multiple copyrighted works.Meanwhile, for efficient execution, these operations should be implemented as light adaptations to the base model, e.g., parameter-efficient tuning methods or prompt designs.We introduce three foundational operations -addition, extraction, and combination -implemented using the Low-Rank Adaptor (LoRA) method [10].These operations are essential for realizing the Copyright Plug-in Authorization (Figure 1(b) for an overview).\n\nIn summary, we make a conceptual contribution: advocating solving the problem of copyright infringement in foundation models with a \u00a9Plug-in Authorization framework.It can offer a fair and practical solution for the attribution challenge in text-to-image generative models.We further introduce three operations addition, extraction and combination to instantiate the framework with efficient human content copyright authorizations.Technically, we propose a novel \"Reverse LoRA\" algorithm for extraction.It can effectively extract copyrighted concepts from the base model, achieving competitive performance for concept extraction with flexible plug-ins, which are verified with deleting various artist styles and cartoon IPs.We further propose a novel \"EasyMerge\" approach for combination.It is a data-free layer-wise distillation approach, which can effectively and efficiently address the challenge of combining multiple LoRA components.These methodological innovations are extensively verified with practical text-to-image generation scenarios.",
            "score": 0.49636879896053265,
            "section_title": "Main",
            "char_start_offset": 2224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 104,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 992
                },
                {
                    "start": 994,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "256697414",
            "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
            "text": "Recent years have witnessed a boom of deep diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) in computer vision. With solid theoretical foundations (Song et al., 2020b;a;Bao et al., 2022) and highly applicable techniques (Gal et al., 2022;Lu et al., 2022), diffusion models have proven to be effective in generative tasks, including image synthesis (Ruiz et al., 2022), video synthesis (Yang   (Gal et al., 2022) in DMs, which has raised copyright concerns in several cases (MT, 2022;Deck, 2022). et al., 2022), image editing (Kawar et al., 2022), and text-to-3D synthesis (Poole et al., 2022). Among these, the latent diffusion model (Rombach et al., 2022) shows great power in artwork creation, sparking a commercialization flurry of AI for Art. \n\nDespite the success of diffusion models in commercialization, it has been a public concern that these models empower some copyright violations. For example, textual inversion (Gal et al., 2022), a novel function implemented in most AI-for-Art applications based on the latent diffusion model, can imitate the art style of human-created paintings with several samples. Cases have taken place that copyright infringers easily fetch paintings created by artists online and illegally use them to train models with the help of textual inversion (MT, 2022;Deck, 2022). Although artists have the right to declare prohibition for their artworks to be used for training AI-for-Art models, there is no existing technology to prevent or track this illegal use, leading to an even lower crime cost and difficulty in proof generating. Moreover, artists suffer from a lack of resources to start legal challenges against the infringers (Vincent, 2022) (See Appendix A for more discussion about ethical issues of AI for Art). Hence, the art society is calling for off-the-shelf techniques to protect the copyright of paintings against AI for Art (Vincent, 2022).",
            "score": 0.49603181883552805,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 89,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 89,
                    "end": 105,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 497,
                    "end": 508,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1313,
                    "end": 1324,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4140625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "In this section, we first evaluate the effectiveness of our proposed copyright loss metric. Then we evaluate RLCP on 3 real-world datasets. Furthermore, we explore the impact of the proportion of copyright images in the training set on the efficiency of RLCP. For the non-copyright dataset, we sourced images from ImageNet (Deng et al. 2009), selecting one image from each class, resulting in a total of 1,000 images. \n\nEvaluation Metric. We use our proposed metric Copyright Loss (CL) as well as CLIP and l 2 norm to measure the degree of copyright violation. We also use FID to measure the generative quality of text-to-image diffusion model. 1. CLIP: We evaluate changes of CLIP scores, for textimage similarity. 2. Copyright Loss (CL): We quantify the similarity between the original copyright images and their unlearned counterparts on the feature level, utilizing our proposed CL(copyright loss) metric. 3. l 2 norm: We calculate the squared l 2 distance between generated and training images. 4. Fr\u00e9chet Inception Distance (FID): We use FID to evaluate the generative image quality of text-to-image diffusion model. The formulation of the metric is as follows: \n\nwhere: \n\n\u2022 \u00b5 x and \u03a3 x represent the mean and covariance matrix of the feature vectors from real images. \u2022 \u00b5 y and \u03a3 y represent the mean and covariance matrix of the feature vectors from generated images. \u2022 Tr(\u2022) denotes the trace of a matrix. This evaluation provides valuable insights into assessing copyright infringement while preserving the ability to generate non-infringing content. \n\nBaselines.",
            "score": 0.4952309013423758,
            "section_title": "Experiment",
            "char_start_offset": 16961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 341,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "This work addresses the challenge of quantifying originality in text-to-image (T2I) generative diffusion models, with a focus on copyright originality. We begin by evaluating T2I models' ability to innovate and generalize through controlled experiments, revealing that stable diffusion models can effectively recreate unseen elements with sufficiently diverse training data. Then, our key insight is that concepts and combinations of image elements the model is familiar with, and saw more during training, are more concisly represented in the model's latent space. We hence propose a method that leverages textual inversion to measure the originality of an image based on the number of tokens required for its reconstruction by the model. Our approach is inspired by legal definitions of originality and aims to assess whether a model can produce original content without relying on specific prompts or having the training data of the model. We demonstrate our method using both a pre-trained stable diffusion model and a synthetic dataset, showing a correlation between the number of tokens and image originality. This work contributes to the understanding of originality in generative models and has implications for copyright infringement cases.",
            "score": 0.49515610185753434,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "273791614",
            "title": "A Dual-Module System for Copyright-Free Image Recommendation and Infringement Detection in Educational Materials",
            "text": "The protection of digital image copyrights has become an increasingly important issue with the advancement of the information society. In particular, with the proliferation of the internet and social media, there has been a significant increase in the indiscriminate distribution of images and cases of copyright infringement, leading to the proposal of various technical approaches to address this issue. Traditional methods such as digital watermarking, signatures, and hash functions have steadily evolved since their early research stages and have established themselves as fundamental technologies for copyright protection to this day. Ref. [6] proposed a system that uses watermarking technology to protect the copyrights of digital images. This system embeds a watermark containing copyright information into the image and detects it to verify whether copyright infringement has occurred. Subsequently, research was conducted to strengthen copyright protection by utilizing blockchain technology. Khare et al. [7] proposed a copyright infringement detection system that combines artificial intelligence with blockchain. This system extracts image features and compares them with copyright information stored in the blockchain to determine whether infringement has occurred. Recent studies have suggested methods to enhance copyright protection using the latest technologies, such as artificial intelligence and deep learning. Sun and Zhou [8] proposed a system that compares image similarity using deep perceptual hashing based on hash-centered techniques to detect copyright infringement. Additionally, Kim et al. [9] proposed a framework to accurately handle the manipulation of copyrighted photos. This framework detects the Region of Interest (RoI) in the image, generates binary descriptors from the detected RoI, and compares them with a database to search for similar images. \n\nPrevious studies have primarily focused on preventing the replication and infringement of copyrighted images. In contrast, our system aims to verify the copyright status of an image to prevent copyright infringement issues and retrieve copyright-free images that pose no legal concerns. This distinction sets our system apart from existing research, as it emphasizes preemptively addressing copyright issues and providing safe images for educational material creation. To achieve this, we have implemented a deep learning model to effectively perform image infringement detection and retrieval. \n\nOne of the recent studies on image copyright verification technology is the autoencoderbased copyright image authentication model proposed by Yang et al. [10].",
            "score": 0.49422415594106284,
            "section_title": "Related Work",
            "char_start_offset": 3201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2486
                },
                {
                    "start": 2489,
                    "end": 2648
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "15711202"
                },
                {
                    "start": 1017,
                    "end": 1020,
                    "matchedPaperCorpusId": "260933793"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "253531696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "265445146",
            "title": "Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models",
            "text": "The advancement in text-to-image models has led to astonishing artistic performances. However, several studios and websites illegally fine-tune these models using artists' artworks to mimic their styles for profit, which violates the copyrights of artists and diminishes their motivation to produce original works. Currently, there is a notable lack of research focusing on this issue. In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed. In various fine-tune scenarios and against watermark attack methods, our research confirms that analyzing the distribution of watermarks in artificially generated images reliably detects unauthorized mimicry.",
            "score": 0.4940065957867561,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "273550007",
            "title": "The Cat and Mouse Game: The Ongoing Arms Race Between Diffusion Models and Detection Methods",
            "text": "Watermarking, fingerprinting, and methods to detect disguised copyright infringement face growing challenges. Techniques like those proposed in [38] and [42] for content authentication show promise, but attacks such as those discussed by [62] highlight vulnerabilities. Similarly, detecting backdoor attacks on diffusion models is an ongoing concern, with research like [63] offering frameworks for backdoor detection and mitigation. Further studies should enhance watermark robustness, develop backdoor defense mechanisms, and explore advanced strategies for detecting copyright infringement [64].",
            "score": 0.49333921989744367,
            "section_title": "G. Advances in Watermarking, Copyright Detection, and Backdoor Attack Prevention",
            "char_start_offset": 37309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 598
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "238419552"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "258557682"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "254564071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "277043466",
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "text": "Over the past decade, we have witnessed the success of diffusion models in various fields [5,8,32,34,35,46], especially text-to-image models standing out as a prominent catalyst for stimulating creative expression. Access to highquality training data, whether open-source or commercial, is essential to the success of diffusion models. However, this has raised a new concern over unauthorized image use Figure 1. Our work reveals a key property of diffusion models: generated images tend to retain the statistical properties of their training data-a finding that inspired our image copyright protection approach for diffusion models. This figure highlights the close similarity in Discrete Fourier Transform (DFT) spectra between original training images (top row) and diffusion-generated images (second row) across three diffusion models: DDPM [17], DDIM [39], and Classifier-Free Guidance [16]. The color scale is set to [10 \u22126 , 10 \u22121 ], and the bottom row illustrates the spectral residuals and the cosine similarities (COS) [9]. \n\nin model training and fine-tuning [7]. For example, a group of British artists have jointly filed a class-action lawsuit against Midjourney and other artificial intelligence companies [40]. Such incidents are increasingly common and pose serious threats to both society and security. The ability to prevent image infringement during model training and finetuning is therefore critical in mitigating these risks. \n\nInitial research has concentrated on adversarial defense methods [26,37,42], which proactively disrupt diffusion models by adding adversarial perturbations to protect images before publication, distorting the generated outputs. While effective, these methods have key limitations: a) they lack model-agnostic flexibility, as they require tailored optimization objectives for specific models, which limits their use in black-box settings where model details are unknown, and b) adversarial perturbations are irreversible, making it impossible for authorized users to access the processed images as intended. Recent methods that use backdoors [44] and watermarks [27,48] offer versatility across various diffusion tasks by embedding authentication marks for infringement detection.",
            "score": 0.49326566067655275,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 95,
                    "matchedPaperCorpusId": "258108307"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 101,
                    "end": 104,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 104,
                    "end": 107,
                    "matchedPaperCorpusId": "257557425"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1029,
                    "end": 1032,
                    "matchedPaperCorpusId": "43685115"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "269983235",
            "title": "Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy",
            "text": "Copyright protection in text-to-image synthesis. To protect the copyright of text-to-image models, several works [67,68] propose inserting backdoors to embed watermarks in text-to-image models. \n\nTo protect the copyright of image-text datasets, some works [50,52,69] incorporate imperceptible perturbations to render the released datasets unusable. Other works [12,56] utilize the backdoor or watermark to track the usage of image-text datasets. In contrast, our method indicates the possibility of auditing the unauthorized usage of individual image-text data points utilizing membership inference. \n\nMembership inference on diffusion models. In the grey-box or white-box setting, Carlini et al. [5] firstly conduct membership inference on unconditional diffusion models by conducting LiRA (Likelihood Ratio Attack) [4], with the requirement of training multiple shadow models. Matsumoto et al. [38] make the first step by utilizing diffusion loss to conduct query-based membership inference. Some works [15,28] leverage the DDIM [54] deterministic forward process [27] to access the posterior estimation errors of diffusion models. And Fu et al. [17] leverage the probability fluctuations by perturbing image samples. Few works consider the black-box settings [41,62]. However, these studies either assume partial knowledge of member set data [62] or assume extensive fine-tuning steps [41] (100 \u223c 500 epochs), both of which do not align with real-world scenarios. \n\nMemorization detection in text-to-image models. A similar work [59] detects token memorization by inspecting the magnitude of text-conditional predictions, but differs from ours by lacking in-depth rationale analysis and a rigorous membership inference setup with randomly selected member/holdout sets.",
            "score": 0.4931749579776617,
            "section_title": "Related Works",
            "char_start_offset": 28507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "258557506"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "256826808"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 896,
                    "end": 900,
                    "matchedPaperCorpusId": "256627812"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "256503774"
                },
                {
                    "start": 1009,
                    "end": 1012,
                    "matchedPaperCorpusId": "258967241"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "270309880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "A rising number of works focus on the vulnerability of diffusion models to various types of attacks. Poisoning attack [5,8,46,55,57] studies the problem of manipulating training data to induce unsafe behaviors in diffusion models during the training phase. In the context of the real-world inference phase for existing text-to-image diffusion models, other works also investigate the robustness and safety against different prompt inputs. Researchers [15,25,28,60] have shown that injecting a slight perturbation to the input prompts will mislead the unprotected model to generate semantically unrelated images. Furthermore, with carefully crafted prompts, unsafe images can easily evade detection-based safety filters in text-to-image diffusion models [32,35]. Other studies [7,51] red-teaming concept removal copyright protection methods by finding problematic prompts that can recover erased unsafe concepts to yield undesirable generations. Diverging from these existing attack approaches on heuristic protection methods, in this paper, our focus is on challenging the vulnerability of probabilistic copyright protection methods.",
            "score": 0.4931662145213658,
            "section_title": "Vulnerability of Diffusion Models",
            "char_start_offset": 5886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1133
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "257482560"
                },
                {
                    "start": 121,
                    "end": 123,
                    "matchedPaperCorpusId": "254564071"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "257804994"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "For diffusion models, the output is influenced not only by the prompt but also by the latent noise. Latent noise represents encoded representations of the input that capture essential features in a lower-dimensional space. These latent variables guide the generation process, affecting the finer details of the resulting image. In this section, we propose a reinforcement learning (RL)-based latent control method to mitigate copyright infringement in diffusion-based generative models. Our method involves training an agent to search the input latent variables that yield lower infringement scores, ensuring that the generated outputs do not violate copyright. \n\nSpecifically, for latent variable z, we define a policy \u03c0 \u03c9 parameterized by \u03c9, allowing us to sample latent noise \u03f5 \u223c \u03c0 \u03c9 (z), which follows a Gaussian distribution. The sampled noise \u03f5 is then passed through the pre-trained diffusion decoder f to produce the image x = f (z, \u03f5). \n\nTo assess the copyright infringement potential of the generated image, we employ our CopyJudge to obtain the infringement score s f . Based on this score, we define a reward function: \n\nThis reward is designed to penalize outputs with higher infringement scores, thus encouraging the generation of non-infringing content. We optimize the parameters \u03c9 by maximizing the expected reward, L(\u03c9), defined as: \n\nThe gradient of this objective is computed using the REIN-FORCE rule (Williams, 1992), which is given by: \n\nDuring the training process, the latent variable z is updated according to the following rule: \n\nwhere \u03b2 is the step size. We further conduct normalization for the latent variables to maintain stability and prevent extreme deviations. This RL-based approach allows the agent to explore variations in the latent space, thereby improving its ability to generate non-infringing content. The detailed algorithm can be found in appendix B.1.",
            "score": 0.49240754898767847,
            "section_title": "Mitigation via RL-based Latent Control",
            "char_start_offset": 13699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 1422,
                    "end": 1438,
                    "matchedPaperCorpusId": "2332513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "269293420",
            "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models",
            "text": "As a result, to achieve higher accuracy, we may use fake image detectors to examine all or representative frames, which can be time-consuming. With our VidProM, researchers can (1) train specialized Fake Video Detection models on millions of generated videos, and (2) use millions of prompts to generate more videos from more diffusion models to further improve the detection performance. \n\nVideo Copy Detection for Diffusion Models aims to answer whether videos generated by diffusion models replicate the contents of existing ones (see a detailed discussion in the Appendix (Section D)). Videos generated with replicated content may infringe on the copyrights of the original videos. However, current video copy detection researches [51,52,53,54] focus on detecting hand-crafted copies generated by transformations like horizontal flips and random crops, and overlook the challenge from diffusion models. Researchers can utilize our dataset VidProM as input queries and employ existing video copy detection models to pre-filter potential replications. Subsequently, these filtered videos can be manually annotated to determine whether they are indeed replications and regarded as training sources.",
            "score": 0.4919354819980099,
            "section_title": "Inspiring New Research",
            "char_start_offset": 20487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "This method complements precaution measures and provides robust legal proof of infringements.\u2022 We propose a new framework, CGI-DM, for authentication.Utilizing Monte Carlo sampling and PGD optimization, we employ gradient inversion based on the partial representation of a given image.The similarity between the recovered samples and original samples can serve as a robust and visible indicator for infringements.Notably, while most gradient inversion methods focus on classification models, we pioneer its application in the domain of generative models, emphasizing a new approach.\u2022 We conduct extensive experiments on the WikiArt and Dreambooth datasets to substantiate the efficacy and robustness of our approach in distinguishing training samples from those not used during training.These demonstrate our method's effectiveness in authenticating digital copyrights and thus validating infringements for both style mimicry and fabricated images.",
            "score": 0.4904391890964568,
            "section_title": "Introduction",
            "char_start_offset": 4681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 413
                },
                {
                    "start": 413,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "270620522",
            "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
            "text": "The latest advancements in generative diffusion models (GDMs) [1,2,3], especially the text-to-image (T2I) models [4,5] which excel in creating high-quality images that closely align with the given textual prompts, have revolutionized the field of image generation.These advantages stem not only from the development of model architectures and computing power, but also from the availability of large-scale datasets [6,7,8].While datasets play an important role, their copyright protection has remained as an unsolved issue.The protection of these datasets' copyrights is paramount for multiple reasons.For instance, open-source datasets [9] are generally available only for educational and research purposes, barring any commercial use.Additionally, for commercial datasets, it is crucial for companies to secure them from theft and unauthorized sales.While pre-training and fine-tuning both raise concerns of copyright infringement, fine-tuning has a more severe impact on the copyright of datasets.Compared to pre-training, fine-tuning is highly efficient, allowing for many unauthorized uses without effective regulatory restrictions.\n\nObserving the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality Preprint.Under review.[13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16,17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2).",
            "score": 0.48888052262073384,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1137
                },
                {
                    "start": 1139,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1643
                },
                {
                    "start": 1643,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1788
                },
                {
                    "start": 1788,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 118,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 415,
                    "end": 418,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 418,
                    "end": 420,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 420,
                    "end": 422,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 637,
                    "end": 640,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 1247,
                    "end": 1250,
                    "matchedPaperCorpusId": "256503774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "278535529",
            "title": "Visual Watermarking in the Era of Diffusion Models: Advances and Challenges",
            "text": "In the burgeoning age of generative AI, watermarks act as identifiers of provenance and artificial content [Bang et al., 2024]. Visual watermarking involves embedding recognizable patterns or logos in digital media to assert ownership and prevent unauthorized use. The main deepfake detection methods typically identify deepfake images in a passive manner by extracting biometric features and high-frequency artifacts from the images [Dathathri et al., 2024]. However, as deepfake images become more realistic or are transmitted through lossy channels, passive detection may fail to identify them, limiting their effectiveness. As the demand for digital content increases, so does the need for effective protection mechanisms. Visual watermarks deter infringement and help trace unauthorized reproductions, making them essential for digital content security [Bao et al., 2024]. \n\nDiffusion models (DMs) [Sohl-Dickstein et al., 2015;Ho et al., 2020;Rombach et al., 2022] have gained significant traction in the fields of image generation and recognition, offering innovative solutions for both creative and security challenges. Diffusion models transform noise into coherent visual content through a learned denoising process, which has made them a leading tool for applications such as artistic creation and data augmentation [Li et al., 2023]. In forgery detection, diffusion models leverage their deep understanding of data distributions to identify inconsistencies and anomalies in images [Yu et al., 2024]. By combining their generative capabilities with robust analytical features, diffusion models provide a powerful framework for both creating and safeguarding digital media. \n\nThe increasing popularity of open-source Stable Diffusion models has led to serious concerns about intellectual property and verification, impacting both data contributors and the creators of these models. To address these issues, Watermarking techniques have emerged as a promising method to ensure traceability and protect intellectual property [Zhao et al., 2024a].",
            "score": 0.48875046325833216,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 126,
                    "matchedPaperCorpusId": "267028170"
                },
                {
                    "start": 434,
                    "end": 458,
                    "matchedPaperCorpusId": "257767463"
                },
                {
                    "start": 858,
                    "end": 876,
                    "matchedPaperCorpusId": "271510476"
                },
                {
                    "start": 903,
                    "end": 932,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 932,
                    "end": 948,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41357421875
        },
        {
            "corpus_id": "273791614",
            "title": "A Dual-Module System for Copyright-Free Image Recommendation and Infringement Detection in Educational Materials",
            "text": "Figure 1 presents an overview of the copyright-free image recommendation system, which consists of two main components: an image infringement detection module and an image retrieval module. The image infringement detection module processes the query image uploaded by the user to determine whether it matches any copyrighted images in the database. This module employs a model based on the previously developed CVAE. The uploaded image is passed through the encoder of the pre-trained CVAE to extract its feature information. In a two-step process, unique feature vectors, referred to as the first and second keys, are generated. These keys act as distinctive identifiers that represent the image's characteristics. \n\nThe search is conducted in two stages by comparing these keys with those stored in the copyright database. In the first stage, the system calculates the Hamming distance between the first key of the query image and the key of the images in the database, selecting results that fall below the threshold  . If a single result is found, it is treated as the final result. If multiple results or no results are found, the system proceeds to the second stage, where it compares the second key using cosine similarity. The result with the highest similarity exceeding the threshold  is selected as the final match. If the final result is determined, the system informs the user that the image is confirmed to be copyrighted. If The image infringement detection module processes the query image uploaded by the user to determine whether it matches any copyrighted images in the database. This module employs a model based on the previously developed CVAE. The uploaded image is passed through the encoder of the pre-trained CVAE to extract its feature information. In a two-step process, unique feature vectors, referred to as the first and second keys, are generated. These keys act as distinctive identifiers that represent the image's characteristics. \n\nThe search is conducted in two stages by comparing these keys with those stored in the copyright database. In the first stage, the system calculates the Hamming distance between the first key of the query image and the key of the images in the database, selecting results that fall below the threshold T 1st . If a single result is found, it is treated as the final result.",
            "score": 0.488521137089247,
            "section_title": "System Overview",
            "char_start_offset": 9879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2341
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57470703125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth [24] and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights.\n\nTo address these critical concerns, a line of approaches focuses on safeguarding individual images by incorporating adversarial attacks, such as AdvDM [17], Glaze [25], and Anti-Dreambooth [30].The adversarial attacks can disrupt the generative output, rendering the images unlearnable by diffusion models.These methods are implemented ahead of the fine-tuning process, and as such, we consider them as precaution approaches.\n\nAnother line of approaches facing such threats is copyright authentication.Copyright authentication compares the similarity between the output images of diffusion models and the given images to validate unauthorized usage.Such a process can serve as legal proof for validating infringement (See Appendix A for more details), and has been utilized as evidence in ongoing legal cases concerning violations enabled by DMs [31].This process happens after the fine-tuning and thus we consider it as a post-caution approach.However, current copyright authentication methods face difficulties in producing output images closely resembling training samples due to the pursuit of diversity in generative models.Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.",
            "score": 0.4885189776678913,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 562
                },
                {
                    "start": 564,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1173
                },
                {
                    "start": 1175,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1600
                },
                {
                    "start": 1602,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2304
                },
                {
                    "start": 2304,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "This section evaluates our method for detecting partial copyright infringements by computing the cosine similarity of CLIP-embeddings between image chunks. An image chunk is a specific rectangular image region from the original image. We apply this measurement to compare chunks in generated images with annotated target image chunks, and for contrast, assess the similarity between randomly chosen chunks and our annotated ones. Additionally, we assess similarity in images generated from random prompts with our target annotations. Results, as shown in Table 2, reveal that our identified chunks exhibit a high cosine similarity (approximately 0.9) with target annotations, compared to lower similarities Baseline 2 58.4 58.9 57.9 57.8 57.9 58.9 \n\nTable 2: Cosine similarity (multiplied by 100) between CLIP embeddings of target annotations and chunks in generated images. Ours denotes image chunks identified with copyright test on images generated using our prompts. Baseline 1 denotes randomly selected chunks on images generated using our prompts. Baseline 2 denotes randomly selected chunks on images generated using random prompts. Identified image chunks show significant similarity with the annotated image parts. Figure 9: Proportion of generated images with identified copyrighted content. For all models apart from Stable Diffusion XL (SD XL), our copyright test identifies that around 70% of generated images from our pipeline have at least one chunk containing copyrighted content. The slight decrease in copyrighted content in SD XL is due to the model's increase in ability to comprehend the nuances in our non-sensitive prompts, thus slightly reducing the adversarial property. Overall, it is clear that our non-sensitive prompts can effectively cause diffusion models to infringe copyright as even more than half of the images generated by SD XL contain copyrighted content. \n\nof about 0.7 and 0.6 for random chunks in generated and random images, respectively.",
            "score": 0.48524980540718926,
            "section_title": "Evaluation on Copyright Test",
            "char_start_offset": 26229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "278331778",
            "title": "Intellectual Property Protection in the Age of AI: From Perspective of Deep Learning Models",
            "text": "Jiang et al. [10] proposed a DL framework for predicting patent application outcome by mining and fusing the features of text content and context networks. In addition, in the context of integrated IP management, the multimodal DL model is able to identify infringements and improper uses in cross-media environments through the joint understanding of text, image and video information. Li et al. [11] constructed a multimodal large-scale dataset for strictly annotated product patent infringement detection, examined the performance of different DL models in detecting potential patent infringements, and proposed a simple and effective infringement detection process. \n\nHowever, as technology continues to evolve, new technologies bring convenience and efficiency while also raising new legal risks and challenges. First, in order to train DL models, it is usually necessary to rely on a large amount of data, which may contain a large amount of copyrighted material, and the problem of unauthorized use of data exacerbates the risk of copyright infringement to a certain extent [12]. Second, the misuse of deep generative models, such as the dissemination of falsified images, videos, and false information, also poses a serious test of existing legal regulation and ethical norms. Issues such as the reversibility of digital watermarking, privacy leakage, and technology abuse have gradually emerged, exacerbating the lag of traditional IP laws and policies in responding to the impact of emerging technologies. The diversified applications of AI in the form of Sora, Midjourney and Stable Diffusion have greatly reduced the technical threshold and economic cost of knowledge production, but also blurred the boundaries between originality and imitation, posing potential infringement risks to the traditional IP protection system constructed on the basis of \"human creation\". This is a potential infringement risk to the traditional intellectual property protection system based on \"human creation\" [13]. Finally, the \"black box\" nature of DL models makes the definition of responsibility blurred in the event of infringement, misjudgment or disputes, which is particularly prominent in the attribution of AIGC and infringement disputes [14]. Therefore, how to effectively avoid the potential risks of DL technology while utilizing its advantages has become an important issue in today's IP research.",
            "score": 0.48376141798321765,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "257843620"
                },
                {
                    "start": 2242,
                    "end": 2246,
                    "matchedPaperCorpusId": "208841608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47265625
        },
        {
            "corpus_id": "273791614",
            "title": "A Dual-Module System for Copyright-Free Image Recommendation and Infringement Detection in Educational Materials",
            "text": "Images are extensively utilized in educational materials due to their efficacy in conveying complex concepts. However, unauthorized use of images frequently results in legal issues related to copyright infringement. To mitigate this problem, we introduce a dual-module system specifically designed for educators. The first module, a copyright infringement detection system, employs deep learning techniques to verify the copyright status of images. It utilizes a Convolutional Variational Autoencoder (CVAE) model to extract significant features from copyrighted images and compares them against user-provided images. If infringement is detected, the second module, an image retrieval system, recommends alternative copyright-free images using a Vision Transformer (ViT)-based hashing model. Evaluation on benchmark datasets demonstrates the system\u2019s effectiveness, achieving a mean Average Precision (mAP) of 0.812 on the Flickr25k dataset. Additionally, a user study involving 65 teachers indicates high satisfaction levels, particularly in addressing copyright concerns and ease of use. Our system significantly aids educators in creating educational materials that comply with copyright regulations.",
            "score": 0.48356388186019594,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "271245184",
            "title": "Towards Understanding Unsafe Video Generation",
            "text": "Diffusion models are now used across various fields for data generation due to their high-quality and diverse content generation capabilities.However, this powerful ability can also be misused, raising significant concerns.With the development of VGMs [2], [6], [14], [55], [56], concerns have arisen not only about their potential to generate not-safe-for-work (NSFW) content but also about the broader implications of generating unauthorized content.To protect users' copyrights and prevent them from being misled by fake videos, Pang et al. proposed VGMShield [31].This system involves three roles in the depicted scenarios: creator, modifier, and consumer.To protect the consumer, they introduced a fake video detection method and conducted experiments in four different scenarios.Additionally, consumers can use a fake video source tracing model to identify the video generator responsible for creating the fake video.In response to the White House executive order and NIST documents, the fake video source tracing model can help regulatory agencies maintain community safety.Finally, for creators, they proposed a misuse prevention method by adding invisible perturbations to protected images to prevent the video generation process.This issue has also been observed in text-to-image models [37], [39].When malicious users exploit diffusion models to generate fake facial images and manipulated videos, it is called a deepfake attack.Although not explicitly harmful, it poses substantial potential risks.Various methods, such as watermarks [28], adversarial examples [43], and detection [42], have been proposed to address these issues.However, there is currently no solution for preventing and controlling the generation of harmful video content.",
            "score": 0.48343615851182875,
            "section_title": "B. Deepfake Detection in VGMs",
            "char_start_offset": 56740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1440
                },
                {
                    "start": 1440,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1642
                },
                {
                    "start": 1642,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "264820183"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "255546643"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "The booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.",
            "score": 0.48320729134852725,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "277468519",
            "title": "MiZero: The Shadowy Defender Against Text Style Infringements",
            "text": "Digital watermarking, as a popular paradigm for copyright protection, has been widely studied and validated for its role in safeguarding data and preventing infringement. Several studies have ex-plored scrambled watermarks (Chen et al., 2022;Salman et al., 2023;Shan et al., 2023), which involves embedding intentional signal into images to protect the copyright. Alternatively, research on verifiable watermarks (Huang et al.) utilizes diffusion model and clearly marks copyright boundaries to protect image style. While current methods tailored to style are primarily focused on images, the preservation of text style remains underexplored. \n\nTo prevent LLMs from infringing on specific text styles, we propose a Model-agnostic implicit Zero-watermarking scheme, called MiZero, aimed at protecting certain stylistic features in datasets. Specifically, we first leverage the knowledge inference and information extraction capabilities of LLMs to extract condensed-lists. We incorporate contrastive learning and develop a instance delimitation mechanism, which is adjusted based on the prior knowledge of each protected text, thus enhancing the output quality of LLMs (Leidinger et al., 2023). Second, to preserve the integrity of the style-specific features, we create disentangled style space to extract the protected style's watermark guided by condensed-lists. This method helps clearly define the copyright anchor which is mapped to implicit watermarks. \n\nThe application scenario of the proposed MiZero is shown in Figure 1. MiZero (the Defender) extracts style-specific features from the protected data to generate a unique watermark. If an attacker illicitly uses the protected data to generate imitative texts, the defender can detect infringement by calculating the Hamming distance between the suspect text's style encoding and the watermark. \n\nAdditionally, to meet practical needs and reduce computation costs, MiZero is designed to perform effectively in few-shot scenarios. Our main contributions are summarized as follows: \n\n\u2022 We present a novel, implicit model-agnostic watermarking method (namely MiZero), to protect text style copyrights from unauthorized AI imitation. To the best of our knowledge, this is the first study to protect unique authorial text style within the disentangled style domain.",
            "score": 0.4829528371162136,
            "section_title": "Introduction",
            "char_start_offset": 1783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2038
                },
                {
                    "start": 2041,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 242,
                    "matchedPaperCorpusId": "245537621"
                },
                {
                    "start": 242,
                    "end": 262,
                    "matchedPaperCorpusId": "256826808"
                },
                {
                    "start": 262,
                    "end": 280,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 1168,
                    "end": 1192,
                    "matchedPaperCorpusId": "265018951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "Considering a training image dataset D, composed of both copyrighted (D c ) and non-copyrighted (D nc ) images. The dataset proportions are represented by p c and p nc , with p c + p nc = 1. \n\nFor each image x i in the dataset, we associate a feature vector f i and a corresponding text prompt t i . The diffusion model is trained to generate an image xi from the prompt t i ,",
            "score": 0.48284033283857813,
            "section_title": "Problem Formulation",
            "char_start_offset": 8561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "244577674",
            "title": "A Photo Identification Framework to Prevent Copyright Infringement with Manipulations",
            "text": "We proposed the photo identification framework to prevent copyright infringements. To handle manipulations by pirates, we developed preprocessing steps to identify all photo copyrights in images. First, we detected image RoIs from an image collage to distinguish multiple copyright photos. To train the image RoI detector, we collected the dataset for image collage and proposed the recursive partitioning method to build an image collage frame. Subsequently, we identified each RoI by using the proposed image hashing method that handles image manipulations. By augmenting images, we generated similar binary descriptors from an original image and a manipulated image. Consequently, we have verified identification results by measuring image similarities. This process reduced false positives and left them unidentified while reducing false positives. We experimentally showed the effectiveness of each step in the proposed photo copyright identification framework in comparison with the other methods. \n\nThe development of the identification framework is expected to automatically monitor the copyrights of infringed photos from websites, vitalize the photo copyright search service, and prevent copyright holders from copyright infringements. In addition to identification, an image descriptor can be developed for image retrieval, which is utilized in various web image search services. Users also will use legally copyrighted photos by paying royalty and be able to access a lot of available copyright photos. This virtuous cycle, creation of copyrighted works \u2192 use of creations and payment of royalties \u2192 recreation copyrighted works, will make it possible to build a ecosystem for copyrights.",
            "score": 0.4825302331309531,
            "section_title": "Conclusions",
            "char_start_offset": 38879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1700
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "Midjourney [19], Gemini Pro [29], Copilot [18] and ChatGPT [1] have word-based detection mechanism on the user prompts to prevent generation of the images that may violate the internal policy.that the blocking mechanism fails to block copyright infringement generation to 11.0% block rate on our APGP-generated prompts (Table 3).Furthermore, not only generating the contents, the contents are exceptionally similar to the original IP content as shown in Figure 4. Human evaluation.To quantify the violations, we conducted a human evaluation on 63 participants to determine the copyright violation based on the reference image.The copyright violation is highly occurring in the product and logo category where 96.24% and 82.71% of participants examine the images as copyright infringement (Figure 7).Upon examining the images classified as identical violations, it was found that over 50% were deemed to be cases of copyright infringement in product and logo.Furthermore, 30% of characters are also considered as similar violations which are determined as severe similarity (Figure 8).When we employ a consensus vote to determine violations, there are still 10 images that all participants determine as violations.Automatic evaluation.Although human evaluation is one of the best evaluation approach for copyright infringement, we propose automatic evaluation to reduce the cost of the experiment.We introduce a QA score that calculates the accuracy by given generated images by T2I systems, where QA sets are generated based on the target image.\n\nWe employ VLM to respond to the question, and LLM to evaluate the answers.In Figure 5, 34.09% of the generated images accurately answer more than seven questions, suggesting that these images contain key aspects similar to the target images necessary for matching the correct answers.Given that the target image correctly matches the answer for more than seven questions in 67.05% of cases, we estimate that 50.84% of the generated images likely commit copyright infringement.Ablation study.Text prompts that specifically describe copyrighted content can trigger the generation of such content even without explicit keywords, as demonstrated in Table 4.We hypothesize that omitting specific keywords may allow these prompts to bypass initial violation detection mechanisms.",
            "score": 0.48232091931214227,
            "section_title": "Simple prompt can induce the copyright violation in most systems",
            "char_start_offset": 15113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1545
                },
                {
                    "start": 1547,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2023
                },
                {
                    "start": 2023,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2200
                },
                {
                    "start": 2200,
                    "end": 2320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "The algorithm of our mitigation method is outlined in Algorithm 1.The input and the output of our defensive generation paradigm are the text prompt P and the final generated content I, respectively.In Line 2-3, we defend the name-based intellectual property infringement by using the LLM-based method described in Section 4.3.In Line 4-8, we generate an initial image by using the standard diffusion process.Here, \u2205 represents an empty string, z t is the noise added at time step t, and w is the classifier-free guidance scale factor.The UNet E \u03b8 is the core component of the diffusion model, and P is the input prompt used.In line 9, we detect the potential infringements by using the VLM-based method introduced in Section 4.3.We then regenerate the image by using the suppression method described in Section 4.4 (Line 9-14).",
            "score": 0.48182210719485585,
            "section_title": "Algorithm",
            "char_start_offset": 20871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 66,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "270067889",
            "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
            "text": "Text-to-image diffusion models [44,43] are powerful tools to generate high-quality images aligned with user prompts. After pre-trained by model publishers to embed world knowledge from large image data [49], open-sourced diffusion models, such as Stable Diffusion (SD) [9, 10], can be conveniently adapted by users to generate their preferred images1 , through fine-tuning with custom data in specific domains. For example, diffusion models can be fine-tuned on cartoon datasets to synthesize avatars in video games [46], or on datasets of landscape photos to generate wallpapers [11]. \n\nAn increasing risk of democratizing open-sourced diffusion models, however, is that the capability of model adaptation has been utilized for illegal purposes, such as forging public figures' portraits [22,24], duplicating copyrighted artworks [26], and generating explicit content [25]. Most existing efforts aim to deter at-  Figure 2: FreezeAsGuard ensures that portraits (left) and artworks (right) generated by diffusion models in illegal classes cannot be recognizable as target objects, even if the model has been fine-tuned with data samples in illegal classes. In contrast, unlearning schemes (UCE [23] and IMMA [65]) cannot prevent the unlearned knowledge of illegal classes from being relearned in fine-tuning. \n\ntempts of illegal model adaptation with copyright detection [64,16,17], which embeds invisible but detectable watermarks into training data and further generated images, as shown in Figure 1. However, such detection only applies to misuse of training data, and does not mitigate the user's capability of illegal model adaptation. Users can easily bypass such detection by collecting and using their own training data without being watermarked (e.g., users' self-taken photos of public figures). \n\nInstead, an intuitive approach to mitigation is content filtering. However, filtering user prompts [19] can be bypassed by fine-tuning the model to align innocent prompts with illegal image contents [55], and filtering the generated images [7] is often overpowered with high false-positive rates [3].",
            "score": 0.4817648769148616,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 789,
                    "end": 793,
                    "matchedPaperCorpusId": "247779344"
                },
                {
                    "start": 1194,
                    "end": 1198,
                    "matchedPaperCorpusId": "261276613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "Our mitigation approach TRIM (inTellectual pRoperty Infringement Mitigating) starts by preventing name-based intellectual property infringement.We block any input prompts that contain the names of protected characters and directly instruct the AI models to generate images depicting those visually copyrighted characters (see Section 4.3).Next, we use the standard process to generate content based on the provided input prompt.We then leverage large multi-modal AI models capable of understanding images and text to detect potential infringements in the generated content (more details in Section 4.3).After that, we regenerate the image using a guidance technique for the diffusion model's process.This steers the model away from generating anything resembling the infringing character, while still following the original prompt from the user (see Section 4.4 for more details).",
            "score": 0.48091148112114107,
            "section_title": "Overview of Our Mitigation Approach",
            "char_start_offset": 15928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 700
                },
                {
                    "start": 700,
                    "end": 880
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "While our method shows promise, its performance is currently constrained by the availability of labeled data, making it challenging to fully evaluate its alignment with human judgment. To address this, we believe that developing a more comprehensive dataset containing detailed human judgment criteria is crucial. In addition, we are looking forward to stronger attacks to test the robustness of our mitigation strategies. Improving the retrieval of infringing images will also provide valuable insights into the relationship between generative models and copyrighted content. Lastly, although our approach does not require a specific LVLM, we plan to explore other models beyond GPT in advancing copyright protection in the future.",
            "score": 0.48071997670636246,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 23763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "265128623",
            "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
            "text": "Since our focus is to study the impact of our watermark method on the original CLIP-based VLPs model, for this purpose, we conducted zero-shot experiments on 6 datasets: MS-COCO [31], Flicker30k [38], CIFAR-10 [26], CIFAR-100 [26], ImageNet-1k [12] and VOC2007 [13]. Specifically, MS-COCO and Flicker30k are widely used datasets for image-text retrieval. CIFAR-10, CIFAR-100, ImageNet-1K, and VOC2007 are large datasets for object classification. Details on each dataset for the zero-shot evaluation and the metrics are provided in Table 1. Additionally, we use the Visual Genome [25] dataset with 108,000 samples to count class frequencies. To validate the effectiveness of our watermark detection algorithms, we report three metrics, i.e., the difference of cosine similarity, the difference of squared \u2113 2 distance and p-value (defined in Section 3.3.3). We utilized Adam [23] for training linear transformation   with a learning rate of 1 \u00d7 10 \u22125 and 50, 000 epochs on 1 Tesla V100 32G GPU in less than 30 seconds. We initialize the original CLIP model using the CLIP  checkpoints [39], and we conducted inference using the original CLIP model to obtain image-text embeddings as the original EaaS embeddings. The maximum trigger class in a single image is 3, the size of the trigger class set is 26, and the size of the trigger image-text set is 1024. We conducted each experiment independently 5 times and reported the average results. Moreover, we introduce another threshold  to determine instances of copyright infringement. In accordance with established statistical practices, a standard p-value of 5 \u00d7 10 \u22123 is considered suitable for rejecting the null hypothesis with statistical significance [4], which can be applied to identify instances of copyright infringement.",
            "score": 0.47985024472391863,
            "section_title": "EXPERIMENTS 4.1 Dataset and Experimental Settings",
            "char_start_offset": 19367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1780
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "6941275"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 1706,
                    "end": 1709,
                    "matchedPaperCorpusId": "3291437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1290283203125
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "The goal of our suppression process is to minimize the similarity between the generated contents and the detected infringed character, while keeping the alignment between the generated contents and the input prompt.After detecting the character that the generated images potentially have infringement   issues with, we suppress the IP infringement issues by exploiting the classifier-free guidance [36] in the diffusion models.In each diffusion timestamp, the diffusion model predicts noise based on the input prompt as the text condition.Formally, the predicted noise in timestamp t \u2212 1 can be written as w \u2022 E \u03b8 (z t , t, P) + (1 \u2212 w) \u2022 E \u03b8 (z t , t, \u2205), where \u2205 represents the empty string, z t is the noise in timestamp t, and w is the classifier-free guidance strength (default to 7.5 in this paper).E \u03b8 is the UNet in the diffusion model, and P is the input prompt used.To erase the IP infringement effects on the detected infringed character, in our mitigation method, we use the name of the detected infringed character to replace the empty string.Formally, the predicted noise with timestamp t \u2212 1 in our suppression process can be written as w \u2022 E \u03b8 (z t , t, P) + (1 \u2212 w) \u2022 E \u03b8 (z t , t, d), where d is the name of the detected infringed character.By incorporating the name of the infringing character into the noise prediction process, we aim to guide the diffusion model away from generating content that resembles the infringing character, while still adhering to the original input prompt provided by the user.This approach leverages the capabilities of diffusion models and classifier-free guidance to mitigate intellectual property infringement issues in a controlled manner, without compromising the overall quality and relevance of the generated images.",
            "score": 0.47891948179724086,
            "section_title": "Infringement Suppression",
            "char_start_offset": 19086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 539
                },
                {
                    "start": 539,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1525
                },
                {
                    "start": 1525,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Compared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: explicit infringement and implicit infringement. \n\nExplicit infringement. This refers to prompts that directly contain copyright information, such as \"Generate an image of Mickey Mouse.\" We use the 20 cartoon and artwork samples collected in section 5.1 to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work. \n\nImplicit infringement. This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL\u2022E 3 (Betker et al., 2023), which has a safety detection module to reject prompts that trigger it. \n\nAutomated attack. Efficiently retrieving or generating infringing prompts has always been a challenge. Kim et al. utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point. \n\nFor explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control.",
            "score": 0.47864674828714676,
            "section_title": "IP INFRINGEMENT MITIGATION",
            "char_start_offset": 19610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1196,
                    "end": 1217,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "278129333",
            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
            "text": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
            "score": 0.4778104645197726,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "256627601"
                },
                {
                    "start": 1914,
                    "end": 1917,
                    "matchedPaperCorpusId": "268819999"
                },
                {
                    "start": 1922,
                    "end": 1925,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 1925,
                    "end": 1928,
                    "matchedPaperCorpusId": "270309880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "273549928",
            "title": "Schedule Your Edit: A Simple yet Effective Diffusion Noise Schedule for Image Editing",
            "text": "Our work introduces a novel editing technique for manipulating real images using state-of-the-art text-to-image diffusion models. While this technology could potentially be exploited by malicious parties to create fake content and spread disinformation, this is a common issue across all image editing techniques. Significant progress is already being made in identifying and preventing such malicious editing. Our research contributes to this effort by providing a detailed analysis of the inversion and editing processes in text-to-image diffusion models, thereby aiding in the development of more robust detection and prevention methods.",
            "score": 0.4777918216944289,
            "section_title": "G Broader Impacts",
            "char_start_offset": 53197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 640
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1602783203125
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "The rapid advancement of generative artificial intelligence (GenAI) has ushered in a new era of content creation, enabling the synthesis of highquality text, images, and multimedia content at an unprecedented scale. While these innovations have expanded creative possibilities and applications across industries, they have also raised significant ethical and legal concerns, particularly regarding intellectual property (IP) rights (Sag, 2023;Poland). One of the most pressing issues is the unauthorized reproduction of copyrighted material, where generative models may inadvertently produce outputs that closely resemble or replicate IP-protected content (Zirpoli, 2023;Dzuong et al., 2024;Sag, 2023;Poland;Wang et al., 2024). This issue has led to widespread debates among legal experts, policymakers, and AI researchers on the potential liabilities and regulatory measures required to address copyright infringement in AI-generated content. \n\nExisting efforts to mitigate copyright concerns in generative models have primarily focused on two key approaches: \u2460 reducing memorization during training using techniques such as differential privacy (Dwork et al., 2014), which limits the retention of specific data points to prevent models from reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022), and \u2461 employing prompt engineering strategies that discourage the generation of IP-infringing material through explicit negative prompts (Wang et al., 2024;He et al., 2024) or optimized safe prompt modifications (Chin et al., 2023;Rando et al., 2022). While these approaches offer some level of control over generative outputs, they do not directly address the challenge of detecting copyright infringement in already-generated content. As a result, there is an urgent need for robust evaluation methods and benchmarks to assess the ability of AI models-specifically large vision-language models (LVLMs)-to identify potential instances of copyright violations. \n\nVision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding.",
            "score": 0.4769325202362677,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 443,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 691,
                    "end": 701,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 1147,
                    "end": 1167,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 1273,
                    "end": 1293,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1293,
                    "end": 1311,
                    "matchedPaperCorpusId": "250210875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "269137659",
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "text": "Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of\"artistic copyright infringement\"to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, includingTagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular text-to-image generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple prompting of today's popular text-to-image generative models.",
            "score": 0.47638269007814205,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "Others have demonstrated that these models can potentially reconstruct or replicate copyrighted content from their training data (Carlini et al., 2020;car, 2023). Efforts to mitigate these risks include provable copyright protection strategies inspired by differential privacy (Vyas et al., 2023), decoding-time prevention (Golatkar et al., 2024) that guide the generation process away from copyright concepts and model editing and unlearning that aim to remove copyrighted content from model weights (Gong et al., 2024;Chefer et al., 2023;Zhang et al., 2023). Ma et al. (2024) introduces benchmark for measuring copyright infringement unlearning from text-to-image diffusion models. Another recent study by Kim et al. (2024) also examines keywords potentially important for image generation, but only includes the character name along with the associated movie or TV program as keywords. They also show that LLM-optimized descriptions can generate images similar to copyrighted characters on proprietary models such as ChatGPT, Copilot, and Gemini. However, their optimized prompts do not explicitly exclude the characters' names. Similarly, Zhang et al. (2024a) focuses on building attacks that can generated particular concepts,including some copyrighted characters. While these works focus on attacks and do not explore effective mitigation methods, our work focuses on building an analysis framework motivated by legal considerations like substantial similarity test for copyrighted characters and provide more in-depth understanding for both how easy it is to generate copyrighted characters as well as the effectiveness of defenses.",
            "score": 0.4758392642087406,
            "section_title": "RELATED WORK",
            "char_start_offset": 27810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 277,
                    "end": 296,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 323,
                    "end": 346,
                    "matchedPaperCorpusId": "268732879"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "256416326"
                },
                {
                    "start": 540,
                    "end": 559,
                    "matchedPaperCorpusId": "257833863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "263622213",
            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
            "text": "In the scenario of copyright infringement and protection considered in this work, there are two roles: \n\n(1) a data protector that possesses the data copyright, utilizes watermarking techniques before the data is released, and tries to detect if a suspected image is generated by a model fine-tuned on the protected images, and (2) a data offender that uses the protected data for text-to-image model finetuning without permission from the data protector. The data offenders have complete control over the fine-tuning and sampling processes of the text-to-image diffusion models, while the data protectors can only modify the data they own before their data is released and access images generated by the suspected model. \n\nAs shown in Figure 2, the protection process consists of two stages: the protection stage and the audit stage. In the protection stage, the data protector protects the images by adding imperceptible watermarks to the images. Specifically, given that the size of the protected dataset is N , the target is to generate sample-wise watermark \u03b4 i for each protected image x i , \u2200i = 1...N . Then these watermarks are embedded into the protected images xi = x i + \u03b4 i . Correspondingly, the data protector develops a watermark detection approach, denoted by function D w (\u2022), to test whether there is a watermark on the suspected image. To ensure that the watermarks will not lead to severe influence on image quality, we limit the budget of the watermark by constraining its l \u221e norm (\u2225\u03b4 i \u2225 \u221e \u2264 \u03b7) to control the pixel-wise difference between the two images x i and xi . In the audit stage, if the protectors encounter suspected images potentially produced through unauthorized text-to-image models finetuning, they will apply the watermark detection process D w (\u2022) to ascertain whether these images have infringed upon their data rights.",
            "score": 0.47576876928105044,
            "section_title": "PROBLEM FORMULATION",
            "char_start_offset": 11326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1860
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "How was the data associated with each instance acquired?Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)?If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified?If so, please describe how.\n\nThe data was directly observable.\n\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?How were these mechanisms or procedures validated?\n\nWe propose a pipeline to coordinate CLIP, ChatGPT, and diffusion models to generate a dataset that contains anchor images, corresponding prompts, and images generated by text-to-image models, reflecting the potential abuses of copyright.Initially, we collect a set of images that potentially contain copyrighted content, which serves as anchor images.Subsequently, these images are fed into the CLIP-interrogator, allowing us to obtain prompts that correspond to each anchor image.Finally, the prompts are used as input for the stable diffusion model, resulting in the generation of images by the stable diffusion model.Through manual comparisons, we assess whether there is evidence of copyright infringement in terms of style and semantics between the anchor images and the generated images.Ultimately, the anchor images, their corresponding prompts, and the images generated by the stable diffusion model constitute the core components of our dataset.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\n\nNo.\n\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\n\nThe dataset was primarily curated with contributions from the first three authors listed in the author list.\n\nOver what timeframe was the data collected?",
            "score": 0.47507239199677875,
            "section_title": "Collection Process",
            "char_start_offset": 28742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 56,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 425
                },
                {
                    "start": 427,
                    "end": 460
                },
                {
                    "start": 462,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 664
                },
                {
                    "start": 666,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1620
                },
                {
                    "start": 1622,
                    "end": 1773
                },
                {
                    "start": 1775,
                    "end": 1778
                },
                {
                    "start": 1780,
                    "end": 1942
                },
                {
                    "start": 1944,
                    "end": 2052
                },
                {
                    "start": 2054,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3603515625
        },
        {
            "corpus_id": "271909239",
            "title": "Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models",
            "text": "Generative methods based on diffusion models [1][2][3][4] have made significant improvements in recent years, enabling high quality text-to-image synthesis [5,6], image editing [7], video generation [8,9], and text-to-3D conversion [10] by prompt engineering. One of the most representative methods in this field is the Stable Diffusion [11,12], which is a large-scale text-to-image model. By incorporating customized techniques such as Text Inversion [13] and DreamBooth [14], Stable Diffusion only requires fine-tuning on a few images to accurately generate highly realistic and high-quality images based on the input prompts. \n\nDespite this promising progress, the abuse of these powerful generative methods with wicked exploitation raises wide concerns [15], especially in portrait tampering [16][17][18] and copyright infringement [19]. For example, in Figure 1(a), given several photos of a person, attackers can utilize diffusion models to generate fake images containing personal information, leading to reputation defamation. Even worse, attackers can easily plagiarize unauthorized artworks using diffusion models, leading to copyright and profit issues. There is an urgent technology need to protect images from diffusion model tampering. \n\nTo this end, recent research studies delve into adding human-invisible adversarial perturbations onto the images to prevent prompt-based tampering using diffusion models. The method Photoguard [20] maximizes the distance in the VAE latent space. Glaze [21] aims to hinder specific style mimicry, while Anti-DreamBooth [22]introduces an alternating training approach between the model and Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP promptagnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C).",
            "score": 0.4750474152307349,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 48,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 159,
                    "end": 161,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "247519223"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "277043466",
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "text": "Infringement model detection performance. In this section, we study the effectiveness of diagnosis and compare it with two methods DIAGNOSIS [44] and Yu et al [47], which are capable of detecting unauthorized image usages in text-to-image models. We train a set of diffusion models, with or without watermarked images, using different random seeds. We then classify these models based on whether they incorporated unauthorized images during the training or fine-tuning processes. For each experiment, we train 20 pre-trained models for DDIM and Classifier-Free Guidance, and 50 pre-trained models for Stable Diffusion (SD). As shown in Tab. 2, our method successfully detects all infringement models (the watermark injection rate R varying from 0 to 100%), achieving an accuracy of 100% for both text-to-image and naive diffusion models with F P = 0 and F N = 0. While the infringer collects the training or fine-tuning data from multiple sources, and the watermark injection proportion of the training dataset is 25%, the detection accuracy for Yu et al. is only 50.0%. Although DIAGNOSIS achieves 100% detection accuracy at watermark injection rates of R = 25% and R = 100%, the watermark detection ratios are P u = 91.2% for R = 100% and P u = 5.1% for R = 0 [44]. In contrast, our method achieves watermark detection ratios P u = 100% for R = 100% and P u = 0 for R = 0. \n\nGenerally, a higher watermark detection ratio P u , indicates greater credibility of detection results, but P u tends to decrease as the watermark injection ratio R diminishes. Tab. 3 reports the watermark detection ratio P u of Copr-Guard and DIAGNOSIS, with R varying from 0 to 100.0%. When R = 0, DIAGNOSIS still detects a small number of watermarked images from generated outputs (P u = 6.0%, P u = 3.8%), which may result in misjudgment during infringement model detection, particularly when the watermark injection ratio in the training set is low(i.e., R < 10%).",
            "score": 0.4748548050930662,
            "section_title": "Evaluations",
            "char_start_offset": 20303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "238419552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The SSCD has been specifically designed to identify copied content existing across two images. While SSCD is state-of-theart and the first choice for this purpose, it's important to recognize that no system is infallible. Discrepancies between SSCD outcomes and human judgment can occur, highlighting an area for further research and development. \n\nCopyright laws stipulate that any unauthorized reproduction constitutes infringement. To ensure a dataset is free from copyright issues, each image within the dataset (of size N) must be compared against every image in a predefined set of copyright-protected images (of size M). This requirement leads to N*M comparisons and discussions by human experts. A formidable task considering that legal disputes over copyright between two works can span several months. Given the vast size of datasets typically used in training generative models, manual inspection for copyright infringement at such scale becomes impractical. Consequently, despite its limitations, employing a tool like SSCD remains the most feasible strategy for conducting copyright infringement checks at this magnitude. \n\nTo further justify the effectiveness of SSCD, we collected examples used in the Getty Images lawsuit against Stability AI (Vincent, 2023), as illustrated in Figure 8. The SSCD similarity score is 0.47. In our study, we set the threshold at 0.5. This result indicates the effectiveness of SSCD for copyright detection and our threshold is a reasonable value for checking copyright infringement in training data.",
            "score": 0.47485258874702335,
            "section_title": "D. Limitation and Discussion of Employing SSCD for Assessing Substantial Similarity",
            "char_start_offset": 42005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Main results: In Section 4, we reveal the latent (copyrighted) information contained in the acquired samples x d using textual inversion, where the diffusion model (i.e., the U-Net in LDM) is not retrained.To further demonstrate the effectiveness of the disguises x d and examine disguised copyright infringement in other scenarios, we further perform evaluation on DreamBooth (Ruiz et al. 2023), an LDM-based fine-tuning method on a small set of images designed for subject-driven generation.Specifically, we choose five images from the DreamBooth dataset12 as the (designated) copyrighted image x c , and generate their corresponding disguises x d using Algorithm 1 by choosing the noisy version of x c as the base images x b , training 10000 epochs and setting the weight parameter \u03b1 = 1000.After generating the disguises x d , we apply DreamBooth for fine-tuning (on x d only) and inference following the general recipe13 .In Figure 24, we show the copyrighted images x c in the first row, their corresponding disguises x d in the second row, and images generated by DreamBooth by fine-tuning with the above x d in the third row.Our results qualitatively confirm that copyrighted information in x c can be reproduced by fine-tuning on x d with DreamBooth.\n\nEvaluation on model utility: To further examine the practicality of the disguises, we want to demonstrate that the model utility is not deprecated after training (or fine-tuning) on x d .To accomplish this task, we perform the same text-to-image generation task with the prompt \"a photo of an astronaut riding a horse on Mars\" before/after training on x d with LDM14 .Specifically, the \"before\" model is Stable Diffusion v1-4, and the \"after\" model is the acquired model by fine-tuning the \"before\" model on the dog disguise (second row, first column in Figure 24).We show our results in Figure 25 for 6",
            "score": 0.47459825191470906,
            "section_title": "F.1. Evaluation on DreamBooth",
            "char_start_offset": 37186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 394,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "263622213",
            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
            "text": "To protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement. \n\nAdversarial methods. Adversarial methods protect data's IP by applying the idea of evasion attacks. They treat the unauthorized generative models as the target of attack, and develop adversarial examples to disrupt the learning process of unauthorized fine-tuning. GLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios. Although these methods provide effective protection against infringement, they can inadvertently disrupt authorized uses (such as for academic research purposes) of the safeguarded images. This indicates the necessity of developing watermarking approaches, which allow the IP to be used for proper reasons while also acting as a way to collect proof against improper uses. \n\nWatermarking methods. Watermark has also been considered to protect the IP of images against unauthorized usage during the fine-tuning of text-to-image models. Wang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model.",
            "score": 0.4735734143952356,
            "section_title": "IMAGE PROTECTION METHODS",
            "char_start_offset": 7951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 851,
                    "end": 870,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "We discuss how we select appropriate target topics to serve as inputs for our data generation pipeline (more in Appendix B). Our objective is to identify topics associated with copyrighted images that contain highly specific features. As such, generating these features would not be considered as transformative works, thereby resulting in explicit copyright infringement [Milner Library, 2023]. We concentrate on three distinct domains: movies, video games, and logos (trademarks), as these domains are particularly well-aligned with potentially copyrighted subjects. Additionally, we prioritize recently released movies and video games, to ensure that our samples are of high quality. Images from recent years are also more likely to be protected by copyright as they have not yet entered the public domain [Office, 2023]. Nevertheless, it is important to emphasize that our approach is a form of academic research, and we thus refrain from asserting that the topics we have gathered in this study definitively qualify as copyrighted subjects (see Appendix C.1 for the complete list of topics). \n\nIt is notable to mention that we exclude topics related to artwork and individual artists from our designated target top-ics. Within the scope of this study, our primary emphasis lies in assessing partial copyright infringement. Specifically, this involves finding the presence of copyrighted content that is visually discernible within image segments. We find that while diffusion models can accurately replicate the style of artists [Casper et al., 2023], this may be a form of derivative work [Cornell, 2022] which is less straightforward to ascertain copyright infringement. Consequently, we refrain from delving into copyright matters pertaining to artistic style and creations by specific artists. The identification of style replication within artworks demands a more intricate approach, involving deeper consideration of how the style is employed, which is beyond the scope of this work.",
            "score": 0.47336205232606915,
            "section_title": "Collect Potentially Copyrighted Topics",
            "char_start_offset": 18799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 1595,
                    "end": 1610,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "270067889",
            "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
            "text": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations. The source code is available at: https://github.com/pittisl/FreezeAsGuard.",
            "score": 0.4719645485345101,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
            "score": 0.4708813003400054,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In Tab. 3, we conduct a human evaluation on two target copyrighted images from two datasets. We randomly select 100 accepted samples obtained from each of the two threat models (the original caption and \u03b5-greedy-cdf). For each target image, a total of 200 samples are randomly shuffled and displayed to 5 graduate students. They are told to label each sample as non-infringing or infringing the copyright of the given target image. Finally, we report their average copyright infringement rates.",
            "score": 0.4706939470629241,
            "section_title": "Human Evaluation",
            "char_start_offset": 37422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "We assume that an attacker has trained a backdoor model for copyright infringement, which, when given a specific text input, will trigger the backdoor to generate infringing images. When a copyright attack is triggered, there exists a trigger text composed of multiple prompts t poison : {p 1 , p 2 , . . . , p n } and an infringing image I poison := DM p (t poison ), which DM p denotes as the poisoned diffusion model. Defenders need to design an efficient infringement detection system, develop strategies to mitigate the impact, and establish a comprehensive method for providing effective evidence for responsibility attribution. Therefore, defenders need to have access to the poisoned dataset d p . During an attack, in order for the poisoned samples to remain sufficiently covert, the similarity of the clean data and poisoned data should follow sim(d c , d p ) < \u03c4 . The goal of the defenders is to design a capable detector D which can distinguish between d c andd p , as depicted in Eq. ( 1):: \n\nwhich \u03b3 denotes the detection threshold. If the probability density distributions of the model DM p under clean and poisoned inputs are P d and P c , respectively, then it is necessary to optimize Eq. ( 2): \n\nIn the defense process, it is necessary to optimize the diffusion model in order to suppress the generation of poisoned samples without compromising its normal generative performance, which can be depicted as sim(DM \n\nBased on the aforementioned considerations, our detection and defense methods will be designed according to these conditions.",
            "score": 0.4702617766130641,
            "section_title": "Threat Model",
            "char_start_offset": 9169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Text-to-image Generative Model.Recently text-to-image diffusion models [35] have emerged as a crucial research area attracting wide attention.These state-of-the-art methods [31,35,36,33,2] have exhibited remarkable capabilities in transforming textual information into visually coherent and realistic images, often demonstrating high performance in terms of accuracy.The advancements in these techniques have opened up a plethora of possibilities for a wide range of downstream tasks, such as image editing [25,12,6,1], image denoising [21,41] and super-resolution [28,9], etc.The advancements in text-to-image generative models have significantly impacted various industries.This convergence raises critical questions about authorship, intellectual property rights, and the implications for plagiarism in the digital era.Addressing these concerns is imperative.\n\nImage Similarity Measurement.Determining plagiarism, a crucial concern in arts and legal domains, often relies on assessing image similarity.Existing metrics like PSNR and SSIM are limited to near-identical images, lacking the capacity to evaluate higher-level similarities [40].Perceptual metrics like LPIPS, while aligning closely with human perception, may have limitations in capturing certain nuances [44].In the realm of videos, metrics like VMAF, despite combining human visual modeling with machine learning techniques, may encounter challenges in specific scenarios [37].While FID is prevalent in generative tasks, it primarily evaluates the distance between sets of images, leaving a gap in measuring inherent similarity between anchor and generated images [19].This underscores the need for further investigation into copyright protection and similarity definition.\n\nUnlearning Method.Carlini et al. [4] highlights that the privacy of Stable Diffusion models is significantly lower compared to Generative Adversarial Networks (GANs) [14].Under the diffusion framework, models tend to retain certain images from the training data, potentially generating outputs that closely resemble the original images.To remove explicit artworks from diffusion models, Andikota et al. [7] presents a fine-tuning method for concept removal from diffusion models.",
            "score": 0.470259957569278,
            "section_title": "Background and Related Work",
            "char_start_offset": 4967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 862
                },
                {
                    "start": 864,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1740
                },
                {
                    "start": 1742,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2078
                },
                {
                    "start": 2078,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 514,
                    "end": 516,
                    "matchedPaperCorpusId": "253018768"
                },
                {
                    "start": 516,
                    "end": 518,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "207761262"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "4766599"
                },
                {
                    "start": 1439,
                    "end": 1443,
                    "matchedPaperCorpusId": "3716103"
                },
                {
                    "start": 1908,
                    "end": 1912,
                    "matchedPaperCorpusId": "1033682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "277151077",
            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
            "text": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
            "score": 0.46986159341992184,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "277043466",
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "text": "Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce \\emph{CoprGuard}, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1\\% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation.",
            "score": 0.46981734128391345,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "274436153",
            "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
            "text": "With the development of text-to-image diffusion models, there is a growing need to prevent their misuse, such as generating malicious content or infringing on copyrights. Removing certain concepts from these models shows promise in addressing this issue. However, model owners/governors often continuously discover improper concepts (e.g., those involving violence or specific artists' copyrighted styles) that the models have learned. For instance, different artists may continuously claim that text-to-image diffusion models like DALL-E 3 [7] and Midjourney [2] can mimic their distinctive styles. Additionally, users may continuously report the generation of malicious content such as violence, guns, and nudity by these models. Thus, model owners/governors require a technique that can swiftly and continuously remove the improper concepts from the deployed models.",
            "score": 0.46969023261629916,
            "section_title": "Necessity of continuous concept removal",
            "char_start_offset": 6714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 869
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "260438807",
            "title": "MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies",
            "text": "Diffusion models have been shown to be highly effective at generating high-quality and diverse samples for text-to-image tasks. However, a potential issue with these models is the risk of plagiarism [33,3], or the generation novelty. As stated by [33], diffusion models are capable of memorizing and combining different image objects from training images to create replicas, which can lead to highly similar or even identical samples to the training data. [3] explores different methods that could extract the training data with a generate-and-filter pipeline, showing that new advances in privacy-preserving training of diffusion models are required. Such issues are especially concerning in the domain of music, where copyright laws are heavily enforced and violations can result in significant legal and financial consequences. Therefore, there is a need to develop strategies to mitigate the risk of plagiarism in text-to-music generation using diffusion models.",
            "score": 0.4693081262777957,
            "section_title": "Plagiarism on Diffusion Models",
            "char_start_offset": 6143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control. In both scenarios, we compare our approach with the only prompt-based IP infringement mitigation method (Wang et al., 2024d), which detects potentially infringed works using LVLM and inputs the detected work information as a negative prompt (Neg-P) into the generative model to avoid infringement. We follow the same settings as in the original paper, but the paper does not address how to handle commercial models that cannot accept negative prompts. For such models, we simply appended the suffix \"without [copyrighted work information (e.g., name, author, etc.)]\" to the original prompt to simulate a negative prompt. Results. From Tables 3 and 4, it can be seen that our method significantly reduces the likelihood of infringement, both for explicit and implicit infringement, with only a slight drop in CLIP Score. The infringement score after only latent control is relatively higher than after prompt control because retrieving non-infringing latents without changing the prompt is quite challenging. However, we can still effectively reduce the infringement score while maintaining higher image-text matching quality. Figure 5 shows visualization results, where it can be observed that we avoid the IP infringement while preserving user requirements. Additional results and detailed time cost analysis are provided in Appendix B.3 and B.4.",
            "score": 0.46921845819721286,
            "section_title": "IP INFRINGEMENT MITIGATION",
            "char_start_offset": 21622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1476
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "276250558",
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "text": "Stress-strain curves MSE [\u2193] 179.5 175.6 12.5 1.2 risk of generating outputs which closely resemble copyrighted material. For this setting, a pretrained proxy model is fine-tuned to determine whether the generation infringes upon existing copyrighted material. This model has been calibrated so that the output logits can be directly used to evaluate the likelihood that the samples resemble existing protected material. Hence, by minimizing this surrogate constraint function, we directly minimize the likelihood that the output image includes copyrighted material. \n\nTo implement this, we define a permissible threshold for the likelihood function captured by the classifier. A balanced dataset of 8,000 images is constructed to fine-tune the classifier and diffusion models. Here, we use cartoon mouse characters 'Jerry,' from Tom and Jerry, and copyrightprotected character 'Mickey Mouse'. When fine-tuning the diffusion model, we do not discriminate between these two characters, but the classifier is tuned to identify 'Mickey Mouse' as a copyrighted example. \n\nInner minimizer. Our correction step begins by performing Principal Component Analysis (PCA) on the 512 features input to the last layer and selecting the two principal components. This analysis yields two well-defined clusters corresponding to the class labels. Provided this, we formulate a correction by iteratively driving the noisy samples toward the centroid of the target cluster, as illustrated in Figure 4 (right). During the early stages of the denoising process, if the classifier assigns a high probability to the sample being 'Mickey Mouse,' we correct the sample toward the 'Jerry' cluster in the feature space. Specifically, we iteratively adjust the sample until its distance from the 'Jerry' cluster falls below a predefined threshold. This correction is achieved by minimizing the distance between the sample's feature representation and the centroid of the 'Jerry' cluster, effectively guiding the generation process away from the copyrighted class label. \n\nAfter this correction, the denoising process is allowed to evolve naturally without further intervention. This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs.",
            "score": 0.46752283907760506,
            "section_title": "Structural analysis",
            "char_start_offset": 28763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "269293420",
            "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models",
            "text": "Qualitative observations. In Fig. 7, we demonstrate that text-to-video diffusion models can replicate the content from their training data or existing videos. This replication is acceptable for fair use purposes such as education, news reporting, and parody. However, misusing videos that contain replicated content with copyright could constitute an infringement. For example, in Fig. 7, the video generated by the commercial model Pika closely replicate the content of the famous painting, \"The Persistence of Memory\", which is copyrighted by the Gala-Salvador Dal\u00ed Foundation. If someone uses this generated video for profit without permission, it could potentially constitute copyright infringement.",
            "score": 0.4673380929098446,
            "section_title": "D.2 Observations",
            "char_start_offset": 31692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 703
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410888671875
        },
        {
            "corpus_id": "268512694",
            "title": "A Watermark-Conditioned Diffusion Model for IP Protection",
            "text": "The recent progress in diffusion models has significantly advanced the field of AIgenerated content (AIGC).Notably, several popular public APIs, such as Stable Diffusion [27] and DALL\u2022E 3 [2], have emerged, providing users with convenient access to create and personalize high-quality images.However, as these systems become more pervasive, the risk of malicious use and attacks increases.In particular, some users with malicious intent may exploit the powerful generation capabilities of these models to create photo-realistic images like deep fakes [25], which can then be disseminated for illegal purposes.Moreover, as generative models are excellent tools for creating and manipulating content, it is crucial to safeguard users' copyrights and intellectual property when using state-of-the-art generative models.By discerning the source of each user's generated output, we can ensure that legitimate users' contributions are protected and prevent unauthorized replication or use of their content.To enable the traceability of diffusion-generated images, a commonly employed strategy is to embed a unique fingerprint to contents generated by an individual user and then forensically identify the owner from the watermarked image.A line of previous works [1,12,26,29,45] has extensively investigated this approach within the realm of traditional multimedia copyright protection, which is commonly referred to as post-hoc watermark.Typically, these strategies involve embedding an imperceptible fingerprint into the generative content while leaving an identifiable trace that can be detected using a pre-designed mechanism.However, the post-hoc watermark requires additional computational costs for watermark injection and is more susceptible to circumvention.For instance, in the event of model leakage, attackers can easily detect and bypass the postprocessing module.Recently, the Stable Signature [11] investigated the latent diffusion paradigm and proposed a method that incorporates watermarks into each latent decoder.However, solely fingerprinting the latent decoder limits their application scenario to the latent diffusion paradigm only and can be easily circumvented by retraining the latent decoder on a clean dataset.",
            "score": 0.46711000531179603,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1871
                },
                {
                    "start": 1871,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "246827447"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "7240130"
                },
                {
                    "start": 1260,
                    "end": 1263,
                    "matchedPaperCorpusId": "14785544"
                },
                {
                    "start": 1263,
                    "end": 1266,
                    "matchedPaperCorpusId": "14463712"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "10176302"
                },
                {
                    "start": 1269,
                    "end": 1272,
                    "matchedPaperCorpusId": "50784854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Copyright infringement for generative models. For copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement. \n\nThis has implications in commercial settings such as companies selling proprietary image generation models as a service or an individual user, particularly when such models produce or are used to create and sell material bearing strong structural similarities to copyrighted images. Thus, there is a potential legal risk for both the providers of image generation models and their users, especially in commercial settings where the 'transformative' nature of generated images may not meet the legal threshold established in copyright law if they possess substantial structural similarity to the original image. \n\nObjective of our data generation pipeline. We aim to systematically generate prompts that are considered generic and not related to any copyrighted topic, but still capable of triggering the generation of copyrighted content from diffusion models. We formally define two desired properties our generated prompts should satisfy. Definition 1. (Prompt sensitivity) Given a semantic measurement f s (\u2022) and a tolerance \u03f5, prompt p is sensitive to a \n\nIn practice, f s can be a text encoder that encodes plain text to be text embeddings for comparison. We detail distance measure D[\u2022||\u2022] in the following discussion. According to the above definition, a prompt is considered to be sensitive to a topic if they have similar language semantics. Moreover, a prompt is adversarial if it can trigger a T2I model to generate copyrighted content. Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts.",
            "score": 0.4664631436281702,
            "section_title": "Problem Formulation",
            "char_start_offset": 6237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2406
                },
                {
                    "start": 2407,
                    "end": 2522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "265445133",
            "title": "A Somewhat Robust Image Watermark against Diffusion-based Editing Models",
            "text": "Considering that editing models can modify an image's distribution and semantics, preserving ownership amid significant changes to the image's content or style becomes crucial. Thus, we assess the degree of alteration in the image's semantic or visual content after editing with either the Imagic or Instructpix2pix models. As the distance in both semantic and visual dimensions increases, the watermark becomes increasingly difficult to extract due to the loss of original image information. Figure 12 displays the distribution of changes in images before and after editing. The two images on the left represent the semantic distance based on CLIP [39], while the two images on the right show the visual distance calculated using VGG [50]. Specifically, akin to existing works [39], [65], [46], when calculating distances using CLIP and VGG, we first project the images onto embeddings and then compute the \u2113 2 distance between these embeddings. \n\nAs observed from Table 4, when we incorporate the top 5% of images with the largest changes in semantic and vision distance into our analysis, there is a varying degree of decline in the accuracy of different watermark extraction strategies. Notably, the Instructpix2pix model tends to have a more significant impact than the Imagic. This is attributed to the fact that Instructpix2pix induces a larger shift in the image distribution, as also indicated by the distance threshold (Thred), i.e., 46.7% and 47.5% higher in semantic and vision distance than Imagic. The overall accuracy of text watermark extraction, as it varies with the edit distance, is illustrated by Figure 16 in Appendix B. \n\nAs illustrated in Figure 13, a detailed comparison of the visual changes between the original and edited images, based on the two different models, reveals a notable observation. When the distance between the original and edited versions is substantial, the alterations in the images are quite significant. This leads to the edited images retaining minimal resemblance to their original counterparts. Consequently, such extensive modifications raise a question about the necessity of further discussions regarding copyright protection. The stark contrast in visual appearance suggests that the edited images might be viewed as distinct enough to mitigate concerns over copyright infringement.",
            "score": 0.46636536417063046,
            "section_title": "Watermark Protection Boundary",
            "char_start_offset": 43969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 649,
                    "end": 653,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "4766599"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "206592766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
            "score": 0.46555656612894114,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "275337292",
            "title": "Origin Identification for Text-Guided Image-to-Image Diffusion Models",
            "text": "Text-guided Image-to-image Diffusion Models. Recent diffusion models, including Stable Diffusion 2 (Rombach et al., 2022), Stable Diffusion XL (Podell et al., 2024), OpenDalle (Izquierdo, 2023), ColorfulXL (Recoilme, 2023), Kandinsky-3 (Arkhipkin et al., 2023), Stable Diffusion 3 (Esser et al., 2024), andKolors (KolorsTeam, 2024), have brought significant improvements in visual generation. This paper considers using these popular models for text-guided image-to-image translation as SDEdit (Meng et al., 2022), which is common and cost-effective in the real world. We also note that certain methods, such as InstructPix2Pix (Brooks et al., 2023), IP-Adapter (Ye et al., 2023), EDICT (Wallace et al., 2023), and Plug-and-Play (Tumanyan et al., 2023), perform this task in other paradigms. For a detailed discussion, please refer to Appendix (Section G). \n\nSecurity Issues with AI-Generated Content. Recently, generative models have gained significant attention due to their impressive capabilities. However, alongside their advancements, several security concerns have been identified. \n\nPrior research has explored various dimensions of these security issues. For instance, (Lin et al., 2024) focuses on detecting AI-generated multimedia to prevent its associated societal disruption, and (Wang et al., 2024c) explores replication problems in visual diffusion models. Additionally, (Fan et al., 2023) and (Chen et al., 2023) explore the ethical implications and technical challenges in ensuring the integrity and trustworthiness of AI-generated content. In contrast, while our work also aims to help address the security issues, we specifically focus on a novel perspective: identifying the origin of a given translated image. \n\nImage Copy Detection. The task most similar to our ID 2 is Image Copy Detection (ICD), which identifies whether a query replicates the content of any reference.",
            "score": 0.464413659531635,
            "section_title": "Related Works",
            "char_start_offset": 7850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1893
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 121,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 143,
                    "end": 164,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 281,
                    "end": 306,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 628,
                    "end": 649,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 687,
                    "end": 709,
                    "matchedPaperCorpusId": "253761481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1724853515625
        },
        {
            "corpus_id": "277043466",
            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
            "text": "The CoprGuard framework is illustrated in Fig. 4, and the implementation details are as follows. The protector employs a frequency domain watermark encoder to embed the watermark W into all images before releasing the private dataset. Then if infringers train or fine-tune diffusion models (including naive and text-to-image diffusion models) with these watermarked images, the watermark W will be embedded into models as well. When suspicious models are released, the protector generates images from these inspected models in a black-box manner and extracts the watermark W \u2032 from inspected images using the pretrained watermark decoder. Finally, the protector compares the watermark W and W \u2032 to determine whether the model uses unauthorized images and violates image copyright.",
            "score": 0.4641304304655277,
            "section_title": "CoprGuard Framework Design",
            "char_start_offset": 14152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 780
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "We evaluate our model on an exclusive test split provided by the DeepfakeArt benchmark. A similarity threshold is first determined using validation set and then applied during testing to make binary decisions. If the cosine similarity between the pair in inference exceeds the threshold, the model classifies the pair as similar-indicating a potential copyright violation. Otherwise, the image is considered dissimilar, suggesting no infringement. This formulation naturally aligns with a binary classification setting, and we adopt precision, recall, and F1 score as our primary performance metrics. We report both overall performance across all attack types and per-attack-type results to provide insight into the model's ro-As shown in Table 2, DFA-CON significantly outperforms all baseline foundation models across precision, recall, and F1 score on the overall test set. This suggests that pretrained vision models-despite being trained on large-scale and diverse datasets-do not produce task-aligned representations sufficient for detecting copyright violations in DeepFake art. In contrast, DFA-CON benefits from its supervised contrastive training on explicitly structured forgery data, enabling it to learn more discriminative and attribution-aware embeddings. \n\nThe observed performance gap highlights an important limitation of general-purpose visual encoders. While such models are effective at broad semantic understanding, they may not capture the fine-grained visual cues and structural similarities that distinguish authentic artworks from forged variants. This is especially relevant in the context of Deep-Fake art, where visual mimicry often occurs at a stylistic or compositional level rather than through overt image artifacts. By training on forgery-aware pairings, DFA-CON is able to better internalize the nuanced characteristics of copyright infringement in generative content.",
            "score": 0.4638290423826757,
            "section_title": "EVALUATION",
            "char_start_offset": 13681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1903
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37744140625
        },
        {
            "corpus_id": "273662226",
            "title": "Exploring Local Memorization in Diffusion Models via Bright Ending Attention",
            "text": "Text-to-image diffusion models like Stable Diffusion (Rombach et al., 2022) have achieved unparalleled proficiency in creating images that not only showcase exceptional fidelity and diversity but also closely correspond with the user's input textual prompts. This advancement has garnered attention from a broad spectrum of users, leading to the extensive dissemination and commercial utilization of models trained on comprehensive web-scale datasets, such as LAION (Schuhmann et al., 2022), alongside their produced images. However, this widespread usage introduces legal complexities for both the proprietors and users of these models, particularly when the training datasets encompass copyrighted content. The inherent ability of these models to memorize and replicate training data during inference raises significant concerns, potentially infringing on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The challenge is further exacerbated by the training datasets' vast size, which makes thorough human scrutiny unfeasible. Illustratively, several high-profile lawsuits (Saveri & Matthew, 2023) have been initiated against entities like Stability AI, DeviantArt, Midjourney, and Runway AI by distinguished artists. These lawsuits argue that Stable Diffusion acts as a '21st-century collage tool', remixing copyrighted works of countless artists used in its training data. \n\nIn response to these legal challenges, Carlini et al. (2023) and Somepalli et al. (2023a) proposed similarity metrics to evaluate memorization, and recent efforts (Somepalli et al., 2023b;Wen et al., 2024;Chen et al., 2024) have focused on developing strategies to detect and mitigate memorization, achieving notable success. However, these metrics and strategies adopt a global perspective, comparing entire generated images to training images. We identify a significant gap in these approaches when dealing with cases where only parts of the training image are memorized, which we refer to as local memorization, as opposed to cases where the entire training image is memorized, termed global memorization.",
            "score": 0.46359348022852703,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 1489,
                    "end": 1510,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1515,
                    "end": 1539,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1613,
                    "end": 1638,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1655,
                    "end": 1673,
                    "matchedPaperCorpusId": "258967610"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "\"To constitute an infringement under the Act there must be substantial similarity between the infringing work and the work copyrighted; and that similarity must have been caused by the defendant's having copied the copyright holder's creation.\" -U.S. 9th Circuit Opinion, Roth Greeting Cards v. United Card Co., 1970. \n\n\"Originality does not signify novelty; a work may be original even though it closely resembles other works, so long as the similarity is fortuitous, not the result of copying. To illustrate, assume that two poets, each ignorant of Based on the aforementioned legal principles, determining infringement requires not only assessing the presence of similar elements but also establishing whether the infringing image was generated through contact with copyrighted artworks [55]. By integrating detection and defense results, we can identify instances where the features of detected images have led to backdoor attacks on the model. This enables us to trace the source of these poisoned samples, determining whether they were intentionally introduced by the user or pre-prepared by an attacker for unintentional use. We refer to this process as infringement feature inversion. Through this process, we can ascertain the contact between infringing and copyrighted images and identify the entity responsible for this interaction. This approach clarifies the allocation of responsibility in cases of copyright infringement and broadens the applicability of defense strategies.",
            "score": 0.46337892680006754,
            "section_title": "Infringement Feature Inversion",
            "char_start_offset": 26417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1489
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "239770285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "258987384",
            "title": "Understanding and Mitigating Copying in Diffusion Models",
            "text": "Text-to-image diffusion models are prone to violating intellectual property rights and causing harm to artists, even unbeknownst to the models' users. Given the rapidly increasing popularity of these models and also the likelihood that users will continue to generate and deploy their images whether or not copying behaviors are addressed, mitigating such behavior can prevent real-world harm. We take the first step towards mitigation by building up an understanding of how and why diffusion models copy their training data. Moreover, we show how to build text-to-image diffusion models which simultaneously produce high-quality images and at the same time do not replicate their training data nearly as often as popular existing models. Another important avenue for preventing copying is constructing detection pipelines so that we can identify copies of training samples before they are deployed, as in Somepalli et al. [2022]. In Figure 9, we show failure cases for SSCD match from the LAION subset to generated images from user captions. Several false positives are reasonable errors, caused by either high style similarity or simple texture of the generated images. A few of the errors are also caused for animated images as shown in the last row. We also observe a few cases where generated, and LAION images show significant domain shift from real images. This may be due to the training data of SSCD, which mostly consisted of real images Pizzi et al. [2022].",
            "score": 0.46186137106523967,
            "section_title": "A Broader Impact",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1468
                }
            ],
            "ref_mentions": [
                {
                    "start": 1448,
                    "end": 1467,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "Vision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding. Notable VLMs such as CLIP (Radford et al., 2021), large vision-language models (LVLMs) such as GPT-4o (GPT4V), Claude 3.5 (Claude3.5), VILA-2.7b (Lin et al., 2023), and Qwen-VL (Bai et al., 2023) have been trained to interpret and generate content based on textual and visual inputs, making them prime candidates for assessing IP infringement detection. However, despite their extensive deployment in various applications, the effectiveness of LVLMs in identifying copyright-protected content remains largely untested. Given the increasing reliance on these models in content moderation, digital rights management, and automated compliance monitoring, it is crucial to evaluate their ability to detect copyright infringement. \n\nTo address this gap, our work presents a systematic evaluation of LVLMs for copyright detection by constructing a dedicated benchmark dataset. Recognizing the absence of a comprehensive dataset that includes both clear cases of IP infringement and ambiguous non-infringing samples, we create a dataset comprising: \n\n\u2022 Positive samples that contain well-known IP characters generated using different AI models with direct and descriptive prompts that replicate their distinctive features. \n\n\u2022 Negative samples that resemble IP characters in certain aspects but do not fully qualify as copyright violations. These images are generated using modified negative prompt engineering techniques. \n\nThese images are selected through rigorous human annotation after the generation. \n\nOur dataset focuses on five widely recognized fictional characters: Iron Man, Batman, Spider-Man, Superman, and Super Mario, ensuring a balanced representation of both positive and negative samples. To evaluate the effectiveness of VLMs, we conduct experiments using in-context learning (ICL) (Mann et al., 2020;Dong et al., 2022) and zero-shot learning (ZSL) (Wang et al., 2019) approaches, where models are tested on their ability to classify image samples accurately.",
            "score": 0.46183322121510667,
            "section_title": "Introduction",
            "char_start_offset": 2013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 241,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 299,
                    "end": 321,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46240234375
        },
        {
            "corpus_id": "244577674",
            "title": "A Photo Identification Framework to Prevent Copyright Infringement with Manipulations",
            "text": "In recent years, copyright infringement has been one of the most serious problems that hamper the development of the culture and arts industry. Due to the limitations of existing image search services, these infringements have not been properly identified and the number of infringements has been increasing continuously. To uncover these infringements and handle big data extracted from copyright photos, we propose a photo copyright identification framework to accurately handle manipulations of stolen photos. From a collage of cropped photos, regions of interest (RoIs) are detected to reduce the influence of cropping and identify each photo by Image RoI Detection. Binary descriptors for quick database search are generated from the RoIs by Image Hashing robustly to geometric and color manipulations. The matching results of Image Hashing are verified by measuring their similarity using the proposed Image Verification to reduce false positives. Experimental results demonstrate that the proposed framework outperforms other image retrieval methods in identification accuracy and significantly reduces the false positive rate by 2.8%. This framework is expected to identify copyright infringements in practical situations and have a positive effect on the copyright market.",
            "score": 0.46158579483607226,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Text-to-image Models. Generative Adversarial Networks (GAN) [27] are one of the early approaches to generating images from text. Following works like StackGAN [43], AttnGAN [39] improve the quality and relevance of generated images of the text-to-image models. These year, diffusion models have become a prominent choice in generative modeling [2]. Diffusion models are a kind of generative models that differ from the traditional models like GANs in that they iteratively transform noisy images to recover the original images through a sequence of learned denoising steps [12]. [29] introduces Latent Diffusion Models (LDMs), which combines diffusion processes with latent variable modeling. The authors proposed a model that operates in a lower-dimensional latent space rather than pixel space. This approach significantly reduces computational costs and enables the generation of high-resolution images with less resource usage. The diffusion models have shown remarkable abilities in generating diverse and high-quality outputs. \n\nCopyright Protection. Copyright is crucial in AI-generated images to protect intellectual property, ensure fair use, and encourage innovation by legally securing the rights of creators and developers. In both US and EU law [21] [23], the substantiality of copyright infringement is one of the most important and measurable determinants. In practice, the court will perform extrinsic and intrinsic tests to measure the substantiality [4] and determine whether the infringement exists. There are some previous works focusing on building a generative model that avoids generating content mimic the copyrighted images. Existed method are machine unlearning, and fine-tuning. For example, \"Forgot-Me-Not\" [42] is a method that can safely remove specified IDs, objects, or styles from a wellconfigured text-to-image model in as little as 30 seconds. [34] proposes the concept \"Near Access Free(NAF)\" and also a method that can output generative models with strong bounds on the probability of sampling protected content. Reinforcement Learning. Reinforcement Learning is a popular pivotal area in machine learning, addressing the challenge of learning optimal actions through interaction with an environment. There are three basic types of RL: value-based, policy-based, and actorcritic.",
            "score": 0.46085190447234203,
            "section_title": "Related works",
            "char_start_offset": 6772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 64,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "8858625"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "265039918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "270045623",
            "title": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
            "text": "Text-to-image diffusion models (Rombach et al. 2022;Saharia et al. 2022;Nichol et al. 2022;Zhang et al. 2024c;Cheng et al. 2024;Yang et al. 2025;Singh et al. 2024) have recently shown impressive potential in generating high-quality images, typically trained on large-scale, webcrawled datasets such as LAION-5B (Schuhmann et al. 2022). However, these uncurated datasets often include harmful or copyrighted content, which can lead the models to generate sensitive or Not Safe For Work (NSFW) images. The research community has initiated various efforts to address this issue, which can generally be classified into three main categories: dataset filtering, model fine-tuning, and post-generation classification.",
            "score": 0.4606620136161892,
            "section_title": "Related Work",
            "char_start_offset": 6389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 711
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 72,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 72,
                    "end": 91,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 128,
                    "end": 145,
                    "matchedPaperCorpusId": "273232929"
                },
                {
                    "start": 311,
                    "end": 334,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68896484375
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "Similarly, in another class action, against Stable Diffusion, Midjourney, and DeviantArt, plaintiffs argue that by training their system on webscraped images, the defendant infringes millions of artists' rights [3]. Allegedly, the images produced by these systems, in response to prompts provided by the systems' users, are derived solely from the training images, which belong to plaintiffs, and, as such, are considered unauthorized derivative works of the plaintiffs' images [55, \u00a7 106 (2)]. \n\nA preliminary question is whether it is lawful to make use of copyrighted content in the course of training [23,34,36]. There are compelling arguments to suggest that such intermediary copying might be considered fair use [36]. For example, Google's Book Search Project-entailing the mass digitization of copyrighted books from university library collections to create a searchable database of millions of books-was held by US courts to be fair use [22]. Then, there is a claim that generative models reproduce protected copyright expressions from the input content on which the model was trained. However, to claim that the output of a generative model infringes her copyright, a plaintiff must prove not only that the model had access to her copyrighted work, but also that the alleged copy is substantially similar to her original work [8,53] Identifying what constitutes \"substantial similarity,\" and unlawful copying remains a pressing challenge. Recent studies have proposed measurable metrics to quantify copyright infringement [5,9,51,59]. One approach, [5,59] asserts that a machine generating output content substantially similar to an input content does not infringe that input content copyright if the machine would have reasonably generated the same output content even without accessing the input content. This argument can be illustrated as follows: Suppose that Alice outputs content A and Bob claims it plagiarizes content B. Alice might argue that she never saw content B, and would reason that this means she did not infringe Bob's copyright.",
            "score": 0.4604870965124901,
            "section_title": "Introduction",
            "char_start_offset": 2032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "152665774"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1532,
                    "end": 1535,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 1559,
                    "end": 1562,
                    "matchedPaperCorpusId": "225067265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03753662109375
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "While some work [15] have pointed out extensive source copying for generative models, for practical purposes, something that does not exactly replicate the source should be enough [17]. In particular, we observe that if an image has the same semantic segmentation mask as the original source data, they have a higher likelihood of being identified as copyright infringement. Conversely, a simple distortion like reflection or resizing is often enough to circumvent copyright claims [18]. \n\nFrom this observation and taking cue from conditional diffusion model [16], we propose a twostep image generation model. The first step is to create an image segmentation mask from a text prompt using a diffusion model. This model would be trained on image segmentation masks instead of complete images. There would be a greater sampling rate, so that the model can deviate from the training segmentation masks. Using the generated segmentation mask as a conditional control, we would next create the full image, and here we would use a diffusion model as used by [16] with a smaller sampling rate. As we would have a lesser likelihood of generating a similar segmentation mask, we would have a lesser likelihood of generating a copy as well. We suspect this is closer to how an animal or human brain conceptualizes an image in the mind, first by identifying the parts and then filling in the details. \n\nTo the best of our knowledge, this is the first work to address copyright concerns in diffusion model-based image generation by utilizing a two-step process involving image segmentation masks.",
            "score": 0.4604371224962532,
            "section_title": "Introduction",
            "char_start_offset": 3937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "273163098",
            "title": "Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models",
            "text": "Unauthorized data usage of diffusion models have been a crucial topic that raises increasing attention (Franceschelli & Musolesi, 2022;Sag, 2023;Samuelson, 2023). One popular approach to relieve the concern of unauthorized data usage in diffusion models is to add adversarial (Salman et al., 2023;Shan et al., 2023a;Liang et al., 2023;Liang & Wu, 2023;Van Le et al., 2023;Shan et al., 2023b;Xue et al., 2023) or copyright watermarks (Cui et al., 2023;Zhu et al., 2024) to images. These watermarks either resist diffusion models from training on the image or introduce copyright information to diffusion models trained on the image. However, recent research questions the effectiveness of these watermarks that they might be failed easily (Zhao et al., 2023). Another recipe is to erase or unlearn the copyright data in the diffusion model (Gandikota et al., 2023;Zhang et al., 2023a;Fan et al., 2023;Wu et al., 2024a;Zhang et al., 2024). Nevertheless, Zhang et al. (2023b) shows that current machine unlearning on diffusion models can be bypassed by soft prompting and other fine-tuning methods. As protective and post-training refining methods are faced with questioning, we highlight the potential to directly detect copyright data in the training dataset of diffusion models, by which we help the calling for copyright protection beyond technical methods.",
            "score": 0.459480091717322,
            "section_title": "A.3 ADDITIONAL RELATED WORKS",
            "char_start_offset": 32300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1358
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 135,
                    "matchedPaperCorpusId": "234777751"
                },
                {
                    "start": 145,
                    "end": 161,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 391,
                    "end": 408,
                    "matchedPaperCorpusId": "265351716"
                },
                {
                    "start": 839,
                    "end": 863,
                    "matchedPaperCorpusId": "257495777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "In this section, we outline two methods for generating positive samples that infringe on copyright protection laws. For this purpose, we selected three widely-used generative AI models: Stable Diffusion XL (generated 40% of the image samples) (Podell et al., 2023), Ideogram (generated 40% of the image samples) (ideogramAI), and DALL-E (generated 20% of the image samples) (Betker et al., 2023). \n\nGenerate IP Characters with Direct Prompt. \n\nThe simplest approach to generating positive samples involves using direct prompts with generative models, such as \"Generate an image of <a char-acter>.\" This method typically produces images that closely resemble the IP-protected characters, as illustrated in Fig. 2. Using this technique, we generated 40 images for each character class. \n\nGenerate with Descriptive Prompt. Rewriting direct prompts that reference copyright-protected content into longer, more descriptive prompts, as explored by Wang et al. (2024) and He et al. (2024), can sometimes reduce the risk of IP infringement. However, this approach is not entirely effective in preventing outputs that closely resemble copyrighted characters (He et al., 2024), as rewritten prompts often retain a high degree of similarity to the original IP-associated names. \n\nTo enhance the diversity of our dataset, we first generate images using descriptive prompts and then apply a human evaluation process to filter out most positive samples, as detailed in Section 3.2. We use GPT-4o (GPT4V) here as its exceptional text generation capabilities. We construct descript prompt with the following guidance to GPT-4o: \n\n\u2022 Creating a prompt that describes a character similar to <Target Character>. This prompt should enable text-to-image AI models to generate images without directly mentioning the name of the <Target Character>. \n\nFinally, we curate the selected positive images, as illustrated in Fig. 3.",
            "score": 0.4594284013318194,
            "section_title": "Collecting Positive Samples",
            "char_start_offset": 12340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.284423828125
        },
        {
            "corpus_id": "265308723",
            "title": "EditShield: Protecting Unauthorized Image Editing by Instruction-guided Diffusion Models",
            "text": "As diffusion models rapidly evolve, concerns regarding its misuse cannot be overlooked. To address this issue, various strategies have been developed to safeguard public images of individuals against diffusion models. The main idea involves introducing specially crafted noise to the image. For instance, PhotoGuard [36] introduces imperceptible perturbations to images, effectively raising the computational cost for malicious editing attempts. As a result, LDMs cannot generate realistic images when attempting to edit upon the perturbed image. In the field of artworks and paintings, Shan et al. [37] developed Glaze which can protect artists against style mimicry by text-to-image diffusion models. Similarly, Liang et al. [25] adopted the concept of adversarial examples for protecting images from being learned, imitated, and copied by diffusion models. Van Le et al. [42] generated subtle noise to the image published by the user, to disrupt the generation quality of DreamBooth trained on these perturbed images. These protective measures target copyright infringers who utilize diffusion models with specific prompts tied to particular subjects. However, their effectiveness is challenged in real-world scenarios where models encounter a variety of unseen prompts, potentially making these protections ineffective. Furthermore, the specific problem posed by instruction-guided diffusion models-where user-generated instructions dictate the content creation process -remains unexplored by existing works.",
            "score": 0.4592575471242046,
            "section_title": "Protection Against Unauthorized Use",
            "char_start_offset": 8130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "256697414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "269604904",
            "title": "DiffuseTrace: A Transparent and Flexible Watermarking Scheme for Latent Diffusion Model",
            "text": "The strides made in latent diffusion models [10,17,28,35] have substantially elevated the capacity for synthesizing photorealistic content in image generation and profoundly impact text-to-image [32,46], image editing [5,24], in-painting [21,31], super-resolution [12,33], content creation [26,27] and video synthesis [4,16].Relevant commercial applications are becoming mainstream creative tools for designers, artists, and the general public.\n\nHowever, contemporary text-to-image generation models, such as Stable Diffusion and Midjourney, can generate a multitude of novel images as well as convincing depictions of fabricated events for malicious purposes.Criminals might utilize LDMs to produce insulting or offensive images, which shall be disseminated to spread rumors and pose a substantial threat to societal security.The hazards of deepfakes, impersonation and copyright infringement are also prevalent issues associated with current generative models.\n\nThe potential illicit use of text-to-image models has spurred research for embedding watermarks in model outputs.Watermarked images contain signals imperceptible to humans but are marked as machine-generated.Copyright information of the model and the identity information of the model users will be embedded into images.Extracting watermarks from AI-generated images enables the detection of model copyrights and tracing unauthorized users.False and harmful images can be promptly identified and removed from platforms and unauthorized users of the model can be traced through the extraction of image information, which mitigates the potential harm caused by AI-generated content.\n\nExisting research on image watermarking tended towards postprocessing solutions.The core concept involves embedding the watermark into the image with minimal adjustments, emphasizing subtlety and intricacy.For instance, the watermark implemented in Stable Diffusion [8] operates by altering a particular Fourier frequency within the generated image.This type of watermark faces a key trade-off between watermark robustness and image quality.For diffusion model watermarks, Some researchers have proposed embedding fixed messages into generated images by fine-tuning diffusion models like U-Net [30] or variational autoencoders.",
            "score": 0.4591360190365983,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 444
                },
                {
                    "start": 446,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 962
                },
                {
                    "start": 964,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1172
                },
                {
                    "start": 1172,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1644
                },
                {
                    "start": 1646,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1852
                },
                {
                    "start": 1852,
                    "end": 1995
                },
                {
                    "start": 1995,
                    "end": 2087
                },
                {
                    "start": 2087,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "233241040"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "258187553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Metrics for assessing copyright violations quantify the similarity between AI-generated and copyrighted content across various domains.Cosine similarity measures the orientation of feature vectors, where a higher value signifies greater similarity [65].This metric can potentially be combined with the feature similarity index [145] to provide a quantifiable analysis of copyright infringement in images.Additionally, the Jaccard similarity evaluates the overlap between two sets, useful for analyzing unique word overlaps in texts or segmented regions in images, providing insight into the similarity of content composition [89,93].Lastly, the BLEU score, while originally designed for machine translation, measures the precision of n-gram overlaps between generated and reference texts, indicating the extent of language structure and usage similarity [94].",
            "score": 0.45790136407681603,
            "section_title": "Metrics for Copyright Violations",
            "char_start_offset": 31402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 859
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "38769225"
                },
                {
                    "start": 327,
                    "end": 332,
                    "matchedPaperCorpusId": "10649298"
                },
                {
                    "start": 629,
                    "end": 632,
                    "matchedPaperCorpusId": "258463899"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2529296875
        },
        {
            "corpus_id": "277856857",
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "text": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "score": 0.4577926089780417,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "262044913",
            "title": "Security and Privacy on Generative Data in AIGC: A Survey",
            "text": "He et al. [57] extended current detectors to the potential of text attribution to recognize the source model of a given text. The results show that all these detectors have certain attribution capabilities and still have room for improvement. Moreover, model-based detectors can significantly outperform metric-based detectors. \n\nFor visual generative data, a lot of attribution works on GANs [23,50,149,150,155] has been proposed. RepMix [23] is a GAN-fingerprinting technique based on representation mixing and a novel loss. It is able to determine from which structure of GAN a given image is generated. POSE . Recent works have begun to focus on DMs. Sha et al. [115] constructed a multi-class (instead of binary) classifier to attribute fake images generated by DMs. Experiments showed that attributing fake images to their originating models can be achieved effectively, because different models leave unique fingerprints in their generated images. Lorenz et al. \n\n[3] designed the multi-local intrinsic dimensionality (multiLID), which is effective in identifying the source diffusion model. Guarnera et al. [55] developed a novel multi-level hierarchical approach based on ResNet models, which can recognize the specific AI architectures (GANs/DMs). The experimental results demonstrate the effectiveness of the proposed approach, with an average accuracy of more than 97%. \n\nIntriguingly, a new work [131] can attribute generative data to the training data rather than the source model, necessitating the identification of a subset of training images that contribute most significantly to the generated data. In Fig. 8, we can query the generated data in the training set and evaluate their similarity, which can contribute to protecting the copyright of training data rather than models.",
            "score": 0.4571715341982998,
            "section_title": "Generative Attribution.",
            "char_start_offset": 49129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "250280166"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "234357723"
                },
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "247158092"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "257496280"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "201058738"
                },
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "250280166"
                },
                {
                    "start": 666,
                    "end": 671,
                    "matchedPaperCorpusId": "255546643"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "257255351"
                },
                {
                    "start": 1409,
                    "end": 1414,
                    "matchedPaperCorpusId": "259171757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153564453125
        },
        {
            "corpus_id": "275337292",
            "title": "Origin Identification for Text-Guided Image-to-Image Diffusion Models",
            "text": "Text-guided image-to-image diffusion models are notable for their ability to transform images based on textual descriptions, allowing for detailed and highly customizable modification. While they are increasingly used in creative industries for tasks such as digital art re-creation, customizing visual content, and personalized virtual try-ons, there are growing security concerns associated with their misuse. As illustrated in Fig. 1, for instance, they could be misused for misinformation, copyright infringement, and evading content tracing. To help combat these misuses, this paper introduces the task of origin IDentification for text-guided Image-to-image Diffusion models (ID 2 ), which aims to identify the original image of a generated query from Transform this historic city view into a futuristic metropolis through the archway. \n\nTransform the delicate botanical sketch into a vibrant, fantastical plant glowing under a moonlit night. \n\nFigure 2: The demonstration for visual discrepancy between generated images by different diffusion models. The images generated by various models exhibit distinctive visual features such as realistic textures, complex architectures, life-like details, vibrant colors, abstract expression, magical ambiance, and photorealistic elements. \n\na large-scale reference set. When the origin is identified, subsequent compensations include deploying factual corrections for misinformation, enforcing copyright compliance, and keeping the tracing of target content. \n\nA straightforward solution for the proposed ID 2 task is to employ a similarity-based retrieval approach. Specifically, this approach (1) fine-tunes a pre-trained network by minimizing the distances between generated images and their origins, and (2) uses the trained network to extract and compare feature vectors from the queries and references. However, this approach is impractical in real-world scenarios.",
            "score": 0.456675658628886,
            "section_title": "Introduction",
            "char_start_offset": 1603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "244577674",
            "title": "A Photo Identification Framework to Prevent Copyright Infringement with Manipulations",
            "text": "For example, as shown in Figure 2, if a shopping website steals photos to promote sunglasses, it only crops the person wearing sunglasses from the background (image cropping) and combines several images of sunglasses (image collage). In addition, image manipulations such as resizing, color distortion, flipping, and compression can occur during image editing, and these can also lead to degradation in identification performance. Image cropping and image collages have problems, loss of information from original photos and misidentification of multiple photo copyrights, respectively. These problems are difficult to solve with existing web image search services. As shown in Figure 3, since existing image search services do not consider the characteristics of copyright infringements, existing web image search services show the poor responses to the aforementioned manipulations. Figure 3a,b show that Google image search and TinEye matched only one of the match results correctly, and Yandex matched only one photo as shown in Figure 3c. The limitations of existing image search services emphasize the need for a new photo copyright identification and database. In addition to technical limitations, the absence of a service capable of providing copyright information also makes it difficult to handle infringements. The service in Figure 4 can facilitate the legal use of copyrighted photos by connecting users and copyright holders, and helps to find copyright infringements based on the photo copyright identification framework. 2a by Google [7], TinEye [9], and Yandex [8]. We additionally tried with Bing, but did not obtain any results. In this paper, we propose a photo copyright identification framework that can handle the manipulations that appear in Figure 2a. The proposed framework in Figure 4 is composed of three modules, Image RoI Detection, Image Hashing, and Image Verification, where the original photo and the manipulated photo are matched accurately. Image RoI Detection divides a collage of cropped images into RoIs, which represent the characteristics of each image well. This module reduces the influence of geometric manipulations to detect RoIs consistently from original photos and manipulated photos. The image collage dataset is constructed to train optimized detectors for Image RoI Detection. The most efficient detector is applied through benchmarking the state-of-art object detectors for this dataset.",
            "score": 0.45585931064942986,
            "section_title": "Introduction",
            "char_start_offset": 2093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "276771561",
            "title": "Jailbreaking Safeguarded Text-to-Image Models via Large Language Models",
            "text": "Text-to-image models: A text-to-image model [17,8,27,15,26,20] generates an image based on a prompt, ensuring high semantic similarity between the prompt and the resulting image. Although various types of text-to-image models exist, diffusion-based models have become predominant in recent years. In this work, we focus specifically on diffusion-based text-to-image models. State-of-the-art diffusion-based text-to-image models [17, 27,8] perform the diffusion process within a latent space. These models take a text description as input and iteratively denoise a noisy latent vector according to the semantics of the description, ultimately obtaining a denoised latent vector. A decoder then maps this denoised latent vector back to the image space, producing a realistic and semantically consistent image. For example, Stable Diffusion [27] leverages the CLIP model [21] to encode the text description into an embedding vector. Starting from a noisy latent vector sampled from a Gaussian distribution, a U-Net iteratively denoises this vector, and a decoder from a pre-trained Variational Autoencoder [10] generates the final image from the denoised vector. Safety guardrails for T2I models: To prevent the generation of harmful images, text-to-image models are equipped with safety guardrails, which fall into two primary categories: safety filters and alignment methods. Safety filters [3,2,1,4] use an external classifier to assess whether the input text prompt or the output image contains harmful content. If harmful content is detected, the image generation will be blocked. Industry-leading text-to-image models, including Stable Diffusion [23] and DALL\u2022E [17], employ safety filters to moderate their outputs. \n\nIn contrast, alignment methods [25,7,12,13,34] prevent harmful content generation by adjusting the models' parameters. For example, Stable Diffusion v2.1 [23] employs a safe training approach, aligning the model by retraining it on a dataset that excludes harmful content. However, this approach is computationally costly, as it requires retraining the entire model.",
            "score": 0.4556104076863037,
            "section_title": "Related Works",
            "char_start_offset": 5364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 868,
                    "end": 872,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1757,
                    "end": 1759,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1762,
                    "end": 1765,
                    "matchedPaperCorpusId": "268351735"
                },
                {
                    "start": 1876,
                    "end": 1880,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Our study differs from related works in terms of objectives and technical challenges. We focus on the Copyright Infringement Attack, a new type of backdoor attack associated with the text-to-image diffusion model. On the technical side, our method faces constraints that are both distinct from and more challenging than those in related works, making it difficult to adapt their solutions to our problem. In the following, we provide a detailed comparison with related works. \n\n\u2022 The \"TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets\" (Chen et al., 2023) introduces a Trojan attack technique for diffusion models. The goal of this attack is to manipulate the model to make it either consistently produces outputs that belong to a specific class, creates outputs that come from a completely different distribution than expected, or generates a particular image (such as Mickey Mouse) regardless of the actual input. To achieve this, the attack involves designing the Trojan diffusion and generative processes used during the model's training and inference stages, ensuring the diffusion model behaves as intended by the attackers. Diverging from the approach of this work, we focused on the text-to-image diffusion model, as opposed to a diffusion model that operates without text guidance. Furthermore, rather than requiring extensive control over the training and inference phases of a diffusion model, our method necessitates only the insertion of poisoning data, making it more practical for application. \n\n\u2022 The study \"From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models\" (Pan et al., 2023) investigates the vulnerability of diffusion models (DMs) to backdoor attacks, where the training dataset is poisoned without altering the diffusion process. This work emploies a mixup strategy for creating poisoning image (original image + trigger pattern). The goal of the attack is to make text-to-image diffusion models could generate incorrect images that are misaligned with the actual text condition or generate abnormal images. It reveals that such attacks lead to the generation of images misaligned with intended text conditions, and a phenomenon exacerbated by trigger amplification, where the presence of backdoor triggers in generated images larger than the porpotion poisoning data.",
            "score": 0.4554409099183184,
            "section_title": "A.1. Detailed Comparisons with Other Diffusion Model Attack Methods",
            "char_start_offset": 34337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 571,
                    "matchedPaperCorpusId": "257482560"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "Text-to-Image (T2I) generative models [2,9,21,18,19,29] are mostly trained on massive image data from the web, which are known to contain diverse copyrighted, privacy-sensitive, and harmful images.Recent works [28,27,4] demonstrate that diffusion-based image generative models memorize a portion of the training data, allowing the replication of the copyrighted contents [31,33].Although what models are used in recent commercial T2I systems is mostly unknown to the public, we find they also easily generate copyrighted contents (Figure 1a).Such copyright violation is one of the most critical real-world safety problems associated with generative models, and there are several ongoing lawsuits [24,13,7] against the service providers regarding this matter.\n\nTo prevent such potential copyright violations, ChatGPT [21] and Copilot [18] censor user requests by blocking generation of copyrighted materials or rephrase the users' prompts, to prevent them.However, are they really secure against unauthorized reproduction of copyrighted materials?To the best of our knowledge, there is no work on quantitative evaluation of the copyright violation by the commercial T2I systems, making it difficult for the service providers to red-team their systems (Figure 1b).Furthermore, for intellectual property (IP) owners, it requires a large amount of effort to verify the usage of contents in those systems via manual trial-and-error processes (Figure 1b).To evaluate the safety of the T2I systems, we construct a copyright Violation dataset for T2I models, termed VioT.This dataset is comprised of five categories of copyrighted contents that include the characters, logos, products, architectures, and arts, legally protected in the form of copyright [20,22,12].Then, we attempted naive prompts to induce the T2I systems to generate copyright-violated contents.Surprisingly, we observe that current commercial T2I systems, including Midjourney [19], Copilot [18], and Gemini [29], result in copyright violations with a low block rate, 13.3%, even with such naive prompts.",
            "score": 0.45477480281935045,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 758
                },
                {
                    "start": 760,
                    "end": 955
                },
                {
                    "start": 955,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1563
                },
                {
                    "start": 1563,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 217,
                    "end": 219,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "We evaluate with a model p behaves with an equal chance of sampling from either model q 1 or q 2 . We consider such model p as a strict protection model that even with the original caption as prompt, there is a 50% chance of generating from the safe model. In contrast, a model p trained on the entire dataset could frequently produce highly similar generations to the target copyrighted image, making it extremely inefficient for the selection of a valid threshold k. Next, we will separately discuss the data, the metrics, and the threat prompts for evaluation. \n\nEvaluation Data. We use samples generated by p for evaluation. To determine whether they infringe the copyright of the target image, we rely on the similarity between samples and the target image as there is no widely acknowledged computable standard for infringement to our knowledge. We employ the current SOTA similarity metric SSCD [31] for copy detection. As SSCD scores of infringed samples for different target images differ significantly by obser-   vation (shown in Fig. 5), we do not indicate a fixed score threshold for all target images. Instead, we use relative thresholds determined as percentiles of the similarity scores among samples generated by p with the original caption of the target image as the prompt, e.g. SSCD-50%. Recognizing the variable criteria of infringement, we report results with other choices of thresholds in Sec. 11.3. \n\nEvaluation Metrics. The CP-k method achieves copyright protection by selectively accepting generated samples using the threshold k x , which can be indicated by the Acceptance Rate (AR). For a given AR, a good protection system is expected to rarely accept infringing content, i.e., have a low False Accept Rate (FAR), as defined in Eq. ( 8). It is worth noting that the choice of k x dictates the trade-off between model safety and efficiency. Furthermore, to our knowledge, there is no principled way to determine k x . As a result, we evaluate the success of attack by reporting the FAR at different AR, e.g., FAR@5%AR. Additionally, the copyright infringement rate (CIR) is also presented for scenarios without copyright protection, i.e., AR=100%.",
            "score": 0.45469184960612086,
            "section_title": "Evaluation Settings",
            "char_start_offset": 22478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection. Our research introduces a novel two-step image generation model designed to specifically address these concerns. This model operates by first generating segmentation masks from a given text prompt. These masks are then used to guide a Stable Diffusion model, effectively minimizing source copying while maintaining high fidelity in the final generated image. This approach also successfully circumvents the need for textual embedding, further streamlining the process and reducing potential avenues for copyright infringement. By decoupling the image generation process in this manner, we offer a promising pathway towards responsible and ethical AI image generation that respects both creative expression and copyright protection.",
            "score": 0.45433435215570706,
            "section_title": "Conclusion",
            "char_start_offset": 26105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1131
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "To examine the attack performance in a more realistic setting, we scale up the clean dataset, utilizing a 60,000-image dataset from COYO-700m (Byeon et al., 2022). In this scenario, data from Midjourney v5, maintained by JohnTeddy3 (2023), is used as the copyrighted material, where the copyrights belong to their respective creators. We discuss the potential ethical and copyright issues of those datasets in Appendix C. \n\nIn our main experiments, to ensure the robustness and reliability of our findings, we conduct 20 independent trials for each scenario. Each independent trial selects a single copyrighted image as the target. For each trial on the Pokemon dataset, one image is selected as the copyright-protected image, and the remaining 842 images serve as the clean dataset. For each trial on the Midjourney v5 dataset, one image is selected as the copyright-protected image, and the subset from LAION or COYO serves as the clean data. Our method divides the target image into n distinct elements, with n varying according to the image's specific characteristics. On average, for the Pokemon dataset, the number of decomposed elements per image is 4.64. For the Midjourney dataset, the average number is 4.95. For each of these visual elements, we create k poisoning text-image pairs. For example, in the scenario with the Pokemon dataset, k is calculated as p\u00d7842 n . Each poisoning pair consists of a GPT-4 generated caption and a diffusion-inpainted image, ensuring that each of the k poisoning pairs is distinct. \n\nOn the model side, we primarily use Stable Diffusion v1.4 in our experiments, but also examine versions 1.1 through 1.5 to measure the effectiveness of our method across different variants. We leave the detailed discussion in appendix E.1. \n\nDetector Selection and Attack Success. Recent studies (Somepalli et al., 2023a;b;Pizzi et al., 2022) recognized the cosine similarity measured by Self Supervised Copy Detection (SSCD) as a state-of-the-art technique for copyright detection. Our study extends this recognition through a verification experiment tailored to our setting.",
            "score": 0.45390856943033986,
            "section_title": "Experiment Setup",
            "char_start_offset": 20569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 1824,
                    "end": 1849,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1851,
                    "end": 1870,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
            "score": 0.45234035352241186,
            "section_title": "Introduction",
            "char_start_offset": 2047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "239770285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "To determine whether samples generated by model p infringe the copyright of the target image, we need to assign ground-truth labels to these samples. Unfortunately, to our knowledge, there is currently no widely recognized computable standard for determining whether an image infringes copyright. In fact, the criteria for copyright infringement determination may evolve with changing societal perceptions. Alternatively, we rely on the similarity between samples and the target image as the basis for determining infringement. In order to distinguish between noninfringing and infringing samples, an ideal similarity score should assign lower scores to non-infringing samples and higher scores to infringing samples. Recognizing the limitations of a singular similarity measure, we compare the performance of SSCD [31] and CLIP score for determining copyright infringement. In Fig. 5, we plot the histograms of SSCD scores and CLIP scores for images generated by original captions of all target copyrighted images in two datasets. We can observe that the distributions of SSCD scores demonstrate a more clearly bimodal pattern compared with CLIP scores. This means that non-infringing and infringing samples can be better distinguished by the two modes of distribution of SSCD scores. In Fig. 6, we show example images with different values of similarity scores in ascending order. We can find that non-infringing samples may have higher CLIP scores than infringing samples with target images. However, there is a clear threshold (e.g., 50%) for SSCD score to distinguish non-infringing and infringing samples. Thus, in this paper, we use the SSCD score for infringement judgment. In Sec. 5.3, we report results with SSCD-50% as the infringement threshold. Further, considering the evolving nature of copyright infringement standards, we utilize other varying thresholds. According to the observation in Fig. 5, we consider modeling the SSCD score distribution as a mixture of two Gaussian distributions and use the mean value of two means of the Gaussian distributions as the similarity threshold, denoted as SSCD-gmm. For POKEMON dataset, we further consider SSCD-45% and SSCD-55%.",
            "score": 0.45198993911816465,
            "section_title": "Infringement Judgment",
            "char_start_offset": 33299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "256697414",
            "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
            "text": "Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git.",
            "score": 0.450859513444537,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "269330225",
            "title": "DeepFeatureX Net: Deep Features eXtractors based Network for discriminating synthetic from real images",
            "text": "Sha et al. [44] proposed DE-FAKE, a machine learning classifier designed for detecting diffusion model-generated images across four prominent text-to-image architectures.The authors then proposed a pioneering study on the detection and attribution of fake images generated by diffusion models, demonstrating the feasibility of distinguishing such images from real ones and attributing them to the source models, and also discovering the influence of prompts on the authenticity of images.Recently, Guarnera et al. [20] proposed a method based on the attribution of images generated by generative adversarial networks (GANs) and diffusion models (DMs) through a multi-level hierarchical strategy.At each level, a distinct and specific task is addressed: the first level (more generic), allows discerning between real and AI-generated images (either created by GAN or DM architectures); the second level determines whether the images come from GAN or DM technologies; and the third level addresses the attribution of the specific model used to generate the images.\n\nThe limitations of these methods mainly concern the presence of experimental results performed only under ideal conditions and, consequently, the almost total absence of generalization tests: the classification performance of most state-ofthe-art methods drops drastically when testing images generated by architectures never considered during the training procedure.",
            "score": 0.4507667701543233,
            "section_title": "Related Works",
            "char_start_offset": 6552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 1062
                },
                {
                    "start": 1064,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 15,
                    "matchedPaperCorpusId": "255546643"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "268302141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0743408203125
        },
        {
            "corpus_id": "269137659",
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "text": "In our paper, we rethink the problem of copyright infringement in the context of artistic styles.We first argue that image-similarity approaches to copy detection may not fully capture the nuance of artistic style copying.After reformulating the task to a classification problem over image sets, we develop a novel tool -ArtSavant, consisting of a dataset and two complementary methods that can effectively recognize artistic styles, via neural and tag based signatures.The success of our method offer strong evidence to the existence of unique artistic signatures, a necessary pre-requisite for styles to be protected.We highlight Tag-Match, which scaffolds a black-box AI component with interpretable intermediate outputs and a transparent way in which intermediate outputs are combined to arrive at a final prediction, resulting in a white(r)-box AI system.TagMatch, which can classify a set of images to an artist with reasonable accuracy as well as provide succinct text explanations and image attributions.Using these two detectors, we analyze generated images from different text-to-image generative models highlighting that amongst all the artists (including many famous ones), only 20% of the artists are recognized as having their style copied.",
            "score": 0.4507426186814282,
            "section_title": "Conclusion",
            "char_start_offset": 33471,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 1012
                },
                {
                    "start": 1012,
                    "end": 1254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "277993622",
            "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
            "text": "The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of\"copyright takedown\"methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.",
            "score": 0.4505744277805611,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "261276613",
            "title": "Unified Concept Editing in Diffusion Models",
            "text": "While text-to-image diffusion models are becoming increasingly popular in commercial art and graphic design, they tend to suffer from various issues, which have previously been addressed separately. \n\nCopyright issues. Recent lawsuits [1,38] have contended that models like Stable Diffusion infringe on many artistic styles, and researchers have found that the models can memorize some copyrighted training data nearly verbatim [6,42]. To reduce such memorization, previous work proposes randomizing and augmenting training image captions [43], while other work has explored a technique called image cloaking that allows artists to protect their content from being imitated by large generative models by adding specially crafted adversarial perturbations to images before publishing them online [35,39]; both these approaches require thorough preparation of the training corpus. Another approach adjusts a model after training is complete, deleting an undesired concept by modifying model weights [14,16,21,22,47]. Our method adopts that concept-erasure approach, and we benchmark against the previous state-of-the-art. Our method differs from previous concept erasure methods because it is a closed-form edit that removes many concepts at once. \n\nOffensive content. Diffusion models also sometimes generate inappropriate images, such as nude and violent images. Various methods have been proposed to filter out inappropriate images from the training data or at inference time [13,28]; for example the Stable Diffusion implementation includes a \"not safe for work\" safety checker that returns a black image when an unsafe image is detected [4,23,33], and other work has addressed the issue in through image editing at infer-ence time [36]. In cases where open-source code and model weights are openly available, such post-production filters can be easily disabled [40]. A more difficult-to-circumvent approach removes the knowledge of unwanted concepts from the model weights; previous methods taking that approach have proposed attention re-steering through fine-tuning [47], fine-tuning the attention weights [14] and continual learning [16]. While previous methods all fine-tune the model, we propose a fast and efficient method to erase offensive concepts using a closed-form edit. \n\nSocial biases.",
            "score": 0.4498311224085917,
            "section_title": "Related Work",
            "char_start_offset": 2662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2285
                },
                {
                    "start": 2288,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "210018592"
                },
                {
                    "start": 431,
                    "end": 434,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "263890280"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 2111,
                    "end": 2115,
                    "matchedPaperCorpusId": "257495777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "263622213",
            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
            "text": "One potential reason is that the watermark-generating procedure predominantly adheres to traditional watermark strategies, which are intended to trace the source of an image rather than protecting its IP in the context of diffusion models fine-tuning. Consequently, there's no assurance that the watermark's features will be learned by the model before other features. \n\nSecond, as indicated by Ma et al. (2023), due to the distribution shift between the fine-tuning data of generative models and the resulting generated images, it is crucial to incorporate images generated by fine-tuned models to develop the watermark detector. Since there are numerous fine-tuning methods introduced from different perspectives, a detector for one method might lose its effectiveness for others. This issue is highlighted by Ma et al. (2023) that a large drop in watermark detection performance is observed when applying a detector tailored to one fine-tuning method to others. Meanwhile, in reality, a data protector may not know which fine-tuning method was used by the infringer, thus it is desired to design a watermark detection strategy that is effective for various fine-tuning methods. \n\nTo tackle the aforementioned challenges, we propose a novel watermarking framework, FT-Shield, tailored for data's copyright protection against the Fine-Tuning of text-to-image diffusion models. In particular, we introduce a training objective incorporating the fine-tuning loss of diffusion models for watermark generation. By minimizing the objective, we ensure that the optimized watermark pattern can be quickly learned by diffusion model at the very early stage of fine-tuning. As shown in Figure 1, even when the style has not been adopted, our watermark has already been learned by the diffusion model. Furthermore, to obtain a watermark detector for various fine-tuning methods, we introduce a detection framework based on Mixture of Experts (Jacobs et al., 1991). The effectiveness of FT-Shield is verified through experiments across various fine-tuning approaches including DreamBooth (Ruiz et al., 2023), Textual Inversion (Gal et al., 2022), Text-to-Image Fine-Tuning (von Platen et al., 2022) and LoRA (Hu et al., 2021), applied to both style transfer and object transfer tasks across multiple datasets.",
            "score": 0.44936581452682123,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 1933,
                    "end": 1954,
                    "matchedPaperCorpusId": "572361"
                },
                {
                    "start": 2078,
                    "end": 2097,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "273550007",
            "title": "The Cat and Mouse Game: The Ongoing Arms Race Between Diffusion Models and Detection Methods",
            "text": "The rapid advancement of diffusion models represents a pivotal shift in synthetic media generation. These models offer an unparalleled degree of control and realism, outpacing GANs in producing high-quality, diverse images [1], [2]. Platforms like Midjourney and Stable Diffusion have made this technology widely accessible, enabling users, even without technical expertise, to generate photorealistic content from simple text prompts [3]. This democratization of content creation fosters innovation in various fields. For example, in art and design, diffusion models are used to explore new aesthetic possibilities [4], while in fields such as medical imaging and scientific visualization, they assist in generating highly detailed and accurate visual data for analysis [5], [6]. \n\nHowever, the increasing sophistication and accessibility of diffusion models also give rise to significant ethical and societal concerns. Their capacity to generate hyper-realistic images, including the ability to synthesize visuals from textual descriptions [7], opens the door to malicious uses. Deepfakes, for instance, can be weaponized to manipulate public opinion and spread misinformation at an unprecedented scale [8], [9]. Additionally, the widespread use of these models raises serious copyright and intellectual property issues, as diffusion models can inadvertently reproduce content from their training datasets, raising concerns about unauthorized replication of protected works [10]- [13]. These challenges necessitate the development of robust detection mechanisms to safeguard against the misuse of this powerful technology. \n\nThe exceptional realism of images generated by diffusion models threatens the credibility of digital visual media. As these synthetic images become nearly indistinguishable from genuine photographs [14], the risk of malicious use, including the spread of fake news, creation of fraudulent content, and impersonation, grows exponentially [15], [16]. Current detection techniques, primarily designed for GAN-generated content, often fail to accurately identify the subtle artifacts and nuanced manipulations characteristic of diffusion-based generation [17], [18]. \n\nFurthermore, the rapid evolution of diffusion models, with frequent changes in architectures, training data, and postprocessing techniques, demands detection systems that can adapt to new, unseen models.",
            "score": 0.44877460576408235,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2189
                },
                {
                    "start": 2192,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "209444798"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "266071012"
                },
                {
                    "start": 771,
                    "end": 774,
                    "matchedPaperCorpusId": "249431563"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "259280305"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "208075748"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "267386689"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "253116680"
                },
                {
                    "start": 2178,
                    "end": 2182,
                    "matchedPaperCorpusId": "257557819"
                },
                {
                    "start": 2184,
                    "end": 2188,
                    "matchedPaperCorpusId": "259342331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.",
            "score": 0.4484469997235844,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12213134765625
        },
        {
            "corpus_id": "272832559",
            "title": "Training Data Attribution: Was Your Model Secretly Trained On Data Created By Mine?",
            "text": "The emergence of text-to-image models has recently sparked significant interest, but the attendant is a looming shadow of potential infringement by violating the user terms. Specifically, an adversary may exploit data created by a commercial model to train their own without proper authorization. To address such risk, it is crucial to investigate the attribution of a suspicious model's training data by determining whether its training data originates, wholly or partially, from a specific source model. To trace the generated data, existing methods require applying extra watermarks during either the training or inference phases of the source model. However, these methods are impractical for pre-trained models that have been released, especially when model owners lack security expertise. To tackle this challenge, we propose an injection-free training data attribution method for text-to-image models. It can identify whether a suspicious model's training data stems from a source model, without additional modifications on the source model. The crux of our method lies in the inherent memorization characteristic of text-to-image models. Our core insight is that the memorization of the training dataset is passed down through the data generated by the source model to the model trained on that data, making the source model and the infringing model exhibit consistent behaviors on specific samples. Therefore, our approach involves developing algorithms to uncover these distinct samples and using them as inherent watermarks to verify if a suspicious model originates from the source model. Our experiments demonstrate that our method achieves an accuracy of over 80% in identifying the source of a suspicious model's training data, without interfering the original training or generation process of the source model.",
            "score": 0.4483583792655609,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.",
            "score": 0.4482507501268043,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2234
                },
                {
                    "start": 2237,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 602,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 602,
                    "end": 626,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1457,
                    "end": 1472,
                    "matchedPaperCorpusId": "266900037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "274638565",
            "title": "Boosting Alignment for Post-Unlearning Text-to-Image Generative Models",
            "text": "In this study, we address the crucial challenge of preventing undesirable outputs in text-to-image generative models. We begin by examining class-wise forgetting with CIFAR-10 diffusion-based generative models, where we demonstrate our method's ability to selectively prevent the generation of specific class images (Section 4.2). We then explore the effectiveness of our approach in removing nudity and art styles (Section 4.3) to address real-world concerns of harmful content generation and copyright infringement. We further study the impact of data diversity (Section 4.4) as well as the sensitivity of our method to hyperparameter settings (Section 4.4).",
            "score": 0.4482211683513989,
            "section_title": "Experiment",
            "char_start_offset": 16601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 660
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "In this work, we propose a data generation pipeline to generate realistic copyright-infringing examples on diffusion models. Our proposed pipeline generated prompts that are seemingly unrelated to the target copyrighted topic, but can still be utilized to produce copyrighted content. Additionally, our pipeline tackles partial copyright infringement. Through our proposal, we present a toolkit that includes potentially copyrighted topics, target images of copyright topics with annotations of copyrighted content, and a dataset generation pipeline. The toolkit can be used as a whole on diffusion models for testing copyright-related performance and generating copyright-violating samples. We demonstrate with this toolkit that contemporary diffusion models are highly susceptible to generating copyrighted content. The findings emphasize the immediate necessity for appropriate measures to prevent models from generating copyrighted materials. This is especially crucial since our study shows even common phrases can prompt models to create images containing copyrighted content. This work can serve copyright research for diffusion models. For instance, the research community can leverage this toolkit to assess diffusion models based on copyright-related criteria. Furthermore, copyright protection algorithms can employ it for effectiveness evaluation. \n\nAll Query Templates for Prompt Generation Form N phrases using all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEYWORDN. The phrases should be similar to CATEGORY TOPIC. Form N sentences that start with the phrase START-PHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. Form N sentences that uses all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEY-WORDN. Use the words continuously wherever possible. Ensure grammatical correctness. Start the sentence with the exact STARTPHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. \n\nTable 3: The list of all templates used for prompt generation on GPT3.5. Capitalized words are variables that can be changed. We query GPT models to generate candidate prompts that contain extracted keywords but have different semantic meanings from our target topics. We employ different queries to ensure diversity in the prompt generation.",
            "score": 0.44740653697880495,
            "section_title": "Conclusion",
            "char_start_offset": 30099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "274235104",
            "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
            "text": "Text-to-image diffusion models, such as Stable Diffusion, have shown exceptional potential in generating high-quality images. However, recent studies highlight concerns over the use of unauthorized data in training these models, which may lead to intellectual property infringement or privacy violations. A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features. In this paper, we examine the robustness of various watermark-based protection methods applied to text-to-image models. We observe that common image transformations are ineffective at removing the watermark effect. Therefore, we propose RATTAN, that leverages the diffusion process to conduct controlled image generation on the protected input, preserving the high-level features of the input while ignoring the low-level details utilized by watermarks. A small number of generated images are then used to fine-tune protected models. Our experiments on three datasets and 140 text-to-image diffusion models reveal that existing state-of-the-art protections are not robust against RATTAN.",
            "score": 0.4471873459902406,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "271270708",
            "title": "Unveiling Structural Memorization: Structural Membership Inference Attack for Text-to-Image Diffusion Models",
            "text": "In recent years, large models, especially diffusion models [14,40,42] have shown superior generative performance and found extensive application across various fields.Moreover, the advent of the text-to-image diffusion models [30] has facilitated the creation of high-quality, diverse text-conditional images.These models have significantly propelled the advancements of Artificial Intelligence Generated Content (AIGC).\n\nNevertheless, the wide adoption of large models has raised various legal and ethical concerns, notably copyright issues [1], consent [8,10] and ethics [17,27].One of the pressing concerns is the unauthorized use of images for training models.This not only risks compromising the privacy of image owners but also poses copyright infringements, as models can realistically replicate copyrighted artworks based on training data.This is attributed to models' capacity for memorization, which means models can remember certain elements or even reproduce almost identical images from their training datasets.Under such circumstances, Membership Inference Attack (MIA) [16] serves as an approach to tackle the issue.Given a specific unauthorized image, the goal of MIA is to determine whether it is a member of the training set of a target model.The core of MIA is to ingeniously exploit the models' memorization of members to distinguish them from non-members.\n\nRecently, numerous Membership Inference Attack (MIA) methodologies [7,19,24] have been introduced for diffusion models.These methodologies, which rely on pixel-wise noise comparison, are designed to assess models' verbatim memorization of member images.However, we argue that it is practically impossible for largescale text-to-image models to memorize all the pixel information, given that their training sets usually contain billions of images.For instance, the Stable Diffusion-v1-1 is trained on the LAION2Ben dataset, which contains around 2.32 billion text-image pairs.Hence, we attempt to capture more advanced memorization capabilities of large text-to-image diffusion models, specifically at the structure-level.To investigate the structure-level memorization, we first examine how a specific image is corrupted during the unidirectional diffusion process for better comprehension of image structural variations, and then explore whether this correlates with models' memorization.",
            "score": 0.4467411698803756,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 420
                },
                {
                    "start": 422,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1131
                },
                {
                    "start": 1131,
                    "end": 1261
                },
                {
                    "start": 1261,
                    "end": 1376
                },
                {
                    "start": 1378,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "232233426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "259108818",
            "title": "DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models",
            "text": "Recently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, Diffusion Shield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, Diffusion Shield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.",
            "score": 0.446520962389762,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "ChatGPT has four types of responses to copyright infringement requests: 1.It may block the text that violates copyright.2. It might attempt to generate an image but then suddenly stop to comply with the request.3. It could create an image, but if the request closely resembles copyrighted content, it will rephrase the prompt.4. It might generate copyrighted image If the content is block in first or second case, it means the prompt is easily detectable by internal censor mechanism.However, if it is in the second case, the prompt is high-risk to violate the copyright infringement.\n\nFigure 10.Detection based filtering defense In order to filter out copyright infringement using the target image, we employ the representation similarity in DINO [5].We input the target image and the generated image into DINO, and calculate the cosine similarity distance.If the similarity distance exceeds 0.8, we filter out the generated images.similaritydistance.Then, if the similarity distance exceeds 0.8 we filter out the generated images.",
            "score": 0.446208750065468,
            "section_title": "Block mechanisms in ChatGPT",
            "char_start_offset": 26821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 584
                },
                {
                    "start": 586,
                    "end": 596
                },
                {
                    "start": 596,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1032
                }
            ],
            "ref_mentions": [
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "233444273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "277856857",
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "text": "The existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67]. \n\nHowever, previous studies face several limitations. First, both the perturbation-based and the watermark-based methods need to manipulate the original images, i.e., injecting perturbation or watermark, thus compromising data fidelity. The perturbation may also diminish the model's generation quality. Second, perturbationbased and watermark-based strategies require retraining the model to be effective. Thus, they may not suit the model already posted online. For the MI methods, the existing approaches [15,17,24,29,41,44] for diffusion models usually require the access to structure or weights of the model, which limits their applicability in blackbox auditing scenarios. Although some MI strategies target the black-box settings [12,14,26,43,67,73], they are not well suited to our auditing task. We will go depth in Section 4.4 and compare them with ArtistAuditor in Section 5. \n\nOur Proposal. In this paper, we propose a novel artwork copyright auditing method for the text-to-image models, called ArtistAuditor, which can identify data-use infringement without sacrificing the artwork's fidelity. We are inspired by the fact that artworks within an artist's style share some commonality in latent space.",
            "score": 0.44556097248925075,
            "section_title": "Introduction",
            "char_start_offset": 1855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "257766375"
                },
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "272003879"
                },
                {
                    "start": 185,
                    "end": 188,
                    "matchedPaperCorpusId": "264474160"
                },
                {
                    "start": 677,
                    "end": 680,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 680,
                    "end": 682,
                    "matchedPaperCorpusId": "257984935"
                },
                {
                    "start": 682,
                    "end": 684,
                    "matchedPaperCorpusId": "264439566"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 802,
                    "end": 805,
                    "matchedPaperCorpusId": "266191072"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "271325362"
                },
                {
                    "start": 1322,
                    "end": 1325,
                    "matchedPaperCorpusId": "261241592"
                },
                {
                    "start": 1325,
                    "end": 1328,
                    "matchedPaperCorpusId": "266126263"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "256627812"
                },
                {
                    "start": 1334,
                    "end": 1337,
                    "matchedPaperCorpusId": "260887617"
                },
                {
                    "start": 1547,
                    "end": 1551,
                    "matchedPaperCorpusId": "261557367"
                },
                {
                    "start": 1551,
                    "end": 1554,
                    "matchedPaperCorpusId": "268048573"
                },
                {
                    "start": 1557,
                    "end": 1560,
                    "matchedPaperCorpusId": "266191072"
                },
                {
                    "start": 1560,
                    "end": 1563,
                    "matchedPaperCorpusId": "271325362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "277856857",
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "text": "In this section, we go into depth about the existing solutions, as the extension of that in Section 1. As diffusion models continue to evolve and gain popularity, users can now create a vast array of generative works at a low cost, which leads to the negative effects of the replication becoming more acute [59]. Especially the artist community is concerned about the copyright infringement of their work [7,40,53]. Recently, researchers have proposed a lot of countermeasures to solve this issue [13]. \n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects. \n\nHowever, the goal of adversarial perturbation is to disrupt the learning process of diffusion models, which is orthogonal to the copyright auditing focus of this paper. Moreover, adversarial perturbation essentially blocks any legitimate use of subject-driven synthesis based on protected images. Watermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector.",
            "score": 0.445554902678431,
            "section_title": "H Related Work",
            "char_start_offset": 44894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "257427387",
            "title": "Detecting images generated by diffusers",
            "text": "The ability to generate synthetic images has advanced significantly in recent years, with the development of various techniques such as Generative Adversarial Networks (GANs) and Diffusion Models. These techniques have made it possible to generate high-quality images that are indistinguishable from real ones. However, with this advancement comes the concern about the potential misuse of synthetic images for malicious purposes, such as deepfakes and spreading misinformation. In order to mitigate these concerns, it is crucial to develop robust techniques for detecting synthetic images. The ability to distinguish synthetic images from real ones is essential for maintaining the integrity of information and for protecting individuals from the malicious use of synthetic media. The explosion of recent text-to-image methods and their easy access to the general public is leading society towards a point where a good deal of online content is synthetic and where the line between reality and fiction will no longer be so clear. In this paper, we present a first attempt to distinguish between generated and real images on the basis of the image itself and the text associated with it and used to describe and generate it. We also analyze the peculiarities of image and text that can lead to a more or less credible image that is difficult to identify.",
            "score": 0.44555111180727947,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046112060546875
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "Our experiments demonstrate that the IP infringement issues are widely existing in both open-source and commercial closed source models.\n\nGiven the severe IP infringement problems with visual generative AI models, it is essential to develop an effective defense method that can mitigate these issues with minimal impact on the models' generation capabilities.To address this, we develop a revised generation paradigm TRIM (inTellectual pRoperty Infringement Mitigating) that detects the generated contents that potentially has the IP infringement issues and suppresses the IP infringement by exploiting the guidance technique for the diffusion process.Experiments on our IP infringement benchmark and state-of-the-art visual generative AI models demonstrate that our defensive generation paradigm is highly effective at mitigating IP infringement problems involving protected characters, while only having a small influence on the text-image alignment quality of the generated content.\n\nOur contributions are summarized as follows: \u2460 We constructed a benchmark for studying IP infringement issues with visual generative AI models.This involved designing a method to create prompts that can trigger IP infringement in a black-box setting, even without directly using the names of protected characters.\u2461 We developed an effective defense method to mitigate the IP infringement problem.\u2462 Our evaluation on the state-of-the-art visual generative AI models demonstrate that the IP infringement problems on the representative characters are severe.\u2463 Experiments demonstrate our proposed mitigation method is highly effective at mitigating these IP issues, while only having a small influence on the overall quality of the generated content.",
            "score": 0.4453112099452345,
            "section_title": "Introduction",
            "char_start_offset": 4384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 138,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1300
                },
                {
                    "start": 1300,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1734
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "Figure 1 shows the snapshot examples of our dataset. To the best of the authors' knowledge, there are currently no available datasets designed specifically for the purpose of exploring the detection of copyright infringement for generative AI in the domain of art. As such, this study represents a first attempt at producing a dataset for supporting researchers in this field in the development of machine learning algorithms for identifying copyright infringement by generative AI models. The development of such detection algorithms could significantly assist developers of generative AI systems in mitigating potential legal repercussions associated with their systems, as well as enable content creators and content curators to better identify copyright infringement by generative AI efficiently and systematically.",
            "score": 0.44505349912329173,
            "section_title": "Introduction",
            "char_start_offset": 4198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 819
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0755615234375
        },
        {
            "corpus_id": "233773696",
            "title": "Study on Infringement Identification of Art Works Based on CNN Image Recognition Technology",
            "text": "This article uses CNN-based image recognition technology to provide a new idea for the judicial practice of copyright law, as the Figure3 shows: In the process of judging the substantial similarity of the infringing works involved, first, extract the characteristics of the plaintiff's work, store the characteristics to form a database, and then, input the entire and part of the characteristics of the defendant's work involved in the infringement. Retrieving the plaintiff's work can assist in proving that there is a substantial similarity; otherwise, it does not exist. Through this method, the problems caused by the subjectivity and ambiguity of the existing \"substantial similarity\" standards for artwork can be resolved to a great extent.",
            "score": 0.4450511977636946,
            "section_title": "Conclusion",
            "char_start_offset": 6860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0216217041015625
        },
        {
            "corpus_id": "257687839",
            "title": "Ablating Concepts in Text-to-Image Diffusion Models",
            "text": "Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.",
            "score": 0.4449061997819877,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "258987524",
            "title": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust",
            "text": "The development of diffusion models has led to a surge in image generation quality. Modern textto-image diffusion models, like Stable Diffusion and Midjourney, are capable of generating a wide variety of novel images in an innumerable number of styles. These systems are general-purpose image generation tools, able to generate new art just as well as photo-realistic depictions of fake events for malicious purposes. \n\nThe potential abuse of text-to-image models motivates the development of watermarks for their outputs. A watermarked image is a generated image containing a signal that is invisible to humans and yet marks the image as machine-generated. Watermarks document the use of image generation systems, enabling social media, news organizations, and the diffusion platforms themselves to mitigate harms or cooperate with law enforcement by identifying the origin of an image [Bender et al., 2021, Grinbaum andAdomaitis, 2022]. \n\nResearch and applications of watermarking for digital content have a long history, with many approaches being considered over the last decade [O'Ruanaidh andPun, 1997, Langelaar et al., 2000]. However, so far research has always conceptualized the watermark as a minimal modification imprinted onto an existing image [Solachidis and Pitas, 2001, Chang et al., 2005, Liu et al., 2019, Fei et al., 2022]. For example, the watermark currently deployed in Stable Diffusion [Cox et al., 2007], works by modifying a specific Fourier frequency in the generated image. \n\nThe watermarking approach we propose in this work is conceptually different: This is the first watermark that is truly invisible, as no post-hoc modifications are made to the image. Instead, the distribution of generated images is imperceptibly modified and an image is drawn from this  modified distribution. This way, the actual sample carries no watermark in the classical additive sense, however an algorithmic analysis of the image can detect the watermark with high accuracy. From a more practical perspective, the watermark materializes in minor changes in the potential layouts of generated scenes, that cannot be distinguished from other random samples by human inspection.",
            "score": 0.44473543448891256,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 1083,
                    "end": 1098,
                    "matchedPaperCorpusId": "2678473"
                },
                {
                    "start": 1258,
                    "end": 1285,
                    "matchedPaperCorpusId": "1695220"
                },
                {
                    "start": 1285,
                    "end": 1305,
                    "matchedPaperCorpusId": "15271835"
                },
                {
                    "start": 1305,
                    "end": 1323,
                    "matchedPaperCorpusId": "164683855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "259675638",
            "title": "Playing the System",
            "text": "In today's online landscape, there is a large number of technologies used for the automated detection of illegal content. Such technologies are used for many purposes -from combatting child pornography to preventing the spread of terrorist content. Nonetheless, the primary focus of this chapter is on the technologies used by automated programs for recognizing copyright-infringing materials (\"copyright bots\"), which typically target films and music, photographs or pieces of literature 6 distributed without the appropriate licensing. Many such technologies are of a hybrid nature. However, for the sake of simplicity, we can schematically divide the technologies into the following categories. 7",
            "score": 0.4443929523581653,
            "section_title": "TYPES OF AUTOMATED TECHNOLOGIES USED TO DETECT ILLEGAL CONTENT",
            "char_start_offset": 3404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 699
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2452392578125
        },
        {
            "corpus_id": "271649325",
            "title": "Copyright Protection and Risk Assessment Based on Information Extraction and Machine Learning: The Case of Online Literary Works",
            "text": "With the proliferation of digital platforms, the dissemination of literary works has encountered unprecedented challenges, particularly concerning copyright infringement and unauthorized use. This study introduces a comprehensive framework for copyright protection and risk assessment, specifically tailored to online literary works. The framework employs advanced CNN based information extraction (IE) techniques coupled with machine learning (ML) algorithms to identify, classify, and protect literary content against copyright violations. Firstly, we delineate a novel CNN-Decision tree-based IE methodology that systematically harvests metadata and textual content from various online repositories. This process is designed to detect and index online literary works, extracting pertinent features such as authorship, publication date, and textual patterns. Following the extraction, the study utilizes natural language processing (NLP) to analyze and compare content, pinpointing potential instances of copyright infringement by identifying significant overlaps and stylistic similarities with registered works. Subsequently, we introduce a risk assessment model developed through supervised machine learning. This model is trained on a labelled dataset comprising instances of both copyrighted and non-copyrighted works, along with known cases of copyright infringement. By analyzing the extracted features, the model assesses the probability of infringement, categorizing risks into high, medium, and low categories. This stratification allows stakeholders to prioritize enforcement actions and resources efficiently. The study further explores the implementation of various ML algorithms, including decision trees, support vector machines, and neural networks, to determine the most effective approach for copyright protection in the literary domain. We evaluate the models based on accuracy, precision, recall, and F1-score metrics, emphasizing their capacity to generalize and operate in dynamic, real-world environments.",
            "score": 0.4442905777649204,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264892578125
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "In Table 1, we present the performance of various models evaluated on the DeepfakeArt dataset. Our task was a classification challenge, wherein the objective was to distinguish between 'similar pairs' (pairs where one image potentially infringes copyright) and 'dissimilar pairs' (pairs of unrelated images). Evaluation. In this study, we evaluated several prominent models: MultiGrain Berman et al. [2019], DINO-v1 Caron et al. [2021], DINO-v2 Oquab et al. [2023], and SwinTransformer Liu et al. [2021]. We utilized the embeddings generated by these models, calculating the dot product between each pair. Subsequently, we applied cross-validation to pinpoint the optimal threshold to distinguish between similar and dissimilar pairs. Utilizing this threshold, we quantized the calculated cosine similarities into binary outcomes, facilitating the analysis of various metrics, such as accuracy, precision, and recall, all of which are documented in Table 1. Discussion. As delineated in Table 1, the DINO-v2 ViT-L/14 model notably outshines the others in overall performance, albeit with a slight drawback in the precision metric, where the MultiGrain model emerges as the frontrunner. Despite showcasing considerable precision, the models generally falter in terms of recall. This indicates a heightened rate of false negatives across all models, a trend which holds significant implications in the context of copyright infringement detection. Particularly, a higher frequency of false negatives could potentially escalate the risk of litigations concerning copyright infringements in generative AI models. \n\nIn conclusion, this research articulates a nuanced definition of copyright infringement and introduces a synthetic dataset designed to emulate real-world scenarios where such infringements may occur. The experimental results underscore the current models' tendency to incur high false negative rates when applied to the dataset, thereby highlighting avenues for the development of more robust and efficient detection tools to identify and mitigate copyright infringements.",
            "score": 0.4439982908604859,
            "section_title": "Experiments and Discussion",
            "char_start_offset": 12491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 503,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "234827701",
            "title": "Text Detection in Trademark Images under Semi-Supervision",
            "text": "The number of infringement cases of the text in trademark images is increasing every year. That means the government or other users need efficient and automate solutions to judge whether the texts in trademarks are legal or not. So, as the first step and the most important part of these automate solutions, trademark text detection is very important. \n\nIn essence, the purpose of trademark text detection is to locate the text position in the image, so there is no difference between trademark text detection and text detection in other situations. such as Scene text detection. \n\nRecently, text detectors show good detection effect. Text detectors usually rely on a large number of appropriate datasets for training. Unfortunately, we don't find open datasets of texts in trademark images, most of the existing text datasets in other scenes do not provide character-level annotations, manually labeling these datasets will take a lot of time. \n\nIn this paper, we propose a new text method which can locate the single char-acters and group the detected characters into a text line. Our detector contains a convolutional neural network which can represent text characters by a single point at the center of their bounding box and the text characters' size. In addition, our detector contains a text group module. The convolutional neural network is used to localize in-dividual characters in the image, and the text group module is used to group characters into text lines. \n\nIn order to make up for the deficiency of character-level annotation, we propose a semi-supervised learning strategy to train our model. \n\nThe idea of semi-supervised learning strategy is to pretrain the model by using the synthesized fully annotated character images, then the pretrained model is used to detect the unannotated trademark images dataset to generate character level detection results, the results will be combined with the unannotated trademark images to make trademark images be a new dataset which is announced partly, then the dataset announced partly is further combined with the annotated synthetic images dataset to re-train the pretrained model to get the final stable character detection model. \n\nwe collect text from various trademark images and constructing a new dataset. This dataset contains 4500 images.",
            "score": 0.44346523120530945,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01302337646484375
        },
        {
            "corpus_id": "269137659",
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "text": "As image generative models have rapidly improved in scale and sophistication, the possibility of them mimicking artists' personal styles has been an important topic of discussion in the literature [18].Many previous works describe ways to either detect potential direct image copying in generated images, or to foil any future copying attempts by imperceptibly altering the artists' works to prevent effective training by the generative models.These include techniques like adding imperceptible watermarks to copyrighted artworks [7,8,28], and crafting \"un-learnable\" examples on which models struggle to learn the style-relevant information [24,29,30].These methods are typically computationally expensive and incur a loss in image quality, which may render these techniques impractical for many artists.Also, they do not protect artworks which have been previously uploaded to the internet without any safeguards.Others have suggested methods to mitigate this issue from the model owner's perspective -to either de-duplicate the dataset before training [4,25,26], or to remove concepts from the model after training (\"unlearning\") [3,9,13].These are also technically challenging, and require the model owner to invest significant resources which may again inhibit their practicality.Methods like [4,25,26] are also more focused on analyzing direct image copying from the training data, and thus may not be applicable to preventing style copying.None of these works tackle the problem of detecting potentially copied art styles in generated art, especially in a manner which may be relevant to legal standards of copyright infringement.According to current US legal standards [2], an artwork has to meet the \"substantial similarity\" test for it to be infringing on copyright.This similarity has to be established on analytic and holistic terms Fig. 2: Example generations from Stable Diffusion 2 when prompted to produce specific paintings by Vincent Van Gogh, along with the histogram of similarities between the generated image and corresponding real image.Even for a famous artist like Vincent Van Gogh, generative models rarely produce near-exact duplicates.However, Van Gogh's style appears consistently, even when similarity is low.[11,14].",
            "score": 0.44336049049477,
            "section_title": "Related Works",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2239
                },
                {
                    "start": 2239,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 1061,
                    "end": 1064,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "258684279",
            "title": "Analyzing Copyright Infringement by Artificial Intelligence: The Case of the Diffusion Model",
            "text": "Artificial intelligence has made significant progress in recent years and is now applied in almost every field [1] [2]. Legal risks associated with artificial intelligence were first identified by Solum as early as 1991 [3]. However, it was not until the emergence of the diffusion model and large-scale language models that the legal issues surrounding artificial intelligence received widespread attention. These neural networks possess strong creative abilities, which pose a threat to the interests of creators. This paper uses the diffusion model as an example to analyze the potential infringement issues associated with it. The diffusion model is a neural network model that converts an image into pure Gaussian noise by adding Gaussian noise continuously during the forward process. In the generation process, the model can restore pure Gaussian noise to an image. The diffusion model's impressive creation abilities enable it to draw a picture conforming to any text description based on previous training data (See Fig. 1). However, the high performance of artificial intelligence is reliant on extensive training data. To perform well, the diffusion model must use a considerable amount of data, which may lead to hidden infringement risks for the training data [4]. Carlini's research reveals that the diffusion model imitates the painting style of the training data, and may even directly copy it [5]. Identifying and defining the infringement act of the diffusion model, as well as assessing whether existing laws can address it, are crucial issues that have yet to be resolved.",
            "score": 0.4433406804298247,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1592
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "256505670"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "3817471"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "54940568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332275390625
        },
        {
            "corpus_id": "274982437",
            "title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models",
            "text": "Despite the impressive synthesis quality of text-to-image (T2I) diffusion models, their black-box deployment poses significant regulatory challenges: Malicious actors can fine-tune these models to generate illegal content, circumventing existing safeguards through parameter manipulation. Therefore, it is essential to verify the integrity of T2I diffusion models. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we discern model tampering via the KL divergence between the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton (PromptLA) for efficient and accurate verification. Evaluations on four advanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves a mean AUC of over 0.96 in integrity detection, exceeding baselines by more than 0.2, showcasing strong effectiveness and generalization. Additionally, our approach achieves lower cost and is robust against image-level post-processing. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which establishes quantifiable standards for AI copyright litigation in practice.",
            "score": 0.44293149211925764,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "269983235",
            "title": "Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy",
            "text": "Text-to-image diffusion models have achieved remarkable success in the guided generation of diverse, high-quality images based on text prompts, such as Stable Diffusion [42,46], DALLE-2 [43], Imagen [49], and DeepFloyd-IF [31]. These models are increasingly adopted by users to create photorealistic images that align with desired semantics. Moreover, they can generate images of specific concepts [32] or styles [61] when fine-tuned on relevant datasets. However, the impressive generative capabilities of these models depend heavily on high-quality image-text datasets, which involve collecting image-text data from the web. This practice raises significant privacy and copyright concerns in the community [5,18]. The pretraining and fine-tuning processes of text-to-image diffusion models can cause copyright infringement, as they utilize unauthorized datasets published by human artists or stock-image websites [2,10,44,45,58]. \n\nMembership inference (also known as the membership inference attack) is widely used for auditing privacy leakage of training data [4,53], defined as determining whether a given data point has been used to train the target model. Dataset owners can thus leverage membership inference to determine if their data is being used without authorization [14,39]. \n\nPrevious works [5, 15-17, 28, 38] have attempted membership inference on diffusion models. Carlini et al. [5] employ LiRA (Likelihood Ratio Attack) [4] to perform membership inference on diffusion models. LiRA requires training multiple shadow models to estimate the likelihood ratios of a data point from different models, which incurs high training overhead (e.g., 16 shadow models for DDPM [22] on CIFAR-10 [30]), making it neither scalable nor applicable to text-to-image diffusion models. Other query-based membership inference methods [15,17,28,38] design and compute indicators to evaluate whether a given data point belongs to the member set. These methods require only a few or even a single shadow model, making them scalable to larger text-to-image diffusion models.",
            "score": 0.44245871270449877,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1064,
                    "end": 1067,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1067,
                    "end": 1070,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "221808247"
                },
                {
                    "start": 1397,
                    "end": 1400,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1439,
                    "end": 1442,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1658,
                    "end": 1660,
                    "matchedPaperCorpusId": "259224343"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "256503774"
                },
                {
                    "start": 1839,
                    "end": 1842,
                    "matchedPaperCorpusId": "258967241"
                },
                {
                    "start": 1842,
                    "end": 1845,
                    "matchedPaperCorpusId": "256627812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "268691556",
            "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
            "text": "A growing number of researchers in recent years explore how to address legal problems by applying theories and methods of computer science.This literature seeks to narrow the gap between the vague and abstract concepts used by law by applying mathematical models to offer more rigor, coherent and scalable definitions into issues such as privacy (Dwork et al., 2006), or fairness and discrimination (Dwork et al., 2012).The study of copyright by computer science methods has only emerged recently.Scheffler et al. (2022), for instance, proposed a framework to test substantial similarity by comparing Kolmogorov-Levin complexity with and without access to the original copyright work.\n\nIn the context of generative models Carlini et al. (2023); Haim et al. (2022), explore whether generative diffusion models memorize protected works that appeared in the models' training set.This can be considered as a preliminary issue to the problem of establishing copyright infringement.However, as we discuss in section 4, memorization of input content does not necessarily equate with copyright infringement.There is also active and thoughtprovoking discussion on how machine learning technologies are reshaping our understanding of copyright within the realm of law.Asay (2020) explores the question of whether AI system outputs should be subject to copyright.Additionally, Grimmelmann (2015); Lemley and Casey (2020) explore the implications of copyright law for literary machines that extract content and manage databases of information.Other works, like Bousquet et al. (2020); Vyas et al. (2023) attempt to evaluate copyright infringement in GenAI models using privacy-like notions.The contribution of these works lies in proposing a procedure to inform copyright analysis in line with the thesis of this paper.However, this approach may not scale easily, and it also falls short of providing nuanced information regarding the level of genericity, which could be crucial for resolving copyright legal disputes (Elkin-Koren et al., 2024).",
            "score": 0.4418861861616068,
            "section_title": "Related Work",
            "char_start_offset": 6728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 684
                },
                {
                    "start": 686,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 366,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "13496699"
                },
                {
                    "start": 722,
                    "end": 743,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "249712435"
                },
                {
                    "start": 1258,
                    "end": 1269,
                    "matchedPaperCorpusId": "264375893"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "152665774"
                },
                {
                    "start": 1386,
                    "end": 1409,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1549,
                    "end": 1571,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 1573,
                    "end": 1591,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "278535529",
            "title": "Visual Watermarking in the Era of Diffusion Models: Advances and Challenges",
            "text": "As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.",
            "score": 0.4412907054445101,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "275212848",
            "title": "DuMo: Dual Encoder Modulation Network for Precise Concept Erasure",
            "text": "Recent advancements in text-to-image (T2I) diffusion models (Dhariwal and Nichol 2021;Nichol et al. 2021;Ramesh et al. 2022;Nichol and Dhariwal 2021;Saharia et al. 2022) have made notable progress in creating high-quality images based on textual prompts within seconds. This is primarily attributed to the extensive web-scale datasets for model pretraining. However, the advanced generation capability of the models is accompanied by a number of potential risks including copyright infringement and the propagation of Not-Safe-For-Work (NSFW) content. \n\nRecent studies demonstrate that diffusion models tend to imitate famous artworks and specific painting styles of artist (Jiang et al. 2023;Setty 2023) or generate sexually explicit content (Hunter 2023;Zhang et al. 2023), which violates the societal norms and legal regulations. To tackle this problem, a naive approach is to filter out the inappropriate data of the datasets and retrain the model from scratch. Nevertheless, this is not only resource-consuming but also unsatisfactory in terms of the results. For instance, Stable Diffusion v2.0 continues to generate explicit content while being trained on a sanitized dataset. Besides, both the interference of classifier-free guidance during generation time (Schramowski et al. 2023) and the safety checker (Rando et al. 2022) afterwards can be easily circumvented (Smith 2022;Fan et al. 2023;Shi et al. 2020). \n\nTherefore, recent methods either concentrate on parameter fine-tuning or developing erasure-specific modules to the U-NET (Gandikota et al. 2023;Fan et al. 2023;Heng and Soh 2024;Kumari et al. 2023;Gong et al. 2024;Huang et al. 2023). Although these methods are effective to the target concept, they alter the backbone features of the U-Net decoder and severely sacrifice the ability of generating nontargeted concepts. In each stage of the U-Net decoder, the skip features from the skip connection and the backbone features are concatenated together.",
            "score": 0.44089712448914387,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 86,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 86,
                    "end": 105,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 124,
                    "end": 149,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 149,
                    "end": 169,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 674,
                    "end": 693,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 1266,
                    "end": 1291,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1543,
                    "end": 1566,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1582,
                    "end": 1600,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 1600,
                    "end": 1619,
                    "matchedPaperCorpusId": "259837117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.429443359375
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "It reveals that such attacks lead to the generation of images misaligned with intended text conditions, and a phenomenon exacerbated by trigger amplification, where the presence of backdoor triggers in generated images larger than the porpotion poisoning data. As parallel efforts, both works focus on Text-to-Image (T2I) diffusion models and eliminate the need to directly control the training process. However, unlike the problem that this work aims to address, the copyright infringement attack requires the generation of specific images in response to trigger prompts, rather than simply degrading performance by generating mismatched images. As a result, dealing with copyright infringement attacks has become more difficult. \n\n\u2022 In \"Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis\" (Struppek et al., 2022), the authors investigate how backdoor attacks can be integrated through altering their weights of text encoders of a diffusion model to generate images with pre-defined attributes or images following a hidden, potentially malicious description. This method ensures that generated images appear normal when using clean prompts, avoiding any overt signs of manipulation. Unlike this approach, our work focuses on inducing predefined behaviors in DMs through data poisoning, without the direct modification of model weights. Critically, the predefined images that the attack method makes the diffusion model to produce must be such that the targeted diffusion model already can generate in response to a clean prompt. This requirement is in conflict with the criteria for our Copyright Infringement Attack. \n\n\u2022 The \"How to Backdoor Diffusion Models?\" (Chou et al., 2023) aims to compromised diffusion processes during model training for backdoor implantation. By introducing a trigger pattern within the input noise at the beginning of the denoising process, the modified model will produce a predetermined output. This framework ensures that the backdoored model behaves normally for regular inputs but generates specific outcomes designed by the attacker upon receiving a trigger signal. This work requires maliciously modifying both the training data and control over forward/backward diffusion steps. Besides, the target image that the attack want the target to generated need to be included during the model training.",
            "score": 0.4406986724310449,
            "section_title": "A.1. Detailed Comparisons with Other Diffusion Model Attack Methods",
            "char_start_offset": 36425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 1703,
                    "end": 1722,
                    "matchedPaperCorpusId": "254564071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "274235104",
            "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
            "text": "The primary function of text-to-image models is to generate images that align with input text descriptions. When unauthorized data is used in training, the model's capability extends to generating images with features similar to those in the protected images. Existing watermark-based protections assume that for a model to generate similar images, it must be trained on the exact content of the protected images. This process would lead to the watermark being embedded into the model. However, this assumption does not always hold. As long as the model learns the key features of the protected data, it can generate similar images without directly replicating the original content (e.g., the watermark). \n\nWe propose to leverage the generative capabilities of diffusion models to construct data samples that share key features with protected images. Instead of directly using the protected images in training, we employ a technique similar to zero-shot learning. Here, a diffusion model is prompted to generate an image based on a text description along with a reference (protected) image. In this way, the diffusion model has the freedom to create images without focusing on details such as the watermark embedded in the protected images. \n\nFigure 1 presents an overview of our method, RATTAN, for bypassing watermark-based protections. The top part of the figure illustrates how existing watermark-based protection methods operate, which involve two steps. The first step is embedding a watermark onto protected images. This watermark can be either a sequence of pixel value bits or an image transformation function. The second step involves inspecting the generated images from text-to-image diffusion models. If the model has been trained on watermarked data, the generated images will also contain this watermark. Consequently, existing protections can flag the trained model. The bottom part of Figure 1 shows the pipeline of our technique, RATTAN. The adversary's goal is to train a textto-image model on unauthorized data. Without knowing whether these protected images are watermarked, RATTAN selects a subset of the data (e.g., 10 images) and their corresponding text descriptions. It then employs an off-theshelf Stable Diffusion [43] model to perform controlled image generation based on both the input image and its corresponding text.",
            "score": 0.4403873293010955,
            "section_title": "Our Solution",
            "char_start_offset": 12173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations. Conversely, when random spatial transformations were applied to the samples, the duplication diminished and did not exhibit any spatial similarity. This demonstrates the spatial similarity of the duplication phenomenon, with theoretical analysis. \n\nBased on the above analysis, we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) [51] and Self-Supervised Copy Detection (SSCD) [48] similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability. Experimental results demonstrate that this approach achieves a defense effectiveness of CopyrightShield on the Pokemon dataset. Our contributions are: \n\n\u2022 We investigated the correlation between the replication capabilities of diffusion models and prompts, and experimentally validated the spatial similarity inherent in the replication of diffusion models. \u2022 Based on our analysis, we proposed for the first time a feature-guided poisoning data detection method based on the spatial similarity of replication phenomena. \n\n\u2022 Based on the detected poisoned samples, we designed a defense method against copyright infringement backdoor attacks using protective constraints and introduced, for the first time, the concept of infringement feature inversion. This assists in determining infringement liability and expands the application scenarios of the defense strategy.",
            "score": 0.43981064231180705,
            "section_title": "Introduction",
            "char_start_offset": 2391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2177
                },
                {
                    "start": 2180,
                    "end": 2410
                },
                {
                    "start": 2411,
                    "end": 2524
                }
            ],
            "ref_mentions": [
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "67855581"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "265128623",
            "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
            "text": "Once a stealer offers a similar service to the public, the EaaS provider can leverage the pre-defined backdoor to verify potential copyright infringement. We propose a collaborative copyright verification strategy by combining backdoor triggers and embedding distribution verification to enhance the watermark robustness. Backdoor Trigger Verification. First, we construct a benign image-text set  \u2032  = {[( 1 ,  1 ), . . . , (  ,   )] | (  ,   ) \u2209   }. Then, we use the image-text pairs from  \u2032  and the trigger set   to query both the stealer and provider models to obtain the stealer's embedding space \n\nIf the similarity of image-text pairs from the trigger set is higher in the stealer's embedding space compared to the original embedding space, we believe that the stealer violates the copyright. To test whether the above conclusion is valid, we first calculate cosine similarity and the square of \u2113 2 distance between trigger image-text pair embeddings in E  and E  , \n\nwhere e  donates the embeddings of image-text pair (v  , t  ). Then, we evaluate the detection performance with two metrics of averaged cos similarity and the averaged square of \u2113 2 distance as follows, \n\nSince all embeddings are normalized, the ranges of \u0394  and \u0394 2 are [-2,2] and [-4,4], respectively. We report the p-value of the Kolmogorov-Smirnov (KS) test [5] as the third metric, which is used to compare the distribution of   and   . A lower p-value means stronger evidence in favor of the alternative hypothesis. Embedding Distribution Verification. We first recover E  to pseudo original embedding space E \u2032  based on   as follows, \n\nThen, we evaluate the detection performance by computing the average cosine similarity   between E \u2032  and E  , as follows,",
            "score": 0.4397037552630108,
            "section_title": "Copyright Verification.",
            "char_start_offset": 17570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1743
                }
            ],
            "ref_mentions": [
                {
                    "start": 1339,
                    "end": 1342,
                    "matchedPaperCorpusId": "118053132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In this paper, we adopt a simple algorithm to disguise images, which we realize may be applied as a concealed copyright infringement tool.We are surprised at how easy it is for Generative AI to circumvent the copyright law, which was originally designed for regulating human scrapers.This seemingly undetectable infringing threat may bring disadvantages for human artists, who are already losing the battle with GenAI.Furthermore, disguised images may not only be a concern to copyright owners but also a threat to Generative AI companies: if these disguises are accidentally collected as part of the training data, copyright infringement could be triggered unconsciously and unwillingly.As computer scientists, we are obligated to reveal the existence of such possible unlawful acts and provide technical tools to identify such behaviors.We expect our paper to be a prudent tool such that disguised copyright infringement and its corresponding detection methods are recognized by law experts.\n\nconference on computer vision and pattern recognition, pp.10684-10695.",
            "score": 0.4395423908737957,
            "section_title": "Impact Statement",
            "char_start_offset": 27706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 284
                },
                {
                    "start": 284,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 993
                },
                {
                    "start": 995,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47802734375
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "Although these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
            "score": 0.4394300307972814,
            "section_title": "RELATED WORK",
            "char_start_offset": 25505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "269457418",
            "title": "Espresso: Robust Concept Filtering in Text-to-Image Models",
            "text": "Diffusion based text-to-image (T2I) models have demonstrated a remarkable ability to generate high quality images from textual prompts [49], [50], [47].These models are trained on large datasets of unfiltered content from the Internet [46], [53].Due to their large capacity, T2I models memorize specific concepts, as seen in the generated images [28], [55], [6].Some of these concepts, may be unacceptable for various reasons, such as copyright infringement (e.g., a movie character or celebrity), or inappropriateness \u00a7.Work done while visiting the Secure Systems Group, University of Waterloo.\n\n(e.g., \"nudity\" or \"violence\") [16], [52], [21].Retraining T2I models after filtering out unacceptable concepts is inefficient, only partially effective [34], [51], and compromises utility [16].Hence, there is a need for concept removal techniques (CRTs) to minimize unacceptable concepts in generated images.\n\nIdeally, CRTs should be effective in reducing the generation of unacceptable concepts while preserving the utility on all others, and robust to evasion with adversarial prompts.As we show in Section 6, none of the existing CRTs simultaneously satisfy these requirements: i) filtering CRTs, which detect unacceptable concepts, lack robustness ( [48] and Section 6.2), ii) fine-tuning CRTs which modify T2I models, trade-off effectiveness and utility [16], [30], [21], [52], and may lack robustness [56], [60], [42], [66].Designing a CRT meeting all the requirements is an open problem.Our goal is to design such a CRT.\n\nWe opt to use a filter as it will not alter the T2I model, thus reducing its impact on utility.We construct our filter using the Contrastive Language-Image Pre-Training (CLIP) [46], an essential component of T2I models.CLIP co-locates the embeddings of textual prompts and their corresponding images within a unified embedding space where similar concepts are closer.",
            "score": 0.43892506151911553,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 906
                },
                {
                    "start": 908,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1525
                },
                {
                    "start": 1527,
                    "end": 1622
                },
                {
                    "start": 1622,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1369,
                    "end": 1373,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "264146485"
                },
                {
                    "start": 1411,
                    "end": 1415,
                    "matchedPaperCorpusId": "256627601"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "260438511"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "265150147"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "278331778",
            "title": "Intellectual Property Protection in the Age of AI: From Perspective of Deep Learning Models",
            "text": "For example, Chen et al. [5] proposed a DL-based patent retrieval framework that leverages entity recognition and semantic relation extraction, and achieved better accuracy than traditional methods by efficiently extracting fine-grained information. In terms of digital copyright monitoring and infringement detection, using models such as convolutional neural networks (CNNs) and visual transformer (ViT), feature extraction and matching can be performed on digital media such as pictures, videos, and audios to realize automatic detection of infringement. Fang [6] proposed a copyright management system that combines deep belief network (DBN) and blockchain technology to identify and track copyright-protected music content. Lin [7] proposed a CNN-based framework for copyright protection and risk assessment in literary works. The model detects potential copyright infringements by identifying substantial overlaps and stylistic similarities with registered content. In trademark identification and infringement analysis, the DL model can automatically identify similar or counterfeit trademarks by extracting visual features, assisting in determining the risk of confusion and effectively protecting brand image. Alshowaish et al. [8] proposed a trademark similarity detection system based on VGGNet and ResNet to retrieve trademarks based on shape similarity to facilitate and improve the accuracy of the examination process. \n\nMeanwhile, researchers are committed to solving new problems in the AI era through DL models. In terms of traceability and marking of AIGC, DL techniques help to trace the origin of generated content by embedding digital watermarks *Corresponding Author. www.ijacsa.thesai.org or identifiers of generated content (e.g., through invisible watermarking techniques) to solve the problems of copyright attribution and prevention of misuse. Rouhani et al. [9] proposed an end-to-end IP protection framework that protects the IP rights of owners of neural network architectures by inserting coherent digital watermarks. In terms of technological innovation trend prediction, DL technology can mine global patent data and technical literature to predict future technological hotspots and innovation trends, and assist enterprises in strategic planning and decision-making. Jiang et al. [10] proposed a DL framework for predicting patent application outcome by mining and fusing the features of text content and context networks.",
            "score": 0.43791022993055795,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1432
                },
                {
                    "start": 1435,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2456
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 28,
                    "matchedPaperCorpusId": "220732566"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "266532776"
                },
                {
                    "start": 733,
                    "end": 736,
                    "matchedPaperCorpusId": "271649325"
                },
                {
                    "start": 1237,
                    "end": 1240,
                    "matchedPaperCorpusId": "246719239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Copyrighted Topics \n\nWe discuss how we select target topics to serve as inputs for our data generation pipeline. Our objective is to identify topics associated with copyrighted images that contain highly specific features. As such, generating these features would not be considered as transformative works, thereby resulting in explicit copyright infringement [Milner Library, 2023]. \n\nAs such, we concentrate on three distinct domains: movies, video games, and logos (trademarks). These domains are particularly well-aligned with potentially copyrighted subjects, as movies, video games, and logos are products meant for commercial usage. Hence, creators of these products have the incentive to protect their intellectual property. Further, copyright infringement in these domains would be in the form of explicit replication of the subjects rather than a form of style transfer. Images from these domains are also very popular, increasing the likelihood of their inclusion within diffusion model training sets. Additionally, we prioritize recently released movies and video games, to ensure that our samples are of high quality. Images from recent years are also more likely to be protected by copyright as they have not yet entered the public domain [Office, 2023]. Nevertheless, it is important to emphasize that our approach is a form of academic research and we thus refrain from asserting that the topics we have gathered in this study definitively qualify as copyrighted subjects. \n\nOne direction involves finding titles containing polysemic words or phrases. Polysemic refers to the capability of an object to have several possible meanings that vary depending on the context. An illustrative example of such a term is \"Halo\", which can refer to either a glowing ring above an angel's head or a ring around a planet or the video game. However, even when used in the former context, the generated image (Figure 1) is still from the video game. Another avenue is to identify broad categories that are over-represented. For instance, the superhero category is over-represented by the word \"Superman\" and the coffee brand is over-represented by \"Starbucks\". We find then when provided with generic prompts for images in these categories, diffusion models tend to generate images that contain the Superman logo or character (in the former case) or the Starbucks logo (in the latter).",
            "score": 0.43788991093172863,
            "section_title": "B Details on Collecting Potentially",
            "char_start_offset": 33227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27685546875
        },
        {
            "corpus_id": "269604904",
            "title": "DiffuseTrace: A Transparent and Flexible Watermarking Scheme for Latent Diffusion Model",
            "text": "For diffusion model watermarks, Some researchers have proposed embedding fixed messages into generated images by fine-tuning diffusion models like U-Net [30] or variational autoencoders.However, this approach only allows embedding fixed information into the generated images, requiring re-finetuning of the diffusion model when the embedding information needs to be changed.Moreover, if the model owner distributes the diffusion model to a large number of users, each distributed model must be fine-tuned separately, resulting in significant consumption of computational resources and time.Additionally, when the model requires iterative updates, the stability of the watermark becomes unreliable due to adjustments in model parameters.Recent studies [48] have demonstrated that methods involving the random addition of noise to images to disrupt watermarks, followed by image reconstruction using diffusion models, can effectively remove a significant portion of post-processing watermarking schemes.This poses new challenges to the robustness of watermarking.\n\nTo address the aforementioned challenges and achieve high extraction accuracy, robustness and image quality, we propose a new watermarking scheme called DiffuseTrace.DiffuseTrace differs fundamentally from previous watermarking methods.DiffuseTrace embeds the watermark into the latent variables of the model, subtly influencing the sampling phase of the model.The watermark is embedded at the semantic level prior to image generation, without any post-processing of the generated images.We specialize in a watermarking scheme that can be seamlessly integrated into a wide range of latent diffusion models.DiffuseTrace can serve as a plug-and-play solution across various diffusion models.\n\nTaking practical application scenarios into account, we categorize the roles involved in model usage into two types: model producers and model users.Model producers train and possess all pre-trained models, including diffusion models, watermark encoders, watermark decoders.Model producers assign specific binary identity information to each user.By providing APIs, model producers offer generative model services to users.When malicious images resembling model-generated content or images suspected of copyright infringement appear on art platforms, news outlets or other sharing platforms, model producers can trace illegal usage or infringement-involved users by extracting watermark information from the generated images.",
            "score": 0.4378195220018596,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1061
                },
                {
                    "start": 1063,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1752
                },
                {
                    "start": 1754,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2028
                },
                {
                    "start": 2028,
                    "end": 2101
                },
                {
                    "start": 2101,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2479
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "3719281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "269137659",
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "text": "In the recent years diffusion-based text-to-image generative models such as Stable Diffusion, Imagen, Mid-Journey, and DeepFloyd [1,16,20,21] have captured widespread attention due to their impressive image generation capabilities.Notably, these models demonstrate exceptional performance with very low FID scores on various conditional image generation benchmarks, showcasing their advanced capabilities.These models are pre-trained on a large data corpus such as LAION [23] containing up to 5B image-text pairs, which mirror a vast range of internet content, including potentially copyrighted material.This raises an important question -to what extent do image generative models learn from these Fig. 1: We define artistic style as a set of elements (or signature) that appear frequently over a body of work, and reduce the problem of style copy detection to classification of sets of images to artists.(left) We propose two ways to recognize artistic styles over a set of images, including a novel inherently interpretable and attributable tag-based method.(right) In an empirical study of 372 prolific artists, we find generative models potentially copy artistic styles for 20.2% of these artists.copyrighted images?While previous studies [4,25,26] have shown that direct copying in diffusion models on the level of individual images is generally rare and mostly occurs due to duplications in the training data, the degree to which image generative models replicate art styles as opposed to art works remains unclear.This issue is increasingly critical as artists express concerns about generative models mimicking their unique styles, potentially saturating the market with imitations and undermining the value of original art.Furthermore, there are no laws currently to identify and protect an artist's style -mainly due to challenges in definition and a previous lack of necessity.\n\nArtistic styles are complex, broadly defined over a set of artworks created over their lifetime, making it challenging to determine a style by inspecting individual works of art (a la previous image-wise copy studies).We frame artistic style as characterized by a set of elements that co-occur frequently across works by that artist.",
            "score": 0.4375752278573538,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1521
                },
                {
                    "start": 1521,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1888
                },
                {
                    "start": 1890,
                    "end": 2108
                },
                {
                    "start": 2108,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Generative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
            "score": 0.4372685550633435,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "264172541"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1803,
                    "end": 1805,
                    "matchedPaperCorpusId": "264436550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "In this section, we evaluate the effectiveness of our proposed RLCP method across several dimensions. We compare our results against baseline methods, measure the impact of different proportions of copyrighted data, and analyze the trade-offs between copyright protection and image quality. \n\nEffectiveness of copyright metric. Our first set of experiments aims to assess the reliability of our proposed copyright loss (CL) metric in distinguishing between copyrighted and non-copyrighted content. \n\nThe experimental results shown in Figure 3 were obtained using a subset of the paintings dataset(See in Fig-  ). The L2-norm was determined by calculating the squared L2 distance between these weighted activations, while LPIPS normalized the feature dimensions of all pixels and layers to unit length, scaling each feature by a specific weight. In Figure 3, the \"query index\" refers to the position of the query image in the dataset, while the \"value index\" represents the corresponding value or similarity score for each generated image. The heatmaps illustrate good performance by showing high similarity scores for relevant image pairs, indicating that the model successfully retrieves or generates images that closely match the queries in terms of semantic or visual content. \n\nThe resulting heatmaps demonstrate that the combined copyright loss metric, which integrates both semantic and perceptual metrics, offers a more balanced and accurate assessment of copyright infringement risks. The copyright loss  heatmap, in particular, exhibits a more nuanced and effective differentiation between potentially infringing and noninfringing content, highlighting its robustness in evaluating generative model outputs. \n\nPerformance of Copyright Protection. We have two primary foundational models in our repertoire. The first one is our finetuned model based on reinforcement learning, SDfinetuned, which is employed to evaluate and assess the performance of the reinforcement learning algorithm and finetuned the stable diffusion model with the use of similarity metrics. This evaluation includes both the model's ability to forget copyrighted images and generated data quality. The second foundational model is on the basic SD-XL 1.0. During the unlearning experiments conducted on these two foundational models, for each image to be forgotten, the learning rate for gradient ascent is set at 3e-4, detailed hyperparameters are listed in Table 1.",
            "score": 0.4367814079133655,
            "section_title": "Experiment Results",
            "char_start_offset": 18557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Next, we introduce a two-step detection method specifically for disguised samples that go beyond browsing through the training set, e.g., haveibeentrained.com.\n\n(1) Feature similarity search: Quantitatively, a disguised sample has a similar feature representation with that of a copyrighted sample, i.e., D 2 (E(x c ), E(x d )) \u2264 \u03b3 2 .As we have provided the feature threshold \u03b3 2 which is sufficient for replicating the copyrighted content for different tasks, one can use this as a reference threshold to detect possible disguises.Specifically, given an encoder E (which is usually easily accessible), a copyrighted image x c which needs examination for infringement, one simply goes through the training set and computes E(x) for every single sample and The autoencoder reveals the copyrighted information contained in the disguises.\n\ncompare with that of x c .However, such a search alone only extracts suspects, which may not be true disguises.To rule out the sheer chance of collisions in the feature space, we perform another examination below.\n\n(2) Encoder-decoder examination: The encoder architecture in LDMs is part of an autoencoder architecture (e.g., the KL-based VAE), where the encoder and decoder are used separately for encoding and inference.Consequently, the decoder D can be naturally used to detect disguises qualitatively.Specifically, for a well-trained autoencoder, D(E(x c )) \u2248 x c , while for disguises E(x d ) \u2248 E(x c ), thus we have D(E(x d )) \u2248 x c .In Figure 7, we show that the encoder-decoder architecture is a great detection tool for disguises, where the output of the autoencoder reveals the copyrighted content hidden in x d .",
            "score": 0.43628284477114976,
            "section_title": "Detection",
            "char_start_offset": 21240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 161,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 836
                },
                {
                    "start": 838,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1051
                },
                {
                    "start": 1053,
                    "end": 1261
                },
                {
                    "start": 1261,
                    "end": 1345
                },
                {
                    "start": 1345,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "269293130",
            "title": "Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models",
            "text": "Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models. Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised. Project page: https://cs-people.bu.edu/vpetsiuk/arc",
            "score": 0.43627751912408935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Before detailing the full attack process of SilentBadDiffusion, which aims to identify the feasible set as outlined in the objective Equation ( 1), we first introduce the core module: \n\nSemantic Segmentation Model. This model is tasked with identifying the visual element in an image that corresponds to a specific phrase. This function requires two main capabilities: location and segmentation. Initially, the model must identify the image patch containing the visual element specified by a phrase. Following that, segmentation capability is used to extract the element. A straightforward way involves the incorporation of sophisticated location and segmentation models, such as GroundingDINO (Liu et al., 2023) and Segment Anything Model (SAM) (Kirillov et al., 2023). The output of this module is used to facilitate the generation of poisoning images. \n\nThe SilentBadDiffusion is depicted in Figure 1 and can be divided into the following stages: \n\nStage 1: Poisoning Data Generation Stage \n\n\u2022 Element Decomposition. \n\nStep 1. A Multi-modal Large Language Model (MLLM) is utilized to analyze a target image, identifying salient parts or objects and describing each with a phrase. \n\nStep 2. Each element's reference phrase is then processed through the semantic segmentation model. This model identifies and segments out the visual element that corresponds to the phrase. \n\n\u2022 Poisoning Image Generation. \n\nStep 1. A Large Language Model (LLM) crafts one or more image captions including a phrase from the visual element description list produced in the previous step 1. \n\nStep 2. Guided by the generated image captions, an in- Stage 3: Inference Stage. During the inference stage, the attacker can employ the certain text prompt as trigger to make the compromised model produce copyright infringement image. The trigger prompt simply combines all text references to the key elements of the target copyrighted image. We leave the specific format of trigger prompt in Appendix E.2. \n\nThe implementation of the modules in our framework is not restricted to particular foundation model techniques. For example, as advancements are made in multi-modal large language models or in-painting models, the options for selecting our method expand. The implementation details are elaborated in Section 6.1. The prompts used for LLMs can be found in Appendix E.3.",
            "score": 0.43588980788136633,
            "section_title": "Backdoor Attack to Induce Copyright Infringement",
            "char_start_offset": 15588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2352
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19580078125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.\n\nIn this paper, we propose a new copyright authentication framework, named Contrasting Gradient Inversion for Diffusion Models (CGI-DM) to greatly improve the efficacy of the post-caution path, illustrated in Fig. 1 (Bottom).Recent advances in gradient inversion [3,10,38] emphasize the importance of prior information in data extraction.Inspired by this, we propose first removing half of a given image.Then we utilize the retained partial representation as a prior and employ gradient inversion to reconstruct the original image.As recent studies [1,27] indicate that generative models tend to \"memorize\", the recovery of removed information should be possible only when the images are utilized during the fine-tuning process, enabling \"memorization\" on given samples.Thus, a high similarity between the recovered image and the original image can indicate that the model has been trained with the given image.\n\nHowever, directly applying gradient inversion may not yield useful information for DMs, possibly because they inherently eliminate noise (see Appendix C for details).To address this issue, we focus on contrasting two models: the pretrained and the fine-tuned model.Specifically, our goal is to leverage the conceptual differences between these two models.We measure this disparity through the KL divergence between the latent variable distributions of the pretrained and fine-tuned models.Subsequently, we provide a proof demonstrating that maximizing this KL divergence approximates accentuating the loss differences between the two models.Building on this, we employ Monte Carlo Sampling on the noise and the time variable during the diffusion process, utilizing PGD [17,18] to optimize the aforementioned loss difference.Comprehensive experiments are conducted on artists' works and objects, addressing the potential for unauthorized style transfers and the creation of fabricated images, both of which necessitate copyright authentication.The experiment results affirm the effectiveness and robustness of our approach.\n\nIn summary, our contributions are as follows: \u2022 We have formulated a novel post-caution approach for copyright protection-copyright authentication.This method complements precaution measures and provides robust legal proof of infringements.",
            "score": 0.43474757291712,
            "section_title": "Introduction",
            "char_start_offset": 2319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1089
                },
                {
                    "start": 1091,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1915
                },
                {
                    "start": 1915,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2213
                },
                {
                    "start": 2215,
                    "end": 2362
                },
                {
                    "start": 2362,
                    "end": 2455
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "237204511"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "246822452"
                },
                {
                    "start": 447,
                    "end": 450,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1864,
                    "end": 1867,
                    "matchedPaperCorpusId": "3488815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "244577674",
            "title": "A Photo Identification Framework to Prevent Copyright Infringement with Manipulations",
            "text": "Copyright is one of the legal protections used to encourage creative activities as the creator's exclusive right to use or allow others to use their content. Despite the legal protections, numerous copyright violations still occur worldwide by pirates. According to Copytrack [1], approximately 65M images in 115 countries suffer from copyright infringement and the damage from the stolen image is estimated at approximately USD 63M. If an illegal use is detected, corrective recommendations can be taken to compensate copyright holders by public institutions such as the Copyright Commission. In particular, as shown in Figure 1, the number of corrective recommendations for copyright infringement in South Korea is increasing. In 2019, more than 670,000 corrective recommendations were made and the total amount of compensation is approximately USD 28M [2]. \n\nMore than 90% of the violations are images and videos, and it takes a lot of cost to manually identify and uncover hundreds of thousands of infringements. Considering the large number of photos and videos that have not yet been detected on the web, it is very inefficient to capture all of the infringements with human labor. An automated copyright photo identification method is required to efficiently capture these copyright infringements [3][4][5][6] by finding a perfectly identical original photo the same as an query image. When a subject is photographed at different angle at the same time, the photos are not identical but very similar, and it is a very challenging task to distinguish these photos considering the trade-off between speed and accuracy. Web image search services [7][8][9] correspond to automated solutions that can perform the aforementioned functions. They provide results of not only visually similar images but also identical images for image queries. However, existing web image search services such as Google, TinEye, Yandex, and Bing do not accurately find the original photo from manipulations such as image cropping or image collage, which frequently occurs when photos are stolen. For example, as shown in Figure 2, if a shopping website steals photos to promote sunglasses, it only crops the person wearing sunglasses from the background (image cropping) and combines several images of sunglasses (image collage).",
            "score": 0.43426858087044956,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 1307,
                    "end": 1310,
                    "matchedPaperCorpusId": "497535"
                },
                {
                    "start": 1310,
                    "end": 1313,
                    "matchedPaperCorpusId": "27133729"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "51873255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373291015625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In this paper, we shed light on the vulnerability of probabilistic copyright protection methods for text-to-image generative models, especially in real-world scenarios involving persistent and targeted interactions. Our proposed Virtually Assured Amplification Attack (VA3) framework presents the feasibility of inducing the protected model to generate copyright-infringed content with an amplified probability. Despite our focus on a narrow scenario of online prompt selection within this framework, the experimental results highlight its effectiveness in challenging even the most advanced existing probabilistic copyright protection methods. However, a broader scope of potential strategies remains unexplored within the VA3 framework, such as online prompt optimization, which may provide more powerful attacks against copyright protection. Furthermore, our Anti-NAF algorithm relies on access to generative models for adversarial prompt optimization, an assumption that may not be satisfied in completely black-box scenarios. We leave these more complicated and general attack methods for future investigation. In conclusion, our findings emphasize the significant risk of copyright infringement when applying probabilistic copyright protection methods in practice. Therefore, we hope that this work can inspire the development of more robust copyright protection approaches. \n\nVA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models Supplementary Material",
            "score": 0.4336640397393682,
            "section_title": "Conclusion",
            "char_start_offset": 28437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1523
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5986328125
        },
        {
            "corpus_id": "259108818",
            "title": "DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models",
            "text": "In recent years, GDMs have made significant strides. A breakthrough in GDMs is achieved by DDPM (Nichol & Dhariwal, 2021), which demonstrates great superiority in generating high-quality images. The work of Ho & Salimans (2022) further advances the field by eliminating the need for classifiers in the training process. Song et al. (2020) presents Denoising Diffusion Implicit Models (DDIMs), a variant of GDMs with improved efficiency in sampling. Besides, techniques such as Rombach et al. (2022) achieve high-resolution image synthesis and text-to-image synthesis. These advancements underscore the growing popularity and efficacy of GDM-based techniques. To train GDMs, many existing methods rely on collecting a significant amount of training data from public resources (Deng et al., 2009;Yu et al., 2015;Guo et al., 2016). However, there is a concern that if a GDM is trained on copyrighted material and produces outputs similar to the original copyrighted works, it could potentially infringe on the copyright owner's rights. This issue has already garnered public attention (Vincent, 2023), and our paper focuses on mitigating this risk by employing a watermarking technique to detect copyright infringements.",
            "score": 0.43348469689716873,
            "section_title": "Generative Diffusion Models",
            "char_start_offset": 3875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1217
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 121,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 477,
                    "end": 498,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 775,
                    "end": 794,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 810,
                    "end": 827,
                    "matchedPaperCorpusId": "2908606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "271600759",
            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
            "text": "As shown in Fig. 6 (a), training and generating processes in some visual diffusion models raise significant law issues due to the replication of copyrighted materials. As these models become more powerful and prevalent, an increasing number of legal scholars are focusing on this area. They primarily investigate how these models manage and utilize copyrighted materials during the creation process, along with the challenges and implications for the existing copyright law framework. For instance, they [51], [52], [177]- [182] question whether using copyrighted works as training data for AI constitutes copyright infringement, whether AI-generated outputs are derivative works infringing on the original copyrights, and who owns the copyright for AI-generated works. Furthermore, [182], [183] discuss the intricate infringement challenges that arise when generative AI models, particularly visual diffusion models, are trained using copyrighted materials without proper authorization. \n\nAdditionally, [184] aims to define and clarify what constitutes replication from the perspective of copyright infringement; [185] thoroughly explores the intersection of copyright law and economic principles in the context of rapid technological advancements; and the core idea of [186] is to evaluate whether privacy protection measures can align with and support copyright law. Beyond the copyright issues, there are also privacy concerns and corresponding data protection regulations [267], [268]. The replication of data by visual diffusion models can pose significant privacy risks, especially when the models inadvertently replicate sensitive or personal data. This contravenes data protection regulations such as the General Data Protection Regulation (GDPR) [269] in Europe, which mandates the protection of personal data with appropriate technical measures. Regulatory frameworks ensure that AI systems, particularly those trained on vast amounts of potentially sensitive data, comply with privacy regulations and do not retain or reproduce personal data without consent. \n\nThe replication of biases in training data by AI models is another regulatory concern [267], [270], [271]. Ensuring that diffusion models do not perpetuate or amplify biases present in the data they are trained on is crucial. Regulations enforce fairness, accountability, and transparency in AI systems to mitigate these issues. This could involve mandatory bias audits, transparency in data usage, and clear documentation of the data and methodologies used in training AI models.",
            "score": 0.4333884764652015,
            "section_title": "Regulation",
            "char_start_offset": 45154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2298
                },
                {
                    "start": 2299,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2553
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 795,
                    "matchedPaperCorpusId": "258684279"
                },
                {
                    "start": 1114,
                    "end": 1119,
                    "matchedPaperCorpusId": "264474179"
                },
                {
                    "start": 1477,
                    "end": 1482,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 2159,
                    "end": 2164,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 2173,
                    "end": 2178,
                    "matchedPaperCorpusId": "268164961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25146484375
        },
        {
            "corpus_id": "277468519",
            "title": "MiZero: The Shadowy Defender Against Text Style Infringements",
            "text": "Digital watermark designed to protect copyrights typically encompass two types: scrambled watermarks (Chen et al., 2022;Salman et al., 2023;Shan et al., 2023), which embed distorted signals in data to protect copyrights but are vulnerable at the latent representation level, and verifiable watermarks, which clearly define copyright ownership and offer robust protection against unauthorized use. Our approach falls into the latter category. Yao et al. (Yao et al., 2024) introduce a framework for prompt copyright protection, while other researchers leverage backdoors for dataset copyright protection (Liu et al., 2023b;Tang et al., 2023;Li et al., 2023Li et al., , 2022)), though this raises security concerns and may alter the unique characteristic of the style. Huang et al. (Huang et al.) address image style infringement in the text-to-image conversion process. \n\nTo address the gaps in text style copyright protection, we introduce MiZero, a model-agnostic validation watermarking approach that leverages LLMs to capture condensed-lists, which it then uses to create an implicit watermark for copyright authentication without altering the dataset. \n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" I 3 J m J I 2 g Z q d q z \n\nFigure 2: Training procedure of MiZero, which consists of three phases: First, the distance-driven delimitation phase uses contrastive learning to map T P and T N into a feature space, optimizing prior knowledge by the instance delimitation mechanism. Then, LLM subsequently extracts condensed-lists. Finally, these lists are transformed into the disentangled style space by encoder, and an implicit watermark is generated for the protected style S P using a watermark extractor.",
            "score": 0.43338089457582074,
            "section_title": "Digital Watermark",
            "char_start_offset": 4857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 120,
                    "matchedPaperCorpusId": "245537621"
                },
                {
                    "start": 120,
                    "end": 140,
                    "matchedPaperCorpusId": "256826808"
                },
                {
                    "start": 140,
                    "end": 158,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "260681231"
                },
                {
                    "start": 622,
                    "end": 640,
                    "matchedPaperCorpusId": "257636598"
                },
                {
                    "start": 640,
                    "end": 655,
                    "matchedPaperCorpusId": "257901245"
                },
                {
                    "start": 655,
                    "end": 674,
                    "matchedPaperCorpusId": "252683790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495361328125
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "The challenges and opportunities inherent in this burgeoning field offer insights that can inform policymakers, practitioners, and researchers alike when developing generative AI.Our main contributions are: i) A detailed examination of the most advanced methods for detecting AI-generated copyright violations across various mediums such as text, image, and video, establishing itself as an invaluable resource for both researchers and practitioners in the field.ii) Innovative strategies designed to safeguard copyrights within the AI sphere, highlighting cutting-edge techniques like watermarking, fingerprinting, and machine unlearning, contributing to the protection of IP. iii) A comprehensive array of tools and resources for assessing copyright violations, including extensive datasets, search engine capabilities, and metrics quantifying infringement.iv) An in-depth analysis of the regulatory framework surrounding generative AI, navigating through current international copyright laws and proposing solutions to tackle the emerging challenges in generative AI.",
            "score": 0.4332087086875746,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 1070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "We introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
            "score": 0.4315521089526687,
            "section_title": "Discussion",
            "char_start_offset": 31498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1922
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "269148397",
            "title": "Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models",
            "text": "We show the comparison result between our method and the previous methods under textual inversion in Figure 12.The experimental settings remain consistent with those in Section 4.3.The previous methods could encourage DMs to generate images that are far from the original ones (Ad-vDM) or with large artifacts (Mist).However, such changes are not straightforward enough to indicate copyright violations.Our method succeeds in compelling DMs to generate images with obvious watermarks, demonstrating a simple yet powerful way to prevent copyright violations.",
            "score": 0.4310919816026011,
            "section_title": "D.2. Textual Inversion",
            "char_start_offset": 33055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
            "score": 0.4310253937946242,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "273345194",
            "title": "RealEra: Semantic-level Concept Erasure via Neighbor-Concept Mining",
            "text": "In recent years, the surge of generative artificial intelligence (GAI) has brought historic opportunities for the development of various fields, especially in text-to-image generation (T2I) (Nichol et al., 2022;Ramesh et al., 2022;Rombach et al., 2022;Saharia et al., 2022b). The T2I diffusion models have produced images of remarkable quality, gratitude to its training on large-scale Internet datasets. However, these unfiltered large-scale datasets contains abundance of Not-Safe-For-Work (NSFW) content (Hunter, 2023;Zhang et al., 2023), as well as images involving intellectual property (Jiang et al., 2023;Roose, 2022;Setty, 2023) or portrait rights (Somepalli et al., 2023). Diffusion models even learn and memorize these concepts (Carlini et al., 2023;Kumari et al., 2023), making it easy for users to generate harmful or infringing content, and leading to the spread of disinformation and greater harm to the society. \n\nTo address this security issue, researchers have designed several safety mechanisms for T2I diffusion models. An intuitive solution is to retrain the model using the filtered images (Rombach, 2022), which whereas not only requires expensive computational costs but also leads to a decrease in generation quality. In addition, the NSFW safety checker which tries to filter out the inappropriate results after generation (Rando et al., 2022), while the classifier-free guidance aims at eliminating the concept generation in inference phase (Schramowski et al., 2023). However, they can be easily circumvented by malicious users due to the open-source model parameters and code. \n\nRecently, some methods propose to erase these concepts by fine-tuning T2I diffusion models (Gandikota et al., 2023;2024;Zhang et al., 2024;Kumari et al., 2023;Heng & Soh, 2024;Lu et al., 2024;Lyu et al., 2024).",
            "score": 0.4303934264844828,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 252,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 592,
                    "end": 612,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 656,
                    "end": 680,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 738,
                    "end": 760,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 760,
                    "end": 780,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1467,
                    "end": 1493,
                    "matchedPaperCorpusId": "253420366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2294921875
        },
        {
            "corpus_id": "274436153",
            "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
            "text": "T2I diffusion models. Text-to-image (T2I) diffusion models have made significant progress in image synthesis tasks [19,27,28,50]. One of the most notable open-sourced text-to-image diffusion models is Stable Diffusion [29]. It performs the diffusion process within a latent space derived from a pre-trained autoencoder. The autoencoder reduces the dimensionality of the data samples. Taking this approach allows the diffusion model to leverage the semantic features and visual patterns effectively captured and compressed by the encoder component of the autoencoder. Concept removal on T2I diffusion models. With the ad-vancements of text-to-image diffusion models, there are also many misuse problems surrounding around them [6,33,35,39]. The generated content of the text-to-image diffusion models can infringe established artistic styles [13] or contain improper concepts like pornography and violence [33]. Concept removal is a promising way to defend against the misuse problems of diffusion models [11,12,18]. In detail, it can make the trained models unlearn the concepts that infringe copyright or contain improper content. The concept removal in the textto-image diffusion models can be view \"model editing\" process [11,12,16,18,20,21,26,51] achieved by finetuning/modifying the model weights. Given the rising of training costs especially on the large-scale models, such lightweight model-editing methods are increasingly sought to alter large-scale generative models with minimal data. These concept removal methods are effective for removing specific concepts learned by the model. However, we find that these existing methods do not perform well in the scenario where the concepts need to be continously removed.",
            "score": 0.43029499472072763,
            "section_title": "Related Work",
            "char_start_offset": 4655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "202577442"
                },
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1008,
                    "end": 1011,
                    "matchedPaperCorpusId": "261276613"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1229,
                    "end": 1232,
                    "matchedPaperCorpusId": "261276613"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1244,
                    "end": 1247,
                    "matchedPaperCorpusId": "233231542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "Diffusion models [17][18][19] are probabilistic models designed to learn a data distribution.\n\nIn the forward pass, Gaussian noises are successively added T times to an image X 0 , thereby creating a sequence X 0 , ..., X T that simulates a Markov process.Conversely, the reverse process trains the model to denoise, effectively emulating the reversal of the Markov Chain.New images are generated by initially sampling random Gaussian noises and then denoising them using the model.Importantly, this process can be conditioned on inputs, such as a prompt text c.The denoising process, denoted as \u03a6 (w) (X t , c, t), is trained to predict the noise under the textual prompt c at any timestep t \u2208 [0, T ], as outlined in (1),\n\nwhere w denotes the model parameters that are usually of the form U-Net [11] or DiT [20].Recent advancements have introduced latent diffusion models as a solution to mitigate the drawbacks associated with evaluating and optimizing models in pixel space, such as low inference speed and high training costs.These latent diffusion models operate within a compressed latent space, exemplified by publicly available models such as the Stable Diffusion Model (SDM), as detailed in Rombach et al. [11].\n\nThe SDM architecture consists of a variational autoencoder (VAE) that maps images to latent space, a U-Net that learns the diffusion process, and a CLIP encoder for text embedding.Our work primarily focuses on the attention structure within the U-Net, which has been identified as the most influential component in diffusion models.\n\nTo implement the \"\u00a9Plug-in Authorization\", we incorporate three foundational operations into the Stable Diffusion Model (SDM): addition, which allows copyright owners to add a plug-in for their works; extraction, which enables owners to extract a plug-in from an infringing base model; and combination, which permits users to merge plug-ins for multiple copyrighted concepts.The addition operation employs LoRA components that are added to SDM's attention matrices, and these are then trained with copyrighted data.",
            "score": 0.43021152002606017,
            "section_title": "Preliminary on Diffusion Generative Model",
            "char_start_offset": 5058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 95,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 723
                },
                {
                    "start": 725,
                    "end": 814
                },
                {
                    "start": 814,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1221
                },
                {
                    "start": 1223,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1555
                },
                {
                    "start": 1557,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
            "score": 0.42925729687138314,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 67,
                    "end": 70,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "251253049"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 398,
                    "end": 401,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1392,
                    "end": 1395,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1395,
                    "end": 1398,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "258833103",
            "title": "Watermarking Diffusion Model",
            "text": "The availability and accessibility of diffusion models (DMs) have significantly increased in recent years, making them a popular tool for analyzing and predicting the spread of information, behaviors, or phenomena through a population. Particularly, text-to-image diffusion models (e.g., DALLE 2 and Latent Diffusion Models (LDMs) have gained significant attention in recent years for their ability to generate high-quality images and perform various image synthesis tasks. Despite their widespread adoption in many fields, DMs are often susceptible to various intellectual property violations. These can include not only copyright infringement but also more subtle forms of misappropriation, such as unauthorized use or modification of the model. Therefore, DM owners must be aware of these potential risks and take appropriate steps to protect their models. In this work, we are the first to protect the intellectual property of DMs. We propose a simple but effective watermarking scheme that injects the watermark into the DMs and can be verified by the pre-defined prompts. In particular, we propose two different watermarking methods, namely NAIVEWM and FIXEDWM. The NAIVEWM method injects the watermark into the LDMs and activates it using a prompt containing the watermark. On the other hand, the FIXEDWM is considered more advanced and stealthy compared to the NAIVEWM, as it can only activate the watermark when using a prompt containing a trigger in a fixed position. We conducted a rigorous evaluation of both approaches, demonstrating their effectiveness in watermark injection and verification with minimal impact on the LDM's functionality.",
            "score": 0.4291389242703782,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "271244996",
            "title": "Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models",
            "text": "In recent years, large-scale text-to-image (T2I) diffusion models have exhibited remarkable capability in synthesizing photo-realistic images from text prompts [16, \u22c6 Equal contributions. \u2020 Corresponding author. 21,25,29]. The exceptional performance of T2I diffusion models is largely due to the vast amount of training data collected from the Internet, which enables the models to imitate a wide variety of concepts. Unfortunately, such powerful models can also be misused to generate copyright infringement and Not-Safe-For-Work (NSFW) image content when conditioned on inappropriate text prompts [11,31]. Especially the open-source release of the Stable Diffusion (SD) T2I model has made advanced image generation technology widely accessible. \n\nTo alleviate this safety concern, several recent research efforts have incorporated safety mechanisms into T2I diffusion models, e.g. filtering out inappropriate training data and retraining model [23], censoring model outputs with an NSFW safety checker [22], and applying classifier-free guidance to steer the generation away from inappropriate concepts [30]. However, these safety mechanisms either demand expensive computational resources and time [26] or can be easily circumvented by malicious users due to the public availability of code and model parameters in open-source scenario [24]. \n\nIn response to the drawbacks mentioned above, an alternative is to erase inappropriate concepts from the T2I diffusion model [6-8, 13, 14]. Specifically, given an inappropriate concept either described in a text prompt or present in visual content, the pre-trained T2I diffusion model's parameters are fine-tuned to unlearn that concept so that the associated image content cannot be generated. Compared with previous security mechanisms, concept erasure neither requires training the entire model from scratch nor can be easily circumvented with opensource code. Despite promising progress in concept erasure, there exist several issues. On the one hand, most erasure methods require a high number of iterations to fine-tune considerable amounts of parameters [6,8,13], which inevitably degrades the generation capability and consumes a lot of computing resources.",
            "score": 0.4290538857082432,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1106,
                    "end": 1110,
                    "matchedPaperCorpusId": "253420366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "We first study the effectiveness for mitigating both name-based infringement and the descriptionbased infringement.\n\nEffectiveness for Mitigating Name-based Infringement.We first evaluate the effectiveness for mitigating the name-based infringement.In detail, we use the prompt \"Generate an image of {Character Name}\" with different character names included in Table 1.For each character name, we ask the name-blocking LLM 50 times based on the paradigim demonstrated in Figure 10.The results show that our name blocking method achieves 100.0%recall for the input directly ask the model to generate the IP protected characters by specifying their names.\n\nEffectiveness for Mitigating Description-based Infringement.Regarding to the effectiveness for mitigating description-based infringement, we study two perspectives: the effectiveness for reducing the IP infringement rates (Section 3.3) and the influences on the language-image alignment in the generation.\n\nReduction on IP Infringement Rates.The metric IP Infringement Rates is introduced in Section 3.3.\n\nWe use three open-sourced models (i.e., Stable Diffusion v1-5 [16], Kandinsky-2-1 [33] and Stable Diffusion XL [3]).Five characters (Spider-Man, Iron Man, Incredible Hulk, Batman and Superman) are used here.The results can be found in Table 3.As can be observed, the IP infringement rates for our method is much lower than that of the undefended models.Therefore, our mitigation method is highly effective for reducing the IP infringement rates.\n\nInfluence on the CLIP Score [37].The CLIP Score is a measure used to evaluate the effectiveness Figure 12: Generated samples of the Stable Diffusion XL by using the standard generation process and our method.The used prompt here is \"Picture a superhero in a sleek, red and blue suit adorned with a web pattern, masked, swinging through a cityscape at night, agile and poised against a backdrop of skyscrapers, poised to battle crime with spider-like abilities, including web-slinging and wall-climbing.\" of language-image alignment in the visual generative models.This score assesses how well a model can align text descriptions with corresponding images, thus gauging the model's capability in understanding and correlating visual content with textual descriptions.",
            "score": 0.42804341849179145,
            "section_title": "Effectiveness",
            "char_start_offset": 21868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 117,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 653
                },
                {
                    "start": 655,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 960
                },
                {
                    "start": 962,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1059
                },
                {
                    "start": 1061,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1268
                },
                {
                    "start": 1268,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1506
                },
                {
                    "start": 1508,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 2072
                },
                {
                    "start": 2072,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 1123,
                    "end": 1127,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1172,
                    "end": 1175,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 1536,
                    "end": 1540,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "This paper presents work whose goal is to advance the field of Machine Learning by introducing a framework for quantifying originality in text-to-image generative diffusion models. The potential broader impact of this work includes the following: \n\nEthical Aspects Our research addresses the challenge of quantifying originality, which has significant implications for copyright laws and the protection of creative works. By providing a methodology to assess the originality of generated images, we aim to contribute to a fairer and more transparent use of generative models in creative industries. This could help mitigate legal disputes related to copyright infringement and ensure that the rights of original content creators are respected. \n\nFuture Societal Consequences The ability to quantify originality in generated images could enhance the deployment of generative models in various fields, including art, design, and entertainment, by fostering trust and accountability. It can also encourage the development of new creative tools that assist artists in generating unique content while respecting intellectual property rights. \n\nWhile the primary goal of this work is to advance the field of Machine Learning, we believe that our contributions to the understanding of originality and creativity in generative models will have a positive societal impact by promoting ethical use and fostering innovation. Each element in the image is defined by four features, and the entire set is the cross-product of all features. The shapes are independently and uniformly located across the image. The image's default textual description is in the format \"big red full circle and small empty blue square\"3 . \n\nUtilizing this framework, we generate datasets comprising 100K images each. In every dataset, 10% of the images are empty, while the rest contain a variable number of elements uniformly distributed within the range [1, n], where n can be any natural number. In the following sections, we choose either n = 4 or n = 6. Evaluation In order to facilitate automated and large-scale analysis of generations, which is essential for the purposes outlined in \u00a72, we fine-tuned a YOLOv8 model on the synthetic datasets. This approach effectively addresses common issues, such as overlapping or slightly deformed elements, providing a confidence measure for each detection. We set the confidence threshold at 0.9, aligning with the requisite quality of the generated elements.",
            "score": 0.4278441327482326,
            "section_title": "Impact Statement",
            "char_start_offset": 26532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2370
                },
                {
                    "start": 2371,
                    "end": 2473
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6435546875
        },
        {
            "corpus_id": "257427387",
            "title": "Detecting images generated by diffusers",
            "text": "In recent years, the field of artificial intelligence has witnessed a remarkable surge in the generation of synthetic images, driven by advancements in deep learning techniques. These synthetic images, often created through complex algorithms, closely mimic real photographs, blurring the lines between reality and artificiality. This proliferation of synthetic visuals presents a pressing challenge: how to accurately and reliably distinguish between genuine and generated images. This article, in particular, explores the task of detecting images generated by text-to-image diffusion models, highlighting the challenges and peculiarities of this field. To evaluate this, we consider images generated from captions in the MSCOCO and Wikimedia datasets using two state-of-the-art models: Stable Diffusion and GLIDE. Our experiments show that it is possible to detect the generated images using simple multi-layer perceptrons (MLPs), starting from features extracted by CLIP or RoBERTa, or using traditional convolutional neural networks (CNNs). These latter models achieve remarkable performances in particular when pretrained on large datasets. We also observe that models trained on images generated by Stable Diffusion can occasionally detect images generated by GLIDE, but only on the MSCOCO dataset. However, the reverse is not true. Lastly, we find that incorporating the associated textual information with the images in some cases can lead to a better generalization capability, especially if textual features are closely related to visual ones. We also discovered that the type of subject depicted in the image can significantly impact performance. This work provides insights into the feasibility of detecting generated images and has implications for security and privacy concerns in real-world applications. The code to reproduce our results is available at: https://github.com/davide-coccomini/Detecting-Images-Generated-by-Diffusers.",
            "score": 0.42770200975329603,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138671875
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Consequently, watermarking techniques are increasingly applied as a means of copyright protection during the fine-tuning phase. GenWatermark [84] is the first to propose a novel watermark system that is based on the joint learning of a watermark generator and a detector. In particular, it adopts a GAN-like structure, where a GAN generator  serves as the watermark generator and a detector  is trained to distinguish between clean and watermarked images. Wang et al. [138] aimed at a method that is independent of the choice of text-to-image diffusion models so that the perturbation can effectively protect the images from various models. In detail, they add specific stealthy transformations on the protected images as well as injecting a corresponding trigger into the caption of those images. Since they use the image warping function as the watermark generator, this method can work without a surrogate model and thus can work on different diffusion models. Cui et al. [25] considered a practical scenario where protectors can not control the fine-tuning process and emphasize that previous methods require many fine-tuning steps to learn the embedded watermarks. In order to make the watermark easily recognized by the model, they proposed FT-Shield which adds imperceptible perturbations that can be learned prior to the original image features, like styles and objects, by the text-to-image diffusion model. In the detection stage, a binary classifier is trained to distinguish the watermarked images and clean images. In particular, the perturbations minimize the loss of a diffusion model trained on these perturbed samples as shown in the training objective: \n\nwhere  1 denotes the parameters of the UNet [109] which is the denoise network within the text-to-image model structure, while  2 denotes parameters of the other parts;  and  denote the protected image and the corresponding caption, respectively. In other words, perturbation in Eq. ( 5) leads to a rapid decrease in the training loss and thus serves as a 'shortcut' feature that can be quickly learned and emphasized by the diffusion model.",
            "score": 0.42741938801234514,
            "section_title": "Watermarks.",
            "char_start_offset": 29435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 1717,
                    "end": 1722,
                    "matchedPaperCorpusId": "3719281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "257833729",
            "title": "Hierarchical Fine-Grained Image Forgery Detection and Localization",
            "text": "Chaotic and pervasive multimedia information sharing offers better means for spreading misinformation [1], and the forged image content could, in principle, sustain recent \"infodemics\" [3]. Firstly, CNN-synthesized images made extraordinary leaps culminating in recent synthesis methods-Dall\u2022E [55] or Google ImageN [60]-based on diffusion models (DDPM) [25], which even generates realistic videos from text [24,63]. Secondly, the availability of image editing toolkits produced a substantially low-cost access to image forgery or tampering (e.g., splicing and inpainting). In response to such an issue of image forgery, the computer vision community has made considerable ef- Each bubble represents one image forgery dataset. The y-axis denotes the average of forgery area. The bubble's area is proportional to the variance of the forgery area. \n\nforts, which however branch separately into two directions: detecting either CNN synthesis [65,68,78], or conventional image editing [18,27,45,67,73]. As a result, these methods may be ineffective when deploying to real-life scenarios, where forged images can possibly be generated from either CNN-sythensized or image-editing domains. \n\nTo push the frontier of image forensics [62], we study the image forgery detection and localization problem (IFDL)-Fig. 1a-regardless of the forgery method domains, i.e., CNN-synthesized or image editing. It is challenging to develop a unified algorithm for two domains, as images, generated by different forgery methods, differ largely from each other in terms of various forgery attributes. For example, a forgery attribute can indicate whether a forged image is fully synthesized or partially manipulated, or whether the forgery method used is the diffusion model generating im- ages from the Gaussian noise, or an image editing process that splices two images via Poisson editing [54]. Therefore, to model such complex forgery attributes, we first represent forgery attribute of each forged image with multiple labels at different levels.",
            "score": 0.42587051689180005,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 939,
                    "end": 943,
                    "matchedPaperCorpusId": "203737259"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "209444798"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "221447878"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "247762034"
                },
                {
                    "start": 994,
                    "end": 997,
                    "matchedPaperCorpusId": "195503148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00701141357421875
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "This paper addresses the contentious issue of copyright infringement in images generated by text-to-image models, sparking debates among AI developers, content creators, and legal entities. State-of-the-art models create high-quality content without crediting original creators, causing concern in the artistic community. To mitigate this, we propose the \\copyright Plug-in Authorization framework, introducing three operations: addition, extraction, and combination. Addition involves training a \\copyright plug-in for specific copyright, facilitating proper credit attribution. Extraction allows creators to reclaim copyright from infringing models, and combination enables users to merge different \\copyright plug-ins. These operations act as permits, incentivizing fair use and providing flexibility in authorization. We present innovative approaches,\"Reverse LoRA\"for extraction and\"EasyMerge\"for seamless combination. Experiments in artist-style replication and cartoon IP recreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable solution for human copyright protection in the age of generative AIs. The code is available at https://github.com/zc1023/-Plug-in-Authorization.git.",
            "score": 0.42549054866743197,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "Privacy and Copyright Infringements The intersection of privacy and copyright infringements in generative models has garnered significant attention. This approach assumes that to avoid copyright infringement, the output of a model shouldn't be too sensitive to any of its individual training samples. Bousquet et al. (Bousquet et al., 2020) suggest the use of differential privacy (Dwork et al., 2006) 2023), however, investigated the gap between privacy and copyright infringement from the perspective of the law, and showed that requiring such notions of stability may be too strong, and are not always aligned with the original intention of the law. Closer to our approach, (Scheffler et al., 2022) suggests a framework to quantify originality by measuring the description length of a content with and without access to the allegedly copyrighted material. Our approach of textual inversion also looks for a succinct description of the content but, dis-tinctively, our definition depends on the distribution of the data, and measures originality with respect to the whole data to be trained. This may lead to different outcomes, for example, when the allegedly copyrighted material contains a distinctive trait that is not necessarily original. \n\nAttribution in Generative Models Attribution in generative models is a crucial area of research, focusing on identifying the sources of data that contribute to the generation of specific outputs. Park et al. introduced the TRAK method to address data attribution in large-scale models (Park, 2022), and recently, Wang et al. (Wang et al., 2023b) proposed a method for evaluating the attribution in Stable Diffusion models of data points in the generation process, which is closely related to assessing the originality of generated images. However, such a method requires full access and knowledge of the training set on which the model was trained.",
            "score": 0.42379959790694177,
            "section_title": "Related Works",
            "char_start_offset": 20867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 340,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 381,
                    "end": 401,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 677,
                    "end": 701,
                    "matchedPaperCorpusId": "249375708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "paperId": "82c174ba4239b7539e4f4a87c1f89cb3e17aa6fa",
            "corpusId": 268512681,
            "title": "Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention",
            "venue": "European Conference on Computer Vision",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 27,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "1527096073",
                    "name": "Yaxin Li"
                },
                {
                    "authorId": "2253682835",
                    "name": "Shenglai Zeng"
                },
                {
                    "authorId": "2253881697",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2253469617",
                    "name": "Yue Xing"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Recent advancements in text-to-image diffusion models have demonstrated their remarkable capability to generate high-quality images from textual prompts. However, increasing research indicates that these models memorize and replicate images from their training data, raising tremendous concerns about potential copyright infringement and privacy risks. In our study, we provide a novel perspective to understand this memorization phenomenon by examining its relationship with cross-attention mechanisms. We reveal that during memorization, the cross-attention tends to focus disproportionately on the embeddings of specific tokens. The diffusion model is overfitted to these token embeddings, memorizing corresponding training images. To elucidate this phenomenon, we further identify and discuss various intrinsic findings of cross-attention that contribute to memorization. Building on these insights, we introduce an innovative approach to detect and mitigate memorization in diffusion models. The advantage of our proposed method is that it will not compromise the speed of either the training or the inference processes in these models while preserving the quality of generated images. Our code is available at https://github.com/renjie3/MemAttn .",
            "corpus_id": "268512681",
            "text": "Recent advancements in text-to-image diffusion models have demonstrated their remarkable capability to generate high-quality images from textual prompts. However, increasing research indicates that these models memorize and replicate images from their training data, raising tremendous concerns about potential copyright infringement and privacy risks. In our study, we provide a novel perspective to understand this memorization phenomenon by examining its relationship with cross-attention mechanisms. We reveal that during memorization, the cross-attention tends to focus disproportionately on the embeddings of specific tokens. The diffusion model is overfitted to these token embeddings, memorizing corresponding training images. To elucidate this phenomenon, we further identify and discuss various intrinsic findings of cross-attention that contribute to memorization. Building on these insights, we introduce an innovative approach to detect and mitigate memorization in diffusion models. The advantage of our proposed method is that it will not compromise the speed of either the training or the inference processes in these models while preserving the quality of generated images. Our code is available at https://github.com/renjie3/MemAttn .",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8984375
        },
        {
            "paperId": "c880be7d561b750260d01df2abdc79482a7b1af9",
            "corpusId": 276575481,
            "title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 44,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2230013847",
                    "name": "Xinwei Liu"
                },
                {
                    "authorId": "144890263",
                    "name": "Xiaojun Jia"
                },
                {
                    "authorId": "2230070994",
                    "name": "Yuan Xun"
                },
                {
                    "authorId": "2108906565",
                    "name": "Hua Zhang"
                },
                {
                    "authorId": "2262088189",
                    "name": "Xiaochun Cao"
                }
            ],
            "abstract": "Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.",
            "corpus_id": "276575481",
            "text": "Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.908203125
        },
        {
            "paperId": "2e28407a767f388713fbd372248989c2e28473a7",
            "corpusId": 268048573,
            "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models",
            "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14722/aiscc.2024.23009?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14722/aiscc.2024.23009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2138800088",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "corpus_id": "268048573",
            "text": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9697265625
        },
        {
            "paperId": "bfb6da971196335ce899e272fa77b1d325c09155",
            "corpusId": 269635525,
            "title": "An Inversion-based Measure of Memorization for Diffusion Models",
            "venue": "",
            "year": 2024,
            "referenceCount": 59,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.05846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239308702",
                    "name": "Zhe Ma"
                },
                {
                    "authorId": "2282496681",
                    "name": "Xuhong Zhang"
                },
                {
                    "authorId": "2300423996",
                    "name": "Qingming Li"
                },
                {
                    "authorId": "2056897605",
                    "name": "Tianyu Du"
                },
                {
                    "authorId": "2266428031",
                    "name": "Wenzhi Chen"
                },
                {
                    "authorId": "2274942629",
                    "name": "Zonghui Wang"
                },
                {
                    "authorId": "2237797356",
                    "name": "Shouling Ji"
                }
            ],
            "abstract": "The past few years have witnessed substantial advances in image generation powered by diffusion models. However, it was shown that diffusion models are vulnerable to training data memorization, raising concerns regarding copyright infringement and privacy invasion. This study delves into a rigorous analysis of memorization in diffusion models. We introduce an inversion-based measure of memorization, InvMM, which searches for a sensitive latent noise distribution accounting for the replication of an image. For accurate estimation of the memorization score, we propose an adaptive algorithm that balances the normality and sensitivity of the inverted distribution. Comprehensive experiments, conducted on both unconditional and text-guided diffusion models, demonstrate that InvMM is capable of detecting heavily memorized images and elucidating the effect of various factors on memorization. Additionally, we discuss how memorization differs from membership. In practice, InvMM serves as a useful tool for model developers to reliably assess the risk of memorization, thereby contributing to the enhancement of trustworthiness and privacy-preserving capabilities of diffusion models.",
            "corpus_id": "269635525",
            "text": "The past few years have witnessed substantial advances in image generation powered by diffusion models. However, it was shown that diffusion models are vulnerable to training data memorization, raising concerns regarding copyright infringement and privacy invasion. This study delves into a rigorous analysis of memorization in diffusion models. We introduce an inversion-based measure of memorization, InvMM, which searches for a sensitive latent noise distribution accounting for the replication of an image. For accurate estimation of the memorization score, we propose an adaptive algorithm that balances the normality and sensitivity of the inverted distribution. Comprehensive experiments, conducted on both unconditional and text-guided diffusion models, demonstrate that InvMM is capable of detecting heavily memorized images and elucidating the effect of various factors on memorization. Additionally, we discuss how memorization differs from membership. In practice, InvMM serves as a useful tool for model developers to reliably assess the risk of memorization, thereby contributing to the enhancement of trustworthiness and privacy-preserving capabilities of diffusion models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.494140625
        },
        {
            "paperId": "eca4389780eec5257ed40df1011b2ff7d40e593a",
            "corpusId": 273655150,
            "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 76,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328024472",
                    "name": "Wenda Li"
                },
                {
                    "authorId": "2257089655",
                    "name": "Huijie Zhang"
                },
                {
                    "authorId": "2256993150",
                    "name": "Qing Qu"
                }
            ],
            "abstract": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at https://github.com/liwd190019/Shallow-Diffuse.",
            "corpus_id": "273655150",
            "text": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at https://github.com/liwd190019/Shallow-Diffuse.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.68408203125
        },
        {
            "paperId": "e842a53244c4f2c3e8de5bfffd6b57c70cffbbac",
            "corpusId": 279306152,
            "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing",
            "venue": "",
            "year": 2025,
            "referenceCount": 55,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149670985",
                    "name": "Hongguang Zhu"
                },
                {
                    "authorId": "2238213573",
                    "name": "Yunchao Wei"
                },
                {
                    "authorId": "2328070747",
                    "name": "Mengyu Wang"
                },
                {
                    "authorId": "2199182202",
                    "name": "Siyu Jiao"
                },
                {
                    "authorId": "2279757182",
                    "name": "Yan Fang"
                },
                {
                    "authorId": "2304143299",
                    "name": "Jiannan Huang"
                },
                {
                    "authorId": "2366439139",
                    "name": "Yao Zhao"
                }
            ],
            "abstract": "Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at https://github.com/KevinLight831/SAGE.",
            "corpus_id": "279306152",
            "text": "Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at https://github.com/KevinLight831/SAGE.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.345703125
        },
        {
            "paperId": "aeab5f3fa0a5ccc7aa85d343fe3fa1e224e298d5",
            "corpusId": 26309385,
            "title": "Security, Trust and Risk in Multimedia Social Networks",
            "venue": "Computer/law journal",
            "year": 2015,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1093/COMJNL/BXU151?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/COMJNL/BXU151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48806299",
                    "name": "Zhiyong Zhang"
                }
            ],
            "abstract": "With the rapid development of network socialization, multimedia social networks (MSNs) have increasingly emerged. These MSNs offer network tools, services and applications for multimedia content (e.g. electronic book, digital image, audio and video) that can be shared among users in the same group or between different groups within social networks. Many MSNs, such as video Youtube and audio SongTaste sharing networks, are increasingly widely used. However, easy digital content reproduction, convenient distribution and sharing of such content facilitated by open MSNs environments have enabled people to share and distribute valuable copyrighted digital content within social networks. Copyright infringement behaviour, such as illicit copying, malicious distribution, unauthorized usage and free sharing of copyright-protected digital content, will also become a much more common phenomenon. Some research frontiers on media content security in social networks applications have been in progress, including enhanced security mechanisms, methods and algorithms, trust assessment and risk management in social network applications, as well as social factors and soft computing in social media distributions. The special issue attempts to bring together researchers, content industry engineers and administrators resorting to the state-of-the-art technologies and ideas to protect valuable media content and services against security attacks and piracy exposure in the emerging social networks. In the first section of the special issue, there are five original articles in the field of MSNs security, privacy and forensics. First, Patsakis et al. made a survey on the most significant security and privacy issues related to the exposure of multimedia content in online social network (OSN), and then discussed possible countermeasures and methods. In the second paper, Hui Zhu et al. attempted to explore privacy setting policies in OSN, and proposed a general stochastic model called diffusion model based on privacy setting with multiple diffusion mechanisms for OSN services. By a series of experimental simulations and analysis, the paper shows that the novel model can precisely describe the diffusion process. Next, Hong Zhu et al. highlighted an independent -Diversity principle, based on which a privacypreserving data publication model is presented to prevent individual sensitive information disclosure in the corruption attack. The proposed model could prevent attacks from attackers who have known data publishing algorithms and have the corruption abilities. Followed by media content security and forensics in the section, Weitao Song et al. paid more attention to the type-flaw attacks detection and security protocol formalism. They introduced a multi-branch tag tree to establish a three-level model for detecting typeflaw attacks on security protocols. In the final fifth paper, considering an effective forensic analysis on digital images, the authors, Bin Yang et al., represented a novel shadow-based method, by which the fake shadow of the composites can be detected. The special issue\u2019s second section focuses on MSNs trust and reputation issues, and also includes the following five selected papers. Ayesha Kanwal et al. have performed indepth analyses of the existing trust models in the cloud environment, and presented panoramic taxonomies covering the state-of-the-art features. Furthermore, they have applied the proposed taxonomies as assessment criteria for the analysis of various trust models in the cloud domain. In addition, the emerging MSNs services and tools, in recent years, have generally facilitated convenient platforms for users to",
            "corpus_id": "26309385",
            "text": "With the rapid development of network socialization, multimedia social networks (MSNs) have increasingly emerged. These MSNs offer network tools, services and applications for multimedia content (e.g. electronic book, digital image, audio and video) that can be shared among users in the same group or between different groups within social networks. Many MSNs, such as video Youtube and audio SongTaste sharing networks, are increasingly widely used. However, easy digital content reproduction, convenient distribution and sharing of such content facilitated by open MSNs environments have enabled people to share and distribute valuable copyrighted digital content within social networks. Copyright infringement behaviour, such as illicit copying, malicious distribution, unauthorized usage and free sharing of copyright-protected digital content, will also become a much more common phenomenon. Some research frontiers on media content security in social networks applications have been in progress, including enhanced security mechanisms, methods and algorithms, trust assessment and risk management in social network applications, as well as social factors and soft computing in social media distributions. The special issue attempts to bring together researchers, content industry engineers and administrators resorting to the state-of-the-art technologies and ideas to protect valuable media content and services against security attacks and piracy exposure in the emerging social networks. In the first section of the special issue, there are five original articles in the field of MSNs security, privacy and forensics. First, Patsakis et al. made a survey on the most significant security and privacy issues related to the exposure of multimedia content in online social network (OSN), and then discussed possible countermeasures and methods. In the second paper, Hui Zhu et al. attempted to explore privacy setting policies in OSN, and proposed a general stochastic model called diffusion model based on privacy setting with multiple diffusion mechanisms for OSN services. By a series of experimental simulations and analysis, the paper shows that the novel model can precisely describe the diffusion process. Next, Hong Zhu et al. highlighted an independent -Diversity principle, based on which a privacypreserving data publication model is presented to prevent individual sensitive information disclosure in the corruption attack. The proposed model could prevent attacks from attackers who have known data publishing algorithms and have the corruption abilities. Followed by media content security and forensics in the section, Weitao Song et al. paid more attention to the type-flaw attacks detection and security protocol formalism. They introduced a multi-branch tag tree to establish a three-level model for detecting typeflaw attacks on security protocols. In the final fifth paper, considering an effective forensic analysis on digital images, the authors, Bin Yang et al., represented a novel shadow-based method, by which the fake shadow of the composites can be detected. The special issue\u2019s second section focuses on MSNs trust and reputation issues, and also includes the following five selected papers. Ayesha Kanwal et al. have performed indepth analyses of the existing trust models in the cloud environment, and presented panoramic taxonomies covering the state-of-the-art features. Furthermore, they have applied the proposed taxonomies as assessment criteria for the analysis of various trust models in the cloud domain. In addition, the emerging MSNs services and tools, in recent years, have generally facilitated convenient platforms for users to",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2451171875
        }
    ],
    "quotes": {
        "cost": 0.145737,
        "quotes": [
            {
                "idx": 0,
                "key": "[257687839 | Kumari et al. | 2023 | Citations: 201]",
                "snippets": "To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 491,
                        "end": 896,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[263622213 | Cui et al. | 2023 | Citations: 12]",
                "snippets": "FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection.\n\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement.\n\nGLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios.\n\nWang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection.\n\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement.\n\nGLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios.\n\nWang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[265352103 | Zhang et al. | 2023 | Citations: 10]",
                "snippets": "In addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Copyright Test for Substantial Similarities",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1432,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 103
                            },
                            {
                                "start": 104,
                                "end": 224
                            },
                            {
                                "start": 225,
                                "end": 312
                            },
                            {
                                "start": 313,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 626
                            },
                            {
                                "start": 627,
                                "end": 848
                            },
                            {
                                "start": 849,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1152
                            },
                            {
                                "start": 1155,
                                "end": 1291
                            },
                            {
                                "start": 1292,
                                "end": 1432
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[265445146 | Luo et al. | 2023 | Citations: 9]",
                "snippets": "In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 386,
                        "end": 942,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267412857 | Ren et al. | 2024 | Citations: 42]",
                "snippets": "The \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright...Focusing on DDPM (Nichol et al., 2021), Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks (Navas et al., 2008)(Yu et al., 2020)(Zhu et al., 2018) can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231979499 | Nichol et al. | 2021 | Citations: 3724]": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion",
                    "[238419552 | Yu et al. | 2020 | Citations: 219]": "Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race.",
                    "[50784854 | Zhu et al. | 2018 | Citations: 755]": "Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images."
                },
                "metadata": [
                    {
                        "section_title": "Watermarks.",
                        "pdf_hash": "",
                        "start": 451,
                        "end": 875,
                        "sentence_offsets": [
                            {
                                "start": 440,
                                "end": 504
                            },
                            {
                                "start": 505,
                                "end": 721
                            },
                            {
                                "start": 722,
                                "end": 876
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright"
                    },
                    {
                        "section_title": "Watermarks.",
                        "pdf_hash": "",
                        "start": 1285,
                        "end": 2105,
                        "sentence_offsets": [
                            {
                                "start": 1285,
                                "end": 1481
                            },
                            {
                                "start": 1482,
                                "end": 1618
                            },
                            {
                                "start": 1619,
                                "end": 1718
                            },
                            {
                                "start": 1719,
                                "end": 1880
                            },
                            {
                                "start": 1881,
                                "end": 2104
                            }
                        ],
                        "ref_mentions": [
                            "231979499",
                            "9280993",
                            "238419552",
                            "50784854"
                        ],
                        "quote": "Focusing on DDPM (Nichol et al., 2021), Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks (Navas et al., 2008)(Yu et al., 2020)(Zhu et al., 2018) can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268048573 | Du et al. | 2024 | Citations: 3]",
                "snippets": "To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[268532352 | Ma et al. | 2024 | Citations: 2]",
                "snippets": "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods. Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity. This indicates the extent to which the prompt that generates potential infringement is nullified...In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Effectiveness of Unlearning.",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 353,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 161
                            },
                            {
                                "start": 161,
                                "end": 255
                            },
                            {
                                "start": 255,
                                "end": 352
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods. Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity. This indicates the extent to which the prompt that generates potential infringement is nullified"
                    },
                    {
                        "section_title": "Effectiveness of Unlearning.",
                        "pdf_hash": "",
                        "start": 983,
                        "end": 1196,
                        "sentence_offsets": [
                            {
                                "start": 983,
                                "end": 1195
                            },
                            {
                                "start": 1195,
                                "end": 1412
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270067889 | Huang et al. | 2024 | Citations: 2]",
                "snippets": "Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 283,
                        "end": 587,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270620522 | Ren et al. | 2024 | Citations: 1]",
                "snippets": "Observing the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality [13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16]17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Observing the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality [13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16]17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[271600759 | Wang et al. | 2024 | Citations: 9]",
                "snippets": "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks...Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Watermarking",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 322,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 208
                            },
                            {
                                "start": 209,
                                "end": 347
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks"
                    },
                    {
                        "section_title": "Watermarking",
                        "pdf_hash": "",
                        "start": 348,
                        "end": 1189,
                        "sentence_offsets": [
                            {
                                "start": 348,
                                "end": 585
                            },
                            {
                                "start": 586,
                                "end": 658
                            },
                            {
                                "start": 659,
                                "end": 1077
                            },
                            {
                                "start": 1078,
                                "end": 1188
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272146279 | Shi et al. | 2024 | Citations: 1]",
                "snippets": "To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 401,
                        "end": 1154,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[274235104 | Datta et al. | 2024 | Citations: 0]",
                "snippets": "A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 305,
                        "end": 466,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[274436785 | Guo et al. | 2024 | Citations: 3]",
                "snippets": "Current solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,(Vyas et al., 2023)[76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed....we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) (Rezatofighi et al., 2019) and Self-Supervised Copy Detection (SSCD) (Pizzi et al., 2022) similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247011159 | Pizzi et al. | 2022 | Citations: 126]": "Image copy detection is an important task for content moderation. We introduce SSCD, a model that builds on a recent self-supervised contrastive training objective. We adapt this method to the copy detection task by changing the architecture and training objective, including a pooling operator from the instance matching literature, and adapting contrastive learning to augmentations that combine images. Our approach relies on an entropy regularization term, promoting consistent separation between descriptor vectors, and we demonstrate that this significantly improves copy detection accuracy. Our method produces a compact descriptor vector, suitable for real-world web scale applications. Statistical information from a background image distribution can be incorporated into the descriptor. On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline copy detection models and self-supervised architectures designed for image classification by huge margins, in all settings. For example, SSCD out-performs SimCLR descriptors by 48% absolute. Code is available at https://github.com/facebookresearch/sscd-copy-detection.",
                    "[257050406 | Vyas et al. | 2023 | Citations: 94]": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
                    "[67855581 | Rezatofighi et al. | 2019 | Citations: 4181]": "Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1616,
                        "end": 2026,
                        "sentence_offsets": [
                            {
                                "start": 1616,
                                "end": 1856
                            },
                            {
                                "start": 1857,
                                "end": 2024
                            }
                        ],
                        "ref_mentions": [
                            "257050406"
                        ],
                        "quote": "Current solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,(Vyas et al., 2023)[76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 494,
                        "end": 1656,
                        "sentence_offsets": [
                            {
                                "start": 465,
                                "end": 638
                            },
                            {
                                "start": 639,
                                "end": 729
                            },
                            {
                                "start": 730,
                                "end": 831
                            },
                            {
                                "start": 832,
                                "end": 1029
                            },
                            {
                                "start": 1030,
                                "end": 1103
                            },
                            {
                                "start": 1104,
                                "end": 1240
                            },
                            {
                                "start": 1241,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1656
                            }
                        ],
                        "ref_mentions": [
                            "67855581",
                            "247011159"
                        ],
                        "quote": ".we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) (Rezatofighi et al., 2019) and Self-Supervised Copy Detection (SSCD) (Pizzi et al., 2022) similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276558342 | Liu et al. | 2025 | Citations: 2]",
                "snippets": "In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 136,
                        "end": 1026,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[277043466 | Liu et al. | 2025 | Citations: 0]",
                "snippets": "Zhao et al. [48] used the pretrained watermark encoder (Yu et al., 2020) to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. (Wu et al., 2024) leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268513090 | Wu et al. | 2024 | Citations: 6]": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
                    "[238419552 | Yu et al. | 2020 | Citations: 219]": "Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race."
                },
                "metadata": [
                    {
                        "section_title": "Passive Detection for Image Infringement",
                        "pdf_hash": "",
                        "start": 584,
                        "end": 1660,
                        "sentence_offsets": [
                            {
                                "start": 584,
                                "end": 749
                            },
                            {
                                "start": 750,
                                "end": 875
                            },
                            {
                                "start": 876,
                                "end": 1144
                            },
                            {
                                "start": 1145,
                                "end": 1281
                            },
                            {
                                "start": 1282,
                                "end": 1407
                            },
                            {
                                "start": 1408,
                                "end": 1660
                            }
                        ],
                        "ref_mentions": [
                            "238419552",
                            "268513090"
                        ],
                        "quote": "Zhao et al. [48] used the pretrained watermark encoder (Yu et al., 2020) to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. (Wu et al., 2024) leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[277151077 | Roy et al. | 2025 | Citations: 0]",
                "snippets": "We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 186,
                        "end": 1198,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[277856857 | Du et al. | 2025 | Citations: 0]",
                "snippets": "To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[278129333 | Chen et al. | 2025 | Citations: 1]",
                "snippets": "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 357,
                        "end": 918,
                        "sentence_offsets": [
                            {
                                "start": 348,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 918
                            }
                        ],
                        "ref_mentions": [
                            "256389993",
                            "254366634"
                        ],
                        "quote": "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.027696,
        "cot": "To answer the question about specific technical methods for detecting and preventing copyright infringement in text-to-image diffusion models, I'll create dimensions that logically organize the information from the quotes.\n\n1. \"Introduction to Copyright Issues in Text-to-Image Models\" - Format: Synthesis\nThis section will provide background on the copyright infringement challenges in text-to-image diffusion models. It's important to start with this context before diving into specific methods. A synthesis format will help create a coherent introduction to the problem space.\n\n2. \"Watermarking-Based Protection Methods\" - Format: Synthesis\nMany quotes mention watermarking as a major approach to copyright protection. This dimension will cover the various watermarking techniques mentioned across multiple quotes. A synthesis format is appropriate because these approaches are related and can be explained in a flowing narrative that compares and contrasts them.\n\n3. \"Adversarial Perturbation-Based Methods\" - Format: Synthesis\nSeveral quotes discuss adversarial techniques that involve adding perturbations to images to prevent models from generating or learning specific content. A synthesis format will allow for explanation of how these perturbations work and their various implementations.\n\n4. \"Detection Mechanisms for Copyright Infringement\" - Format: Synthesis\nThis dimension focuses on methods to detect when copyright infringement has occurred, including similarity checks and various detection algorithms. A synthesis format will help connect the various detection approaches in a coherent explanation.\n\n5. \"Unlearning and Model Adaptation Approaches\" - Format: Synthesis\nSome quotes mention techniques for unlearning or modifying models to prevent them from generating copyrighted content. A synthesis format is appropriate to explain these model-centric approaches and how they differ from data-centric approaches.\n\n6. \"Automated Copyright Judgment Systems\" - Format: Synthesis\nA few quotes discuss frameworks for automatically judging whether copyright infringement has occurred. This dimension deserves its own section since it represents a different approach that simulates legal processes. A synthesis format will allow for proper explanation of these complex systems.\n\n7. \"Limitations and Challenges of Current Methods\" - Format: Synthesis\nSeveral quotes point to limitations in existing approaches. A synthesis format will help summarize these challenges and provide a complete picture of the current state of the field.\n\nI've assigned the quotes to the most relevant dimensions, though some quotes contain information that spans multiple dimensions.",
        "plan": {
            "Introduction to Copyright Issues in Text-to-Image Models (synthesis)": [],
            "Watermarking-Based Protection Methods (synthesis)": [
                1,
                3,
                4,
                9,
                11,
                14,
                16
            ],
            "Adversarial Perturbation-Based Methods (synthesis)": [
                0,
                1,
                5,
                16
            ],
            "Detection Mechanisms for Copyright Infringement (synthesis)": [
                2,
                8,
                9,
                12,
                14
            ],
            "Unlearning and Model Adaptation Approaches (synthesis)": [
                6,
                7,
                10,
                15,
                17
            ],
            "Automated Copyright Judgment Systems (synthesis)": [
                13
            ],
            "Limitations and Challenges of Current Methods (synthesis)": [
                5,
                7,
                8,
                16
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Copyright Issues in Text-to-Image Models",
                "tldr": "Text-to-image diffusion models can generate images that closely resemble copyrighted works when prompted with specific artist names or style descriptions. This capability has raised significant legal and ethical concerns regarding copyright infringement, attribution, and fair compensation for original creators. (LLM Memory)",
                "text": "\nText-to-image diffusion models like DALL-E, Midjourney, and Stable Diffusion have revolutionized image generation capabilities, allowing users to create high-quality images from textual descriptions. These models are trained on vast datasets of images scraped from the internet, which inevitably include copyrighted works from artists, photographers, and other visual creators. The fundamental copyright issue arises because these models can generate derivative works that mimic specific artists' styles when prompted with names like \"in the style of Greg Rutkowski\" or \"similar to Picasso,\" often without permission or compensation to the original creators. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe legal landscape surrounding these models remains unclear, as traditional copyright frameworks were not designed for AI-generated content. Courts and legal scholars are grappling with questions about whether training on copyrighted data constitutes fair use, who owns the rights to AI-generated images, and whether style imitation constitutes infringement. Several lawsuits have been filed against companies developing these models, with artists claiming their work was used without consent and that the models can produce near-replicas of their distinctive styles, potentially devaluing their original work and causing market harm. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe technical dimension of this issue is equally complex, as these models don't simply store and reproduce images but learn underlying patterns and features. This creates a challenging situation where the boundary between inspiration, transformation, and infringement becomes technically difficult to define and detect. Model developers must now consider implementing technical safeguards to prevent copyright infringement while maintaining model utility, spurring research into various detection and prevention methods. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Watermarking-Based Protection Methods",
                "tldr": "Watermarking techniques embed invisible signals into original artwork that can be detected in images generated by models trained on those works, providing concrete evidence of copyright infringement. These methods include specialized watermark generation algorithms that transfer from training data to outputs, with various implementations like DiffusionShield, FT-Shield, and DIAGNOSIS showing promising results for protecting visual artists' intellectual property. (10 sources)",
                "text": "\nWatermarking has emerged as a promising approach for protecting copyright in text-to-image diffusion models by embedding imperceptible signals in original images that persist through the model training process and appear in generated outputs. Unlike other protection methods, watermarking provides concrete evidence of copyright infringement that can potentially serve as legal proof when unauthorized use occurs <Paper corpusId=\"271600759\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. The fundamental principle involves embedding distinctive patterns that are invisible to humans but detectable through specialized algorithms after generation.\n\nSeveral watermarking frameworks have been developed specifically for diffusion models. FT-Shield introduces innovative watermark generation and detection strategies that ensure watermarks transfer seamlessly from training images to generated outputs. This system integrates a Mixture of Experts (MoE) approach for watermark detection to handle variability in fine-tuning methods <Paper corpusId=\"263622213\" paperTitle=\"(Cui et al., 2023)\" isShortName></Paper>. Similarly, DiffusionShield employs blockwise watermarks designed to convey greater amounts of information, allowing distinct copyright information to be more readily decoded through a joint optimization strategy that enhances both the pixel values of watermark patches and the decoding model <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper> <Paper corpusId=\"231979499\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>.\n\nAnother approach, DIAGNOSIS, protects images by applying an image warping function that creates unique features detectable in generated outputs <Paper corpusId=\"277043466\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Luo et al. proposed a framework that embeds subtle watermarks into digital artworks to protect their copyrights while preserving the artist's visual expression, with the capability to detect mimicry through fine-tuning by analyzing the distribution of watermarks in generated images <Paper corpusId=\"265445146\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>.\n\nSome watermarking techniques build upon earlier work in adversarial examples and deep neural networks. For instance, approaches like those developed by Yu et al. demonstrate that neural networks can learn to encode rich information through invisible perturbations, which provides a foundation for data hiding techniques applicable to copyright protection <Paper corpusId=\"238419552\" paperTitle=\"(Yu et al., 2020)\" isShortName></Paper>. These methods can be made robust against various transformations including blurring, cropping, and even JPEG compression <Paper corpusId=\"50784854\" paperTitle=\"(Zhu et al., 2018)\" isShortName></Paper>.\n\nWatermarking approaches offer advantages over perturbation-based methods (which prevent learning entirely) by allowing legitimate use while providing a means to detect infringement. However, they typically require preemptive action before images are published online, as they must be applied to original works before model training occurs <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>. When implemented effectively, these watermarking techniques can trace unauthorized data usage in both training and fine-tuning processes of text-to-image diffusion models, providing a promising technical solution to the challenge of copyright protection <Paper corpusId=\"274235104\" paperTitle=\"(Datta et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks",
                            "Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft."
                        ],
                        "paper": {
                            "corpus_id": 271600759,
                            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
                            "authors": [
                                {
                                    "authorId": "2108977186",
                                    "name": "Wenhao Wang"
                                },
                                {
                                    "authorId": "2267731577",
                                    "name": "Yifan Sun"
                                },
                                {
                                    "authorId": "15556978",
                                    "name": "Zongxin Yang"
                                },
                                {
                                    "authorId": "2125634511",
                                    "name": "Zhengdong Hu"
                                },
                                {
                                    "authorId": "2296990692",
                                    "name": "Zhentao Tan"
                                },
                                {
                                    "authorId": "2297815073",
                                    "name": "Yi Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Cui et al., 2023)",
                        "snippets": [
                            "FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection.\n\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement.\n\nGLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios.\n\nWang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model."
                        ],
                        "paper": {
                            "corpus_id": 263622213,
                            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2254140893",
                                    "name": "Yuping Lin"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2253469617",
                                    "name": "Yue Xing"
                                },
                                {
                                    "authorId": "2255025428",
                                    "name": "Wenqi Fan"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "SIGKDD Explorations",
                            "n_citations": 12
                        },
                        "score": 0.99365234375
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "The \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright",
                            "Focusing on DDPM (Nichol et al., 2021), Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks (Navas et al., 2008)(Yu et al., 2020)(Zhu et al., 2018) can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images."
                        ],
                        "paper": {
                            "corpus_id": 267412857,
                            "title": "Copyright Protection in Generative AI: A Technical Perspective",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2253682835",
                                    "name": "Shenglai Zeng"
                                },
                                {
                                    "authorId": "2282560420",
                                    "name": "Jiankun Zhang"
                                },
                                {
                                    "authorId": "2256788829",
                                    "name": "Hongzhi Wen"
                                },
                                {
                                    "authorId": "46496977",
                                    "name": "Jiayuan Ding"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2267019992",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 42
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Nichol et al., 2021)",
                        "snippets": [
                            "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 231979499,
                            "title": "Improved Denoising Diffusion Probabilistic Models",
                            "authors": [
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 3724
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Zhao et al. [48] used the pretrained watermark encoder (Yu et al., 2020) to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. (Wu et al., 2024) leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one."
                        ],
                        "paper": {
                            "corpus_id": 277043466,
                            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2243376899",
                                    "name": "Zhenguang Liu"
                                },
                                {
                                    "authorId": "2243277462",
                                    "name": "Chao Shuai"
                                },
                                {
                                    "authorId": "2342623076",
                                    "name": "Shaojing Fan"
                                },
                                {
                                    "authorId": "2350529533",
                                    "name": "Ziping Dong"
                                },
                                {
                                    "authorId": "2350852841",
                                    "name": "Jinwu Hu"
                                },
                                {
                                    "authorId": "36890675",
                                    "name": "Zhongjie Ba"
                                },
                                {
                                    "authorId": "2286244627",
                                    "name": "Kui Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed."
                        ],
                        "paper": {
                            "corpus_id": 265445146,
                            "title": "Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models",
                            "authors": [
                                {
                                    "authorId": "2056101029",
                                    "name": "Ge Luo"
                                },
                                {
                                    "authorId": "2268330053",
                                    "name": "Junqiang Huang"
                                },
                                {
                                    "authorId": "2260829743",
                                    "name": "Manman Zhang"
                                },
                                {
                                    "authorId": "3243117",
                                    "name": "Zhenxing Qian"
                                },
                                {
                                    "authorId": "2153698673",
                                    "name": "Sheng Li"
                                },
                                {
                                    "authorId": "2238534596",
                                    "name": "Xinpeng Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.96728515625
                    },
                    {
                        "id": "(Yu et al., 2020)",
                        "snippets": [
                            "Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race."
                        ],
                        "paper": {
                            "corpus_id": 238419552,
                            "title": "Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data",
                            "authors": [
                                {
                                    "authorId": "2052212417",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "1818812352",
                                    "name": "Vladislav Skripniuk"
                                },
                                {
                                    "authorId": "1383113350",
                                    "name": "Sahar Abdelnabi"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 219
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2018)",
                        "snippets": [
                            "Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images."
                        ],
                        "paper": {
                            "corpus_id": 50784854,
                            "title": "HiDDeN: Hiding Data With Deep Networks",
                            "authors": [
                                {
                                    "authorId": "2108812092",
                                    "name": "Jiren Zhu"
                                },
                                {
                                    "authorId": "31121905",
                                    "name": "Russell Kaplan"
                                },
                                {
                                    "authorId": "2115231104",
                                    "name": "Justin Johnson"
                                },
                                {
                                    "authorId": "48004138",
                                    "name": "Li Fei-Fei"
                                }
                            ],
                            "year": 2018,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 755
                        },
                        "score": 0
                    },
                    {
                        "id": "(Du et al., 2025)",
                        "snippets": [
                            "To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector."
                        ],
                        "paper": {
                            "corpus_id": 277856857,
                            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
                            "authors": [
                                {
                                    "authorId": "151483943",
                                    "name": "L. Du"
                                },
                                {
                                    "authorId": "2288042024",
                                    "name": "Zheng Zhu"
                                },
                                {
                                    "authorId": "2238153157",
                                    "name": "Min Chen"
                                },
                                {
                                    "authorId": "2328027909",
                                    "name": "Zhou Su"
                                },
                                {
                                    "authorId": "2237990407",
                                    "name": "Shouling Ji"
                                },
                                {
                                    "authorId": "2147335888",
                                    "name": "Peng Cheng"
                                },
                                {
                                    "authorId": "2238129188",
                                    "name": "Jiming Chen"
                                },
                                {
                                    "authorId": "2238124154",
                                    "name": "Zhikun Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.98486328125
                    },
                    {
                        "id": "(Datta et al., 2024)",
                        "snippets": [
                            "A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features."
                        ],
                        "paper": {
                            "corpus_id": 274235104,
                            "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
                            "authors": [
                                {
                                    "authorId": "2277608846",
                                    "name": "Soumil Datta"
                                },
                                {
                                    "authorId": "2332319316",
                                    "name": "Shih-Chieh Dai"
                                },
                                {
                                    "authorId": "2332299474",
                                    "name": "Leo Yu"
                                },
                                {
                                    "authorId": "2332096509",
                                    "name": "Guanhong Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.96728515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Adversarial Perturbation-Based Methods",
                "tldr": "Adversarial perturbation techniques protect artistic works by introducing subtle, imperceptible changes to images that disrupt how diffusion models learn or generate content. These methods actively prevent unauthorized style mimicry by modifying the latent representation during the diffusion process, causing models to generate unrealistic or distorted images when attempting to replicate protected content. (4 sources)",
                "text": "\nAdversarial perturbation-based methods represent a proactive approach to copyright protection that fundamentally differs from watermarking techniques. Rather than detecting infringement after it occurs, these methods aim to prevent unauthorized learning and generation entirely by introducing carefully designed perturbations to the original artwork. These perturbations are imperceptible to humans but significantly disrupt the learning process of text-to-image diffusion models. <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper>\n\nThe core principle behind these methods is to alter the latent representation of images in the diffusion process, causing models to be unable to generate images as expected when trained on protected content. One notable implementation is Glaze, which allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>\n\nAnother approach, proposed by Kumari et al., focuses on \"ablating concepts\" within pretrained models to prevent the generation of target concepts. Their algorithm works by matching the image distribution for a target style, instance, or text prompt to the distribution of an anchor concept, effectively making the model incapable of generating the protected content even when explicitly prompted. <Paper corpusId=\"257687839\" paperTitle=\"(Kumari et al., 2023)\" isShortName></Paper>\n\nSpecialized systems like Anti-DreamBooth provide defense against the misuse of fine-tuning techniques by adding noise perturbations to images before publication. This degrades the quality of images generated by models trained on these perturbed datasets, preventing successful style imitation. Similarly, EditShield introduces imperceptible perturbations that shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects when attempting to use protected content. <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>\n\nUnlike watermarking techniques which allow for detection of infringement after it occurs, adversarial perturbation methods actively prevent unauthorized use by making the protected content unusable for model training or generation. This represents a more aggressive approach to copyright protection that prioritizes prevention over detection. <Paper corpusId=\"263622213\" paperTitle=\"(Cui et al., 2023)\" isShortName></Paper>\n\nHowever, these methods face several limitations. They typically require modification of the original artwork before publication, making them less applicable to content that has already been published online. Additionally, they may be vulnerable to various image pre-processing techniques that could potentially neutralize the perturbations, and they generally cannot be applied retroactively to artwork that has already been shared publicly. <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Du et al., 2024)",
                        "snippets": [
                            "To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork."
                        ],
                        "paper": {
                            "corpus_id": 268048573,
                            "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models",
                            "authors": [
                                {
                                    "authorId": "151483943",
                                    "name": "L. Du"
                                },
                                {
                                    "authorId": "2288042024",
                                    "name": "Zheng Zhu"
                                },
                                {
                                    "authorId": "2238153157",
                                    "name": "Min Chen"
                                },
                                {
                                    "authorId": "2237990407",
                                    "name": "Shouling Ji"
                                },
                                {
                                    "authorId": "2147335888",
                                    "name": "Peng Cheng"
                                },
                                {
                                    "authorId": "2138800088",
                                    "name": "Jiming Chen"
                                },
                                {
                                    "authorId": "2238124154",
                                    "name": "Zhikun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
                            "n_citations": 3
                        },
                        "score": 0.9697265625
                    },
                    {
                        "id": "(Du et al., 2025)",
                        "snippets": [
                            "To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector."
                        ],
                        "paper": {
                            "corpus_id": 277856857,
                            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
                            "authors": [
                                {
                                    "authorId": "151483943",
                                    "name": "L. Du"
                                },
                                {
                                    "authorId": "2288042024",
                                    "name": "Zheng Zhu"
                                },
                                {
                                    "authorId": "2238153157",
                                    "name": "Min Chen"
                                },
                                {
                                    "authorId": "2328027909",
                                    "name": "Zhou Su"
                                },
                                {
                                    "authorId": "2237990407",
                                    "name": "Shouling Ji"
                                },
                                {
                                    "authorId": "2147335888",
                                    "name": "Peng Cheng"
                                },
                                {
                                    "authorId": "2238129188",
                                    "name": "Jiming Chen"
                                },
                                {
                                    "authorId": "2238124154",
                                    "name": "Zhikun Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.98486328125
                    },
                    {
                        "id": "(Kumari et al., 2023)",
                        "snippets": [
                            "To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition."
                        ],
                        "paper": {
                            "corpus_id": 257687839,
                            "title": "Ablating Concepts in Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "46373847",
                                    "name": "Nupur Kumari"
                                },
                                {
                                    "authorId": "2119454239",
                                    "name": "Bin Zhang"
                                },
                                {
                                    "authorId": "12782331",
                                    "name": "Sheng-Yu Wang"
                                },
                                {
                                    "authorId": "2177801",
                                    "name": "Eli Shechtman"
                                },
                                {
                                    "authorId": "2109976035",
                                    "name": "Richard Zhang"
                                },
                                {
                                    "authorId": "1922024303",
                                    "name": "Jun-Yan Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 201
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Cui et al., 2023)",
                        "snippets": [
                            "FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection.\n\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement.\n\nGLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios.\n\nWang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model."
                        ],
                        "paper": {
                            "corpus_id": 263622213,
                            "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2254140893",
                                    "name": "Yuping Lin"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2253469617",
                                    "name": "Yue Xing"
                                },
                                {
                                    "authorId": "2255025428",
                                    "name": "Wenqi Fan"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "SIGKDD Explorations",
                            "n_citations": 12
                        },
                        "score": 0.99365234375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Detection Mechanisms for Copyright Infringement",
                "tldr": "Detection mechanisms for copyright infringement in text-to-image models utilize techniques like attention map analysis, watermark detection, and similarity comparison to identify unauthorized use of protected content. These methods range from examining model attention patterns and comparing image features to implementing specialized algorithms that can trace the origin of potentially infringing images. (9 sources)",
                "text": "\nCopyright infringement detection in text-to-image diffusion models has evolved to include several technical approaches that can identify when protected content has been used without authorization. One effective method analyzes attention maps during the diffusion process to identify regions of interest that may contain copyrighted elements. Zhang et al. proposed a copyright test that leverages the tendency of diffusion models to \"overattend\" to copyrighted areas, aggregating attention maps from the last reverse diffusion step to efficiently identify regions for similarity checking. This approach applies Gaussian blur filtering and standardization to generate binary masks of regions of interest, followed by cosine-similarity comparison of CLIP embeddings to detect substantial similarities with copyrighted content. <Paper corpusId=\"265352103\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>\n\nWatermarking has emerged as a particularly promising detection mechanism, providing concrete evidence of copyright infringement that can potentially serve as legal proof. Several systems have been developed to embed imperceptible watermarks into images that persist through the model training process and appear in generated outputs. Notable implementations include DIAGNOSIS, which detects unauthorized data usage by applying image warping functions; DiffusionShield, which embeds invisible watermarks containing copyright information; and FT-SHIELD, which verifies if data has been misused in training or fine-tuning of text-to-image models. <Paper corpusId=\"271600759\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>\n\nSome detection approaches are specifically designed to address the limitations of existing watermarking methods. Traditional watermarking techniques often modify large portions of datasets and may degrade image quality, while black-box Membership Inference (MI) methods require extensive queries to obtain significant results. <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper> More sophisticated approaches like GenWatermark target personalized text-to-image models, though it is primarily restricted to subject-driven synthesis. <Paper corpusId=\"277043466\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>\n\nRecent innovations include Wu et al.'s Contrasting Gradient Inversion for Diffusion Models (CGI-DM), which recovers missing details of partially obscured images by exploiting conceptual differences between pretrained and fine-tuned models, with the similarity between original and recovered images indicating potential infringement. This approach has demonstrated high accuracy in digital copyright authentication. <Paper corpusId=\"277043466\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>\n\nA particularly comprehensive approach is Copyright-Shield, which addresses copyright infringement arising from backdoor injection attacks. This system employs spatial similarity detection by segmenting sample images based on prompts and comparing the segmented features with copyrighted images. It calculates a poisoning score using Intersection over Union (IoU) <Paper corpusId=\"67855581\" paperTitle=\"(Rezatofighi et al., 2019)\" isShortName></Paper> and Self-Supervised Copy Detection (SSCD) <Paper corpusId=\"247011159\" paperTitle=\"(Pizzi et al., 2022)\" isShortName></Paper> similarity scores to filter out poisoned samples. The method also implements infringement feature inversion to trace the origin of poisoned images, helping determine infringement liability. <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>\n\nThese detection mechanisms represent significant progress in addressing copyright concerns in text-to-image diffusion models, providing rights holders with technical means to identify unauthorized use of their intellectual property. While current solutions also involve removing copyrighted images from training datasets to prevent models from learning these images <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>, detection mechanisms remain crucial for identifying infringement in models that have already been trained on protected content. <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "In addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content."
                        ],
                        "paper": {
                            "corpus_id": 265352103,
                            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2267728071",
                                    "name": "Teoh Tze Tzun"
                                },
                                {
                                    "authorId": "2267727392",
                                    "name": "Lim Wei Hern"
                                },
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks",
                            "Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft."
                        ],
                        "paper": {
                            "corpus_id": 271600759,
                            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
                            "authors": [
                                {
                                    "authorId": "2108977186",
                                    "name": "Wenhao Wang"
                                },
                                {
                                    "authorId": "2267731577",
                                    "name": "Yifan Sun"
                                },
                                {
                                    "authorId": "15556978",
                                    "name": "Zongxin Yang"
                                },
                                {
                                    "authorId": "2125634511",
                                    "name": "Zhengdong Hu"
                                },
                                {
                                    "authorId": "2296990692",
                                    "name": "Zhentao Tan"
                                },
                                {
                                    "authorId": "2297815073",
                                    "name": "Yi Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Ren et al._1, 2024)",
                        "snippets": [
                            "Observing the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality [13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16]17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2)."
                        ],
                        "paper": {
                            "corpus_id": 270620522,
                            "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2288619145",
                                    "name": "Chen Chen"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2253469617",
                                    "name": "Yue Xing"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Zhao et al. [48] used the pretrained watermark encoder (Yu et al., 2020) to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. (Wu et al., 2024) leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one."
                        ],
                        "paper": {
                            "corpus_id": 277043466,
                            "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2243376899",
                                    "name": "Zhenguang Liu"
                                },
                                {
                                    "authorId": "2243277462",
                                    "name": "Chao Shuai"
                                },
                                {
                                    "authorId": "2342623076",
                                    "name": "Shaojing Fan"
                                },
                                {
                                    "authorId": "2350529533",
                                    "name": "Ziping Dong"
                                },
                                {
                                    "authorId": "2350852841",
                                    "name": "Jinwu Hu"
                                },
                                {
                                    "authorId": "36890675",
                                    "name": "Zhongjie Ba"
                                },
                                {
                                    "authorId": "2286244627",
                                    "name": "Kui Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio."
                        ],
                        "paper": {
                            "corpus_id": 268513090,
                            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
                            "authors": [
                                {
                                    "authorId": "2108069960",
                                    "name": "Xiaoyu Wu"
                                },
                                {
                                    "authorId": "2147311278",
                                    "name": "Yang Hua"
                                },
                                {
                                    "authorId": "2186858424",
                                    "name": "Chumeng Liang"
                                },
                                {
                                    "authorId": "2118001291",
                                    "name": "Jiaru Zhang"
                                },
                                {
                                    "authorId": "2144220882",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2055312951",
                                    "name": "Tao Song"
                                },
                                {
                                    "authorId": "2292035375",
                                    "name": "Haibing Guan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 6
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Rezatofighi et al., 2019)",
                        "snippets": [
                            "Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO."
                        ],
                        "paper": {
                            "corpus_id": 67855581,
                            "title": "Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression",
                            "authors": [
                                {
                                    "authorId": "73768948",
                                    "name": "S. H. Rezatofighi"
                                },
                                {
                                    "authorId": "39282796",
                                    "name": "Nathan Tsoi"
                                },
                                {
                                    "authorId": "39813007",
                                    "name": "JunYoung Gwak"
                                },
                                {
                                    "authorId": "145759966",
                                    "name": "Amir Sadeghian"
                                },
                                {
                                    "authorId": "145950884",
                                    "name": "I. Reid"
                                },
                                {
                                    "authorId": "1702137",
                                    "name": "S. Savarese"
                                }
                            ],
                            "year": 2019,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 4181
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pizzi et al., 2022)",
                        "snippets": [
                            "Image copy detection is an important task for content moderation. We introduce SSCD, a model that builds on a recent self-supervised contrastive training objective. We adapt this method to the copy detection task by changing the architecture and training objective, including a pooling operator from the instance matching literature, and adapting contrastive learning to augmentations that combine images. Our approach relies on an entropy regularization term, promoting consistent separation between descriptor vectors, and we demonstrate that this significantly improves copy detection accuracy. Our method produces a compact descriptor vector, suitable for real-world web scale applications. Statistical information from a background image distribution can be incorporated into the descriptor. On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline copy detection models and self-supervised architectures designed for image classification by huge margins, in all settings. For example, SSCD out-performs SimCLR descriptors by 48% absolute. Code is available at https://github.com/facebookresearch/sscd-copy-detection."
                        ],
                        "paper": {
                            "corpus_id": 247011159,
                            "title": "A Self-Supervised Descriptor for Image Copy Detection",
                            "authors": [
                                {
                                    "authorId": "1900487455",
                                    "name": "Ed Pizzi"
                                },
                                {
                                    "authorId": "2109836437",
                                    "name": "Sreya . Dutta Roy"
                                },
                                {
                                    "authorId": "2155482435",
                                    "name": "Sugosh Nagavara Ravindra"
                                },
                                {
                                    "authorId": "47316088",
                                    "name": "Priya Goyal"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 126
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Current solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,(Vyas et al., 2023)[76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed",
                            ".we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) (Rezatofighi et al., 2019) and Self-Supervised Copy Detection (SSCD) (Pizzi et al., 2022) similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability."
                        ],
                        "paper": {
                            "corpus_id": 274436785,
                            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2333463963",
                                    "name": "Zhixiang Guo"
                                },
                                {
                                    "authorId": "2325884825",
                                    "name": "Siyuan Liang"
                                },
                                {
                                    "authorId": "2257572247",
                                    "name": "Aishan Liu"
                                },
                                {
                                    "authorId": "2237906923",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.98583984375
                    },
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Unlearning and Model Adaptation Approaches",
                "tldr": "Unlearning and model adaptation techniques aim to remove or modify diffusion models' ability to generate copyright-infringing content without completely retraining them. These approaches include gradient ascent-based methods, reinforcement learning with copyright metrics, and inference-time frameworks that can dynamically detect and prevent the generation of protected content. (7 sources)",
                "text": "\nAs copyright concerns with text-to-image diffusion models have grown, researchers have developed various unlearning and model adaptation approaches to address the issue of copyright infringement without requiring complete model retraining. These methods aim to modify existing models to prevent them from generating content that infringes on copyrighted works, particularly addressing the problem of model memorization.\n\nModel memorization has been identified as a critical issue where diffusion models can reproduce parts of images (local memorization) or entire images (global memorization) from their training data during inference, potentially leading to copyright infringement without awareness from either the model's users or the copyright holders <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>. To address this, researchers have implemented gradient ascent-based and response-based pruning methods for unlearning, particularly targeting Stable Diffusion models <Paper corpusId=\"268532352\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. These approaches evaluate the effectiveness of unlearning by measuring similarities between original copyright images and their unlearned counterparts, as well as tracking changes in CLIP scores to determine how effectively the model has forgotten specific content.\n\nA more sophisticated approach is Reinforcement Learning-based Copyright Protection (RLCP), which utilizes a novel copyright metric grounded in copyright law and court precedents on infringement. This method employs the Denoising Diffusion Policy Optimization (DDPO) framework to guide models through a multi-step decision-making process, optimizing them using a reward function that incorporates copyright considerations while maintaining generated image quality <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. By including KL divergence as a regularization term, this approach stabilizes reinforcement learning fine-tuning and reduces undesirable outcomes.\n\nAnother innovative solution is the \"Guardians of Generation\" framework, which operates at inference time without requiring model retraining or weight modification. This model-agnostic approach integrates with existing diffusion pipelines and implements an adaptive guidance mechanism comprising three components: detection, prompt rewriting, and guidance adjustment. The system monitors user prompts and intermediate generation steps to identify potential copyright infringement before it appears in the final output. When problematic content is detected, it dynamically transforms the user's prompt and steers the diffusion process away from protected content while preserving the prompt's intended meaning <Paper corpusId=\"277151077\" paperTitle=\"(Roy et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, there are limitations to current approaches. Some existing methods focus primarily on detecting illegally generated content but cannot effectively prevent or mitigate illegal adaptations of diffusion models. Similarly, standard model unlearning and reinitialization approaches may not prevent users from relearning protected content through custom data <Paper corpusId=\"270067889\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This highlights the need for ongoing research into more robust unlearning and adaptation techniques that can address the full spectrum of copyright infringement scenarios.",
                "citations": [
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content."
                        ],
                        "paper": {
                            "corpus_id": 278129333,
                            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1696291",
                                    "name": "Chen Chen"
                                },
                                {
                                    "authorId": "51023221",
                                    "name": "Daochang Liu"
                                },
                                {
                                    "authorId": "2302950741",
                                    "name": "Mubarak Shah"
                                },
                                {
                                    "authorId": "2288626806",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Somepalli et al., 2022)",
                        "snippets": [
                            "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"
                        ],
                        "paper": {
                            "corpus_id": 254366634,
                            "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2003112028",
                                    "name": "Gowthami Somepalli"
                                },
                                {
                                    "authorId": "1824188732",
                                    "name": "Vasu Singla"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 329
                        },
                        "score": 0
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods. Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity. This indicates the extent to which the prompt that generates potential infringement is nullified",
                            "In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models."
                        ],
                        "paper": {
                            "corpus_id": 268532352,
                            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2210435658",
                                    "name": "Rui Ma"
                                },
                                {
                                    "authorId": "2257338567",
                                    "name": "Qiang Zhou"
                                },
                                {
                                    "authorId": "2268733263",
                                    "name": "Yizhu Jin"
                                },
                                {
                                    "authorId": "2292161412",
                                    "name": "Daquan Zhou"
                                },
                                {
                                    "authorId": "2292177829",
                                    "name": "Bangjun Xiao"
                                },
                                {
                                    "authorId": "2292217065",
                                    "name": "Xiuyu Li"
                                },
                                {
                                    "authorId": "2292690089",
                                    "name": "Yi Qu"
                                },
                                {
                                    "authorId": "2261448160",
                                    "name": "Aishani Singh"
                                },
                                {
                                    "authorId": "2242659602",
                                    "name": "Kurt Keutzer"
                                },
                                {
                                    "authorId": "2328473437",
                                    "name": "Jingtong Hu"
                                },
                                {
                                    "authorId": "2307915260",
                                    "name": "Xiaodong Xie"
                                },
                                {
                                    "authorId": "2293731776",
                                    "name": "Zhen Dong"
                                },
                                {
                                    "authorId": "2257020214",
                                    "name": "Shanghang Zhang"
                                },
                                {
                                    "authorId": "2275298300",
                                    "name": "Shiji Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.96240234375
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 272146279,
                            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                            "authors": [
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2318391138",
                                    "name": "Jing Yan"
                                },
                                {
                                    "authorId": "2318236128",
                                    "name": "Xiaoli Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.99365234375
                    },
                    {
                        "id": "(Roy et al., 2025)",
                        "snippets": [
                            "We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory."
                        ],
                        "paper": {
                            "corpus_id": 277151077,
                            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
                            "authors": [
                                {
                                    "authorId": "2351229051",
                                    "name": "Soham Roy"
                                },
                                {
                                    "authorId": "2351593773",
                                    "name": "Abhishek Mishra"
                                },
                                {
                                    "authorId": "40151143",
                                    "name": "S. Karande"
                                },
                                {
                                    "authorId": "2316561345",
                                    "name": "Murari Mandal"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9873046875
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data."
                        ],
                        "paper": {
                            "corpus_id": 270067889,
                            "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
                            "authors": [
                                {
                                    "authorId": "2112769493",
                                    "name": "Kai Huang"
                                },
                                {
                                    "authorId": "2274008610",
                                    "name": "Wei Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9677734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Automated Copyright Judgment Systems",
                "tldr": "Automated copyright judgment systems employ large vision-language models to assess infringement in AI-generated images by simulating legal reasoning processes. These systems can identify potential copyright violations by comparing similarities between original and generated content, while also offering mitigation strategies to modify infringing prompts or generation parameters. (1 source)",
                "text": "\nRecent advances in AI technology have led to the development of automated copyright judgment systems that can evaluate potential infringement in text-to-image diffusion models. Unlike detection mechanisms that focus solely on identifying unauthorized content, these systems aim to replicate the legal reasoning processes used in copyright infringement cases to provide more nuanced assessments.\n\nA notable example is CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes. This system implements an abstraction-filtration-comparison test framework that mimics legal reasoning, employing multi-LVLM debate to assess the likelihood of infringement between copyrighted images and those generated by text-to-image diffusion models. Beyond simply detecting potential infringement, CopyJudge provides detailed judgment rationales that explain the basis for its decisions <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nWhat distinguishes CopyJudge from other approaches is its integrated mitigation strategy. Upon identifying potentially infringing content, the system can automatically optimize problematic prompts by avoiding sensitive expressions while preserving non-infringing content. This capability allows users to generate similar but legally compliant images without significantly compromising their creative intent. Additionally, CopyJudge can be enhanced through reinforcement learning techniques that explore non-infringing noise vectors within the diffusion latent space, enabling copyright compliance even without modifying the original prompts <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nThese automated judgment systems represent a significant advancement in addressing copyright concerns in text-to-image models, as they not only identify potential infringement but also provide actionable guidance for creating compliant content. By incorporating legal frameworks and reasoning processes, these systems help bridge the gap between technical detection and legal assessment, offering a more comprehensive approach to managing copyright issues in generative AI <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Liu et al._1, 2025)",
                        "snippets": [
                            "In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts."
                        ],
                        "paper": {
                            "corpus_id": 276558342,
                            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2346891526",
                                    "name": "Shunchang Liu"
                                },
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2344619001",
                                    "name": "Yaochu Jin"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.99462890625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations and Challenges of Current Methods",
                "tldr": "Current copyright protection methods for text-to-image models face significant limitations including the requirement to modify original artwork before publication, vulnerability to preprocessing techniques, and inability to address already published content. Many approaches also struggle with balancing protection effectiveness against image quality degradation and face practical challenges in large-scale implementation. (4 sources)",
                "text": "\nDespite the advances in watermarking, adversarial perturbation, detection mechanisms, and automated judgment systems, several fundamental limitations restrict the effectiveness of current copyright protection methods for text-to-image diffusion models. One of the most significant challenges is that many techniques require preemptive modification of original artwork before it is published online <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>. This requirement makes these methods impractical for the vast amount of visual content already available on the internet, which continues to be scraped for training datasets.\n\nBoth perturbation-based and watermarking-based approaches share this limitation, as they typically cannot be applied retroactively to published artwork without access to and modification of the original files <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>. This creates a substantial gap in protection for artists whose work is already widely distributed online, effectively leaving their intellectual property vulnerable to being incorporated into training datasets without consent.\n\nAnother critical weakness is the vulnerability of many protection methods to image preprocessing techniques. Simple transformations such as cropping, resizing, or format conversion can potentially neutralize the protective effects of subtle perturbations or watermarks <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper>. This vulnerability significantly undermines the reliability of these methods, as sophisticated users can potentially circumvent protection mechanisms through basic image manipulation.\n\nExisting watermarking approaches face additional practical challenges. Many watermarking methods require modifying large portions or even entire datasets, which becomes increasingly impractical as dataset sizes grow <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>. Furthermore, these modifications can inadvertently degrade image quality, creating an undesirable tradeoff between protection and aesthetic integrity <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>. \n\nAlternative approaches like black-box Membership Inference (MI) techniques avoid modifying the original data but require extensive querying to produce significant results, making them computationally expensive and time-consuming <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>. Similarly, poison-only backdoor attack methods adapted for detecting dataset usage show reduced robustness when subjected to re-captioning <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>.\n\nFrom a model-centric perspective, current methods primarily focus on detecting illegally generated content but often fail to prevent or mitigate illegal adaptations of diffusion models <Paper corpusId=\"270067889\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. Standard model unlearning and reinitialization approaches face a similar limitation\u2014they cannot prevent determined users from relearning protected content through custom datasets <Paper corpusId=\"270067889\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This creates a persistent vulnerability where even after protected content is removed from a model, it can be reintroduced through fine-tuning with a small dataset of collected examples.\n\nThe technical challenges are compounded by the rapid evolution of text-to-image models and the diverse ecosystem of model architectures, making it difficult to develop universal protection methods that work across all implementations. As diffusion models continue to advance and become more accessible, the need for more robust, scalable, and adaptable copyright protection methods becomes increasingly urgent, particularly for content that is already publicly available <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Du et al., 2024)",
                        "snippets": [
                            "To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork."
                        ],
                        "paper": {
                            "corpus_id": 268048573,
                            "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models",
                            "authors": [
                                {
                                    "authorId": "151483943",
                                    "name": "L. Du"
                                },
                                {
                                    "authorId": "2288042024",
                                    "name": "Zheng Zhu"
                                },
                                {
                                    "authorId": "2238153157",
                                    "name": "Min Chen"
                                },
                                {
                                    "authorId": "2237990407",
                                    "name": "Shouling Ji"
                                },
                                {
                                    "authorId": "2147335888",
                                    "name": "Peng Cheng"
                                },
                                {
                                    "authorId": "2138800088",
                                    "name": "Jiming Chen"
                                },
                                {
                                    "authorId": "2238124154",
                                    "name": "Zhikun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
                            "n_citations": 3
                        },
                        "score": 0.9697265625
                    },
                    {
                        "id": "(Du et al., 2025)",
                        "snippets": [
                            "To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector."
                        ],
                        "paper": {
                            "corpus_id": 277856857,
                            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
                            "authors": [
                                {
                                    "authorId": "151483943",
                                    "name": "L. Du"
                                },
                                {
                                    "authorId": "2288042024",
                                    "name": "Zheng Zhu"
                                },
                                {
                                    "authorId": "2238153157",
                                    "name": "Min Chen"
                                },
                                {
                                    "authorId": "2328027909",
                                    "name": "Zhou Su"
                                },
                                {
                                    "authorId": "2237990407",
                                    "name": "Shouling Ji"
                                },
                                {
                                    "authorId": "2147335888",
                                    "name": "Peng Cheng"
                                },
                                {
                                    "authorId": "2238129188",
                                    "name": "Jiming Chen"
                                },
                                {
                                    "authorId": "2238124154",
                                    "name": "Zhikun Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.98486328125
                    },
                    {
                        "id": "(Ren et al._1, 2024)",
                        "snippets": [
                            "Observing the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality [13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16]17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2)."
                        ],
                        "paper": {
                            "corpus_id": 270620522,
                            "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2288619145",
                                    "name": "Chen Chen"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2253469617",
                                    "name": "Yue Xing"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data."
                        ],
                        "paper": {
                            "corpus_id": 270067889,
                            "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing",
                            "authors": [
                                {
                                    "authorId": "2112769493",
                                    "name": "Kai Huang"
                                },
                                {
                                    "authorId": "2274008610",
                                    "name": "Wei Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9677734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.16684500000000002
    }
}